{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#what-is-arcticdb","title":"What is ArcticDB?","text":"<p>ArcticDB is a serverless DataFrame database engine designed for the Python Data Science ecosystem. </p> <p>ArcticDB enables you to store, retrieve and process DataFrames at scale, backed by commodity object storage (S3-compatible storages and Azure Blob Storage).</p> <p>ArcticDB requires zero additional infrastructure beyond a running Python environment and access to object storage and can be installed in seconds.</p> <p>ArcticDB is:</p> <ul> <li>Fast<ul> <li>Process up to 100 million rows per second for a single consumer</li> <li>Process a billion rows per second across all consumers</li> <li>Quick and easy to install: <code>pip install arcticdb</code></li> </ul> </li> <li>Flexible<ul> <li>Data schemas are not required</li> <li>Supports streaming data ingestion</li> <li>Bitemporal - stores all previous versions of stored data</li> <li>Easy to setup both locally and on cloud</li> <li>Scales from dev/research to production environments</li> </ul> </li> <li>Familiar<ul> <li>ArcticDB is the world's simplest shareable database</li> <li>Easy to learn for anyone with Python and Pandas experience</li> <li>Just you and your data - the cognitive overhead is very low.</li> </ul> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This section will cover installation, setup and basic usage. More details on basics and advanced features can be found in the tutorials section.</p>"},{"location":"#installation","title":"Installation","text":"<p>ArcticDB supports Python 3.8 - 3.13.</p> <p>To install, simply run:</p> <pre><code>pip install arcticdb\n</code></pre>"},{"location":"#setup","title":"Setup","text":"<p>ArcticDB is a storage engine designed for object storage and also supports local-disk storage using LMDB.</p> <p>Storage</p> <p>ArcticDB supports any S3 API compatible storage, including AWS and Azure, and storage appliances like VAST Universal Storage and Pure Storage.</p> <p>ArcticDB also supports LMDB for local/file based storage - to use LMDB, pass an LMDB path as the URI: <code>adb.Arctic('lmdb://path/to/desired/database')</code>.</p> <p>To get started, we can import ArcticDB and instantiate it:</p> <pre><code>import arcticdb as adb\n# this will set up the storage using the local file system\nuri = \"lmdb://tmp/arcticdb_intro\"\nac = adb.Arctic(uri)\n</code></pre> <p>For more information on how to correctly format the <code>uri</code> string for other storages, please view the docstring (<code>help(Arctic)</code>) or read the storage access section (click the link or keep reading below this section).</p>"},{"location":"#library-setup","title":"Library Setup","text":"<p>ArcticDB is geared towards storing many (potentially millions) of tables. Individual tables (DataFrames) are called symbols and  are stored in collections called libraries. A single library can store many symbols.</p> <p>Libraries must first be initialized prior to use:</p> <p><pre><code>ac.create_library('intro')  # static schema - see note below\nac.list_libraries()\n</code></pre> output <pre><code>['intro']\n</code></pre></p> <p>The library must then be instantiated in the code ready to read/write data:</p> <pre><code>library = ac['intro']\n</code></pre> <p>Sometimes it is more convenient to combine library creation and instantiation using this form, which will automatically create the library if needed, to save you checking if it exists already:</p> <pre><code>library = ac.get_library('intro', create_if_missing=True)\n</code></pre> <p>ArcticDB Static &amp; Dynamic Schemas</p> <p>ArcticDB does not need data schemas, unlike many other databases. You can write any DataFrame and read it back later. If the shape of the data is changed and then written again, it will all just work. Nice and simple.</p> <p>The one exception where schemas are needed is in the case of functions that modify existing symbols: <code>update</code> and <code>append</code>. When modifying a symbol, the new data must have the same schema as the existing data. The schema here means the index type and the name, order, and type of each column in the DataFrame. In other words when you are appending new rows they must look like the existing rows. This is the default option and is called <code>static schema</code>.</p> <p>However, if you need to add, remove or change the type of columns via <code>update</code> or <code>append</code>, then you can do that. You simply need to create the library with the <code>dynamic_schema</code> option set. See the <code>library_options</code> parameter of the (<code>create_library</code>) method.</p> <p>So you have the best of both worlds - you can choose to either enforce a static schema on your data so it cannot be changed by modifying operations, or allow it to be flexible.</p> <p>The choice to use static or dynamic schemas must be set at library creation time.</p> <p>In this section we are using <code>static schema</code>, just to be clear.</p>"},{"location":"#reading-and-writing-data","title":"Reading And Writing Data","text":"<p>Now we have a library set up, we can get to reading and writing data. ArcticDB has a set of simple functions for DataFrame storage.</p> <p>Let's write a DataFrame to storage.</p> <p>First create the data:</p> <p><pre><code># 50 columns, 25 rows, random data, datetime indexed.\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\ncols = ['COL_%d' % i for i in range(50)]\ndf = pd.DataFrame(np.random.randint(0, 50, size=(25, 50)), columns=cols)\ndf.index = pd.date_range(datetime(2000, 1, 1, 5), periods=25, freq=\"h\")\ndf.head(5)\n</code></pre> output (the first 5 rows of the data) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     18     48     10     16     38     34     25     44  ...\n2000-01-01 06:00:00     48     10     24     45     22     36     30     19  ...\n2000-01-01 07:00:00     25     16     36     29     25      9     48      2  ...\n2000-01-01 08:00:00     38     21      2      1     37      6     31     31  ...\n2000-01-01 09:00:00     45     17     39     47     47     11     33     31  ...\n</code></pre></p> <p>Then write to the library:</p> <p><pre><code>library.write('test_frame', df)\n</code></pre> output (information about what was written) <pre><code>VersionedItem(symbol=test_frame,library=intro,data=n/a,version=0,metadata=None,host=&lt;host&gt;)\n</code></pre></p> <p>The <code>'test_frame'</code> DataFrame will be used for the remainder of this guide.</p> <p>ArcticDB index</p> <p>When writing Pandas DataFrames, ArcticDB supports the following index types:</p> <ul> <li><code>pandas.Index</code> containing <code>int64</code> (or the corresponding dedicated types <code>Int64Index</code>, <code>UInt64Index</code>)</li> <li><code>RangeIndex</code></li> <li><code>DatetimeIndex</code></li> <li><code>MultiIndex</code> composed of above supported types</li> </ul> <p>The \"row\" concept in <code>head()/tail()</code> refers to the row number ('iloc'), not the value in the <code>pandas.Index</code> ('loc').</p> <p>Read the data back from storage:</p> <p><pre><code>from_storage_df = library.read('test_frame').data\nfrom_storage_df.head(5)\n</code></pre> output (the first 5 rows but read from the database) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     18     48     10     16     38     34     25     44  ...\n2000-01-01 06:00:00     48     10     24     45     22     36     30     19  ...\n2000-01-01 07:00:00     25     16     36     29     25      9     48      2  ...\n2000-01-01 08:00:00     38     21      2      1     37      6     31     31  ...\n2000-01-01 09:00:00     45     17     39     47     47     11     33     31  ...\n</code></pre></p> <p>The data read matches the original data, of course.</p>"},{"location":"#slicing-and-filtering","title":"Slicing and Filtering","text":"<p>ArcticDB enables you to slice by row and by column. </p> <p>ArcticDB indexing</p> <p>ArcticDB will construct a full index for ordered numerical and timeseries (e.g. DatetimeIndex) Pandas indexes. This will enable optimised slicing across index entries. If the index is unsorted or not numeric your data can still be stored but row-slicing will be slower.</p>"},{"location":"#row-slicing","title":"Row-slicing","text":"<p><pre><code>library.read('test_frame', date_range=(df.index[5], df.index[8])).data\n</code></pre> output (the rows in the data range requested) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 10:00:00     23     39      0     45     15     28     10     17  ...\n2000-01-01 11:00:00     36     28     22     43     23      6     10      1  ...\n2000-01-01 12:00:00     18     42      1     15     19     36     41     36  ...\n2000-01-01 13:00:00     28     32     47     37     17     44     29     24  ...\n</code></pre></p>"},{"location":"#column-slicing","title":"Column slicing","text":"<p><pre><code>_range = (df.index[5], df.index[8])\n_columns = ['COL_30', 'COL_31']\nlibrary.read('test_frame', date_range=_range, columns=_columns).data\n</code></pre> output (the rows in the date range and columns requested) <pre><code>                     COL_30  COL_31\n2000-01-01 10:00:00      31       2\n2000-01-01 11:00:00       3      34\n2000-01-01 12:00:00      24      43\n2000-01-01 13:00:00      18       8\n</code></pre></p>"},{"location":"#filtering-and-analytics","title":"Filtering and Analytics","text":"<p>ArcticDB supports many common DataFrame analytics operations, including filtering, projections, group-bys, aggregations, and resampling. The most intuitive way to access these operations is via the <code>LazyDataFrame</code> API, which should feel familiar to experienced users of Pandas or Polars.</p> <p>The legacy <code>QueryBuilder</code> class can also be created directly and passed into <code>read</code> calls with the same effect.</p> <p>ArcticDB Analytics Philosphy</p> <p>In most cases this is more memory efficient and performant than the equivalent Pandas operation as the processing is within the C++ storage engine and parallelized over multiple threads of execution. </p> <p><pre><code>import arcticdb as adb\n_range = (df.index[5], df.index[8])\n_cols = ['COL_30', 'COL_31']\n# Using lazy evaluation\nlazy_df = library.read('test_frame', date_range=_range, columns=_cols, lazy=True)\nlazy_df = lazy_df[(lazy_df[\"COL_30\"] &gt; 10) &amp; (lazy_df[\"COL_31\"] &lt; 40)]\ndf = lazy_df.collect().data\n# Using the legacy QueryBuilder class gives the same result\nq = adb.QueryBuilder()\nq = q[(q[\"COL_30\"] &gt; 10) &amp; (q[\"COL_31\"] &lt; 40)]\nlibrary.read('test_frame', date_range=_range, columns=_cols, query_builder=q).data\n</code></pre> output (the data filtered by date range, columns and the query which filters based on the data values) <pre><code>                     COL_30  COL_31\n2000-01-01 10:00:00      31       2\n2000-01-01 13:00:00      18       8\n</code></pre></p>"},{"location":"#modifications-versioning-aka-time-travel","title":"Modifications, Versioning (aka Time Travel)","text":"<p>ArcticDB fully supports modifying stored data via two primitives: update and append.</p> <p>These operations are atomic but do not lock the symbol. Please see the section on transactions for more on this.</p>"},{"location":"#append","title":"Append","text":"<p>Let's append data to the end of the timeseries.</p> <p>To start, we will take a look at the last few records of the data (before it gets modified)</p> <p><pre><code>library.tail('test_frame', 4).data\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 02:00:00     46     12     38     47      4     31      1     42  ...\n2000-01-02 03:00:00     46     20      5     42      8     35     12      2  ...\n2000-01-02 04:00:00     17     48     36     43      6     46      5      8  ...\n2000-01-02 05:00:00     20     19     24     44     29     32      2     19  ...\n</code></pre> Then create 3 new rows to append. For append to work the new data must have its first <code>datetime</code> starting after the existing data.</p> <p><pre><code>random_data = np.random.randint(0, 50, size=(3, 50))\ndf_append = pd.DataFrame(random_data, columns=['COL_%d' % i for i in range(50)])\ndf_append.index = pd.date_range(datetime(2000, 1, 2, 7), periods=3, freq=\"h\")\ndf_append\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 07:00:00      9     15      4     48     48     35     34     49  ...\n2000-01-02 08:00:00     35      4     12     30     30     12     38     25  ...\n2000-01-02 09:00:00     25     17      3      1      1     15     33     49  ...\n</code></pre></p> <p>Now append that DataFrame to what was written previously</p> <p><pre><code>library.append('test_frame', df_append)\n</code></pre> output <pre><code>VersionedItem(symbol=test_frame,library=intro,data=n/a,version=1,metadata=None,host=&lt;host&gt;)\n</code></pre> Then look at the final 5 rows to see what happened</p> <p><pre><code>library.tail('test_frame', 5).data\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 04:00:00     17     48     36     43      6     46      5      8  ...\n2000-01-02 05:00:00     20     19     24     44     29     32      2     19  ...\n2000-01-02 07:00:00      9     15      4     48     48     35     34     49  ...\n2000-01-02 08:00:00     35      4     12     30     30     12     38     25  ...\n2000-01-02 09:00:00     25     17      3      1      1     15     33     49  ...\n</code></pre></p> <p>The final 5 rows consist of the last two rows written previously followed by the 3 new rows that we have just appended.</p> <p>Append is very useful for adding new data to the end of a large timeseries.</p>"},{"location":"#update","title":"Update","text":"<p>The update primitive enables you to overwrite a contiguous chunk of data. This results in modifying some rows and deleting others as we will see in the example below.</p> <p>Here we create a new DataFrame for the update, with only 2 rows that are 2 hours apart</p> <p><pre><code>random_data = np.random.randint(0, 50, size=(2, 50))\ndf = pd.DataFrame(random_data, columns=['COL_%d' % i for i in range(50)])\ndf.index = pd.date_range(datetime(2000, 1, 1, 5), periods=2, freq=\"2h\")\ndf\n</code></pre> output (rows 0 and 2 only as selected by the <code>iloc[]</code>) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     47     49     15      6     22     48     45     22  ...\n2000-01-01 07:00:00     46     10      2     49     24     49      8      0  ...\n</code></pre> now update the symbol <pre><code>library.update('test_frame', df)\n</code></pre> output (information about the update) <pre><code>VersionedItem(symbol=test_frame,library=intro,data=n/a,version=2,metadata=None,host=&lt;host&gt;)\n</code></pre></p> <p>Now let's look at the first 4 rows in the symbol:</p> <p><pre><code>library.head('test_frame', 4).data  # head/tail are similar to the equivalent Pandas operations\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     47     49     15      6     22     48     45     22  ...\n2000-01-01 07:00:00     46     10      2     49     24     49      8      0  ... \n2000-01-01 08:00:00     38     21      2      1     37      6     31     31  ... \n2000-01-01 09:00:00     45     17     39     47     47     11     33     31  ...\n</code></pre></p> <p>Let's unpack how we end up with that result. The update has</p> <ul> <li>replaced the data in the symbol with the new data where the index matched (in this case the 05:00 and 07:00 rows)</li> <li>removed any rows within the date range of the new data that are not in the index of the new data (in this case the 06:00 row)</li> <li>kept the rest of the data the same (in this case 09:00 onwards)</li> </ul> <p>Logically, this corresponds to replacing the complete date range of the old data with the new data, which is what you would expect from an update.</p>"},{"location":"#versioning","title":"Versioning","text":"<p>You might have noticed that <code>read</code> calls do not return the data directly - but instead returns a <code>VersionedItem</code> structure. You may also have noticed that modification operations (<code>write</code>, <code>append</code> and <code>update</code>) increment the version number. ArcticDB versions all modifications, which means you can retrieve earlier versions of data - it is a bitemporal database:</p> <p><pre><code>library.tail('test_frame', 7, as_of=0).data\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 23:00:00     16     46      3     45     43     14     10     27  ...\n2000-01-02 00:00:00     37     37     20      3     49     38     23     46  ...\n2000-01-02 01:00:00     42     47     40     27     49     41     11     26  ...\n2000-01-02 02:00:00     46     12     38     47      4     31      1     42  ...\n2000-01-02 03:00:00     46     20      5     42      8     35     12      2  ...\n2000-01-02 04:00:00     17     48     36     43      6     46      5      8  ...\n2000-01-02 05:00:00     20     19     24     44     29     32      2     19  ...\n</code></pre></p> <p>Note the timestamps - we've read the data prior to the <code>append</code> operation. Please note that you can also pass a <code>datetime</code> into any <code>as_of</code> argument, which will result in reading the last version earlier than the <code>datetime</code> passed.</p> <p>Versioning, Prune Previous &amp; Snapshots</p> <p>By default, <code>write</code>, <code>append</code>, and <code>update</code> operations will not remove the previous versions. Please be aware that this will consume more space.</p> <p>This behaviour can be can be controlled via the <code>prune_previous_versions</code> keyword argument. Space will be saved but the previous versions will then not be available.</p> <p>A compromise can be achieved by using snapshots, which allow states of the library to be saved and read back later. This allows certain versions to be protected from deletion, they will be deleted when the snapshot is deleted. See snapshot documentation for details.</p>"},{"location":"#storage-access","title":"Storage Access","text":""},{"location":"#s3-configuration","title":"S3 configuration","text":"<p>There are two methods to configure S3 access. If you happen to know the access and secret key, simply connect as follows:</p> <pre><code>import arcticdb as adb\nac = adb.Arctic('s3://ENDPOINT:BUCKET?region=blah&amp;access=ABCD&amp;secret=DCBA')\n</code></pre> <p>Otherwise, you can delegate authentication to the AWS SDK (obeys standard AWS configuration options):</p> <pre><code>ac = adb.Arctic('s3://ENDPOINT:BUCKET?aws_auth=true')\n</code></pre> <p>Same as above, but using HTTPS:</p> <pre><code>ac = adb.Arctic('s3s://ENDPOINT:BUCKET?aws_auth=true')\n</code></pre> <p>S3</p> <p>Use <code>s3s</code> if your S3 endpoint used HTTPS</p>"},{"location":"#connecting-to-a-defined-storage-endpoint","title":"Connecting to a defined storage endpoint","text":"<p>Connect to local storage (not AWS - HTTP endpoint of s3.local) with a pre-defined access and storage key:</p> <pre><code>ac = adb.Arctic('s3://s3.local:arcticdb-test-bucket?access=EFGH&amp;secret=HGFE')\n</code></pre>"},{"location":"#connecting-to-aws","title":"Connecting to AWS","text":"<p>Connecting to AWS with a pre-defined region:</p> <pre><code>ac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arcticdb-test-bucket?aws_auth=true')\n</code></pre> <p>Note that no explicit credential parameters are given. When <code>aws_auth</code> is passed, authentication is delegated to the AWS SDK which is responsible for locating the appropriate credentials in the <code>.config</code> file or in environment variables. You can manually configure which profile is being used by setting the <code>AWS_PROFILE</code> environment variable as described in the AWS Documentation.</p>"},{"location":"#using-a-specific-path-within-a-bucket","title":"Using a specific path within a bucket","text":"<p>You may want to restrict access for the ArcticDB library to a specific path within the bucket. To do this, you can use the <code>path_prefix</code> parameter:</p> <pre><code>ac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arcticdb-test-bucket?path_prefix=test&amp;aws_auth=true')\n</code></pre>"},{"location":"#azure","title":"Azure","text":"<p>ArcticDB uses the Azure connection string to define the connection: </p> <pre><code>import arcticdb as adb\nac = adb.Arctic('azure://AccountName=ABCD;AccountKey=EFGH;BlobEndpoint=ENDPOINT;Container=CONTAINER')\n</code></pre> <p>For example: </p> <pre><code>import arcticdb as adb\nac = adb.Arctic(\"azure://CA_cert_path=/etc/ssl/certs/ca-certificates.crt;BlobEndpoint=https://arctic.blob.core.windows.net;Container=acblob;SharedAccessSignature=sp=awd&amp;st=2001-01-01T00:00:00Z&amp;se=2002-01-01T00:00:00Z&amp;spr=https&amp;rf=g&amp;sig=awd%3D\")\n</code></pre> <p>For more information, see the Arctic class reference.</p>"},{"location":"#lmdb","title":"LMDB","text":"<p>LMDB supports configuring its map size. See its documentation.</p> <p>You may need to tweak it on Windows, whereas on Linux the default is much larger and should suffice. This is because Windows allocates physical space for the map file eagerly, whereas on Linux the map size is an upper bound to the physical space that will be used.</p> <p>You can set a map size in the connection string:</p> <pre><code>import arcticdb as adb\nac = adb.Arctic('lmdb://path/to/desired/database?map_size=2GB')\n</code></pre> <p>The default on Windows is 2GiB. Errors with <code>lmdb errror code -30792</code> indicate that the map is getting full and that you should increase its size. This will happen if you are doing large writes.</p> <p>In each Python process, you should ensure that you only have one Arctic instance open over a given LMDB database.</p> <p>LMDB does not work with remote filesystems.</p>"},{"location":"#in-memory-configuration","title":"In-memory configuration","text":"<p>An in-memory backend is provided mainly for testing and experimentation. It could be useful when creating files with LMDB is not desired.</p> <p>There are no configuration parameters, and the memory is owned solely by the Arctic instance.</p> <p>For example:</p> <pre><code>import arcticdb as adb\nac = adb.Arctic('mem://')\n</code></pre> <p>For concurrent access to a local backend, we recommend LMDB connected to tmpfs, see LMDB and In-Memory Tutorial.</p>"},{"location":"#transactions","title":"Transactions","text":"<ul> <li>Transactions can be be very useful but are often expensive and slow</li> <li>If we unpack ACID: Atomicity, Consistency and Durability are useful, Isolation less so</li> <li>Most analytical workflows can be constructed to run without needing transactions at all</li> <li>So why pay the cost of transactions when they are often not needed?</li> <li>ArcticDB doesn't have transactions because it is designed for high throughput analytical workloads</li> </ul>"},{"location":"aws/","title":"Getting Started with AWS S3","text":"<p>For a detailed guide on configuring Amazon S3, refer to the official AWS documentation: Creating Your First Bucket.</p>"},{"location":"aws/#best-practices-for-setting-up-aws-s3-using-an-access-key","title":"Best Practices for Setting Up AWS S3 Using an Access Key","text":"<p>The best practices for using AWS S3 depend on your specific use case. If you're looking to try ArcticDB on AWS S3, a simple and effective approach is outlined below.</p>"},{"location":"aws/#1-create-a-policy-for-accessing-s3-buckets","title":"1. Create a Policy for Accessing S3 Buckets","text":"<ol> <li>Navigate to IAM -&gt; Policies -&gt; Create Policy.  </li> <li>Choose S3 as the service, and select the following Access Level permissions:</li> </ol> Permission Description <code>s3:GetObject</code> Read objects <code>s3:PutObject</code> Write objects <code>s3:DeleteObject</code> Delete objects <code>s3:ListBucket</code> List bucket contents <ol> <li>Specify the S3 buckets this policy will allow access to. </li> </ol> <ol> <li>Click Next, review, and give the policy a meaningful name.  </li> </ol> <ol> <li>Click Create Policy.  </li> </ol>"},{"location":"aws/#2-create-an-iam-user","title":"2. Create an IAM User","text":"<p>It is best practice to manage and access S3 buckets using a non-root IAM account in AWS. For trial purposes, it is recommended to create a dedicated IAM account specifically for testing ArcticDB.</p> <p>Follow these steps to create an IAM user:</p> <ol> <li>Navigate to IAM &gt; Users.</li> <li>Click Add users.</li> <li>Choose a username, such as <code>arcticdbtrial</code>, and click Next.</li> <li>Select Attach policies directly and attach the policy created in step 1 to the user.</li> <li>Click Next to review.</li> <li>Click Create user.</li> </ol>"},{"location":"aws/#3-create-the-access-key","title":"3. Create the Access Key","text":"<ol> <li>Navigate to the IAM &gt; Users table and click on the newly created user.  </li> <li>Go to the Security credentials tab and locate the Access keys section.  </li> <li>Click Create access key.  </li> <li>Select the Local code option (ArcticDB is the local code).  </li> <li>Check the box for I understand... and click Next.  </li> <li>Click Create access key.  </li> <li>Record the Access key and Secret access key securely, as they will not be displayed again after closing the screen.</li> </ol>"},{"location":"aws/#4-create-the-bucket","title":"4. Create the Bucket","text":"<p>The remaining steps can be performed using commands on your client machine. </p>"},{"location":"aws/#step-1-install-the-aws-cli","title":"Step 1: Install the AWS CLI","text":"<p>If you don\u2019t already have the AWS Command Line Interface (CLI) installed, follow the official AWS CLI installation guide.</p>"},{"location":"aws/#step-2-configure-the-aws-cli","title":"Step 2: Configure the AWS CLI","text":"<p>Use the AWS CLI to configure your client machine with the Access Key and Secret Access Key created earlier. You will also need to select an AWS region. For optimal performance, choose a region local to your ArcticDB client.</p> <p>Run the following command and provide the required details:</p> <pre><code>$ aws configure\nAWS Access Key ID [None]: &lt;ACCESS_KEY&gt;\nAWS Secret Access Key [None]: &lt;SECRET_KEY&gt;\nDefault region name [None]: &lt;REGION&gt;\nDefault output format [None]:\n</code></pre> <p>Bucket names must be globally unique, so you will need to create your own unique name. Use the following command to create the bucket: <pre><code>$ aws s3 mb s3://&lt;BUCKET_NAME&gt;\n</code></pre></p>"},{"location":"aws/#5-connect-to-the-bucket","title":"5. Connect to the bucket","text":"<ul> <li>Install ArcticDB.</li> <li>Use your <code>&lt;REGION&gt;</code> and <code>&lt;BUCKET_NAME&gt;</code>.</li> <li>Setup <code>~/.aws/credentials</code> with <code>aws configure</code>, as above. <pre><code>import arcticdb as adb\narctic = adb.Arctic('s3://s3.&lt;REGION&gt;.amazonaws.com:&lt;BUCKET_NAME&gt;?aws_auth=true')\n</code></pre></li> </ul>"},{"location":"aws/#6-checking-connectivity-to-the-s3-bucket-troubleshooting","title":"6. Checking Connectivity to the S3 Bucket - Troubleshooting","text":""},{"location":"aws/#1-permissions","title":"1. Permissions","text":"<p>ArcticDB relies on the following five S3 methods for its operations:</p> Method Permission required GetObject s3:GetObject HeadObject s3:GetObject PutObject s3:PutObject DeleteObject s3:DeleteObject ListObjectsV2 s3:ListBucket"},{"location":"aws/#2-verification-script","title":"2. Verification Script","text":"<p>You can use the following script to verify connectivity to an S3 bucket from your client machine. If the script executes successfully, the configuration should be correct for read and write operations with ArcticDB.</p>"},{"location":"aws/#3-prerequisites","title":"3. Prerequisites:","text":"<ol> <li>Replace <code>&lt;BUCKET_NAME&gt;</code> in the script with your bucket name.  </li> <li>Install boto3.  </li> <li>Set up your AWS credentials in <code>~/.aws/credentials</code> using the <code>aws configure</code> command (as described earlier).  </li> </ol>"},{"location":"aws/#4-python-script","title":"4. Python Script:","text":"<pre><code>import io\nimport boto3\n\n# Initialize the S3 client\ns3 = boto3.client('s3')\n\n# Replace '&lt;BUCKET_NAME&gt;' with your actual bucket name\nbucket = '&lt;BUCKET_NAME&gt;'\n\n# Perform operations to check connectivity\ns3.put_object(Bucket=bucket, Key='_arctic_check/check.txt', Body=io.BytesIO(b'check file contents'))\ns3.list_objects_v2(Bucket=bucket, Prefix='_arctic_check/')\ns3.head_object(Bucket=bucket, Key='_arctic_check/check.txt')\ns3.get_object(Bucket=bucket, Key='_arctic_check/check.txt')\ns3.delete_object(Bucket=bucket, Key='_arctic_check/check.txt')\n</code></pre>"},{"location":"aws/#5-notes","title":"5. Notes:","text":"<p>The check object written by this script (_arctic_check/check.txt) is temporary and will not interfere with normal ArcticDB operations on the bucket. If any of the operations fail, verify the permissions assigned to your IAM role or user, and ensure the bucket is correctly configured.</p>"},{"location":"aws/#best-practices-for-setting-up-aws-s3-using-sts","title":"Best Practices for Setting Up AWS S3 Using STS","text":"<p>AWS Security Token Service (STS) enables users to assume specific roles to gain temporary access to AWS resources. This guide details the steps required for setting up STS to access Amazon S3 buckets.</p>"},{"location":"aws/#getting-started-with-sts-setup","title":"Getting Started with STS Setup","text":""},{"location":"aws/#1-create-a-policy-for-accessing-s3-buckets_1","title":"1. Create a Policy for Accessing S3 Buckets","text":"<ol> <li>Navigate to IAM -&gt; Policies -&gt; Create Policy.  </li> <li>Choose S3 as the service, and select the following Access Level permissions:</li> </ol> Permission Description <code>s3:GetObject</code> Read objects <code>s3:PutObject</code> Write objects <code>s3:DeleteObject</code> Delete objects <code>s3:ListBucket</code> List bucket contents <ol> <li>Specify the S3 buckets this policy will allow access to. </li> </ol> <ol> <li>Click Next, review, and give the policy a meaningful name.  </li> </ol> <ol> <li>Click Create Policy.  </li> </ol>"},{"location":"aws/#2-create-a-role-to-access-s3-buckets","title":"2. Create a Role to Access S3 Buckets","text":"<ol> <li>Navigate to IAM -&gt; Roles -&gt; Create Role.  </li> <li>Select the appropriate Trusted Entity Type for your use case.  </li> </ol> <ol> <li>In the permissions step, attach the policy created in the previous step.  </li> </ol> <ol> <li>Give the role a meaningful name and click Create Role.  </li> <li>Copy the Role ARN for use in the next step.</li> </ol>"},{"location":"aws/#3-create-a-policy-to-assume-the-role-using-sts","title":"3. Create a Policy to Assume the Role Using STS","text":"<ol> <li>Navigate to IAM -&gt; Policies -&gt; Create Policy.  </li> <li>Select STS as the service, and grant the AssumeRole permission.  </li> <li>Specify the Role ARN created in the previous step.  </li> </ol> <ol> <li>Name the policy and click Create Policy.</li> </ol>"},{"location":"aws/#4-create-an-iam-user-optional-and-attach-the-sts-policy","title":"4. Create an IAM User (Optional) and Attach the STS Policy","text":"<p>In many cases, a dedicated IAM user is not required, as it is typically managed by the organization's IT services. However, if needed, follow these steps to create an IAM user for your services (e.g., ArcticDB).  </p> <ol> <li>Navigate to IAM -&gt; Users -&gt; Create User.  </li> <li>Enter a username and proceed to the next step.  </li> </ol> <p> </p> <ol> <li>Select the Attach policies directly option and attach the policy created in Step 3.  </li> </ol> <p> </p> <ol> <li>Click Create User.  </li> <li>After the user is created, navigate to the Security Credentials tab for the user and create an Access Key.  </li> <li>Copy the Access Key and Secret Access Key immediately, as these will not be shown again.</li> </ol>"},{"location":"aws/#5-configure-the-arcticdb-client-to-access-aws-and-assume-the-role","title":"5. Configure the ArcticDB Client to Access AWS and Assume the Role","text":"<p>To use the setup with ArcticDB, configure the credentials in the AWS shared config file.</p>"},{"location":"aws/#file-locations","title":"File Locations:","text":"Platform File Location Linux and macOS <code>~/.aws/config</code> Windows <code>%USERPROFILE%\\.aws\\config</code>"},{"location":"aws/#example-configuration","title":"Example Configuration:","text":"<pre><code>[profile PROFILE]\nrole_arn = ROLE_ARN_TO_BE_ASSUMED\nsource_profile = BASE_PROFILE\n\n[profile BASE_PROFILE]\naws_access_key_id = YOUR_ACCESS_KEY_ID\naws_secret_access_key = YOUR_SECRET_ACCESS_KEY\n</code></pre> <p>Use the configuration in ArcticDB: <pre><code>&gt;&gt;&gt; import arcticdb as adb\n&gt;&gt;&gt; arctic = adb.Arctic('s3://s3.REGION.amazonaws.com:BUCKET?aws_auth=sts&amp;aws_profile=PROFILE')\n</code></pre></p> <p>For more in-depth documentation please refer to the offical website.</p>"},{"location":"aws/#common-errors-when-working-with-sts-troubleshooting","title":"Common Errors When Working with STS - Troubleshooting","text":"<p>When using STS with ArcticDB, the following common errors may occur. These are typically caused by issues with the AWS C++ SDK, incorrect IAM account setup, or misconfigured files. To enable additional logging, refer to this guide.</p>"},{"location":"aws/#1-assertion-error-in-aws-c-sdk","title":"1. Assertion Error in AWS C++ SDK","text":"<p>If ArcticDB fails to obtain a temporary token by assuming a role, you may encounter an assertion error like the one below:</p> <pre><code>virtual void Aws::Auth::STSProfileCredentialsProvider::Reload(): Assertion `!profileIt-&gt;second.GetCredentials().IsEmpty()' failed.\n</code></pre>"},{"location":"aws/#cause","title":"Cause:","text":"<p>This error is usually caused by: - An incorrect IAM account setup. - An invalid or misconfigured AWS credentials file.</p>"},{"location":"aws/#solution","title":"Solution:","text":"<ul> <li>Verify that the IAM Role ARN is correct.</li> <li>Ensure the AWS credentials file (<code>~/.aws/config</code> or <code>%USERPROFILE%\\.aws\\config</code>) is properly configured with the correct role and base profile.</li> </ul>"},{"location":"aws/#2-permission-error","title":"2. Permission Error","text":"<p>You may encounter a permission error like the following:</p> <pre><code>arcticdb_ext.exceptions.PermissionException: E_PERMISSION Permission error: S3Error#15 AccessDenied: Access Denied for object '_arctic_cfg/cref/'\n</code></pre>"},{"location":"aws/#cause-1","title":"Cause #1:","text":"<p>This error indicates a problem with the configuration file. Specifically: - The Role ARN or Base Profile in the AWS configuration file is incorrect.</p>"},{"location":"aws/#solution_1","title":"Solution:","text":"<ul> <li>Double-check the <code>role_arn</code> and <code>source_profile</code> values in your AWS configuration file.</li> <li>Ensure that the IAM Role has the necessary permissions to access the required S3 bucket and objects.</li> </ul>"},{"location":"aws/#cause-2","title":"Cause #2:","text":"<p>A known issue in the AWS C++ SDK</p>"},{"location":"aws/#affected-users","title":"Affected users","text":"<ul> <li>Use STS authentication, and</li> <li>Use below opearting systems</li> <li>RHEL distributions with custom CA certificates</li> <li>Other Linux distributions</li> </ul>"},{"location":"aws/#workaround","title":"Workaround","text":"<p>You need to create symbolic links for the CA certificate in use to the required <code>/etc/pki/tls/certs</code> directory. Below is an example of how to do this for the default CA certificate on Ubuntu:</p> <pre><code>ln -s /usr/lib/ssl/cert.pem /etc/pki\nln -s /usr/lib/ssl/certs /etc/pki/tls/certs\nln -s /etc/ssl/certs/ca-certificates.crt /etc/pki/tls/certs/ca-bundle.crt\n</code></pre>"},{"location":"aws/#3-retryable-storage-error","title":"3. Retryable Storage Error","text":"<p>If there is a network connectivity issue, you might see the following error:</p> <pre><code>arcticdb_ext.exceptions.StorageException: E_S3_RETRYABLE Retry-able error: S3Error#99 : Encountered network error when sending http request for object '_arctic_cfg/cref/'\n</code></pre>"},{"location":"aws/#cause_1","title":"Cause:","text":"<p>This error occurs when: - There is a loss of network connectivity during an S3 operation. - ArcticDB is unable to re-establish a connection after several retry attempts.</p>"},{"location":"aws/#solution_2","title":"Solution:","text":"<ul> <li>Verify your network connection.</li> <li>Ensure that the S3 endpoint is reachable from your environment.</li> </ul> <p>A loss of network connectivity could trigger such an error. Note, that this error will appear after several attempts to re-establish the connection</p>"},{"location":"aws_permissions/","title":"Dynamic library permissions with ArticDB on AWS S3","text":""},{"location":"aws_permissions/#goal","title":"Goal","text":"<p>One of the advantages of ArcticDB is how easy it is to setup and use as a personal database.  But how can we extend this pattern to an organisation?  How can we keep it trivial to use as an individual, but allow for secure sharing of data with team-mates and groups across your organisation?  We also want this to be easy to maintain, a key challange for permissions generally.</p> <p>Here we model a small two team organisation, 'Acme', in AWS and create some flexible permissions that allow users and teams to create and use private and shared data without any per-library setup.</p> <ul> <li>Data Team<ul> <li>Jane</li> <li>Samir</li> </ul> </li> <li>Quant Team<ul> <li>Alan</li> <li>Diana</li> </ul> </li> </ul> <p>Each user should be able to, </p> <ul> <li>list all ArcticDB libraries (but not their content)</li> <li>create personal libraries that only they can read and write to</li> <li>create team libraries that only those in their team can read and write to</li> </ul> <p>Users will follow an ArcticDB library name convention.  The library name should one of <code>&lt;USERNAME&gt;/&lt;LIBRARY&gt;</code> or <code>&lt;TEAM&gt;/&lt;MYLIBRARY&gt;</code>, so if Jane wants to create a personal library for weather data, they would use <code>lib.create_library('jane@acme/weather')</code> (assuming the AWS S3 setup below).</p> <p>We can do this with path-prefix permissions in AWS S3.  Other backends, such as Minio, support path based permissioning.</p>"},{"location":"aws_permissions/#aws-s3","title":"AWS S3","text":"<p>You should have a number of users setup in AWS IAM already along with a group containing all the users.  Follow the IAM docs for help with that.</p> <p>For Acme we've setup four users and the users are tagged with the teams they are a member of:</p> aws:username aws:PrincipalTag/team jane@acme data samir@acme data alan@acme quant diana@acme quant <p>We've also created a user group, <code>acme</code> with all four users in.</p> <p>Let's create an S3 bucket for Acme, <code>acme-arcticdb</code>, using cloudshell. <pre><code>aws s3 mb s3://acme-arcticdb\n</code></pre></p>"},{"location":"aws_permissions/#setting-up-the-permissions-policy","title":"Setting up the permissions policy","text":"<p>In general for read access our users will need <code>s3:ListBucket</code> and <code>s3:GetObject</code> permissions and for write access our users will additionally need <code>s3:PutObject</code> and <code>s3:DeleteObject</code>.</p> <p>Then setup the following access policy.  We will save this snippet to <code>policy.json</code>. <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjects\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::acme-arcticdb\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"_arctic_cfg/*\",\n                        \"${aws:username}/*\",\n                        \"${aws:PrincipalTag/team}/*\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"PutGetDeleteObjects\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::acme-arcticdb/${aws:username}/*\",\n                \"arn:aws:s3:::acme-arcticdb/${aws:PrincipalTag/team}/*\",\n                \"arn:aws:s3:::acme-arcticdb/_arctic_cfg/cref/?sUt?${aws:username}/*\",\n                \"arn:aws:s3:::acme-arcticdb/_arctic_cfg/cref/?sUt?${aws:PrincipalTag/team}/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>Create the policy in AWS. <pre><code>aws iam create-policy --policy-name acme-arcticdb-access --policy-document file://policy.json\n</code></pre></p> <p>Take note of the <code>Arn</code> in the output to the last command as you'll need it to attach the policy to a group with all your users in, for this example the group is <code>acme</code>. <pre><code>aws iam attach-group-policy --policy-arn &lt;ARN&gt; --group-name acme\n</code></pre></p> <p>If you intend to adapt that example policy to your own situation then please note that,</p> <ul> <li><code>acme-arcticdb</code> is the name of the bucket and will need to be replaced everywhere</li> <li><code>s3:ListBucket</code> is used to permission <code>ListObjectsV2</code> and needs its own section, as it applies to the bucket as a whole.  We control access to paths by checking the <code>s3:prefix</code> argument that's part of the <code>ListObjectsV2</code> request.</li> <li><code>Put</code>, <code>Get</code> and <code>Delete</code> can be specifed for object paths in the second section.</li> <li><code>_arctic_cfg/cref/*</code> is where the ArcticDB library configuration is stored and the data for each library is stored in the root of the bucket with a path that starts with the library name.</li> <li>By using <code>${aws:username}</code> and <code>${aws:PrincipalTag/team}</code> we've restricted library access to those with a matching AWS IAM username or a matched user 'team' tag.</li> </ul>"},{"location":"aws_permissions/#security-note","title":"Security note","text":"<p>Because <code>${aws:username}</code>, <code>${aws:PrincipalTag/team}</code> and <code>_arctic_cfg</code>... are at the beginning of the path, it's important they don't contain values that can overlap. For example if you have a team called 'data' and a username for an application called 'data', they will have the same permissions, or if a username can be created that starts with <code>_arctic_cfg</code>... then that user will be able to modify all library configs.</p>"},{"location":"aws_permissions/#usage","title":"Usage","text":"<p>Jane can now list and read and write to their own libraries and team libraries, but not to others in Acme.</p> <pre><code>import numpy as np\nimport arcticdb as adb\n\n# jane@acme team=data\naccess = '&lt;REDACTED&gt;'\nsecret = '&lt;REDACTED&gt;'\nbucket='acme-arcticdb'\nregion='eu-west-2'\n\narctic = adb.Arctic(f's3://s3.{region}.amazonaws.com:{bucket}?access={access}&amp;secret={secret}')\n\n# Create library as me\narctic.create_library('jane@acme/weather')\nlib = arctic.get_library('jane@acme/weather')\nlib.write('test', np.arange(100))\n\n# Create library as data team\narctic.create_library('data/forecast')\nlib = arctic.get_library('data/forecast')\nlib.write('test', np.arange(100))\n\n# See all libraries\narctic.list_libraries()\n# ['alan@acme/bonds', 'data/forecast', 'jane@acme/weather', 'quant/stocks']\n\n# Can't use or delete Alan or Quant team data\narctic.get_library('alan@acme/bonds')\narctic.get_library('quant/stocks')\narctic.delete_libraru('alan@acme/bonds')\n# All raise:\n# PermissionException: E_PERMISSION Permission error: S3Error#15 : No response body.\n</code></pre>"},{"location":"azure/","title":"Getting started with Azure Blob Storage","text":"<p>Azure Blob Storage has an extensive set of configuration and access settings. There are detailed guides in the Azure Blob Storage documentation. Best practice can depend on your situation. This guide is intended as a quick-start, to help you get going with ArcticDB.</p> <p>You will need an azure account with permission to create storage-accounts. - Install Azure CLI or use the browser based Cloud Shell. - If you installed Azure CLI then you will also need to login.</p>"},{"location":"azure/#1-select-a-region","title":"1. Select a region.","text":"<p>A region close to your client will mean greater performance. You can list your available regions with. <pre><code>az account list-locations -o table\n</code></pre></p>"},{"location":"azure/#2-create-a-resource-group","title":"2. Create a resource-group","text":"<p>This is not required but best practice would be to create a new resource-group to try out arcticdb.  Resource groups are there to help you collect together and manage related resources in Azure. Set your chosen <code>&lt;REGION&gt;</code> here.  If you use an existing resource-group then replace that in the examples below. <pre><code>az group create --name arcticdb --location &lt;REGION&gt;\n</code></pre></p>"},{"location":"azure/#3-create-a-blob-storage-account","title":"3. Create a blob storage account","text":"<p>This is created within your resource-group.  Choose a <code>&lt;STORAGE_NAME&gt;</code>, it needs to be globally unique across all of Azure. <pre><code>az storage account create -g arcticdb --allow-blob-public-access false --sku Standard_LRS -n &lt;STORAGE_NAME&gt;\n</code></pre> <code>-g arcticdb</code> is the resource-group you created in the last step.</p>"},{"location":"azure/#4-create-a-container","title":"4. Create a container","text":"<p>Create a container within the storage account.  Depending on your account and CLI setup you may need to provide authorization for this step. <pre><code>az storage container create  --name data --account-name &lt;STORAGE_NAME&gt;\n</code></pre></p>"},{"location":"azure/#5-connect-to-the-storage-account","title":"5. Connect to the storage account","text":"<ul> <li> <p>Get the connection string. <pre><code>az storage account show-connection-string -g arcticdb --query connectionString -n &lt;STORAGE_NAME&gt; | sed 's,\",,g'\n</code></pre> The connection string includes the <code>AccountKey</code> for authentication and so you should store it securely.</p> </li> <li> <p>Install ArcticDB.</p> </li> <li>Find your CA_CERT_PATH path. See the ArcticAB API docs for more information.</li> <li>Replace <code>&lt;CONNECTION_STRING&gt;</code> and <code>&lt;CA_CERT_PATH&gt;</code> in the following example. <pre><code>import arcticdb as adb\nconnection_string = '&lt;CONNECTION_STRING&gt;'\nca_cert_path = '&lt;CA_CERT_PATH&gt;'\narctic = adb.Arctic(f\"azure://{connection_string};Container=data;CA_cert_path={ca_cert_path}\")\n</code></pre></li> </ul> <p>Please check the URI manual for more details about Azure Connection String</p>"},{"location":"error_messages/","title":"Error Messages","text":"<p>This page details the exceptions and associated error messages users are most likely to encounter, what they mean, and what (if anything) can be done to resolve the issue.</p> <p>For legacy reasons, the terms <code>symbol</code>, <code>stream</code>, and <code>stream ID</code> are used interchangeably.</p>"},{"location":"error_messages/#errors-with-numeric-error-codes","title":"Errors with numeric error codes","text":"<p>Note</p> <p>We are in the process of adding error codes to all user-facing errors. As a result, this section will expand as error codes are added to existing errors.</p>"},{"location":"error_messages/#internal-errors","title":"Internal Errors","text":"Error Code Cause Resolution 1000 An invalid date range has been passed in. ArcticDB date ranges must be in increasing order. Ensure the requested range is sorted. 1001 Invalid Argument An invalid argument has been passed in. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1002 An internal ArcticDB assertion has failed. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1003 ArcticDB has encountered an internal error. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1004 Unsupported config found in storage Follow the instructions in the error message to repair configuration within your Arctic instance."},{"location":"error_messages/#normalization-errors","title":"Normalization Errors","text":"Error Code Cause Resolution 2000 Attempting to update or append an existing type with an incompatible object type NumPy arrays or Pandas DataFrames can only be mutated by a matching type. Read the latest version of the symbol and update/append with the corresponding type. 2001 Input type cannot be converted to an ArcticDB type. Please ensure all input types match supported ArcticDB types. 2003 A write of an incompatible index type has been attempted. ArcticDB only supports defined Pandas index types. Please see the documentation for more information on what types are supported. 2004 A NumPy append is attempting to change the shape of the previous version. When storing NumPy arrays, append operations must have the same shape as the previous version."},{"location":"error_messages/#missing-data-errors","title":"Missing Data Errors","text":"Error Code Cause Resolution 3000 A missing version has been requested of a symbol. Please request a valid version - see the documentation for the <code>list_versions</code> method to enumerate existing versions. 3001 A symbol from an incomplete library without any versions was requested and no incomplete segments were found. Append incomplete data to the symbol."},{"location":"error_messages/#schema-error","title":"Schema Error","text":"Error Code Cause Resolution 4000 The number, type, or name of the columns has been changed. Ensure that the type and order of the columns has not changed when appending or updating the previous version. This restriction only applies when <code>Dynamic Schema</code> is disabled - if you require the columns sets to change, please enable the <code>Dynamic Schema</code> option on your library. 4001 The specified column does not exist. Please specify a valid column - use the <code>get_description</code> method to see all of the columns associated with a given symbol. 4002 The requested operation is not supported with the type of column provided. Certain operations are not supported over all column types e.g. arithmetic in the processing pipeline over string columns - use the <code>get_description</code> method to see all of the columns associated with a given symbol, along with their types. 4003 The requested operation is not supported with the index type of the symbol provided. Certain operations are not supported over all index types e.g. column statistics generation with a string index - use the <code>get_description</code> method to see the index(es) associated with a given symbol, along with their types. 4004 The requested operation is not supported with pickled data. Certain operations are not supported with pickled data e.g. <code>date_range</code> filtering. If such operations are required, you must ensure that the data is of a normalizable type, such that it can be written using the <code>write</code> method, and does not require the <code>write_pickle</code> method."},{"location":"error_messages/#storage-errors","title":"Storage Errors","text":"Error Code Cause Resolution 5000 A missing key has been requested. ArcticDB has requested a key that does not exist in storage. Ensure that you have requested a <code>symbol</code>, <code>snapshot</code>, <code>version</code>, or column statistic that exists. 5001 ArcticDB is attempting to write to an already-existing key in storage. This error is unexpected - please ensure that no other tools are writing data the same storage location that may conflict with ArcticDB. 5002 The symbol being worked on does not exist. ArcticDB has requested a key that does not exist in storage. Ensure that the symbol exists. 5003 Don't have permissions to carry out the operation. Ensure that you have the permissions to perform the requested operation on the given key. 5010 The LMDB map is full. Close and reopen your LMDB backed Arctic instance with a larger map size. For example to open <code>/tmp/a/b/</code> with a map size of 5GB, use <code>adb.Arctic(\"lmdb:///tmp/a/b?map_size=5GB\")</code>. Also see the LMDB documentation. 5011 An unexpected LMDB error occurred. e.g. File corruption, Environment version mismatch, Page type mismatch etc. Varies depending on the type of failure. Read more on LMDB: return codes 5020 An unexpected S3 error occurred. e.g. Network error, Service not available, Throttling failure etc. Varies depending on the type of failure. 5021 An unexpected S3 error occurred which is retryable. Varies depending on the type of failure. 5030 An unexpected Azure error occurred with a given status code and error code. Varies depending on the type of failure. Read more on Azure Blob Storage error code docs. 5050 Mongo didn't acknowledge the operation. This means that the mongo apis didn't confirm whether the operation was successful. Retry running the previous operation. 5051 An unexpected Mongo error occurred with a given error code. Varies depending on the type of failure. Check MongoDB error codes."},{"location":"error_messages/#sorting-errors","title":"Sorting Errors","text":"Error Code Cause Resolution 6000 Data should be sorted for this operation. The requested operation requires data to be sorted. If this is a modification operation such as update, sort the input data. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the data to be sorted"},{"location":"error_messages/#user-input-errors","title":"User Input Errors","text":"Error Code Cause Resolution 7000 The input provided by the user is invalid in some fashion. The resolution will depend on the nature of the incorrect input, and should be explained in the associated error message. 7001 The input was expected to be a valid decimal string but it is not a valid decimal string. Pass a valid decimal string. 7002 An unsupported character was found in a symbol or library name. We support only the ASCII characters between 32-127 inclusive and exclude <code>&lt;</code>, <code>&gt;</code> and <code>*</code> specifically. Change your name so it contains only valid characters. If you want to bypass this check for symbol names, you can define an environment variable called - ARCTICDB_VersionStore_NoStrictSymbolCheck_int=1. 7003 The library or symbol name was too long. We currently only support names up to 255 characters long. Change your name so it not longer than 255 characters."},{"location":"error_messages/#compatibility-errors","title":"Compatibility Errors","text":"Error Code Cause Resolution 8000 The version of ArcticDB being used to read the column statistics does not understand the statistics format. Update ArcticDB to (at least) the same version as that being used to create the column statistics."},{"location":"error_messages/#errors-without-numeric-error-codes","title":"Errors without numeric error codes","text":""},{"location":"error_messages/#pickling-errors","title":"Pickling errors","text":"<p>These errors relate to data being pickled, which limits the operations available. Internally, pickled symbols are stored as opaque, serialised binary blobs in the data layer. No index or column information is maintained in this serialised object which is in contrast to non-pickled data, where this information is stored in the index layer.</p> <p>Furthermore, it is not possible to partially read/update/append the data using the ArcticDB API or use the processing pipeline with pickled symbols. </p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot append to pickled data  Cannot update pickled data A symbol has been created with the <code>write_pickle</code> method, and now <code>append</code>/<code>update</code> has been called on this symbol. Pickled data cannot be appended to or updated, due to the lack of indexing or column information in the index layer as explained above. If appending is required, the symbol must be created with <code>write</code>, and must therefore only contain normalizeable data types. Cannot delete date range of pickled data  Cannot use head/tail/row_range with pickled data, use plain read instead  Cannot filter pickled data   The data for this symbol is pickled and does not support date_range, row_range, or column queries A symbol has been created with the <code>write_pickle</code> method, and now <code>delete_data_in_range</code>/<code>head</code>/<code>tail</code>/<code>read</code> with a <code>QueryBuilder argument</code> has been called on this symbol. For reading operations, unpickling is inherently a Python-layer process. Therefore any operation that would cut down the amount of data returned to a user compared to a call to <code>read</code> with no optional parameters cannot be performed in the C++ layer, and would be no faster than calling <code>read</code> and then filtering the result down in Python."},{"location":"error_messages/#snapshot-errors","title":"Snapshot errors","text":"<p>Errors that can be encountered when creating  and deleting snapshots, or trying to read data from a specific snapshot.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Snapshot with name &lt;name&gt; already exists The <code>snapshot</code> method was called, but a snapshot with the specified name already exists. The old snapshot must first be deleted with <code>delete_snapshot</code>. Cannot snapshot version(s) that have been deleted... A <code>versions</code> dictionary was provided to the <code>snapshot</code> method, but one of the symbol-version pairs specified does not exist. The <code>list_versions</code> method can be used to see which versions of which symbols are in which snapshots. Only one of skip_symbols and versions can be set The <code>snapshot</code> method was called with both the <code>skip_symbols</code> and <code>versions</code> optional arguments set. Just specify <code>versions</code> on its own in this case."},{"location":"error_messages/#require-live-version-errors","title":"Require live version errors","text":"<p>A select few operations with ArcticDB require the symbol to exist and have at least one live version. These errors occur when this is not the case.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot update non-existent stream &lt;symbol&gt; The <code>update</code> method was called with the optional <code>upsert</code> defaulted or set to <code>False</code>, but this symbol has no live versions. If the symbol is expected to have a live version, then this is a genuine error. Otherwise, set <code>upsert</code> to <code>True</code>."},{"location":"error_messages/#date-range-related-errors","title":"Date-range related errors","text":"<p>All calls to <code>delete_data_in_range</code> and <code>update</code>, and calls to <code>read</code> using the <code>date_range</code> optional argument, require the existing data to have a sorted timestamp index. ArcticDB does not check this condition at write time.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot apply date range filter to symbol with non-timestamp index <code>read</code> method called with the optional <code>date_range</code> argument specified, but the symbol does not have a timestamp index. None, the <code>date_range</code> parameter does not make sense without a timestamp index. Non-contiguous rows, range search on unsorted data?... <code>read</code> method called with the optional <code>date_range</code> argument specified, and the symbol has a timestamp index, but it is not sorted. To use the <code>date_range</code> argument to <code>read</code>, the user must ensure the data is sorted on the index at write time. Delete in range will not work as expected with a non-timeseries index <code>delete_data_in_range</code> method called, but the symbol does not have a timestamp index. None, the <code>delete_data_in_range</code> method does not make sense without a timestamp index."},{"location":"error_messages/#processing-pipeline-errors","title":"Processing pipeline errors","text":"<p>Due to the client-only nature of ArcticDB, it is not possible to know if a processing operation applied to a <code>LazyDataFrame</code>, or provided to <code>read</code> with a <code>QueryBuilder</code> object, makes sense for the given symbol without interacting with the storage. In particular, we do not know:</p> <ul> <li>Whether a specified column exists</li> <li>What the type of the data held in a specified column is if it does exist</li> </ul> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Unexpected column name A column name was specified for a processing operation that does not exist for this symbol, and the library has dynamic schema disabled. Use <code>get_description</code> to ensure that column names provided in processing operations exist for the symbol. Non-numeric type provided to binary operation: &lt;typename&gt; Error messages like this imply that an operation that ArcticDB does not support was provided in a processing operation e.g. adding two string columns together. The <code>get_description</code> method can be used to inspect the types of the columns. A full list of supported operations are provided in the <code>QueryBuilder</code> API documentation. Cannot compare &lt;typename 1&gt; to &lt;typename 2&gt; (possible categorical?) If <code>get_description</code> indicates that a column is of categorical type, and this categorical is being used to store string values, then comparisons to other strings will fail with an error message like this one. Categorical support in ArcticDB is extremely limited, but may be added in the future."},{"location":"error_messages/#encoding-errors","title":"Encoding errors","text":"<p>These errors should be extremely rare, however it is possible that the encoding in the storage may change from time to time. Whilst the changes will always be backwards compatible (new clients can always read the old data), it's possible they may not be forward-compatible, and data that has been written by a new client cannot be read by an older one</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Error decoding A column was unable to be decoded by the compression algorithm. Upgrade to a later version of the client."},{"location":"error_messages/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>ArcticDB exceptions are exposed in <code>arcticdb.exceptions</code> and sit in a hierarchy:</p> <pre><code>RuntimeError\n\u2514-- ArcticException\n    |-- ArcticDbNotYetImplemented\n    |-- MissingDataException\n    |-- NoDataFoundException\n    |-- NoSuchVersionException\n    |-- NormalizationException\n    |-- SchemaException\n    |-- SortingException\n    |   \u2514-- UnsortedDataException\n    |-- StorageException\n    |   \u2514-- LmdbMapFullError\n    |   \u2514-- PermissionException\n    |   \u2514-- DuplicateKeyException\n    |-- StreamDescriptorMismatch\n    \u2514-- InternalException\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Note</p> <p>This FAQ document covers multiple topic areas - please see the contents table on the  right for more information.</p>"},{"location":"faq/#product","title":"Product","text":""},{"location":"faq/#what-is-arcticdb","title":"What is ArcticDB?","text":"<p>ArcticDB is a high performance DataFrame database built for the modern Python Data Science ecosystem. ArcticDB is an embedded database engine - which means that installing ArcticDB is as simple as installing a Python package. This also means that ArcticDB does not require any server infrastructure to function.</p> <p>ArcticDB is optimised for numerical datasets spanning millions of rows and columns, enabling you to store and retrieve massive datasets within a Pythonic, DataFrame-like API that researchers, data scientists and software engineers will find immediately familiar.</p>"},{"location":"faq/#how-does-arcticdb-differ-from-the-version-of-arctic-on-github","title":"How does ArcticDB differ from the version of Arctic on GitHub?","text":"<p>Please see the history page.</p>"},{"location":"faq/#how-does-arcticdb-differ-from-apache-parquet","title":"How does ArcticDB differ from Apache Parquet?","text":"<p>Both ArcticDB and Parquet enable the storage of columnar data without requiring additional infrastructure.</p> <p>ArcticDB however uses a custom storage format that means it offers the following functionality over Parquet:</p> <ul> <li>Versioned modifications (\"time travel\") - ArcticDB is bitemporal.</li> <li>Timeseries indexes. ArcticDB is a timeseries database and as such is optimised for slicing  and dicing timeseries data containing billions of rows.</li> <li>Data discovery - ArcticDB is built for teams. Data is structured into libraries and symbols rather  than raw filepaths.</li> <li>Support for streaming data. ArcticDB is a fully functional streaming/tick database, enabling the storage  of both batch and streaming data.</li> <li>Support for \"dynamic schemas\" - ArcticDB supports datasets with changing schemas (column sets) over time.</li> <li>Support for automatic data deduplication.</li> </ul>"},{"location":"faq/#what-sort-of-data-is-arcticdb-best-suited-to","title":"What sort of data is ArcticDB best suited to?","text":"<p>ArcticDB is an OLA(nalytical)P DBMS, rather than an OLT(ransactional)P DBMS.</p> <p>In practice, this means that ArcticDB is optimised for large numerical datasets and for queries that operate over many rows at a time.</p>"},{"location":"faq/#does-arcticdb-require-a-server","title":"Does ArcticDB require a server?","text":"<p>No. ArcticDB is a fully fledged embedded analytical database system, designed for modern cloud and on-premises object storage that does not require a server for any of the core features.</p>"},{"location":"faq/#what-languages-can-i-use-arcticdb-with","title":"What languages can I use ArcticDB with?","text":"<p>Bindings are currently only available for Python. </p>"},{"location":"faq/#what-is-the-best-practice-for-saving-data-to-arcticdb","title":"What is the best practice for saving Data to ArcticDB?","text":"<p>Users should consider how they store data based on their specific use cases. See the guide here.</p>"},{"location":"faq/#what-are-the-limitations-of-arcticdb-being-client-side","title":"What are the Limitations of ArcticDB being client-side?","text":"<p>The serverless nature of ArcticDB provides excellent performance, making it ideal for data science applications where speed and efficiency are key.  It ensures atomicity, consistency, and durability but does not isolate transactions. Changes to symbols are performed on a last-writer-wins principle, and without isolation, write-after-read transactions are not supported, which is not suitable for use cases requiring strong transactional guarantees. </p>"},{"location":"faq/#what-storage-options-does-arcticdb-support","title":"What storage options does ArcticDB support?","text":"<p>ArcticDB offers compatibility with a wide array of storage choices, both on-premises and in the cloud. It is verified to work with multiple storage systems such as AWS S3, Azure Blob Storage, LMDB, In-memory, Ceph, MinIO (Linux), Pure Flashblade S3, Scality S3, and VAST Data S3, with plans to support additional options soon. </p>"},{"location":"faq/#what-are-the-trade-offs-with-arcticdb-versioning","title":"What are the trade offs with ArcticDB Versioning?","text":"<p>ArcticDB versions data by default, allowing for point-in-time analysis and efficient data updates, including daily appends and historical corrections, making it ideal for research datasets. The database is capable of de-duplicating data that has not changed between versions, using storage space efficiently. The ArcticDB enterprise tools including data pruning and compaction to help manage storage and data-fragmentation as new versions are created. Storing large numbers of versions of data does require more storage.</p> <p>More information can be found here!</p>"},{"location":"faq/#what-granularity-of-authorization-does-arcticdb-support","title":"What granularity of authorization does ArcticDB support?","text":"<p>Authentication is at storage account level, and authorization can be done at ArcticDB Library level for most S3 backends (with directory/path permissions), otherwise also at storage account level.  There are many third-party authentication and authorization integrations available for the backends.</p>"},{"location":"faq/#how-is-arcticdb-data-catalogued-and-discoverable-by-consumers","title":"How is ArcticDB data catalogued and discoverable by consumers?","text":"<p>ArcticDB offers capabilities to list libraries and symbols, complete with metadata. You can use these functions to discover and browse data stored in ArcticDB.</p>"},{"location":"faq/#how-can-i-get-started-using-arcticdb","title":"How can I get started using ArcticDB?","text":"<p>Please see our getting started guide!</p>"},{"location":"faq/#technical","title":"Technical","text":""},{"location":"faq/#does-arcticdb-use-sql","title":"Does ArcticDB use SQL?","text":"<p>No. ArcticDB enables data access and modifications with a Python API that speaks in terms of Pandas DataFrames. See the reference documentation for more details.</p>"},{"location":"faq/#does-arcticdb-de-duplicate-data","title":"Does ArcticDB de-duplicate data?","text":"<p>Yes.</p> <p>On each <code>write</code>, ArcticDB will check the previous version of the symbol that you are writing (and only this version - other symbols will not be scanned!) and skip the write of identical segments. Please keep in mind however that this is most effective when version <code>n</code> is equal to version <code>n-1</code> plus additional data at the end - and only at the end! If there is additional data inserted into the in the middle, then all segments occuring after that modification will almost certainly differ. ArcticDB segments data at fixed intervals and data is only de-duplicated if the hashes of the data segments are identical - as a result, a one row offset will prevent effective de-duplication.</p> <p>Note that this is a library configuration option that is off by default, see <code>help(LibraryOptions)</code> for details of how to enable it.</p>"},{"location":"faq/#how-does-arcticdb-enable-advanced-analytics","title":"How does ArcticDB enable advanced analytics?","text":"<p>ArcticDB is primarily focused on filtering and transfering data from storage through to memory - at which point Pandas, NumPy, or other standard analytical packages can be utilised for analytics.</p> <p>That said, ArcticDB does offer a limited set of analytical functions that are executed inside the C++ storage engine offering significant performance benefits over Pandas. For more information, see the documentation for the <code>LazyDataFrame</code>, <code>LazyDataFrameCollection</code>, and <code>QueryBuilder</code> classes.</p>"},{"location":"faq/#what-does-pickling-mean","title":"What does Pickling mean?","text":"<p>ArcticDB has two means for storing data:</p> <ol> <li>ArcticDB can store your data using the Arctic On-Disk Storage Format.</li> <li>ArcticDB can Pickle your data, storing it as a giant binary blob.</li> </ol> <p>(1) is vastly more performant (i.e. reads and writes are faster), space efficient and unlocks data slicing as described in the getting started guide. There are no practical advantages to storing your data as a Pickled binary-blob - other than certain data types must be Pickled for ArcticDB to be able to store them at all!</p> <p>ArcticDB is only able to store the following data types natively:</p> <ol> <li>Pandas DataFrames</li> <li>NumPy arrays</li> <li>Integers (including timestamps - though timezone information in timestamps is removed)</li> <li>Floats</li> <li>Bools</li> <li>Strings (written as part of a DataFrame/NumPy array)</li> </ol> <p>Note that ArcticDB cannot efficiently store custom Python objects, even if inserted into a Pandas DataFrames/NumPy array.  Pickled data cannot be index or column-sliced, and neither <code>update</code> nor <code>append</code> primitives will function on pickled data. </p>"},{"location":"faq/#how-does-indexing-work-in-arcticdb","title":"How does indexing work in ArcticDB?","text":"<p>See the Getting Started page for details of supported index types.</p>"},{"location":"faq/#can-i-append-with-additional-columns-what-is-dynamic-schema","title":"Can I <code>append</code> with additional columns / What is Dynamic Schema?","text":"<p>You can <code>append</code> (or <code>update</code>) with differing column sets to symbols for which the containing library has <code>Dynamic Schema</code> enabled. See the documentation for the <code>create_library</code> method for more information.</p> <p>You can also change the type of numerical columns - for example, integers will be promoted to floats on read.</p>"},{"location":"faq/#how-does-arcticdb-segment-data","title":"How does ArcticDB segment data?","text":"<p>See On Disk Storage Format and the documentation for the <code>rows_per_segment</code> and <code>columns_per_segment</code> library configuration options for more details. </p>"},{"location":"faq/#how-does-arcticdb-handle-streaming-data","title":"How does ArcticDB handle streaming data?","text":"<p>ArcticDB support for streaming data is on our roadmap for the coming months</p>"},{"location":"faq/#how-does-arcticdb-handle-concurrent-writers","title":"How does ArcticDB handle concurrent writers?","text":"<p>Without a centralised server, ArcticDB does not support transactions. Instead, ArcticDB supports concurrent writers across symbols - but not to a single symbol (unless \"staging the writes\"). It is up to the writer to ensure that clients do not concurrently modify a single symbol.</p> <p>In the case of concurrent writers to a single symbol, the behaviour will be last-writer-wins. Data is not lost per se, but only the version of the last-writer will be accessible through the version chain.</p> <p>To reiterate, ArcticDB supports concurrent writers to multiple symbols, even within a single library.</p> <p>Note</p> <p>ArcticDB does support staging multiple single-symbol concurrent writes. See the documentation for <code>staged</code>. </p>"},{"location":"faq/#does-arcticdb-cache-any-data-locally","title":"Does ArcticDB cache any data locally?","text":"<p>Yes, please see the Runtime Configuration page for details.</p>"},{"location":"faq/#how-can-i-enable-detailed-logging","title":"How can I enable detailed logging?","text":"<p>Please see the Runtime Configuration page for details.</p>"},{"location":"faq/#how-can-i-tune-the-performance-of-arcticdb","title":"How can I tune the performance of ArcticDB?","text":"<p>Please see the Runtime Configuration page for details.</p>"},{"location":"faq/#does-arcticdb-support-categorical-data","title":"Does ArcticDB support categorical data?","text":"<p>ArcticDB currently offers extremely limited support for categorical data. Series and DataFrames with categorical columns can be provided to the <code>write</code> and <code>write_batch</code> methods, and will then behave as expected on <code>read</code>. However, <code>append</code> and <code>update</code> are not yet supported with categorical data, and will raise an exception if attempted. Analytics such as filtering using the <code>LazyDataFrame</code> or <code>QueryBuilder</code> classes is also not supported with categorical data, and will either raise an exception, or give incorrect results, depending on the exact operations requested.</p>"},{"location":"faq/#how-does-arcticdb-handle-nan","title":"How does ArcticDB handle <code>NaN</code>?","text":"<p>The handling of <code>NaN</code> in ArcticDB depends on the type of the column under consideration:</p> <ul> <li>For string columns, <code>NaN</code>, as well as Python <code>None</code>, are fully supported.</li> <li>For floating-point numeric columns, <code>NaN</code> is also fully supported.</li> <li>For integer numeric columns <code>NaN</code> is not supported. A column that otherwise contains only integers will be treated as a floating point column if a <code>NaN</code> is encountered by ArcticDB, at which point the usual rules around type promotion for libraries configured with or without dynamic schema all apply as usual.</li> </ul>"},{"location":"history/","title":"History of ArcticDB","text":""},{"location":"history/#arctic","title":"Arctic","text":"<p>ArcticDB is built upon the foundations of Arctic,  an open-source, high performance datastore written in Python which utilises MongoDB as the backend  storage. Arctic has been under development within Man Group since 2012 and from its very first  release has underpinned the research and trading environments at Man Group.</p> <p>Arctic was open sourced in 2015 and has since racked up over  2,800 GitHub stars and over a million package downloads.</p> <p></p>"},{"location":"history/#to-arcticdb","title":"... to ArcticDB!","text":"<p>In 2018, Man Group embarked on a ground-up rewrite in order to improve upon some of the  foundational limitations of Arctic. We called this version ArcticDB, and is differentiated from Arctic in three main ways:</p> <ol> <li>ArcticDB does not depend on Mongo. Instead, ArcticDB is designed to work with consumer grade S3 - on prem or in the cloud.</li> <li>ArcticDB is written in C++, enabling signficant performance improvements. ArcticDB is an order of magnitude faster than Arctic whilst being vastly easier to set up and get started with. </li> <li>ArcticDB unifies streaming and batch workflows behind the same easy to use, consistent API. </li> </ol> <p>All in all, ArcticDB offers tremendous, scalable and portable performance with the same intuitive  Python and Pandas-centric API as Arctic. Behind the scenes, it utilises a custom C++ storage engine, along with  modern S3-compatible object storage. Both bulk and streaming data workflows have unified APIs, offering a bi-temporal view of data history with no performance penalty. </p> <p>ArcticDB's versatility and ease-of-use have made it the database of choice for all front-office timeseries analysis at Man Group.</p> <p>For more information on how ArcticDB is licensed, please see the licensing FAQ.</p>"},{"location":"lib_config/","title":"Library Configuration","text":"<p>When creating a new library, all configuration options will default to those specified in the <code>LibraryOptions</code> documentation defaults, unless overridden by an explicitly specified <code>LibraryOptions</code> object passed in to the call to <code>create_library</code>.</p> <p>Note that currently library configuration options cannot be changed once the library has been created. The ability to modify library options for existing libraries will be added soon.</p>"},{"location":"licensing/","title":"Licensing &amp; Commercial FAQs","text":"<p>ArcticDB is licensed under the Business Source License 1.1 (BSL), reverting to an Apache 2.0 License after a two-year term. </p>"},{"location":"licensing/#is-arcticdb-free-to-use","title":"Is ArcticDB free to use?","text":"<p>Free Usage: ArcticDB is available at no cost for non-commercial, personal, or academic use.</p> <p>For BSL versions of ArcticDB, a commercial agreement is required for any business use. That includes use in research or dev environments, or where any economic benefit is being derived. Please contact us for any additional information.</p> <p>For pricing queries please email us at info@arcticdb.io</p> <p>Getting Started: If you're interested in evaluating ArcticDB for business use, please contact us to discuss carrying out a proof of concept.</p>"},{"location":"licensing/#how-does-our-licensing-work","title":"How does our licensing work?","text":"<p>Our licensing structure is designed to scale with your organisation's needs:</p> <p>Small Team License: For teams of up to 5 users of the API, we offer a small use case license.</p> <p>Enterprise Licensing: For larger teams and organisations, licensing is customised based on your specific use case and requirements. </p> <p>Enterprise agreements may include: -   SLA guarantees with defined response times -   Dedicated consultancy services -   Custom support arrangements -   Enterprise features </p>"},{"location":"licensing/#do-we-provide-free-trials","title":"Do we provide free trials?","text":"<p>Yes, we provide free trials to help you evaluate ArcticDB for your specific needs. Our trial program is designed to support you through a comprehensive proof of concept, ensuring you can fully assess how ArcticDB fits your use case.</p> <p>During the trial period, our team will work closely with you to:</p> <ul> <li>Guide you through optimal ArcticDB implementation for your requirements</li> <li>Provide hands-on support and best practices</li> <li>Help you determine if ArcticDB is the right solution for your organisation</li> </ul> <p>To get started with your free trial, simply contact us and  we'll set up a tailored evaluation period that works for your timeline and objectives.</p>"},{"location":"licensing/#how-does-the-free-trial-work","title":"How does the free trial work?","text":"<p>Step 1: Strategic Consultation</p> <p>Connect with our team for a personalised consultation where we will discuss more about why you are interested in ArcticDB and your current tech stack. This session ensures you're equipped with everything needed for a successful evaluation, tailored specifically to your organisational requirements.</p> <p>Step 2: Exploration</p> <p>Experience ArcticDB's full potential with complimentary access during your proof-of-concept period. Test real-world scenarios, explore your specific use cases, and see firsthand how our solution integrates seamlessly with your existing infrastructure.</p> <p>Step 3: Expert Support</p> <p>Our dedicated technical specialists can provide support every step of the way. From initial setup to advanced implementation guidance, we ensure your POC experience is smooth, productive, and delivers meaningful insights.</p> <p>Step 4: Licensing</p> <p>Upon completion of your evaluation, we'll collaborate on the optimal path forward. Whether that's advancing to full licensing or simply gathering valuable feedback.</p> <p>Contact us on info@arcticdb.io to start your proof of concept today</p>"},{"location":"licensing/#do-we-provide-support","title":"Do we provide support?","text":"<p>Yes, we can provide comprehensive support tailored to your needs:</p> <p>Standard Support: We offer general assistance through our Slack channel, which typically meets the needs of smaller teams and addresses common questions effectively.</p> <p>Enhanced Support Options: For teams and organisations requiring more customised support, we can provide:</p> <ul> <li>SLA-backed support with guaranteed response times</li> <li>Dedicated consultancy services for hands-on guidance</li> </ul>"},{"location":"licensing/#license-conversion-timeline","title":"License conversion timeline","text":"<p>This is stored in the README.md of the project. </p>"},{"location":"licensing/#can-i-contribute-to-arcticdb","title":"Can I contribute to ArcticDB?","text":"<p>Yes! Contributions are welcome - please see our GitHub readme for more information.</p>"},{"location":"licensing/#is-arcticdb-part-of-man-group","title":"Is ArcticDB part of Man Group?","text":"<p>ArcticDB operates as a separate entity \"ArcticDB Limited\" incubated at Man Group PLC.</p>"},{"location":"runtime_config/","title":"Runtime Configuration","text":"<p>ArcticDB features a variety of options that can be tuned at runtime. This page details the most commonly modified options, and how to configure them.</p>"},{"location":"runtime_config/#configuration-methods","title":"Configuration methods","text":"<p>All of the integer options detailed on this page can be configured using the following two methods. All of the options listed on this page are integer options except for log levels, which will be explained in their own section.</p>"},{"location":"runtime_config/#in-code","title":"In code","text":"<p>For integer options, the following code snippet demonstrates how to set values in code:</p> <pre><code>from arcticdb.config import set_config_int\nset_config_int(setting, value)\n</code></pre> <p>where <code>setting</code> is a string containing the setting name (e.g. <code>VersionMap.ReloadInterval</code>), and <code>value</code> is an int to set the option to.</p>"},{"location":"runtime_config/#environment-variables","title":"Environment variables","text":"<p>For integer options, environment variables can be used to set options as follows:</p> <pre><code>ARCTICDB_&lt;setting&gt;_int=&lt;value&gt;\n</code></pre> <p>e.g. <code>ARCTICDB_VersionMap_ReloadInterval_int=0</code>. Note that <code>.</code> characters in setting names are replaced with underscores when setting them by environment variables.</p>"},{"location":"runtime_config/#priority","title":"Priority","text":"<p>If both the environment variable is set, and <code>set_config_int</code> is called, then the latter takes priority.</p>"},{"location":"runtime_config/#reactivity","title":"Reactivity","text":"<p>Configuration options are read once when the <code>Library</code> instance is created, and are not monitored after that point, so all options should be configured before the <code>Library</code> object is constructed.</p>"},{"location":"runtime_config/#configuration-options","title":"Configuration options","text":""},{"location":"runtime_config/#versionmapreloadinterval","title":"VersionMap.ReloadInterval","text":"<p>ArcticDB library instances maintain a short-lived cache containing what it believes is the latest version for every encountered symbol.  This cache is invalidated after 5 seconds by default.</p> <p>As a result of this caching, it is theoretically possible for two independent library instances to disagree as to what the latest version of a symbol is for a short period of time.</p> <p>This caching is designed to reduce load on storage - if this is not a concern it can be safely disabled by setting this option to <code>0</code>.</p> <p>Other than this, there is no client-side caching in ArcticDB.</p>"},{"location":"runtime_config/#symbollistmaxdelta","title":"SymbolList.MaxDelta","text":"<p>The symbol list cache is compacted when there are more than <code>SymbolList.MaxDelta</code> objects on disk in the symbol list cache.</p> <p>The default is 500.</p>"},{"location":"runtime_config/#s3storagedeletebatchsize","title":"S3Storage.DeleteBatchSize","text":"<p>The S3 API supports the <code>DeleteObjects</code> method, whereby a single HTTP request can be used to delete multiple objects. This parameter can be used to control how many objects are requested to be deleted at a time.</p> <p>The default is 1000.</p>"},{"location":"runtime_config/#s3storageverifyssl","title":"S3Storage.VerifySSL","text":"<p>Control whether the client should verify the SSL certificate of the storage. If set, this will override the library option set upon library creation.</p> <p>Values: * 0: Do not perform SSL verification. * 1: Perform SSL verification.</p>"},{"location":"runtime_config/#s3storageusewininet","title":"S3Storage.UseWinINet","text":"<p>This setting only has an effect on the Windows operating system.</p> <p>Control whether the client should use the WinINet HTTP backend rather than the default WinHTTP backend.</p> <p>WinINet can provide better error messages in AWS SDK debug logs, for example for diagnosing SSL issues. See the logging configuration section below for notes on how to set up AWS SDK debug logs.</p> <p>The INet backend does not allow SSL verification to be disabled with the current AWS SDK.</p> <p>Values: * 0: Use WinHTTP * 1: Use WinINet</p>"},{"location":"runtime_config/#versionstorenumcputhreads-and-versionstorenumiothreads","title":"VersionStore.NumCPUThreads and VersionStore.NumIOThreads","text":"<p>ArcticDB uses two threadpools in order to manage computational resources:</p> <ul> <li>CPU - used for CPU intensive tasks such as decompressing or filtering data</li> <li>IO - used to read/write data from/to the underlying storage</li> </ul> <p>By default, ArcticDB attempts to infer sensible sizes for these threadpools based on the number of cores<sup>*</sup> available on the host machine. The CPU threadpool size defaults to the number of cores available on the host machine, while the IO threadpool size defaults to x1.5 the CPU threadpool size. If these defaults are not suitable for a particular use case, these threadpool sizes can be set directly .</p> <p>If only <code>NumCPUThreads</code> is set, <code>NumIOThreads</code> will still default to x1.5 <code>NumCPUThreads</code>.</p> <p><sup>*</sup>On Linux machines, this core count takes cgroups into account. In particular, this means that CPU limits are respected in processes running in Kubernetes.</p>"},{"location":"runtime_config/#versionstorewillitembepickledwarningmsg","title":"VersionStore.WillItemBePickledWarningMsg","text":"<p>Control whether a detailed message explaining how the item is normalized is logged when calling the <code>will_item_be_pickled</code> function. Please note that this message is logged as a warning. Therefore, setting the log level to below <code>warning</code> will also suppress the log in the <code>will_item_be_pickled</code> function.</p> <p>Values: * 0: Disable * 1: Enable (Default)</p>"},{"location":"runtime_config/#versionstorerecursivenormalizermetastructure","title":"VersionStore.RecursiveNormalizerMetastructure","text":"<p>Controls whether the recursive normalizer will use meta structure V2</p> <p>Read Compatibility:</p> Meta Structure Version Read Support V1 All existing and future ArcticDB releases V2 ArcticDB v6.7.0 and later <p>V1 meta structure phase-out plan:</p> Version Change &gt;= v6.7.0 Deprecation warning when writing V1 meta structure; V2 meta structure can be enabled optionally &gt;= v7.0.0 V2 meta structure will be enabled by default <p>Values: * 1: V1 (Default) * 2: V2  </p>"},{"location":"runtime_config/#versionstoreversionstorerecursivenormalizermetastructurev1deprecationwarning","title":"VersionStore.VersionStore.RecursiveNormalizerMetastructureV1DeprecationWarning","text":"<p>Control whether deprecation warning will be given if meta structure V1 for recursive normalizer is still in use</p> <p>Values: * 0: Disable * 1: Enable (Default)</p> <p>Please note that if meta structure V2 is read by &lt; v6.7.0, exception KeyError will be raised</p>"},{"location":"runtime_config/#logging-configuration","title":"Logging configuration","text":"<p>ArcticDB has multiple log streams, and the verbosity of each can be configured independently.  The available streams are visible in the source code, although the most commonly useful logs are in:</p> <ul> <li><code>version</code> - contains information about versions being read, created, or destroyed, and traversal of the version layer linked list</li> <li><code>storage</code> - contains information about individual operations that interact with the storage device (read object, write object, delete object, etc)</li> </ul> <p>The available log levels in decreasing order of verbosity are are <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>CRITICAL</code>, <code>OFF</code>.  By default, all streams are set to the <code>INFO</code> level.</p> <p>There are two ways to configure log levels: </p> <ol> <li>Setting an environment variable: <code>ARCTICDB_&lt;stream&gt;_loglevel=&lt;level&gt;</code>, for example: <code>ARCTICDB_version_loglevel=DEBUG</code>. All streams can be configured together via <code>ARCTICDB_all_loglevel</code>. </li> <li> <p>In code: Calling <code>set_log_level</code> from the <code>arcticdb.config</code> module. This takes two optional arguments:</p> </li> <li> <p><code>default_level</code> - the default level for all streams. Should be a string such as <code>\"DEBUG\"</code></p> </li> <li><code>specific_log_levels</code> - a dictionary from stream names to log levels used to override the default such as <code>{\"version\": \"DEBUG\"\"}</code>.</li> </ol> <p>If both environment variables are set, and <code>set_log_level</code> is called, then the latter takes priority.</p> <p>S3 logging can also be enabled by setting the environment variable <code>ARCTICDB_AWS_LogLevel_int=6</code>, which will output all S3 logs to a file in the present working directory.  See the AWS documentation for more details.</p>"},{"location":"runtime_config/#logging-destinations","title":"Logging destinations","text":"<p>By default, all logging from ArcticDB goes to <code>stderr</code>. This can be configured using the <code>set_log_level</code> method.</p> <p>To configure logging to only a file:</p> <pre><code>from arcticdb.config import set_log_level\nset_log_level(console_output=False, file_output_path=\"/tmp/arcticdb.log\")\n</code></pre> <p>To configure logging to both <code>stderr</code> and a file:</p> <pre><code>from arcticdb.config import set_log_level\nset_log_level(console_output=True, file_output_path=\"/tmp/arcticdb.log\")\n</code></pre> <p>To configure logging to only <code>stderr</code> (this is the default configuration):</p> <pre><code>from arcticdb.config import set_log_level\nset_log_level(console_output=True, file_output_path=None)\n</code></pre>"},{"location":"api/","title":"ArcticDB Python Documentation","text":""},{"location":"api/#introduction","title":"Introduction","text":"<p>This part of the site documents the Python API of ArcticDB.</p> <p>The API is structured into the following components:</p> <ul> <li>Arctic: Arctic is the primary API used for accessing and manipulating ArcticDB libraries.</li> <li>Library: The Library API enables reading and manipulating symbols inside ArcticDB libraries.</li> <li>DataFrame Processing Operations API: Details the advanced DataFrame processing operations available within ArcticDB.</li> </ul> <p>Most of the code snippets in the API docs require importing <code>arcticdb</code> as <code>adb</code>:</p> <pre><code>import arcticdb as adb\n</code></pre>"},{"location":"api/admin_tools/","title":"Admin Tools API","text":"<p>This page documents the <code>arcticdb.version_store.admin_tools</code> module. This exposes administrative functionality aimed at power users and DBAs.</p> <p>These tools are intended for adhoc use. The API is not currently stable and not governed by the semver version numbers of ArcticDB releases.</p> <p>To get an <code>arcticdb.version_store.admin_tools.AdminTools</code> instance, use <code>arcticdb.version_store.Library.get_admin_tools</code>.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.Size","title":"arcticdb.version_store.admin_tools.Size  <code>dataclass</code>","text":"<p>Information about the size of objects.</p> ATTRIBUTE DESCRIPTION <code>bytes_compressed</code> <p>Compressed size in bytes.</p> <p> TYPE: <code>int</code> </p> <code>count</code> <p>The number of objects contributing to the size.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.Size.bytes_compressed","title":"bytes_compressed  <code>instance-attribute</code>","text":"<pre><code>bytes_compressed: int\n</code></pre> <p>Compressed size in bytes.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.Size.count","title":"count  <code>instance-attribute</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of objects contributing to the size.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType","title":"arcticdb.version_store.admin_tools.KeyType","text":"<p>               Bases: <code>Enum</code></p> <p>This is a subset of all the key types that ArcticDB uses, covering the most important types.</p> <p>More information about the ArcticDB data layout is available here.</p> ATTRIBUTE DESCRIPTION <code>APPEND_DATA</code> <p>Only used for staged writes. Data that has been staged but not yet finalized.</p> <p> </p> <code>LOG</code> <p>Only used for enterprise replication. Small objects recording a stream of changes to the library.</p> <p> </p> <code>LOG_COMPACTED</code> <p>Only used for some enterprise replication installations. A compacted form of the LOG keys.</p> <p> </p> <code>MULTI_KEY</code> <p>Only used for \"recursively normalized\" data, which cannot currently be written with the <code>Library</code> API. </p> <p> </p> <code>SNAPSHOT_REF</code> <p>Metadata used by ArcticDB to store the contents of a snapshot (the structure created when you call <code>lib.snapshot</code>).</p> <p> </p> <code>SYMBOL_LIST</code> <p>A collection of keys that together store the total set of symbols stored in a library. Used for <code>list_symbols</code>.</p> <p> </p> <code>TABLE_DATA</code> <p>Where the contents of data are stored, in a tiled format.</p> <p> </p> <code>TABLE_INDEX</code> <p>Metadata used by ArcticDB to select the TABLE_DATA blocks to read. One per un-deleted version of the symbol.</p> <p> </p> <code>VERSION</code> <p>Metadata used by ArcticDB to store the chain of versions associated with a symbol. It is possible to have</p> <p> </p> <code>VERSION_REF</code> <p>A pointer to the latest version of a symbol. One per symbol.</p> <p> </p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.APPEND_DATA","title":"APPEND_DATA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>APPEND_DATA = 5\n</code></pre> <p>Only used for staged writes. Data that has been staged but not yet finalized.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.LOG","title":"LOG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOG = 8\n</code></pre> <p>Only used for enterprise replication. Small objects recording a stream of changes to the library.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.LOG_COMPACTED","title":"LOG_COMPACTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>LOG_COMPACTED = 9\n</code></pre> <p>Only used for some enterprise replication installations. A compacted form of the LOG keys.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.MULTI_KEY","title":"MULTI_KEY  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MULTI_KEY = 6\n</code></pre> <p>Only used for \"recursively normalized\" data, which cannot currently be written with the <code>Library</code> API. </p> <p>Records all the TABLE_INDEX keys used to compose the overall structure. For example, if you save a list of two dataframes with recursive normalizers, this key would refer to the two index keys used to serialize the two dataframes.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.SNAPSHOT_REF","title":"SNAPSHOT_REF  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SNAPSHOT_REF = 7\n</code></pre> <p>Metadata used by ArcticDB to store the contents of a snapshot (the structure created when you call <code>lib.snapshot</code>).</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.SYMBOL_LIST","title":"SYMBOL_LIST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SYMBOL_LIST = 10\n</code></pre> <p>A collection of keys that together store the total set of symbols stored in a library. Used for <code>list_symbols</code>.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.TABLE_DATA","title":"TABLE_DATA  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TABLE_DATA = 1\n</code></pre> <p>Where the contents of data are stored, in a tiled format.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.TABLE_INDEX","title":"TABLE_INDEX  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TABLE_INDEX = 2\n</code></pre> <p>Metadata used by ArcticDB to select the TABLE_DATA blocks to read. One per un-deleted version of the symbol.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.VERSION","title":"VERSION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VERSION = 3\n</code></pre> <p>Metadata used by ArcticDB to store the chain of versions associated with a symbol. It is possible to have more VERSION keys than the version number of a symbol, as we also write a VERSION key when we delete data.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.KeyType.VERSION_REF","title":"VERSION_REF  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VERSION_REF = 4\n</code></pre> <p>A pointer to the latest version of a symbol. One per symbol.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.AdminTools","title":"arcticdb.version_store.admin_tools.AdminTools","text":"<p>A collection of tools for administrative tasks on an ArcticDB library.</p> <p>This API is not currently stable and not governed by the semver version numbers of ArcticDB releases.</p> See Also <p>Library.admin_tools: The API to get a handle on this object from a Library.</p> METHOD DESCRIPTION <code>get_sizes</code> <p>A breakdown of compressed sizes (in bytes) in the library, grouped by key type.</p> <code>get_sizes_by_symbol</code> <p>A breakdown of compressed sizes (in bytes) in the library, grouped by symbol and then key type.</p> <code>get_sizes_for_symbol</code> <p>A breakdown of compressed sizes (in bytes) used by the given symbol, grouped by key type.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.AdminTools.get_sizes","title":"get_sizes","text":"<pre><code>get_sizes() -&gt; Dict[KeyType, Size]\n</code></pre> <p>A breakdown of compressed sizes (in bytes) in the library, grouped by key type.</p> <p>All the key types in KeyType are always included in the output.</p>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.AdminTools.get_sizes_by_symbol","title":"get_sizes_by_symbol","text":"<pre><code>get_sizes_by_symbol() -&gt; Dict[str, Dict[KeyType, Size]]\n</code></pre> <p>A breakdown of compressed sizes (in bytes) in the library, grouped by symbol and then key type.</p> <p>The following key types (and only these) are always included in the output,</p> <pre><code>VERSION_REF\nVERSION\nTABLE_INDEX\nTABLE_DATA\nAPPEND_DATA\n</code></pre>"},{"location":"api/admin_tools/#arcticdb.version_store.admin_tools.AdminTools.get_sizes_for_symbol","title":"get_sizes_for_symbol","text":"<pre><code>get_sizes_for_symbol(symbol: str) -&gt; Dict[KeyType, Size]\n</code></pre> <p>A breakdown of compressed sizes (in bytes) used by the given symbol, grouped by key type.</p> <p>The following key types (and only these) are always included in the output:</p> <pre><code>VERSION_REF\nVERSION\nTABLE_INDEX\nTABLE_DATA\nAPPEND_DATA\n</code></pre> <p>Does not raise if the symbol does not exist.</p>"},{"location":"api/arctic/","title":"Arctic API","text":"<p>The primary API used to access and manage ArcticDB libraries. Use this to get a handle to a <code>Library</code> instance, which can then be used for subsequent operations as documented in the Library API section.</p> Class Description Arctic Top-level library management class. LibraryOptions Configuration options that can be applied when libraries are created."},{"location":"api/arctic/#arcticdb.Arctic","title":"arcticdb.Arctic","text":"<p>Top-level library management class. Arctic instances can be configured against an S3 environment and enable the creation, deletion and retrieval of Arctic libraries.</p> METHOD DESCRIPTION <code>__init__</code> <p>Initializes a top-level Arctic library management instance.</p> <code>create_library</code> <p>Creates the library named <code>name</code>.</p> <code>delete_library</code> <p>Removes the library called <code>name</code>. This will remove the underlying data contained within the library and as</p> <code>get_library</code> <p>Returns the library named <code>name</code>.</p> <code>get_uri</code> <p>Returns the URI that was used to create the Arctic instance.</p> <code>has_library</code> <p>Query if the given library exists</p> <code>list_libraries</code> <p>Lists all libraries available.</p> <code>modify_library_option</code> <p>Modify an option for a library.</p>"},{"location":"api/arctic/#arcticdb.Arctic.__init__","title":"__init__","text":"<pre><code>__init__(\n    uri: str,\n    encoding_version: EncodingVersion = DEFAULT_ENCODING_VERSION,\n    output_format: Union[OutputFormat, str] = PANDAS,\n    arrow_string_format_default: Union[\n        ArrowOutputStringFormat, DataType\n    ] = LARGE_STRING,\n)\n</code></pre> <p>Initializes a top-level Arctic library management instance.</p> <p>For more information on how to use Arctic Library instances please see the documentation on Library.</p> PARAMETER DESCRIPTION <code>uri</code> <p>URI specifying the backing store used to access, configure, and create Arctic libraries. For more details about the parameters, please refer to the Arctic URI Documentation.</p> <p> TYPE: <code>str</code> </p> <code>encoding_version</code> <p>When creating new libraries with this Arctic instance, the default encoding version to use. Can be overridden by specifying the encoding version in the LibraryOptions argument to create_library.</p> <p> TYPE: <code>EncodingVersion</code> DEFAULT: <code>DEFAULT_ENCODING_VERSION</code> </p> <code>output_format</code> <p>Default output format for all read operations on libraries created from this <code>Arctic</code> instance. Can be overridden per library or per read operation. See <code>OutputFormat</code> documentation for details on available formats.</p> <p> TYPE: <code>Union[OutputFormat, str]</code> DEFAULT: <code>PANDAS</code> </p> <code>arrow_string_format_default</code> <p>Default string column format when using <code>PYARROW</code> or <code>POLARS</code> output formats. Can be overridden per library or per read operation. See <code>ArrowOutputStringFormat</code> documentation for details on available string formats.</p> <p> TYPE: <code>Union[ArrowOutputStringFormat, DataType]</code> DEFAULT: <code>LARGE_STRING</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ac = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')  # Leave AWS to derive credential information\n&gt;&gt;&gt; ac = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET?region=YOUR_REGION&amp;access=ABCD&amp;secret=DCBA') # Manually specify creds\n&gt;&gt;&gt; ac = adb.Arctic('azure://CA_cert_path=/etc/ssl/certs/ca-certificates.crt;BlobEndpoint=https://arctic.blob.core.windows.net;Container=acblob;SharedAccessSignature=sp=sig')\n&gt;&gt;&gt; ac.create_library('travel_data')\n&gt;&gt;&gt; ac.list_libraries()\n['travel_data']\n&gt;&gt;&gt; travel_library = ac['travel_data']\n&gt;&gt;&gt; ac.delete_library('travel_data')\n</code></pre>"},{"location":"api/arctic/#arcticdb.Arctic.create_library","title":"create_library","text":"<pre><code>create_library(\n    name: str,\n    library_options: Optional[LibraryOptions] = None,\n    enterprise_library_options: Optional[\n        EnterpriseLibraryOptions\n    ] = None,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n) -&gt; Library\n</code></pre> <p>Creates the library named <code>name</code>.</p> <p>Arctic libraries contain named symbols which are the atomic unit of data storage within Arctic. Symbols contain data that in most cases strongly resembles a DataFrame and are versioned such that all modifying operations can be tracked and reverted.</p> <p>Arctic libraries support concurrent writes and reads to multiple symbols as well as concurrent reads to a single symbol. However, concurrent writers to a single symbol are not supported other than for primitives that explicitly state support for single-symbol concurrent writes.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the library that you wish to create.</p> <p> TYPE: <code>str</code> </p> <code>library_options</code> <p>Options to use in configuring the library. Defaults if not provided are the same as documented in LibraryOptions.</p> <p> TYPE: <code>Optional[LibraryOptions]</code> DEFAULT: <code>None</code> </p> <code>enterprise_library_options</code> <p>Enterprise options to use in configuring the library. Defaults if not provided are the same as documented in EnterpriseLibraryOptions. These options are only relevant to ArcticDB enterprise users.</p> <p> TYPE: <code>Optional[EnterpriseLibraryOptions]</code> DEFAULT: <code>None</code> </p> <code>output_format</code> <p>Default output format for all read operations on this library. If <code>None</code>, uses the output format from the <code>Arctic</code> instance. Can be overridden per read operation. See <code>OutputFormat</code> documentation for details on available formats.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>Default string column format when using <code>PYARROW</code> or <code>POLARS</code> output formats on this library. If <code>None</code>, uses the <code>arrow_string_format_default</code> from the <code>Arctic</code> instance. Can be overridden per read operation. See <code>ArrowOutputStringFormat</code> documentation for details on available string formats. Note that this setting is only applied to the runtime <code>Library</code> instance and is not stored as part of the library configuration.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.create_library('test.library')\n&gt;&gt;&gt; my_library = arctic['test.library']\n</code></pre> RETURNS DESCRIPTION <code>Library that was just created</code>"},{"location":"api/arctic/#arcticdb.Arctic.delete_library","title":"delete_library","text":"<pre><code>delete_library(name: str) -&gt; None\n</code></pre> <p>Removes the library called <code>name</code>. This will remove the underlying data contained within the library and as such will take as much time as the underlying delete operations take.</p> <p>If no library with <code>name</code> exists then this is a no-op. In particular this method does not raise in this case.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the library to delete.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/arctic/#arcticdb.Arctic.get_library","title":"get_library","text":"<pre><code>get_library(\n    name: str,\n    create_if_missing: Optional[bool] = False,\n    library_options: Optional[LibraryOptions] = None,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n) -&gt; Library\n</code></pre> <p>Returns the library named <code>name</code>.</p> <p>This method can also be invoked through subscripting. <code>adb.Arctic('bucket').get_library(\"test\")</code> is equivalent to <code>adb.Arctic('bucket')[\"test\"]</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the library that you wish to retrieve.</p> <p> TYPE: <code>str</code> </p> <code>create_if_missing</code> <p>If True, and the library does not exist, then create it.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>library_options</code> <p>If create_if_missing is True, and the library does not already exist, then it will be created with these options, or the defaults if not provided. If create_if_missing is True, and the library already exists, ensures that the existing library options match these. Unused if create_if_missing is False.</p> <p> TYPE: <code>Optional[LibraryOptions]</code> DEFAULT: <code>None</code> </p> <code>output_format</code> <p>Default output format for all read operations on this library. If <code>None</code>, uses the output format from the <code>Arctic</code> instance. Can be overridden per read operation. See <code>OutputFormat</code> documentation for details on available formats.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>Default string column format when using <code>PYARROW</code> or <code>POLARS</code> output formats on this library. If <code>None</code>, uses the <code>arrow_string_format_default</code> from the <code>Arctic</code> instance. Can be overridden per read operation. See <code>ArrowOutputStringFormat</code> documentation for details on available string formats.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.create_library('test.library')\n&gt;&gt;&gt; my_library = arctic.get_library('test.library')\n&gt;&gt;&gt; my_library = arctic['test.library']\n</code></pre> RETURNS DESCRIPTION <code>Library</code>"},{"location":"api/arctic/#arcticdb.Arctic.get_uri","title":"get_uri","text":"<pre><code>get_uri() -&gt; str\n</code></pre> <p>Returns the URI that was used to create the Arctic instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.get_uri()\n</code></pre> RETURNS DESCRIPTION <code>s3</code> <p> TYPE: <code>//MY_ENDPOINT:MY_BUCKET</code> </p>"},{"location":"api/arctic/#arcticdb.Arctic.has_library","title":"has_library","text":"<pre><code>has_library(name: str) -&gt; bool\n</code></pre> <p>Query if the given library exists</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the library to check the existence of.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>True if the library exists, False otherwise.</code>"},{"location":"api/arctic/#arcticdb.Arctic.list_libraries","title":"list_libraries","text":"<pre><code>list_libraries() -&gt; List[str]\n</code></pre> <p>Lists all libraries available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.list_libraries()\n['test.library']\n</code></pre> RETURNS DESCRIPTION <code>A list of all library names that exist in this Arctic instance.</code>"},{"location":"api/arctic/#arcticdb.Arctic.modify_library_option","title":"modify_library_option","text":"<pre><code>modify_library_option(\n    library: Library,\n    option: Union[\n        ModifiableLibraryOption,\n        ModifiableEnterpriseLibraryOption,\n    ],\n    option_value: Any,\n)\n</code></pre> <p>Modify an option for a library.</p> <p>See <code>LibraryOptions</code> and <code>EnterpriseLibraryOptions</code> for descriptions of the meanings of the various options.</p> <p>After the modification, this process and other processes that open the library will use the new value. Processes that already have the library open will not see the configuration change until they restart.</p> PARAMETER DESCRIPTION <code>library</code> <p>The library to modify.</p> <p> TYPE: <code>Library</code> </p> <code>option</code> <p>The library option to change.</p> <p> TYPE: <code>Union[ModifiableLibraryOption, ModifiableEnterpriseLibraryOption]</code> </p> <code>option_value</code> <p>The new setting for the library option.</p> <p> TYPE: <code>Any</code> </p>"},{"location":"api/arctic/#arcticdb.LibraryOptions","title":"arcticdb.LibraryOptions","text":"<p>Configuration options for ArcticDB libraries.</p> ATTRIBUTE DESCRIPTION <code>dynamic_schema</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>bool</code> </p> <code>dedup</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>bool</code> </p> <code>rows_per_segment</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>int</code> </p> <code>columns_per_segment</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>int</code> </p> <code>recursive_normalizers</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>bool</code> </p> METHOD DESCRIPTION <code>__init__</code> <p>Parameters</p>"},{"location":"api/arctic/#arcticdb.LibraryOptions.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    dynamic_schema: bool = False,\n    dedup: bool = False,\n    rows_per_segment: int = 100000,\n    columns_per_segment: int = 127,\n    encoding_version: Optional[EncodingVersion] = None,\n    recursive_normalizers: bool = False\n)\n</code></pre> PARAMETER DESCRIPTION <code>dynamic_schema</code> <p>Controls whether the library supports dynamically changing symbol schemas.</p> <p>The schema of a symbol refers to the order of the columns and the type of the columns.</p> <p>If False, then the schema for a symbol is set on each <code>write</code> call, and cannot then be modified by successive updates or appends. Each successive update or append must contain the same column set in the same order with the same types as the initial write.</p> <p>When disabled, ArcticDB will tile stored data across both the rows and columns. This enables highly efficient retrieval of specific columns regardless of the total number of columns stored in the symbol.</p> <p>If True, then updates and appends can contain columns not originally seen in the most recent write call. The data will be dynamically backfilled on read when required for the new columns. Furthermore, Arctic will support numeric type promotions should the type of a column change - for example, should column A be of type int32 on write, and of type float on the next append, the column will be returned as a float to Pandas on read. Supported promotions include (narrow) integer to (wider) integer, and integer to float.</p> <p>When enabled, ArcticDB will only tile across the rows of the data. This will result in slower column subsetting when storing a large number of columns (&gt;1,000).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dedup</code> <p>Controls whether calls to write and write_batch will attempt to deduplicate data segments against the previous live version of the specified symbol.</p> <p>If False, new data segments will always be written for the new version of the symbol being created.</p> <p>If True, the content hash, start index, and end index of data segments associated with the previous live version of this symbol will be compared with those about to be written, and will not be duplicated in the storage device if they match.</p> <p>Keep in mind that this is most effective when version n is equal to version n-1 plus additional data at the end - and only at the end! If there is additional data inserted at the start or into the the middle, then all segments occuring after that modification will almost certainly differ. ArcticDB creates new segments at fixed intervals and data is only de-duplicated if the hashes of the data segments are identical. A one row offset will therefore prevent this de-duplication.</p> <p>Note that these conditions will also be checked with write_pickle and write_pickle_batch. However, pickled objects are always written as a single data segment, and so dedup will only occur if the written object is identical to the previous version.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rows_per_segment</code> <p>Together with columns_per_segment, controls how data being written, appended, or updated is sliced into separate data segment objects before being written to storage.</p> <p>By splitting data across multiple objects in storage, calls to read and read_batch that include the date_range and/or columns parameters can reduce the amount of data read from storage by only reading those data segments that contain data requested by the reader.</p> <p>For example, if writing a dataframe with 250,000 rows and 200 columns, by default, this will be sliced into 6 data segments: 1 - rows 1-100,000 and columns 1-127 2 - rows 100,001-200,000 and columns 1-127 3 - rows 200,001-250,000 and columns 1-127 4 - rows 1-100,000 and columns 128-200 5 - rows 100,001-200,000 and columns 128-200 6 - rows 200,001-250,000 and columns 128-200</p> <p>Data segments that cover the same range of rows are said to belong to the same row-slice (e.g. segments 2 and 5 in the example above). Data segments that cover the same range of columns are said to belong to the same column-slice (e.g. segments 2 and 3 in the example above).</p> <p>Note that this slicing is only applied to the new data being written, existing data segments from previous versions that can remain the same will not be modified. For example, if a 50,000 row dataframe with a single column is written, and then another dataframe also with 50,000 rows and one column is appended to it, there will still be two data segments each with 50,000 rows.</p> <p>Note that for libraries with dynamic_schema enabled, columns_per_segment does not apply, and there is always a single column-slice. However, rows_per_segment is used, and there will be multiple row-slices.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>columns_per_segment</code> <p>See rows_per_segment</p> <p> TYPE: <code>int</code> DEFAULT: <code>127</code> </p> <code>encoding_version</code> <p>The encoding version to use when writing data to storage. v2 is faster, but still experimental, so use with caution.</p> <p> TYPE: <code>Optional[EncodingVersion]</code> DEFAULT: <code>None</code> </p> <code>recursive_normalizers</code> <p>Whether to recursively normalize nested data structures when writing sequence-like or dict-like data. The data structure can be nested or a mix of lists and dictionaries. Note: If the leaf nodes cannot be natively normalized and must be written using write_pickle, those leaf nodes will be pickled, resulting in the overall data being only partially normalized and partially pickled. Example:     data = {\"a\": np.arange(5), \"b\": pd.DataFrame({\"col\": [1, 2, 3]})}     lib = ac.create_library(lib_name)     lib.write(symbol, data) # ArcticUnsupportedDataTypeException will be thrown by default     lib2 = ac.create_library(lib_name, LibraryOptions(recursive_normalizers=True))     lib2.write(symbol, data) # data will be successfully written</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"api/arctic_uri/","title":"Arctic URI","text":"<p>The URI provided to an <code>Arctic</code> instance is used to specify the storage backend and its configuration.</p>"},{"location":"api/arctic_uri/#s3","title":"S3","text":"<p>The S3 URI connection scheme has the form <code>s3(s)://&lt;s3 end point&gt;:&lt;s3 bucket&gt;[?options]</code>.</p> <p>Use s3s as the protocol if communicating with a secure endpoint.</p> <p>Options is a query string that specifies connection specific options as <code>&lt;name&gt;=&lt;value&gt;</code> pairs joined with <code>&amp;</code>.</p> <p>Available options for S3:</p> Option Description port port to use for S3 connection region S3 region use_virtual_addressing Whether to use virtual addressing to access the S3 bucket access S3 access key secret S3 secret access key path_prefix Path within S3 bucket to use for data storage aws_auth AWS authentication method. If setting is <code>default</code> (or <code>true</code> for backward compatibility), authentication to endpoint will be computed via AWS default credential provider chain. If setting is <code>sts</code>, AWS Security Token Service (STS) will be the authentication method used. If no options are provided AWS authentication will not be used and you should specify access and secret in the URI. More info about <code>sts</code> is provided here aws_profile Only when <code>aws_auth</code> is set to be <code>sts</code>. AWS profile to be used with AWS Security Token Service (STS). More info about <code>sts</code> is provided here <p>Note: When connecting to AWS, <code>region</code> can be automatically deduced from the endpoint if the given endpoint specifies the region and <code>region</code> is not set.</p>"},{"location":"api/arctic_uri/#azure","title":"Azure","text":"<p>The Azure URI connection scheme has the form <code>azure://[options]</code>. It is based on the Azure Connection String, with additional options for configuring ArcticDB. Please refer to Azure for more details.</p> <p><code>options</code> is a string that specifies connection specific options as <code>&lt;name&gt;=&lt;value&gt;</code> pairs joined with <code>;</code> (the final key value pair should not include a trailing <code>;</code>).</p>"},{"location":"api/arctic_uri/#notable-options","title":"Notable options","text":"Option Description DefaultEndpointsProtocol Indicate whether the connection to the storage account through https or http AccountName Name of the storage account AccountKey Access key of the storage account BlobEndpoint Endpoint of the storage account; Usually this should end with <code>blob.core.windows.net</code> SharedAccessSignature Shared Access Signature for authentication EndpointSuffix Suffix of the endpoint"},{"location":"api/arctic_uri/#additional-options-specific-for-arcticdb","title":"Additional options specific for ArcticDB","text":"Option Description Container Azure container for blobs Path_prefix Path within Azure container to use for data storage CA_cert_path (Linux platform only) Azure CA certificate path. If not set, python <code>ssl.get_default_verify_paths().cafile</code> will be used. If the certificate cannot be found in the provided path, an Azure exception with no meaningful error code will be thrown. For more details, please see here. For example, <code>Failed to iterate azure blobs 'C' 0:</code>. CA_cert_dir (Linux platform only) Azure CA certificate directory. If not set, python <code>ssl.get_default_verify_paths().capath</code> will be used. Certificates can only be used if corresponding hash files exist. If the certificate cannot be found in the provided path, an Azure exception with no meaningful error code will be thrown. For more details, please see here. For example, <code>Failed to iterate azure blobs 'C' 0:</code>."},{"location":"api/arctic_uri/#example","title":"Example","text":""},{"location":"api/arctic_uri/#with-access-key-authentication","title":"With Access Key Authentication","text":"<p><code>azure://DefaultEndpointsProtocol=https;AccountName=arcticdb;AccountKey=KEY;EndpointSuffix=core.windows.net;Container=test</code></p>"},{"location":"api/arctic_uri/#with-shared-access-signature-authentication","title":"With Shared Access Signature Authentication","text":"<p><code>azure://BlobEndpoint=https://arcticdb.blob.core.windows.net/;SharedAccessSignature=KEY;Container=test</code></p>"},{"location":"api/arctic_uri/#notes","title":"Notes","text":"<p>For non-Linux platforms, neither <code>CA_cert_path</code> nor <code>CA_cert_dir</code> may be set. Please set CA certificate related options using operating system settings. For Windows, please see here</p> <p>Exception: Azure exceptions message always ends with <code>{AZURE_SDK_HTTP_STATUS_CODE}:{AZURE_SDK_REASON_PHRASE}</code>.</p> <p>Please refer to azure-sdk-for-cpp for more details of provided status codes.</p> <p>Note that due to a bug in Azure C++ SDK, Azure may not give meaningful status codes and reason phrases in the exception. To debug these instances, please set the environment variable <code>export AZURE_LOG_LEVEL</code> to <code>1</code> to turn on the SDK debug logging.</p>"},{"location":"api/arctic_uri/#lmdb","title":"LMDB","text":"<p>The LMDB connection scheme has the form <code>lmdb:///&lt;path to store LMDB files&gt;[?options]</code>.</p> <p>Options is a query string that specifies connection specific options as <code>&lt;name&gt;=&lt;value&gt;</code> pairs joined with <code>&amp;</code>.</p> Option Description map_size LMDB map size (see here). String. Supported formats are:\"150MB\" / \"20GB\" / \"3TB\"The only supported units are MB / GB / TB.On Windows and MacOS, LMDB will materialize a file of this size, so you need to set it to a reasonable value that your system has room for, and it has a small default (order of 1GB). On Linux, this is an upper bound on the space used by LMDB and the default is large (order of 100GB).  ArcticDB creates an LMDB database per library, so the <code>map_size</code> will be per library. <p>Example connection strings are <code>lmdb:///home/user/my_lmdb</code> or <code>lmdb:///home/user/my_lmdb?map_size=2GB</code>.</p>"},{"location":"api/arctic_uri/#in-memory","title":"In-Memory","text":"<p>The in-memory connection scheme has the form <code>mem://</code>.</p> <p>The storage is local to the <code>Arctic</code> instance.</p>"},{"location":"api/config/","title":"Config API","text":""},{"location":"api/config/#arcticdb.config.get_config_int","title":"arcticdb.config.get_config_int","text":"<pre><code>get_config_int(label: str) -&gt; int | None\n</code></pre> <p>Get configured value, returns None if not set.</p>"},{"location":"api/config/#arcticdb.config.set_config_int","title":"arcticdb.config.set_config_int","text":"<pre><code>set_config_int(label: str, value: int) -&gt; None\n</code></pre> <p>Set configured value.</p>"},{"location":"api/config/#arcticdb.config.set_log_level","title":"arcticdb.config.set_log_level","text":"<pre><code>set_log_level(\n    default_level: str = DEFAULT_LOG_LEVEL,\n    specific_log_levels: Optional[Dict[str, str]] = None,\n    console_output: bool = True,\n    file_output_path: Optional[str] = None,\n)\n</code></pre> <p>Set log levels, overwriting any existing config.</p> PARAMETER DESCRIPTION <code>default_level</code> <p>Default log level for all the loggers unless overriden with specific_log_levels. Valid values are \"TRACE\" (most verbose), \"DEBUG\", \"INFO\", \"WARN\", \"ERROR\", \"CRITICAL\", \"OFF\" (no logging).</p> <p> TYPE: <code>str</code> DEFAULT: <code>DEFAULT_LOG_LEVEL</code> </p> <code>specific_log_levels</code> <p>Optional overrides for specific logger(s). The possible logger names can be found in <code>arcticdb.log.logger_by_name.keys()</code> (subject to change).</p> <p> TYPE: <code>Optional[Dict[str, str]]</code> DEFAULT: <code>None</code> </p> <code>console_output</code> <p>Boolean indicating whether to output logs to the terminal.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>file_output_path</code> <p>If None, logs will not be written to a file. Otherwise, this value should be set to the path of a file to which logging output will be written.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#arcticdb.DataError","title":"arcticdb.DataError","text":"<p>Return value for batch methods which fail in some way.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>Read or modified symbol.</p> <p> TYPE: <code>str</code> </p> <code>version_request_type</code> <p>For operations that support as_of, the type of version query provided. <code>None</code> otherwise.</p> <p> TYPE: <code>Optional[VersionRequestType]</code> </p> <code>version_request_data</code> <p>For operations that support as_of, the value provided in the version query:     None - Operation does not support as_of, or latest version was requested     str - The name of the snapshot provided to as_of     int - The specific version requested if version_request_type == VersionRequestType::SPECIFIC, or           nanoseconds since epoch if version_request_type == VersionRequestType::TIMESTAMP</p> <p> TYPE: <code>Optional[Union[str, int]]</code> </p> <code>error_code</code> <p>For the most common error types, the ErrorCode is included here. e.g. ErrorCode.E_NO_SUCH_VERSION if the version requested has been deleted Please see the Error Messages section of the docs for more detail.</p> <p> TYPE: <code>Optional[ErrorCode]</code> </p> <code>error_category</code> <p>If error_code is provided, the category is also provided. e.g.  ErrorCategory.MISSING_DATA if error_code is ErrorCode.E_NO_SUCH_VERSION</p> <p> TYPE: <code>Optional[ErrorCategory]</code> </p> <code>exception_string</code> <p>The string associated with the exception that was originally raised.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/exceptions/#arcticdb.version_store.library.ArcticDuplicateSymbolsInBatchException","title":"arcticdb.version_store.library.ArcticDuplicateSymbolsInBatchException","text":"<p>               Bases: <code>ArcticInvalidApiUsageException</code></p> <p>Exception indicating that duplicate symbols were passed to a batch method of this module.</p>"},{"location":"api/exceptions/#arcticdb.version_store.library.ArcticInvalidApiUsageException","title":"arcticdb.version_store.library.ArcticInvalidApiUsageException","text":"<p>               Bases: <code>ArcticException</code></p> <p>Exception indicating an invalid call made to the Arctic API.</p>"},{"location":"api/exceptions/#arcticdb.version_store.library.ArcticUnsupportedDataTypeException","title":"arcticdb.version_store.library.ArcticUnsupportedDataTypeException","text":"<p>               Bases: <code>ArcticInvalidApiUsageException</code></p> <p>Exception indicating that a method does not support the type of data provided.</p>"},{"location":"api/library/","title":"Library API","text":"<p>This page documents the <code>arcticdb.version_store.library</code> module. This module is the main interface exposing read/write functionality within a given Arctic instance.</p> <p>The key functionality is exposed through <code>arcticdb.version_store.library.Library</code> instances. See the Arctic API section for notes on how to create these. The other types exposed in this module are less important and are used as part of the signature of <code>arcticdb.version_store.library.Library</code> instance methods.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library","title":"arcticdb.version_store.library.Library","text":"<p>The main interface exposing read/write functionality within a given Arctic instance.</p> <p>Arctic libraries contain named symbols which are the atomic unit of data storage within Arctic. Symbols contain data that in most cases resembles a DataFrame and are versioned such that all modifying operations can be tracked and reverted.</p> <p>Instances of this class provide a number of primitives to write, modify and remove symbols, as well as also providing methods to manage library snapshots. For more information on snapshots please see the <code>snapshot</code> method.</p> <p>Arctic libraries support concurrent writes and reads to multiple symbols as well as concurrent reads to a single symbol. However, concurrent writers to a single symbol are not supported other than for primitives that explicitly state support for single-symbol concurrent writes.</p> METHOD DESCRIPTION <code>admin_tools</code> <p>Administrative utilities that operate on this library.</p> <code>append</code> <p>Appends the given data to the existing, stored data. Append always appends along the index. A new version will</p> <code>append_batch</code> <p>Append data to multiple symbols in a batch fashion. This is more efficient than making multiple <code>append</code> calls in</p> <code>compact_symbol_list</code> <p>Compact the symbol list cache into a single key in the storage</p> <code>defragment_symbol_data</code> <p>Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer).</p> <code>delete</code> <p>Delete all versions of the symbol from the library, unless <code>version</code> is specified, in which case only those</p> <code>delete_batch</code> <p>Delete multiple symbols in a batch fashion.</p> <code>delete_data_in_range</code> <p>Delete data within the given date range, creating a new version of <code>symbol</code>.</p> <code>delete_snapshot</code> <p>Delete a named snapshot. This may take time if the given snapshot is the last reference to the underlying</p> <code>delete_staged_data</code> <p>Removes staged data.</p> <code>enterprise_options</code> <p>Enterprise library options set on this library. See also <code>options</code> for non-enterprise options.</p> <code>finalize_staged_data</code> <p>Finalizes staged data, making it available for reads. All staged segments must be ordered and non-overlapping.</p> <code>get_description</code> <p>Returns descriptive data for <code>symbol</code>.</p> <code>get_description_batch</code> <p>Returns descriptive data for a list of <code>symbols</code>.</p> <code>get_staged_symbols</code> <p>Returns all symbols with staged, unfinalized data.</p> <code>has_symbol</code> <p>Whether this library contains the given symbol.</p> <code>head</code> <p>Read the first n rows of data for the named symbol. If n is negative, return all rows except the last n rows.</p> <code>is_symbol_fragmented</code> <p>Check whether the number of segments that would be reduced by compaction is more than or equal to the</p> <code>list_snapshots</code> <p>List the snapshots in the library.</p> <code>list_symbols</code> <p>Return the symbols in this library.</p> <code>list_versions</code> <p>Get the versions in this library, filtered by the passed in parameters.</p> <code>merge_experimental</code> <p>Merge new data into an existing symbol's DataFrame according to a specified strategy.</p> <code>options</code> <p>Library options set on this library. See also <code>enterprise_options</code>.</p> <code>prune_previous_versions</code> <p>Removes all (non-snapshotted) versions from the database for the given symbol, except the latest.</p> <code>read</code> <p>Read data for the named symbol.  Returns a VersionedItem object with a data and metadata element (as passed into</p> <code>read_batch</code> <p>Reads multiple symbols.</p> <code>read_batch_and_join</code> <p>Reads multiple symbols in a batch, and then joins them together using the first clause in the <code>query_builder</code></p> <code>read_metadata</code> <p>Return the metadata saved for a symbol.  This method is faster than read as it only loads the metadata, not the</p> <code>read_metadata_batch</code> <p>Reads the metadata of multiple symbols.</p> <code>reload_symbol_list</code> <p>Forces the symbol list cache to be reloaded.</p> <code>snapshot</code> <p>Creates a named snapshot of the data within a library.</p> <code>sort_and_finalize_staged_data</code> <p>Sorts and merges all staged data, making it available for reads. This differs from <code>finalize_staged_data</code> in that it</p> <code>stage</code> <p>Similar to <code>write</code> but the written segments are left in an \"incomplete\" state, unable to be read until they</p> <code>tail</code> <p>Read the last n rows of data for the named symbol. If n is negative, return all rows except the first n rows.</p> <code>update</code> <p>Overwrites existing symbol data with the contents of <code>data</code>. The entire range between the first and last index</p> <code>update_batch</code> <p>Perform an update operation on a list of symbols in parallel. All constrains on</p> <code>write</code> <p>Write <code>data</code> to the specified <code>symbol</code>. If <code>symbol</code> already exists then a new version will be created to</p> <code>write_batch</code> <p>Write a batch of multiple symbols.</p> <code>write_metadata</code> <p>Write metadata under the specified symbol name to this library. The data will remain unchanged.</p> <code>write_metadata_batch</code> <p>Write metadata to multiple symbols in a batch fashion. This is more efficient than making multiple <code>write_metadata</code> calls</p> <code>write_pickle</code> <p>See <code>write</code>. This method differs from <code>write</code> only in that <code>data</code> can be of any type that is serialisable via</p> <code>write_pickle_batch</code> <p>Write a batch of multiple symbols, pickling their data if necessary.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name of this library.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of this library.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.admin_tools","title":"admin_tools","text":"<pre><code>admin_tools() -&gt; AdminTools\n</code></pre> <p>Administrative utilities that operate on this library.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.append","title":"append","text":"<pre><code>append(\n    symbol: str,\n    data: NormalizableType,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    validate_index: bool = True,\n    index_column: Optional[str] = None,\n) -&gt; VersionedItem\n</code></pre> <p>Appends the given data to the existing, stored data. Append always appends along the index. A new version will be created to reference the newly-appended data. Append only accepts data for which the index of the first row is equal to or greater than the index of the last row in the existing data.</p> <p>Appends containing differing column sets to the existing data are only possible if the library has been configured to support dynamic schemas.</p> <p>If <code>append</code> is called on a symbol that does not exist, it will create it. This is convenient when setting up a new symbol, but be careful - it will not work for creating a new version of an existing symbol. Use <code>write</code> in that case.</p> <p>Note that <code>append</code> is not designed for multiple concurrent writers over a single symbol.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the new symbol version. Note that the metadata is not combined in any way with the metadata stored in the previous version.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>If True, verify that the index of <code>data</code> supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing. Note that no checks are performed for Arrow input data.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>index_column</code> <p>Optional specification of timeseries index column if data is an Arrow table. Ignored if data is not an Arrow table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store.</p> RAISES DESCRIPTION <code>UnsortedDataException</code> <p>If data is unsorted, when validate_index is set to True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...    {'column': [1,2,3]},\n...    index=pd.date_range(start='1/1/2018', end='1/03/2018')\n... )\n&gt;&gt;&gt; df\n            column\n2018-01-01       1\n2018-01-02       2\n2018-01-03       3\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; to_append_df = pd.DataFrame(\n...    {'column': [4,5,6]},\n...    index=pd.date_range(start='1/4/2018', end='1/06/2018')\n... )\n&gt;&gt;&gt; to_append_df\n            column\n2018-01-04       4\n2018-01-05       5\n2018-01-06       6\n&gt;&gt;&gt; lib.append(\"symbol\", to_append_df)\n&gt;&gt;&gt; lib.read(\"symbol\").data\n            column\n2018-01-01       1\n2018-01-02       2\n2018-01-03       3\n2018-01-04       4\n2018-01-05       5\n2018-01-06       6\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.append_batch","title":"append_batch","text":"<pre><code>append_batch(\n    append_payloads: List[WritePayload],\n    prune_previous_versions: bool = False,\n    validate_index=True,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Append data to multiple symbols in a batch fashion. This is more efficient than making multiple <code>append</code> calls in succession as some constant-time operations can be executed only once rather than once for each element of <code>append_payloads</code>. Note that this isn't an atomic operation - it's possible for one symbol to be fully written and readable before another symbol.</p> PARAMETER DESCRIPTION <code>append_payloads</code> <p>Symbols and their corresponding data. There must not be any duplicate symbols in <code>append_payloads</code>.</p> <p> TYPE: <code>`List[WritePayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>Verify that each entry in the batch has an index that supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing. Note that no checks are performed for Arrow input data.</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. i-th entry corresponds to i-th element of <code>append_payloads</code>. Each result correspond to a structure containing metadata and version number of the affected symbol in the store. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> <code>ArcticUnsupportedDataTypeException</code> <p>If data that is not of NormalizableType appears in any of the payloads.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.compact_symbol_list","title":"compact_symbol_list","text":"<pre><code>compact_symbol_list() -&gt; None\n</code></pre> <p>Compact the symbol list cache into a single key in the storage</p> RETURNS DESCRIPTION <code>The number of symbol list keys prior to compaction</code> RAISES DESCRIPTION <code>PermissionException</code> <p>Library has been opened in read-only mode</p> <code>InternalException</code> <p>Storage lock required to compact the symbol list could not be acquired</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.defragment_symbol_data","title":"defragment_symbol_data","text":"<pre><code>defragment_symbol_data(\n    symbol: str,\n    segment_size: Optional[int] = None,\n    prune_previous_versions: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer). This method calls <code>is_symbol_fragmented</code> to determine whether to proceed with the defragmentation operation.</p> <p>CAUTION - Please note that a major restriction of this method at present is that any column slicing present on the data will be removed in the new version created as a result of this method. As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of the symbol (by using the <code>columns</code> parameter) may be negatively impacted in the defragmented version. If your symbol has less than 127 columns this caveat does not apply. For more information, please see <code>columns_per_segment</code> here:</p> <p>https://docs.arcticdb.io/api/arcticdb/arcticdb.LibraryOptions</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>segment_size</code> <p>Target for maximum no. of rows per segment, after compaction. If parameter is not provided, library option - \"segment_row_size\" will be used Note that no. of rows per segment, after compaction, may exceed the target. It is for achieving smallest no. of segment after compaction. Please refer to below example for further explanation.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the defragmented symbol in the store.</p> RAISES DESCRIPTION <code>1002 ErrorCategory.INTERNAL:E_ASSERTION_FAILURE</code> <p>If <code>is_symbol_fragmented</code> returns false.</p> <code>2001 ErrorCategory.NORMALIZATION:E_UNIMPLEMENTED_INPUT_TYPE</code> <p>If library option - \"bucketize_dynamic\" is ON</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"symbol\", pd.DataFrame({\"A\": [0]}, index=[pd.Timestamp(0)]))\n&gt;&gt;&gt; lib.append(\"symbol\", pd.DataFrame({\"A\": [1, 2]}, index=[pd.Timestamp(1), pd.Timestamp(2)]))\n&gt;&gt;&gt; lib.append(\"symbol\", pd.DataFrame({\"A\": [3]}, index=[pd.Timestamp(3)]))\n&gt;&gt;&gt; lib_tool = lib._dev_tools.library_tool()\n&gt;&gt;&gt; lib_tool.read_index(sym)\n                    start_index                     end_index  version_id stream_id          creation_ts          content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000001          20    b'sym'  1678974096622685727   6872717287607530038          84         2          1        2          0        1\n1970-01-01 00:00:00.000000001 1970-01-01 00:00:00.000000003          21    b'sym'  1678974096931527858  12345256156783683504          84         2          1        2          1        3\n1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          22    b'sym'  1678974096970045987   7952936283266921920          84         2          1        2          3        4\n&gt;&gt;&gt; lib.version_store.defragment_symbol_data(\"symbol\", 2)\n&gt;&gt;&gt; lib_tool.read_index(sym)  # Returns two segments rather than three as a result of the defragmentation operation\n                    start_index                     end_index  version_id stream_id          creation_ts         content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000003          23    b'sym'  1678974097067271451  5576804837479525884          84         2          1        2          0        3\n1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          23    b'sym'  1678974097067427062  7952936283266921920          84         2          1        2          3        4\n</code></pre> Notes <p>Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting in the future. This API will allow overriding the setting as well.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete","title":"delete","text":"<pre><code>delete(\n    symbol: str,\n    versions: Optional[Union[int, Iterable[int]]] = None,\n) -&gt; None\n</code></pre> <p>Delete all versions of the symbol from the library, unless <code>version</code> is specified, in which case only those versions are deleted.</p> <p>This may not actually delete the underlying data if a snapshot still references the version. See <code>snapshot</code> for more detail.</p> <p>Note that this may require data to be removed from the underlying storage which can be slow.</p> <p>This method does not remove any staged data, use <code>delete_staged_data</code> for that.</p> <p>If no symbol called <code>symbol</code> exists then this is a no-op. In particular this method does not raise in this case.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to delete.</p> <p> TYPE: <code>str</code> </p> <code>versions</code> <p>Version or versions of symbol to delete. If <code>None</code> then all versions will be deleted.</p> <p> TYPE: <code>Optional[Union[int, Iterable[int]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_batch","title":"delete_batch","text":"<pre><code>delete_batch(\n    delete_requests: List[Union[str, DeleteRequest]],\n) -&gt; List[Optional[DataError]]\n</code></pre> <p>Delete multiple symbols in a batch fashion.</p> PARAMETER DESCRIPTION <code>delete_requests</code> <p>List of symbols to delete. Can be either: - String symbols (delete all versions of the symbol) - DeleteRequest objects (delete specific versions of the symbol, must have at least one version)</p> <p> TYPE: <code>List[Union[str, DeleteRequest]]</code> </p> RETURNS DESCRIPTION <code>List[DataError]</code> <p>List of DataError objects, one for each symbol that was not deleted due to an error. If the symbol was already deleted, there will be no error, just a warning.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_data_in_range","title":"delete_data_in_range","text":"<pre><code>delete_data_in_range(\n    symbol: str,\n    date_range: Tuple[\n        Optional[Timestamp], Optional[Timestamp]\n    ],\n    prune_previous_versions: bool = False,\n) -&gt; None\n</code></pre> <p>Delete data within the given date range, creating a new version of <code>symbol</code>.</p> <p>The existing symbol version must be timeseries-indexed.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>date_range</code> <p>The date range in which to delete data. Leaving any part of the tuple as None leaves that part of the range open ended.</p> <p> TYPE: <code>Tuple[Optional[Timestamp], Optional[Timestamp]]</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"column\": [5, 6, 7, 8]}, index=pd.date_range(start=\"1/1/2018\", end=\"1/4/2018\"))\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.delete_data_in_range(\"symbol\", date_range=(datetime.datetime(2018, 1, 1), datetime.datetime(2018, 1, 2)))\n&gt;&gt;&gt; lib[\"symbol\"].version\n1\n&gt;&gt;&gt; lib[\"symbol\"].data\n                column\n2018-01-03       7\n2018-01-04       8\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_snapshot","title":"delete_snapshot","text":"<pre><code>delete_snapshot(snapshot_name: str) -&gt; None\n</code></pre> <p>Delete a named snapshot. This may take time if the given snapshot is the last reference to the underlying symbol(s) as the underlying data will be removed as well.</p> PARAMETER DESCRIPTION <code>snapshot_name</code> <p>The snapshot name to delete.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>Exception</code> <p>If the named snapshot does not exist.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_staged_data","title":"delete_staged_data","text":"<pre><code>delete_staged_data(symbol: str) -&gt; None\n</code></pre> <p>Removes staged data.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to remove staged data for.</p> <p> TYPE: <code>`str`</code> </p> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.enterprise_options","title":"enterprise_options","text":"<pre><code>enterprise_options() -&gt; EnterpriseLibraryOptions\n</code></pre> <p>Enterprise library options set on this library. See also <code>options</code> for non-enterprise options.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.finalize_staged_data","title":"finalize_staged_data","text":"<pre><code>finalize_staged_data(\n    symbol: str,\n    mode: Optional[\n        Union[StagedDataFinalizeMethod, str]\n    ] = WRITE,\n    prune_previous_versions: bool = False,\n    metadata: Any = None,\n    validate_index=True,\n    delete_staged_data_on_failure: bool = False,\n    stage_results: Optional[List[StageResult]] = None,\n) -&gt; VersionedItem\n</code></pre> <p>Finalizes staged data, making it available for reads. All staged segments must be ordered and non-overlapping. <code>finalize_staged_data</code> is less time-consuming than <code>sort_and_finalize_staged_data</code>.</p> <p>If <code>mode</code> is <code>StagedDataFinalizeMethod.APPEND</code> or <code>append</code> the index of the first row of the new segment must be equal to or greater than the index of the last row in the existing data.</p> <p>If <code>Static Schema</code> is used all staged block must have matching schema (same column names, same dtype, same column ordering) and must match the existing data if mode is <code>StagedDataFinalizeMethod.APPEND</code>. For more information about schema options see documentation for <code>arcticdb.LibraryOptions.dynamic_schema</code></p> <p>If the symbol does not exist both <code>StagedDataFinalizeMethod.APPEND</code> and <code>StagedDataFinalizeMethod.WRITE</code> will create it.</p> <p>Calling <code>finalize_staged_data</code> without having staged data for the symbol will throw <code>UserInputException</code>. Use <code>get_staged_symbols</code> to check if there are staged segments for the symbol.</p> <p>Calling <code>finalize_staged_data</code> if any of the staged segments contains NaT in its index will throw <code>SortingException</code>.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to finalize data for.</p> <p> TYPE: <code>`str`</code> </p> <code>mode</code> <p>Finalize mode. Valid options are WRITE or APPEND. Write collects the staged data and writes them to a new timeseries. Append collects the staged data and appends them to the latest version.</p> <p>Also accepts strings \"write\" or \"append\" (case-insensitive).</p> <p> TYPE: <code>Union[str, StagedDataFinalizeMethod]</code> DEFAULT: <code>StagedDataFinalizeMethod.WRITE</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>validate_index</code> <p>If True, and staged segments are timeseries, will verify that the index of the symbol after this operation supports date range searches and update operations. This requires that the indexes of the staged segments are non-overlapping with each other, and, in the case of <code>StagedDataFinalizeMethod.APPEND</code>, fall after the last index value in the previous version.  Note that no checks are performed for Arrow input data.</p> <p> DEFAULT: <code>True</code> </p> <code>delete_staged_data_on_failure</code> <p>Determines the handling of staged data when an exception occurs during the execution of the <code>finalize_staged_data</code> function.</p> <ul> <li>If set to True, all staged data for the specified symbol will be deleted if an exception occurs.    If <code>stage_results</code> is provided, only the provided ones will be deleted.</li> <li>If set to False, the staged data will be retained and will be used in subsequent calls to   <code>finalize_staged_data</code>.</li> </ul> <p>To manually delete staged data, use the <code>delete_staged_data</code> function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stage_results</code> <p>If specified, only the data corresponding to the provided <code>StageResult</code>s will be finalized. See <code>stage</code>.</p> <p> TYPE: <code>Optional[List[StageResult]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store. The data member will be None.</p> RAISES DESCRIPTION <code>SortingException</code> <ul> <li>If any two staged segments for a given symbol have overlapping indexes</li> <li>If any staged segment for a given symbol is not sorted</li> <li>If the first index value of the new segment is not greater or equal than the last index value of     the existing data when <code>StagedDataFinalizeMethod.APPEND</code> is used.</li> <li>If any staged segment contains NaT in the index</li> </ul> <code>UserInputException</code> <ul> <li>If there are no staged segments when <code>finalize_staged_data</code> is called</li> <li> <p>If all of the following conditions are met:</p> <ol> <li>Static schema is used.</li> <li>The width of the DataFrame exceeds the value of <code>LibraryOptions.columns_per_segment</code>.</li> <li>The symbol contains data that was not written by <code>finalize_staged_data</code>.</li> <li>Finalize mode is append</li> </ol> </li> </ul> <code>SchemaException</code> <ul> <li>If static schema is used and not all staged segments have matching schema.</li> <li>If static schema is used, mode is <code>StagedDataFinalizeMethod.APPEND</code> and the schema of the new segment     is not the same as the schema of the existing data</li> <li>If dynamic schema is used and different segments have the same column names but their dtypes don't have a     common type (e.g string and any numeric type)</li> <li>If a different index name is encountered in the staged data, regardless of the schema mode</li> </ul> See Also <p>stage     Documentation on the <code>stage</code> method explains the concept of staged data in more detail.</p> <p>Examples:</p> <p>Finalizing all of the staged data</p> <pre><code>&gt;&gt;&gt; result1 = lib.stage(\"sym\", pd.DataFrame({\"col\": [3, 4]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 3), pd.Timestamp(2024, 1, 4)])))\n&gt;&gt;&gt; result2 = lib.stage(\"sym\", pd.DataFrame({\"col\": [1, 2]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 1), pd.Timestamp(2024, 1, 2)])))\n&gt;&gt;&gt; lib.finalize_staged_data(\"sym\")\n&gt;&gt;&gt; lib.read(\"sym\").data\n            col\n2024-01-01    1\n2024-01-02    2\n2024-01-03    3\n2024-01-04    4\n</code></pre> <p>Finalizing only some of the staged data</p> <pre><code>&gt;&gt;&gt; lib.finalize_staged_data(\"staged\", StagedDataFinalizeMethod.WRITE, stage_results=[result1])\n&gt;&gt;&gt; lib.read(\"staged\").data\n            col\n2000-01-03    3\n2000-01-04    4\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.get_description","title":"get_description","text":"<pre><code>get_description(\n    symbol: str, as_of: Optional[AsOf] = None\n) -&gt; SymbolDescription\n</code></pre> <p>Returns descriptive data for <code>symbol</code>.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SymbolDescription</code> <p>Named tuple containing the descriptive data.</p> See Also <p>SymbolDescription     For documentation on each field.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.get_description_batch","title":"get_description_batch","text":"<pre><code>get_description_batch(\n    symbols: List[Union[str, ReadInfoRequest]],\n) -&gt; List[Union[SymbolDescription, DataError]]\n</code></pre> <p>Returns descriptive data for a list of <code>symbols</code>.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read.</p> <p> TYPE: <code>List[Union[str, ReadInfoRequest]]</code> </p> RETURNS DESCRIPTION <code>List[Union[SymbolDescription, DataError]]</code> <p>A list of the descriptive data, whose i-th element corresponds to the i-th element of the <code>symbols</code> parameter. If the specified version does not exist, a DataError object is returned, with symbol, version_request_type, version_request_data properties, error_code, error_category, and exception_string properties. If a key error or any other internal exception occurs, the same DataError object is also returned.</p> See Also <p>SymbolDescription     For documentation on each field.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.get_staged_symbols","title":"get_staged_symbols","text":"<pre><code>get_staged_symbols() -&gt; List[str]\n</code></pre> <p>Returns all symbols with staged, unfinalized data.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>Symbol names.</p> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.has_symbol","title":"has_symbol","text":"<pre><code>has_symbol(\n    symbol: str, as_of: Optional[AsOf] = None\n) -&gt; bool\n</code></pre> <p>Whether this library contains the given symbol.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name for the item</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>Return the data as it was as_of the point in time. See <code>read</code> for more documentation. If absent then considers symbols that are live in the library as of the current time.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the symbol is in the library, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"symbol\", pd.DataFrame())\n&gt;&gt;&gt; lib.has_symbol(\"symbol\")\nTrue\n&gt;&gt;&gt; lib.has_symbol(\"another_symbol\")\nFalse\n</code></pre> <p>The contains operator also checks whether a symbol exists in this library as of now:</p> <pre><code>&gt;&gt;&gt; \"symbol\" in lib\nTrue\n&gt;&gt;&gt; \"another_symbol\" in lib\nFalse\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.head","title":"head","text":"<pre><code>head(\n    symbol: str,\n    n: int = 5,\n    as_of: Optional[AsOf] = None,\n    columns: List[str] = None,\n    lazy: bool = False,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n    arrow_string_format_per_column: Optional[\n        Dict[str, Union[ArrowOutputStringFormat, DataType]]\n    ] = None,\n) -&gt; Union[VersionedItem, LazyDataFrame]\n</code></pre> <p>Read the first n rows of data for the named symbol. If n is negative, return all rows except the last n rows.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>as_of</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> <code>columns</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_format</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_per_column</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>Optional[Dict[str, Union[ArrowOutputStringFormat, DataType]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[VersionedItem, LazyDataFrame]</code> <p>If lazy is False, VersionedItem object that contains a .data and .metadata element. If lazy is True, a LazyDataFrame object on which further querying can be performed prior to collect.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.is_symbol_fragmented","title":"is_symbol_fragmented","text":"<pre><code>is_symbol_fragmented(\n    symbol: str, segment_size: Optional[int] = None\n) -&gt; bool\n</code></pre> <p>Check whether the number of segments that would be reduced by compaction is more than or equal to the value specified by the configuration option \"SymbolDataCompact.SegmentCount\" (defaults to 100).</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>segment_size</code> <p>Target for maximum no. of rows per segment, after compaction. If parameter is not provided, library option for segments's maximum row size will be used</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> Notes <p>Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting in the future. This API will allow overriding the setting as well.</p> RETURNS DESCRIPTION <code>bool</code>"},{"location":"api/library/#arcticdb.version_store.library.Library.list_snapshots","title":"list_snapshots","text":"<pre><code>list_snapshots(\n    load_metadata: Optional[bool] = True,\n) -&gt; Union[List[str], Dict[str, Any]]\n</code></pre> <p>List the snapshots in the library.</p> PARAMETER DESCRIPTION <code>load_metadata</code> <p>Load the snapshot metadata. May be slow so opt for false if you don't need it.</p> <p> TYPE: <code>`Optional[bool]`</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Union[List[str], Dict[str, Any]]</code> <p>Snapshots in the library. Returns a list of snapshot names if load_metadata is False, otherwise returns a dictionary where keys are snapshot names and values are metadata associated with that snapshot.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.list_symbols","title":"list_symbols","text":"<pre><code>list_symbols(\n    snapshot_name: Optional[str] = None,\n    regex: Optional[str] = None,\n) -&gt; List[str]\n</code></pre> <p>Return the symbols in this library.</p> PARAMETER DESCRIPTION <code>regex</code> <p>If passed, returns only the symbols which match the regex.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>snapshot_name</code> <p>Return the symbols available under the snapshot. If None then considers symbols that are live in the library as of the current time.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>Symbols in the library.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.list_versions","title":"list_versions","text":"<pre><code>list_versions(\n    symbol: Optional[str] = None,\n    snapshot: Optional[str] = None,\n    latest_only: bool = False,\n    skip_snapshots: bool = False,\n) -&gt; Dict[SymbolVersion, VersionInfo]\n</code></pre> <p>Get the versions in this library, filtered by the passed in parameters.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to return versions for.  If None returns versions across all symbols in the library.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>snapshot</code> <p>Only return the versions contained in the named snapshot.</p> <p>Limitation: If this is specified then every VersionInfo in the result will have <code>deleted=False</code>. This may change in future.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>latest_only</code> <p>Only include the latest version for each returned symbol.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>skip_snapshots</code> <p>Don't populate version list with snapshot information. Can improve performance significantly if there are many snapshots.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dict[SymbolVersion, VersionInfo]</code> <p>Dictionary describing the version for each symbol-version pair in the library. Since symbol version is a (named) tuple you can index in to the dictionary simply as shown in the examples below.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame()\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata=10)\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata=11, prune_previous_versions=False)\n&gt;&gt;&gt; lib.snapshot(\"snapshot\")\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata=12, prune_previous_versions=False)\n&gt;&gt;&gt; lib.delete(\"symbol\", versions=(1, 2))\n&gt;&gt;&gt; versions = lib.list_versions(\"symbol\")\n&gt;&gt;&gt; versions[\"symbol\", 1].deleted\nTrue\n&gt;&gt;&gt; versions[\"symbol\", 1].snapshots\n[\"my_snap\"]\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.merge_experimental","title":"merge_experimental","text":"<pre><code>merge_experimental(\n    symbol: str,\n    source: NormalizableType,\n    strategy: MergeStrategy = MergeStrategy(),\n    on: Optional[List[str]] = None,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    upsert: bool = False,\n)\n</code></pre> <p>Merge new data into an existing symbol's DataFrame according to a specified strategy.</p> <p>See Merge Notebook for usage examples.</p> <p>Warning</p> <p>This API is under development and is subject to change. The API is not subject to semver and can change in minor or patch releases.</p> <p>Only date time indexed symbols and sources are supported at the moment.</p> <p>Dynamic schema is not supported.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>The symbol to merge data into.</p> <p> TYPE: <code>str</code> </p> <code>source</code> <p>The new data to merge. In the case of timeseries, the index must be sorted.</p> <p> TYPE: <code>DataFrame or Series</code> </p> <code>strategy</code> <p>Warning</p> <p>Only <code>MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\")</code> is implemented</p> <p>Determines how to handle matched and unmatched rows. Accepted strategies are:     - MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\"): Update matched rows, leave others unchanged.     - MergeStrategy(matched=\"do_nothing\", not_matched_by_target=\"insert\"): Insert unmatched rows from source.     - MergeStrategy(matched=\"update\", not_matched_by_target=\"insert\"): Update matched rows and insert unmatched rows. Note: If the strategy includes \"update\" on matched, a row in the target cannot be matched by multiple rows in the source.</p> <p>The elements of <code>MergeStrategy</code> can be either values of the <code>MergeAction</code> enum or case-insensitive strings representing the enum values.</p> <p> TYPE: <code>Optional[MergeStrategy]</code> DEFAULT: <code>MergeStrategy(matched=\"update\", not_matched_by_target=\"insert\")</code> </p> <code>on</code> <p>Warning</p> <p>Not yet implemented</p> <p>Columns which are used to determine row equality between source and target. A row is considered matched when all specified columns have equal values in both source and target.</p> <p>IMPORTANT: For date-time indexed data, the index is always included in matching and cannot be excluded.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>metadata</code> <p>Metadata to save alongside the new version.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>If True, removes previous versions from the version list.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upsert</code> <p>Warning</p> <p>Not yet implemented</p> <p>If True and <code>not_matched_by_target=\"insert\"</code>, creates the symbol if it does not exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store. The data attribute will not be populated.</p> RAISES DESCRIPTION <code>StorageException</code> <p>If symbol doesn't exist and <code>upsert=False</code></p> <code>UserInputException</code> <p>If strategy is not one of the supported strategies listed above</p> <code>SortingException</code> <p>If date-time index is used and source or target are not sorted</p> <code>SchemaException</code> <p>If dynamic schema is used or if source's schema is incompatible with target's schema</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"symbol\", pd.DataFrame({'a': [1, 2, 3]}, index=pd.DatetimeIndex([pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)])))\n&gt;&gt;&gt; lib.merge_experimental(\"symbol\", pd.DataFrame({\"a\": [100, 200]}, index=pd.DatetimeIndex([pd.Timestamp(2), pd.Timestamp(4)])), strategy=MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\"))))\n&gt;&gt;&gt; lib.read(\"symbol\").data\n                               a\n1970-01-01 00:00:00.000000001  1\n1970-01-01 00:00:00.000000002  100\n1970-01-01 00:00:00.000000003  3\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.options","title":"options","text":"<pre><code>options() -&gt; LibraryOptions\n</code></pre> <p>Library options set on this library. See also <code>enterprise_options</code>.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.prune_previous_versions","title":"prune_previous_versions","text":"<pre><code>prune_previous_versions(symbol) -&gt; None\n</code></pre> <p>Removes all (non-snapshotted) versions from the database for the given symbol, except the latest.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name to prune.</p> <p> TYPE: <code>`str`</code> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.read","title":"read","text":"<pre><code>read(\n    symbol: str,\n    as_of: Optional[AsOf] = None,\n    date_range: Optional[\n        Tuple[Optional[Timestamp], Optional[Timestamp]]\n    ] = None,\n    row_range: Optional[Tuple[int, int]] = None,\n    columns: Optional[List[str]] = None,\n    query_builder: Optional[QueryBuilder] = None,\n    lazy: bool = False,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n    arrow_string_format_per_column: Optional[\n        Dict[str, Union[ArrowOutputStringFormat, DataType]]\n    ] = None,\n) -&gt; Union[VersionedItem, LazyDataFrame]\n</code></pre> <p>Read data for the named symbol.  Returns a VersionedItem object with a data and metadata element (as passed into write).</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>Return the data as it was as of the point in time. <code>None</code> means that the latest version should be read. The various types of this parameter mean: - <code>int</code>: specific version number. Negative indexing is supported, with -1 representing the latest version, -2 the version before that, etc. - <code>str</code>: snapshot name which contains the version - <code>datetime.datetime</code> : the version of the data that existed <code>as_of</code> the requested point in time</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> <code>date_range</code> <p>DateRange to restrict read data to.</p> <p>Applicable only for time-indexed Pandas dataframes or series. Returns only the part of the data that falls within the given range (inclusive). None on either end leaves that part of the range open-ended. Hence specifying <code>(None, datetime(2025, 1, 1)</code> declares that you wish to read all data up to and including 20250101. The same effect can be achieved by using the date_range clause of the QueryBuilder class, which will be slower, but return data with a smaller memory footprint. See the QueryBuilder.date_range docstring for more details.</p> <p>Only one of date_range or row_range can be provided.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]]</code> DEFAULT: <code>None</code> </p> <code>row_range</code> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound. lib.read(symbol, row_range=(start, end)).data should behave the same as df.iloc[start:end], including in the handling of negative start/end values. Leaving either element as None leaves that side of the range open-ended. For example (5, None) would include everything from the 5th row onwards. Only one of date_range or row_range can be provided.</p> <p> TYPE: <code>`Optional[Tuple[Optional[int], Optional[int]]]`</code> DEFAULT: <code>None</code> </p> <code>columns</code> <p>Applicable only for Pandas data. Determines which columns to return data for. Special values: - <code>None</code>: Return a dataframe containing all columns - <code>[]</code>: Return a dataframe containing only the index columns</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>query_builder</code> <p>A QueryBuilder object to apply to the dataframe before it is returned. For more information see the documentation for the QueryBuilder class (<code>from arcticdb import QueryBuilder; help(QueryBuilder)</code>).</p> <p> TYPE: <code>Optional[QueryBuilder]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>Defer query execution until <code>collect</code> is called on the returned <code>LazyDataFrame</code> object. See documentation on <code>LazyDataFrame</code> for more details.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_format</code> <p>Output format for the returned dataframe. If <code>None</code>, uses the output format from the <code>Library</code> instance. See <code>OutputFormat</code> documentation for details on available formats.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>String column format when using <code>PYARROW</code> or <code>POLARS</code> output formats. If <code>None</code>, uses the <code>arrow_string_format_default</code> from the <code>Library</code> instance. See <code>ArrowOutputStringFormat</code> documentation for details on available string formats.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_per_column</code> <p>Per-column overrides for <code>arrow_string_format_default</code>. Keys are column names.</p> <p> TYPE: <code>Optional[Dict[str, Union[ArrowOutputStringFormat, DataType]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[VersionedItem, LazyDataFrame]</code> <p>If lazy is False, VersionedItem object that contains a .data and .metadata element. If lazy is True, a LazyDataFrame object on which further querying can be performed prior to collect.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'column': [5,6,7]})\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata={'my_dictionary': 'is_great'})\n&gt;&gt;&gt; lib.read(\"symbol\").data\n   column\n0       5\n1       6\n2       7\n</code></pre> <p>The default read behaviour is also available through subscripting:</p> <pre><code>&gt;&gt;&gt; lib[\"symbol\"].data\n   column\n0       5\n1       6\n2       7\n</code></pre> <p>Passing an output_format can change the resulting dataframe type. For example, to return a PyArrow table:</p> <pre><code>&gt;&gt;&gt; lib.read(\"symbol\", output_format=\"PYARROW\").data\npyarrow.Table\ncolumn: int64\n----\ncolumn: [[5,6,7]]\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_batch","title":"read_batch","text":"<pre><code>read_batch(\n    symbols: List[Union[str, ReadRequest]],\n    query_builder: Optional[QueryBuilder] = None,\n    lazy: bool = False,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n    arrow_string_format_per_column: Optional[\n        Dict[str, Union[ArrowOutputStringFormat, DataType]]\n    ] = None,\n) -&gt; Union[\n    List[Union[VersionedItem, DataError]],\n    LazyDataFrameCollection,\n]\n</code></pre> <p>Reads multiple symbols.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read.</p> <p> TYPE: <code>List[Union[str, ReadRequest]]</code> </p> <code>query_builder</code> <p>A single QueryBuilder to apply to all the dataframes before they are returned. If this argument is passed then none of the <code>symbols</code> may have their own query_builder specified in their request.</p> <p> TYPE: <code>Optional[QueryBuilder]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>Defer query execution until <code>collect</code> is called on the returned <code>LazyDataFrameCollection</code> object. See documentation on <code>LazyDataFrameCollection</code> for more details.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_format</code> <p>Output format for the returned dataframes. If <code>None</code>, uses the output format from the <code>Library</code> instance. See <code>OutputFormat</code> documentation for details on available formats.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>String column format when using <code>PYARROW</code> or <code>POLARS</code> output formats. Serves as the default for the entire batch. String format settings in individual <code>ReadRequest</code> objects override this batch-level setting. If <code>None</code>, uses the <code>arrow_string_format_default</code> from the <code>Library</code> instance. See <code>ArrowOutputStringFormat</code> documentation for details on available string formats.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_per_column</code> <p>Per-column overrides for <code>arrow_string_format_default</code>. Keys are column names. Only applied to symbols that don't have <code>arrow_string_format_per_column</code> set in their <code>ReadRequest</code>.</p> <p> TYPE: <code>Optional[Dict[str, Union[ArrowOutputStringFormat, DataType]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[List[Union[VersionedItem, DataError]], LazyDataFrameCollection]</code> <p>If lazy is False: A list of the read results, whose i-th element corresponds to the i-th element of the <code>symbols</code> parameter. If the specified version does not exist, a DataError object is returned, with symbol, version_request_type, version_request_data properties, error_code, error_category, and exception_string properties. If a key error or any other internal exception occurs, the same DataError object is also returned. If lazy is True: A LazyDataFrameCollection object on which further querying can be performed prior to collection.</p> RAISES DESCRIPTION <code>ArcticInvalidApiUsageException</code> <p>If kwarg query_builder and per-symbol query builders both used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"s1\", pd.DataFrame())\n&gt;&gt;&gt; lib.write(\"s2\", pd.DataFrame({\"col\": [1, 2, 3]}))\n&gt;&gt;&gt; lib.write(\"s2\", pd.DataFrame(), prune_previous_versions=False)\n&gt;&gt;&gt; lib.write(\"s3\", pd.DataFrame())\n&gt;&gt;&gt; batch = lib.read_batch([\"s1\", adb.ReadRequest(\"s2\", as_of=0), \"s3\", adb.ReadRequest(\"s2\", as_of=1000)])\n&gt;&gt;&gt; batch[0].data.empty\nTrue\n&gt;&gt;&gt; batch[1].data.empty\nFalse\n&gt;&gt;&gt; batch[2].data.empty\nTrue\n&gt;&gt;&gt; batch[3].symbol\n\"s2\"\n&gt;&gt;&gt; isinstance(batch[3], adb.DataError)\nTrue\n&gt;&gt;&gt; batch[3].version_request_type\nVersionRequestType.SPECIFIC\n&gt;&gt;&gt; batch[3].version_request_data\n1000\n&gt;&gt;&gt; batch[3].error_code\nErrorCode.E_NO_SUCH_VERSION\n&gt;&gt;&gt; batch[3].error_category\nErrorCategory.MISSING_DATA\n</code></pre> See Also <p>read</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_batch_and_join","title":"read_batch_and_join","text":"<pre><code>read_batch_and_join(\n    symbols: List[ReadRequest],\n    query_builder: QueryBuilder,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n    arrow_string_format_per_column: Optional[\n        Dict[str, Union[ArrowOutputStringFormat, DataType]]\n    ] = None,\n) -&gt; VersionedItemWithJoin\n</code></pre> <p>Reads multiple symbols in a batch, and then joins them together using the first clause in the <code>query_builder</code> argument. If there are subsequent clauses in the <code>query_builder</code> argument, then these are applied to the joined data.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read.</p> <p> TYPE: <code>List[Union[str, ReadRequest]]</code> </p> <code>query_builder</code> <p>The first clause must be a multi-symbol join, such as <code>concat</code>. Any subsequent clauses must work on individual dataframes, and will be applied to the joined data.</p> <p> TYPE: <code>QueryBuilder</code> </p> <code>output_format</code> <p>Output format for the returned joined dataframe. If <code>None</code>, uses the output format from the <code>Library</code> instance. See <code>OutputFormat</code> documentation for details on available formats.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>String column format when using <code>PYARROW</code> or <code>POLARS</code> output formats. If <code>None</code>, uses the <code>arrow_string_format_default</code> from the <code>Library</code> instance. See <code>ArrowOutputStringFormat</code> documentation for details on available string formats.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_per_column</code> <p>Per-column overrides for <code>arrow_string_format_default</code>. Keys are column names.</p> <p> TYPE: <code>Optional[Dict[str, Union[ArrowOutputStringFormat, DataType]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItemWithJoin</code> <p>Contains a .data field with the joined together data, and a list of VersionedItem objects describing the version number, metadata, etc., of the symbols that were joined together.</p> RAISES DESCRIPTION <code>UserInputException</code> <ul> <li>If the first clause in <code>query_builder</code> is not a multi-symbol join</li> <li>If any subsequent clauses in <code>query_builder</code> are not single-symbol clauses</li> <li>If any of the specified symbols are recursively normalized</li> </ul> <code>MissingDataException</code> <ul> <li>If a symbol or the version of symbol specified in as_ofs does not exist or has been deleted</li> </ul> <code>SchemaException</code> <ul> <li>If the schema of symbols to be joined are incompatible. Examples of incompatible schemas include:<ul> <li>Trying to join a Series to a DataFrame</li> <li>Different index types, including MultiIndexes with different numbers of levels</li> <li>Incompatible column types e.g. joining a string column to an integer column</li> </ul> </li> </ul> <p>Examples:</p> <p>Join 2 symbols together without any pre or post processing.</p> <pre><code>&gt;&gt;&gt; df0 = pd.DataFrame(\n    {\n        \"col1\": [0.5],\n        \"col2\": [1],\n    },\n    index=[pd.Timestamp(\"2025-01-01\")],\n)\n&gt;&gt;&gt; df1 = pd.DataFrame(\n    {\n        \"col3\": [\"hello\"],\n        \"col2\": [2],\n    },\n    index=[pd.Timestamp(\"2025-01-02\")],\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.concat(\"outer\")\n&gt;&gt;&gt; lib.write(\"symbol0\", df0)\n&gt;&gt;&gt; lib.write(\"symbol1\", df1)\n&gt;&gt;&gt; lib.read_batch_and_join([\"symbol0\", \"symbol1\"], query_builder=q).data\n</code></pre> <pre><code>                       col1     col2     col3\n2025-01-01 00:00:00     0.5        1     None\n2025-01-02 00:00:00     NaN        2  \"hello\"\n</code></pre> <pre><code>&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.concat(\"inner\")\n&gt;&gt;&gt; lib.read_batch_and_join([\"symbol0\", \"symbol1\"], query_builder=q).data\n</code></pre> <pre><code>                       col2\n2025-01-01 00:00:00       1\n2025-01-02 00:00:00       2\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(\n    symbol: str, as_of: Optional[AsOf] = None\n) -&gt; VersionedItem\n</code></pre> <p>Return the metadata saved for a symbol.  This method is faster than read as it only loads the metadata, not the data itself.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>Return the metadata as it was as of the point in time. See documentation on <code>read</code> for documentation on the different forms this parameter can take.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the affected symbol in the store. The data attribute will be None.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_metadata_batch","title":"read_metadata_batch","text":"<pre><code>read_metadata_batch(\n    symbols: List[Union[str, ReadInfoRequest]],\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Reads the metadata of multiple symbols.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read metadata.</p> <p> TYPE: <code>List[Union[str, ReadInfoRequest]]</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>A list of the read metadata results, whose i-th element corresponds to the i-th element of the <code>symbols</code> parameter. A VersionedItem object with the metadata field set as None will be returned if the requested version of the symbol exists but there is no metadata If the specified version does not exist, a DataError object is returned, with symbol, version_request_type, version_request_data properties, error_code, error_category, and exception_string properties. If a key error or any other internal exception occurs, the same DataError object is also returned.</p> See Also <p>read_metadata</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.reload_symbol_list","title":"reload_symbol_list","text":"<pre><code>reload_symbol_list() -&gt; None\n</code></pre> <p>Forces the symbol list cache to be reloaded.</p> <p>This can take a long time on large libraries or certain S3 implementations, and once started, it cannot be safely interrupted. If the call is interrupted somehow (exception/process killed), please call this again ASAP.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.snapshot","title":"snapshot","text":"<pre><code>snapshot(\n    snapshot_name: str,\n    metadata: Any = None,\n    skip_symbols: Optional[List[str]] = None,\n    versions: Optional[Dict[str, int]] = None,\n) -&gt; None\n</code></pre> <p>Creates a named snapshot of the data within a library.</p> <p>By default, the latest version of every symbol that has not been deleted will be contained within the snapshot. You can change this behaviour with either <code>versions</code> (an allow-list) or with <code>skip_symbols</code> (a deny-list). Concurrent writes with prune previous versions set while the snapshot is being taken can potentially lead to corruption of the affected symbols in the snapshot.</p> <p>The symbols and versions contained within the snapshot will persist regardless of new symbols and versions being written to the library afterwards. If a version or symbol referenced in a snapshot is deleted then the underlying data will be preserved to ensure the snapshot is still accessible. Only once all referencing snapshots have been removed will the underlying data be removed as well.</p> <p>At most one of <code>skip_symbols</code> and <code>versions</code> may be truthy.</p> PARAMETER DESCRIPTION <code>snapshot_name</code> <p>Name of the snapshot.</p> <p> TYPE: <code>str</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the snapshot.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>skip_symbols</code> <p>Optional symbols to be excluded from the snapshot.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>versions</code> <p>Optional dictionary of versions of symbols to snapshot. For example <code>versions={\"a\": 2, \"b\": 3}</code> will snapshot version 2 of symbol \"a\" and version 3 of symbol \"b\".</p> <p> TYPE: <code>Optional[Dict[str, int]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>InternalException</code> <p>If a snapshot already exists with <code>snapshot_name</code>. You must explicitly delete the pre-existing snapshot.</p> <code>MissingDataException</code> <p>If a symbol or the version of symbol specified in versions does not exist or has been deleted in the library, or, the library has no symbol.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.sort_and_finalize_staged_data","title":"sort_and_finalize_staged_data","text":"<pre><code>sort_and_finalize_staged_data(\n    symbol: str,\n    mode: Optional[\n        Union[StagedDataFinalizeMethod, str]\n    ] = WRITE,\n    prune_previous_versions: bool = False,\n    metadata: Any = None,\n    delete_staged_data_on_failure: bool = False,\n    stage_results: Optional[List[StageResult]] = None,\n) -&gt; VersionedItem\n</code></pre> <p>Sorts and merges all staged data, making it available for reads. This differs from <code>finalize_staged_data</code> in that it can support staged segments with interleaved time periods and staged segments which are not internally sorted. The end result will be sorted. This requires performing a full sort in memory so can be time-consuming.</p> <p>If <code>mode</code> is <code>StagedDataFinalizeMethod.APPEND</code> the index of the first row of the sorted block must be equal to or greater than the index of the last row in the existing data.</p> <p>If <code>Static Schema</code> is used all staged block must have matching schema (same column names, same dtype, same column ordering) and must match the existing data if mode is <code>StagedDataFinalizeMethod.APPEND</code>. For more information about schema options see documentation for <code>arcticdb.LibraryOptions.dynamic_schema</code></p> <p>If the symbol does not exist both <code>StagedDataFinalizeMethod.APPEND</code> and <code>StagedDataFinalizeMethod.WRITE</code> will create it.</p> <p>Calling <code>sort_and_finalize_staged_data</code> without having staged data for the symbol will throw <code>UserInputException</code>. Use <code>get_staged_symbols</code> to check if there are staged segments for the symbol.</p> <p>Calling <code>sort_and_finalize_staged_data</code> if any of the staged segments contains NaT in its index will throw <code>SortingException</code>.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to finalize data for.</p> <p> TYPE: <code>str</code> </p> <code>mode</code> <p>Finalize mode. Valid options are WRITE or APPEND. Write collects the staged data and writes them to a new timeseries. Append collects the staged data and appends them to the latest version.</p> <p>Also accepts strings \"write\" or \"append\" (case-insensitive).</p> <p> TYPE: <code>Union[str, StagedDataFinalizeMethod]</code> DEFAULT: <code>StagedDataFinalizeMethod.WRITE</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>delete_staged_data_on_failure</code> <p>Determines the handling of staged data when an exception occurs during the execution of the <code>sort_and_finalize_staged_data</code> function.</p> <ul> <li>If set to True, all staged data for the specified symbol will be deleted if an exception occurs.</li> <li>If set to False, the staged data will be retained and will be used in subsequent calls to   <code>sort_and_finalize_staged_data</code>.</li> </ul> <p>To manually delete staged data, use the <code>delete_staged_data</code> function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stage_results</code> <p>If specified, only the data corresponding to the provided <code>StageResult</code>s will be finalized. See <code>stage</code>.</p> <p> TYPE: <code>Optional[List[StageResult]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store. The data member will be None.</p> RAISES DESCRIPTION <code>SortingException</code> <ul> <li>If the first index value of the sorted block is not greater or equal than the last index value of     the existing data when <code>StagedDataFinalizeMethod.APPEND</code> is used.</li> <li>If any staged segment contains NaT in the index</li> </ul> <code>UserInputException</code> <ul> <li>If there are no staged segments when <code>sort_and_finalize_staged_data</code> is called</li> <li> <p>If all of the following conditions are met:</p> <ol> <li>Static schema is used.</li> <li>The width of the DataFrame exceeds the value of <code>LibraryOptions.columns_per_segment</code>.</li> <li>The symbol contains data that was not written by <code>sort_and_finalize_staged_data</code>.</li> <li>Finalize mode is append</li> </ol> </li> </ul> <code>SchemaException</code> <ul> <li>If static schema is used and not all staged segments have matching schema.</li> <li>If static schema is used, mode is <code>StagedDataFinalizeMethod.APPEND</code> and the schema of the sorted and merged     staged segment is not the same as the schema of the existing data</li> <li>If dynamic schema is used and different segments have the same column names but their dtypes don't have a     common type (e.g string and any numeric type)</li> <li>If a different index name is encountered in the staged data, regardless of the schema mode</li> </ul> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"sym\", pd.DataFrame({\"col\": [2, 4]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 2), pd.Timestamp(2024, 1, 4)])), staged=True)\n&gt;&gt;&gt; lib.write(\"sym\", pd.DataFrame({\"col\": [3, 1]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 3), pd.Timestamp(2024, 1, 1)])), staged=True)\n&gt;&gt;&gt; lib.sort_and_finalize_staged_data(\"sym\")\n&gt;&gt;&gt; lib.read(\"sym\").data\n            col\n2024-01-01    1\n2024-01-02    2\n2024-01-03    3\n2024-01-04    4\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.stage","title":"stage","text":"<pre><code>stage(\n    symbol: str,\n    data: NormalizableType,\n    validate_index=True,\n    sort_on_index=False,\n    sort_columns: List[str] = None,\n    index_column: Optional[str] = None,\n) -&gt; StageResult\n</code></pre> <p>Similar to <code>write</code> but the written segments are left in an \"incomplete\" state, unable to be read until they are finalized. This enables multiple writers to a single symbol - all writing staged data at the same time - with one process able to later finalize all staged data rendering the data readable by clients. To finalize staged data see <code>finalize_staged_data</code> or <code>sort_and_finalize_staged_data</code>.</p> <p>Check out the demo notebook for more info and examples.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written. Staged data must be normalizable.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>validate_index</code> <p>Check that the index is sorted prior to writing. In the case of unsorted data, throw an UnsortedDataException. Note that no checks are performed for Arrow input data.</p> <p> DEFAULT: <code>True</code> </p> <code>sort_on_index</code> <p>If an appropriate index is present, sort the data on it. In combination with sort_columns the index will be used as the primary sort column, and the others as secondaries.</p> <p> DEFAULT: <code>False</code> </p> <code>sort_columns</code> <p>Sort the data by specific columns prior to writing.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>index_column</code> <p>Optional specification of timeseries index column if data is an Arrow table. Ignored if data is not an Arrow table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>StageResult</code> <p>Structure describing the segments that were staged and which can later be passed to <code>finalize_staged_data</code> or <code>sort_and_finalize_staged_data</code> to specify which data to finalize.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.tail","title":"tail","text":"<pre><code>tail(\n    symbol: str,\n    n: int = 5,\n    as_of: Optional[Union[int, str]] = None,\n    columns: List[str] = None,\n    lazy: bool = False,\n    output_format: Optional[\n        Union[OutputFormat, str]\n    ] = None,\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n    arrow_string_format_per_column: Optional[\n        Dict[str, Union[ArrowOutputStringFormat, DataType]]\n    ] = None,\n) -&gt; Union[VersionedItem, LazyDataFrame]\n</code></pre> <p>Read the last n rows of data for the named symbol. If n is negative, return all rows except the first n rows.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>as_of</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> <code>columns</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_format</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>Optional[Union[OutputFormat, str]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_default</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, DataType]]</code> DEFAULT: <code>None</code> </p> <code>arrow_string_format_per_column</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>Optional[Dict[str, Union[ArrowOutputStringFormat, DataType]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[VersionedItem, LazyDataFrame]</code> <p>If lazy is False, VersionedItem object that contains a .data and .metadata element. If lazy is True, a LazyDataFrame object on which further querying can be performed prior to collect.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.update","title":"update","text":"<pre><code>update(\n    symbol: str,\n    data: Union[DataFrame, Series],\n    metadata: Any = None,\n    upsert: bool = False,\n    date_range: Optional[\n        Tuple[Optional[Timestamp], Optional[Timestamp]]\n    ] = None,\n    prune_previous_versions: bool = False,\n    index_column: Optional[str] = None,\n) -&gt; VersionedItem\n</code></pre> <p>Overwrites existing symbol data with the contents of <code>data</code>. The entire range between the first and last index entry in <code>data</code> is replaced in its entirety with the contents of <code>data</code>, adding additional index entries if required. <code>update</code> only operates over the outermost index level - this means secondary index rows will be removed if not contained in <code>data</code>.</p> <p>Both the existing symbol version and <code>data</code> must be timeseries-indexed.</p> <p>In the case where <code>data</code> has zero rows, nothing will be done and no new version will be created. This means that <code>update</code> cannot be used with <code>date_range</code> to just delete a subset of the data. We have <code>delete_data_in_range</code> for exactly this purpose and to make it very clear when deletion is intended.</p> <p>Note that <code>update</code> is not designed for multiple concurrent writers over a single symbol.</p> <p>If using static schema then all the column names of <code>data</code>, their order, and their type must match the columns already in storage.</p> <p>If dynamic schema is used then data will override everything in storage for the entire index of <code>data</code>. Update will not keep columns from storage which are not in <code>data</code>.</p> <p>The update will split the first and last segments in the storage that intersect with 'data'. Therefore, frequent calls to update might lead to data fragmentation (see the example below).</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Timeseries indexed data to use for the update.</p> <p> TYPE: <code>Union[DataFrame, Series]</code> </p> <code>metadata</code> <p>Metadata to persist along with the new symbol version.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>upsert</code> <p>If True, will write the data even if the symbol does not exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>date_range</code> <p>If a range is specified, it will delete the stored value within the range and overwrite it with the data in <code>data</code>. This allows the user to update with data that might only be a subset of the stored value. Leaving any part of the tuple as None leaves that part of the range open ended. Only data with date_range will be modified, even if <code>data</code> covers a wider date range.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]]</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>index_column</code> <p>Optional specification of timeseries index column if data is an Arrow table. Ignored if data is not an Arrow table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...    {'column': [1,2,3,4]},\n...    index=pd.date_range(start='1/1/2018', end='1/4/2018')\n... )\n&gt;&gt;&gt; df\n            column\n2018-01-01       1\n2018-01-02       2\n2018-01-03       3\n2018-01-04       4\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; update_df = pd.DataFrame(\n...    {'column': [400, 40]},\n...    index=pd.date_range(start='1/1/2018', end='1/3/2018', freq='2D')\n... )\n&gt;&gt;&gt; update_df\n            column\n2018-01-01     400\n2018-01-03      40\n&gt;&gt;&gt; lib.update(\"symbol\", update_df)\n&gt;&gt;&gt; # Note that 2018-01-02 is gone despite not being in update_df\n&gt;&gt;&gt; lib.read(\"symbol\").data\n            column\n2018-01-01     400\n2018-01-03      40\n2018-01-04       4\n</code></pre> <p>Update will split the first and the last segment intersecting with <code>data</code></p> <pre><code>&gt;&gt;&gt; index = pd.date_range(pd.Timestamp(\"2024-01-01\"), pd.Timestamp(\"2024-02-01\"))\n&gt;&gt;&gt; df = pd.DataFrame({f\"col_{i}\": range(len(index)) for i in range(1)}, index=index)\n&gt;&gt;&gt; lib.write(\"test\", df)\n&gt;&gt;&gt; lt=lib._dev_tools.library_tool()\n&gt;&gt;&gt; print(lt.read_index(\"test\"))\nstart_index                     end_index  version_id stream_id          creation_ts         content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n2024-01-01  2024-02-01 00:00:00.000000001           0   b'test'  1738599073224386674  9652922778723941392          84         2          1        2          0       32\n&gt;&gt;&gt; update_index=pd.date_range(pd.Timestamp(\"2024-01-10\"), freq=\"ns\", periods=200000)\n&gt;&gt;&gt; update = pd.DataFrame({f\"col_{i}\": [1] for i in range(1)}, index=update_index)\n&gt;&gt;&gt; lib.update(\"test\", update)\n&gt;&gt;&gt; print(lt.read_index(\"test\"))\nstart_index                                    end_index  version_id stream_id          creation_ts          content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n2024-01-01 00:00:00.000000 2024-01-09 00:00:00.000000001           1   b'test'  1738599073268200906  13838161946080117383          84         2          1        2          0        9\n2024-01-10 00:00:00.000000 2024-01-10 00:00:00.000100000           1   b'test'  1738599073256354553  15576483210589662891          84         2          1        2          9   100009\n2024-01-10 00:00:00.000100 2024-01-10 00:00:00.000200000           1   b'test'  1738599073256588040  12429442054752910013          84         2          1        2     100009   200009\n2024-01-11 00:00:00.000000 2024-02-01 00:00:00.000000001           1   b'test'  1738599073268493107   5975110026983744452          84         2          1        2     200009   200031\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.update_batch","title":"update_batch","text":"<pre><code>update_batch(\n    update_payloads: List[UpdatePayload],\n    upsert: bool = False,\n    prune_previous_versions: bool = False,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Perform an update operation on a list of symbols in parallel. All constrains on update apply to this call as well.</p> PARAMETER DESCRIPTION <code>update_payloads</code> <p>List of <code>UpdatePayload</code>. Each element of the list describes an update operation for a particular symbol. Providing the symbol name, data, etc. The same symbol should not appear twice in this list.</p> <p> TYPE: <code>List[UpdatePayload]</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the library.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upsert</code> <p>If True any symbol in <code>update_payloads</code> which is not already in the library will be created.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. i-th entry corresponds to i-th element of <code>update_payloads</code>. Each result correspond to a structure containing metadata and version number of the affected symbol in the store. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> <code>ArcticUnsupportedDataTypeException</code> <p>If data that is not of NormalizableType appears in any of the payloads.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df1 = pd.DataFrame({'column_1': [1, 2, 3]}, index=pd.date_range(\"2025-01-01\", periods=3))\n&gt;&gt;&gt; df1\n            column_1\n2025-01-01         1\n2025-01-02         2\n2025-01-03         3\n&gt;&gt;&gt; df2 = pd.DataFrame({'column_2': [10, 11]}, index=pd.date_range(\"2024-01-01\", periods=2))\n&gt;&gt;&gt; df2\n            column_2\n2024-01-01        10\n2024-01-02        11\n&gt;&gt;&gt; lib.write(\"symbol_1\", df1)\n&gt;&gt;&gt; lib.write(\"symbol_2\", df1)\n&gt;&gt;&gt; lib.update_batch([arcticdb.library.UpdatePayload(\"symbol_1\", pd.DataFrame({\"column_1\": [4, 5]}, index=pd.date_range(\"2025-01-03\", periods=2))), arcticdb.library.UpdatePayload(\"symbol_2\", pd.DataFrame({\"column_2\": [-1]}, index=pd.date_range(\"2023-01-01\", periods=1)))])\n[VersionedItem(symbol='symbol_1', library='test', data=n/a, version=1, metadata=(None,), host='LMDB(path=...)', timestamp=1737542783853861819), VersionedItem(symbol='symbol_2', library='test', data=n/a, version=1, metadata=(None,), host='LMDB(path=...)', timestamp=1737542783851798754)]\n&gt;&gt;&gt; lib.read(\"symbol_1\").data\n            column_1\n2025-01-01         1\n2025-01-02         2\n2025-01-03         4\n2025-01-04         5\n&gt;&gt;&gt; lib.read(\"symbol_2\").data\n            column_2\n2023-01-01        -1\n2024-01-01        10\n2024-01-02        11\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write","title":"write","text":"<pre><code>write(\n    symbol: str,\n    data: NormalizableType,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    staged=False,\n    validate_index=True,\n    index_column: Optional[str] = None,\n    recursive_normalizers: bool = None,\n) -&gt; VersionedItem\n</code></pre> <p>Write <code>data</code> to the specified <code>symbol</code>. If <code>symbol</code> already exists then a new version will be created to reference the newly written data. For more information on versions see the documentation for the <code>read</code> primitive.</p> <p><code>data</code> must be of a format that can be normalised into Arctic's internal storage structure. Pandas DataFrames, Pandas Series and Numpy NDArrays can all be normalised. Normalised data will be split along both the columns and rows into segments. By default, a segment will contain 100,000 rows and 127 columns.</p> <p>If this library has <code>write_deduplication</code> enabled then segments will be deduplicated against storage prior to write to reduce required IO operations and storage requirements. Data will be effectively deduplicated for all segments up until the first differing row when compared to storage. As a result, modifying the beginning of <code>data</code> with respect to previously written versions may significantly reduce the effectiveness of deduplication.</p> <p>Note that <code>write</code> is not designed for multiple concurrent writers over a single symbol. For that case see <code>stage()</code>.</p> <p>Note: ArcticDB will use the 0-th level index of the Pandas DataFrame for its on-disk index.</p> <p>Any non-<code>DatetimeIndex</code> will converted into an internal <code>RowCount</code> index. That is, ArcticDB will assign each row a monotonically increasing integer identifier and that will be used for the index.</p> <p>See the Metadata section of our online documentation for details about how metadata is persisted and caveats.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written. To write non-normalizable data, use <code>write_pickle</code>.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>staged</code> <p>Deprecated. Use stage() instead. Whether to write to a staging area rather than immediately to the library. See documentation on <code>finalize_staged_data</code> for more information.</p> <p> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>If True, verify that the index of <code>data</code> supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing. Note that no checks are performed for Arrow input data.</p> <p> DEFAULT: <code>True</code> </p> <code>index_column</code> <p>Optional specification of timeseries index column if data is an Arrow table. Ignored if data is not an Arrow table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>recursive_normalizers</code> <p>Whether to recursively normalize nested data structures when writing sequence-like or dict-like data. If None, falls back to the corresponding setting in the library configuration. For libraries created with &lt; v6.4.0, the default library configuration is True, otherwise it is False. The library configuration can be modified via Arctic.modify_library_option(). Please refer to https://docs.arcticdb.io/latest/api/arctic/#arcticdb.Arctic.modify_library_option for more details. The data structure can be nested or a mix of lists, dictionaries and tuples. Example:     data = {\"a\": np.arange(5), \"b\": pd.DataFrame({\"col\": [1, 2, 3]})}     lib.write(symbol, data, recursive_normalizers=False) # ArcticUnsupportedDataTypeException will be thrown     lib.write(symbol, data, recursive_normalizers=True) # The data will be successfully written     ac.modify_library_option(lib, ModifiableLibraryOption.RECURSIVE_NORMALIZERS, True)     lib.write(symbol, data) # The data will be successfully written Please refer to https://docs.arcticdb.io/latest/notebooks/arcticdb_demo_recursive_normalizers for more details of this feature. Please check https://docs.arcticdb.io/latest/runtime_config/#versionstorerecursivenormalizermetastructure for the plan of introducing meta structure v2. V2 has removed the dependency on pickle for normalizing meta structure. Please consider switching to V2 as V1 will be deprecated in future v7.0.0 release. However note that reading V2 meta structure requires ArcticDB version &gt;= 6.7.0, or KeyError will be raised.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store.</p> RAISES DESCRIPTION <code>ArcticUnsupportedDataTypeException</code> <p>If <code>data</code> is not of NormalizableType.</p> <code>UnsortedDataException</code> <p>If data is unsorted and validate_index is set to True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'column': [5,6,7]})\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata={'my_dictionary': 'is_great'})\n&gt;&gt;&gt; lib.read(\"symbol\").data\n   column\n0       5\n1       6\n2       7\n</code></pre> <p>WritePayload objects can be unpacked and used as parameters:</p> <pre><code>&gt;&gt;&gt; w = adb.WritePayload(\"symbol\", df, metadata={'the': 'metadata'})\n&gt;&gt;&gt; lib.write(*w, staged=True)\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_batch","title":"write_batch","text":"<pre><code>write_batch(\n    payloads: List[WritePayload],\n    prune_previous_versions: bool = False,\n    validate_index=True,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Write a batch of multiple symbols.</p> PARAMETER DESCRIPTION <code>payloads</code> <p>Symbols and their corresponding data. There must not be any duplicate symbols in <code>payload</code>.</p> <p> TYPE: <code>`List[WritePayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>Verify that each entry in the batch has an index that supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing. Note that no checks are performed for Arrow input data.</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. The data attribute will be None for each versioned item. i-th entry corresponds to i-th element of <code>payloads</code>. Each result correspond to a structure containing metadata and version number of the written symbols in the store, in the same order as <code>payload</code>. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> <code>ArcticUnsupportedDataTypeException</code> <p>If data that is not of NormalizableType appears in any of the payloads.</p> See Also <p>write: For more detailed documentation.</p> <p>Examples:</p> <p>Writing a simple batch:</p> <pre><code>&gt;&gt;&gt; df_1 = pd.DataFrame({'column': [1,2,3]})\n&gt;&gt;&gt; df_2 = pd.DataFrame({'column': [4,5,6]})\n&gt;&gt;&gt; payload_1 = adb.WritePayload(\"symbol_1\", df_1, metadata={'the': 'metadata'})\n&gt;&gt;&gt; payload_2 = adb.WritePayload(\"symbol_2\", df_2)\n&gt;&gt;&gt; items = lib.write_batch([payload_1, payload_2])\n&gt;&gt;&gt; lib.read(\"symbol_1\").data\n   column\n0       1\n1       2\n2       3\n&gt;&gt;&gt; lib.read(\"symbol_2\").data\n   column\n0       4\n1       5\n2       6\n&gt;&gt;&gt; items[0].symbol, items[1].symbol\n('symbol_1', 'symbol_2')\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(\n    symbol: str,\n    metadata: Any,\n    prune_previous_versions: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Write metadata under the specified symbol name to this library. The data will remain unchanged. A new version will be created.</p> <p>If the symbol is missing, it causes a write with empty data (None, pickled, can't append) and the supplied metadata.</p> <p>This method should be faster than <code>write</code> as it involves no data segment read/write operations.</p> <p>See the Metadata section of our online documentation for details about how metadata is persisted and caveats.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name for the item</p> <p> TYPE: <code>str</code> </p> <code>metadata</code> <p>Metadata to persist along with the symbol</p> <p> TYPE: <code>Any</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database. Note that metadata is versioned alongside the data it is referring to, and so this operation removes old versions of data as well as metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the affected symbol in the store.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_metadata_batch","title":"write_metadata_batch","text":"<pre><code>write_metadata_batch(\n    write_metadata_payloads: List[WriteMetadataPayload],\n    prune_previous_versions: bool = False,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Write metadata to multiple symbols in a batch fashion. This is more efficient than making multiple <code>write_metadata</code> calls in succession as some constant-time operations can be executed only once rather than once for each element of <code>write_metadata_payloads</code>. Note that this isn't an atomic operation - it's possible for the metadata for one symbol to be fully written and readable before another symbol.</p> <p>See the Metadata section of our online documentation for details about how metadata is persisted and caveats.</p> PARAMETER DESCRIPTION <code>write_metadata_payloads</code> <p>Symbols and their corresponding metadata. There must not be any duplicate symbols in <code>payload</code>.</p> <p> TYPE: <code>`List[WriteMetadataPayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database. Note that metadata is versioned alongside the data it is referring to, and so this operation removes old versions of data as well as metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. The data attribute will be None for each versioned item. i-th entry corresponds to i-th element of <code>write_metadata_payloads</code>. Each result correspond to a structure containing metadata and version number of the affected symbol in the store. If any internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in write_metadata_payloads.</p> <p>Examples:</p> <p>Writing a simple batch:</p> <pre><code>&gt;&gt;&gt; payload_1 = adb.WriteMetadataPayload(\"symbol_1\", {'the': 'metadata_1'})\n&gt;&gt;&gt; payload_2 = adb.WriteMetadataPayload(\"symbol_2\", {'the': 'metadata_2'})\n&gt;&gt;&gt; items = lib.write_metadata_batch([payload_1, payload_2])\n&gt;&gt;&gt; lib.read_metadata(\"symbol_1\")\n{'the': 'metadata_1'}\n&gt;&gt;&gt; lib.read_metadata(\"symbol_2\")\n{'the': 'metadata_2'}\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_pickle","title":"write_pickle","text":"<pre><code>write_pickle(\n    symbol: str,\n    data: Any,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    staged=False,\n    recursive_normalizers: bool = None,\n) -&gt; VersionedItem\n</code></pre> <p>See <code>write</code>. This method differs from <code>write</code> only in that <code>data</code> can be of any type that is serialisable via the Pickle library. There are significant downsides to storing data in this way:</p> <ul> <li>Retrieval can only be done in bulk. Calls to <code>read</code> will not support <code>date_range</code>, <code>query_builder</code> or <code>columns</code>.</li> <li>The data cannot be updated or appended to via the update and append methods.</li> <li>Writes cannot be deduplicated in any way.</li> </ul> PARAMETER DESCRIPTION <code>symbol</code> <p>See documentation on <code>write</code>.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written.</p> <p> TYPE: <code>`Any`</code> </p> <code>metadata</code> <p>See documentation on <code>write</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>See documentation on <code>write</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>staged</code> <p>See documentation on <code>write</code>.</p> <p> DEFAULT: <code>False</code> </p> <code>recursive_normalizers</code> <p>See documentation on <code>write</code>. If enabled, attempts to recursively normalize data before falling back to pickling. If the leaf nodes cannot be natively normalized, they will be pickled, resulting in the overall data being recursively normalized and partially pickled. Example:     data = {\"a\": np.arange(5), \"b\": ABC()} # ABC is some custom class that cannot be natively normalized     # Exception will be thrown, as the leaf node requires pickling to normalize     lib.write(symbol, data, recursive_normalizers=True)     # The data will be successfully written by partially pickling the leaf node     lib.write_pickle(symbol, data, recursive_normalizers=True)     # The data will be successfully written by pickling the whole object     lib.write_pickle(symbol, data)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>See documentation on <code>write</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write_pickle(\"symbol\", [1,2,3])\n&gt;&gt;&gt; lib.read(\"symbol\").data\n[1, 2, 3]\n</code></pre> See Also <p>write: For more detailed documentation.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_pickle_batch","title":"write_pickle_batch","text":"<pre><code>write_pickle_batch(\n    payloads: List[WritePayload],\n    prune_previous_versions: bool = False,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Write a batch of multiple symbols, pickling their data if necessary.</p> PARAMETER DESCRIPTION <code>payloads</code> <p>Symbols and their corresponding data. There must not be any duplicate symbols in <code>payload</code>.</p> <p> TYPE: <code>`List[WritePayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>Structures containing metadata and version number of the written symbols in the store, in the same order as <code>payload</code>. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> See Also <p>write: For more detailed documentation. write_pickle: For information on the implications of providing data that needs to be pickled.</p>"},{"location":"api/library_types/","title":"Library Related Objects","text":""},{"location":"api/library_types/#arcticdb.version_store.library.NormalizableType","title":"arcticdb.version_store.library.NormalizableType  <code>module-attribute</code>","text":"<pre><code>NormalizableType = Union[NORMALIZABLE_TYPES]\n</code></pre> <p>Types that can be normalised into Arctic's internal storage structure.</p> See Also <p>Library.write: for more documentation on normalisation.</p>"},{"location":"api/library_types/#arcticdb.ReadInfoRequest","title":"arcticdb.ReadInfoRequest","text":"<p>               Bases: <code>NamedTuple</code></p> <p>ReadInfoRequest is useful for batch methods like read_metadata_batch and get_description_batch, where we only need to specify the symbol and the version information. Therefore, construction of this object is only required for these batch operations.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>See <code>read_metadata</code> method.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>See <code>read_metadata</code> method.</p> <p> TYPE: <code>Optional[AsOf], default=none</code> </p> See Also <p>Library.read: For documentation on the parameters.</p>"},{"location":"api/library_types/#arcticdb.ReadRequest","title":"arcticdb.ReadRequest","text":"<p>               Bases: <code>NamedTuple</code></p> <p>ReadRequest is designed to enable batching of read operations with an API that mirrors the singular <code>read</code> API. Therefore, construction of this object is only required for batch read operations.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[AsOf], default=none</code> </p> <code>date_range</code> <p>See <code>read</code>method.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]], default=none</code> </p> <code>row_range</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[Tuple[Optional[int], Optional[int]]], default=none</code> </p> <code>columns</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[List[str]], default=none</code> </p> <code>query_builder</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[Querybuilder], default=none</code> </p> <code>arrow_string_format_default</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[Union[ArrowOutputStringFormat, \"pa.DataType\"]], default=None</code> </p> <code>arrow_string_format_per_column</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[Dict[str, Union[ArrowOutputStringFormat, \"pa.DataType\"]]], default=None</code> </p> See Also <p>Library.read: For documentation on the parameters.</p>"},{"location":"api/library_types/#arcticdb.DeleteRequest","title":"arcticdb.DeleteRequest","text":"<p>               Bases: <code>NamedTuple</code></p> <p>DeleteRequest is designed to enable batching of delete operations with an API that mirrors the singular <code>delete</code> API. Therefore, construction of this object is only required for batch delete operations.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>See <code>delete</code> and <code>delete_batch</code> methods.</p> <p> TYPE: <code>str</code> </p> <code>version_ids</code> <p>See <code>delete</code> and <code>delete_batch</code> methods.</p> <p> TYPE: <code>List[int]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If version_ids is empty.</p>"},{"location":"api/library_types/#arcticdb.version_store.library.SymbolDescription","title":"arcticdb.version_store.library.SymbolDescription","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple. Descriptive information about the data stored under a particular symbol.</p> ATTRIBUTE DESCRIPTION <code>columns</code> <p>Columns stored under the symbol.</p> <p> TYPE: <code>Tuple[NameWithDType]</code> </p> <code>index</code> <p>Index of the symbol.</p> <p> TYPE: <code>Tuple[NameWithDType]</code> </p> <code>index_type</code> <p>Whether the index is a simple index or a multi_index. <code>NA</code> indicates that the stored data does not have an index.</p> <p> TYPE: <code>str {\"NA\", \"index\", \"multi_index\"}</code> </p> <code>row_count</code> <p>Number of rows, or None if the symbol is pickled.</p> <p> TYPE: <code>Optional[int]</code> </p> <code>last_update_time</code> <p>The time of the last update to the symbol, in UTC.</p> <p> TYPE: <code>datetime</code> </p> <code>date_range</code> <p>The values of the index column in the first and last row of this symbol. Both values will be NaT if: - the symbol is not timestamp indexed - the symbol is timestamp indexed, but the sorted field of this class is UNSORTED (see below)</p> <p> TYPE: <code>Tuple[Union[Timestamp], Union[Timestamp]]</code> </p> <code>sorted</code> <p>One of \"ASCENDING\", \"DESCENDING\", \"UNSORTED\", or \"UNKNOWN\": ASCENDING - The data has a timestamp index, and is sorted in ascending order. Guarantees that operations such as             append, update, and read with date_range work as expected. DESCENDING - The data has a timestamp index, and is sorted in descending order. Update and read with date_range              will not work. UNSORTED - The data has a timestamp index, and is not sorted. Can only be created by calling write, write_batch,            append, or append_batch with validate_index set to False. Update and read with date_range will not            work. UNKNOWN - Either the data does not have a timestamp index, or the data does have a timestamp index, but was           written by a client that predates this information being stored.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/library_types/#arcticdb.version_store.library.SymbolVersion","title":"arcticdb.version_store.library.SymbolVersion","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple. A symbol name - version pair.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the symbol.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/library_types/#arcticdb.version_store.library.StagedDataFinalizeMethod","title":"arcticdb.version_store.library.StagedDataFinalizeMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/library_types/#arcticdb.VersionedItem","title":"arcticdb.VersionedItem","text":"<p>Return value for many operations that captures the result and associated information.</p> ATTRIBUTE DESCRIPTION <code>library</code> <p>Library this result relates to.</p> <p> TYPE: <code>str</code> </p> <code>symbol</code> <p>Read or modified symbol.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>For data retrieval (read) operations, contains the data read. For data modification operations, the value might not be populated.</p> <p> TYPE: <code>Any</code> </p> <code>version</code> <p>For data retrieval operations, the version the <code>as_of</code> argument resolved to. In the special case where no versions have been written yet, but data is being read exclusively from incomplete segments, this will be 2^64-1. For data modification operations, the version the data has been written under.</p> <p> TYPE: <code>int</code> </p> <code>metadata</code> <p>The metadata saved alongside <code>data</code>. Availability depends on the method used and may be different from that of <code>data</code>.</p> <p> TYPE: <code>Any</code> </p> <code>host</code> <p>Informational / for backwards compatibility.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>timestamp</code> <p>The time in nanoseconds since epoch that this version was written. In the special case where no versions have been written yet, but data is being read exclusively from incomplete segments, this will be 0.</p> <p> TYPE: <code>Optional[int]</code> </p>"},{"location":"api/library_types/#arcticdb.version_store.library.VersionInfo","title":"arcticdb.version_store.library.VersionInfo","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple. Descriptive information about a particular version of a symbol.</p> ATTRIBUTE DESCRIPTION <code>date</code> <p>Time that the version was written in UTC.</p> <p> TYPE: <code>datetime</code> </p> <code>deleted</code> <p>True if the version has been deleted and is only being kept alive via a snapshot.</p> <p> TYPE: <code>bool</code> </p> <code>snapshots</code> <p>Snapshots that refer to this version.</p> <p> TYPE: <code>List[str]</code> </p>"},{"location":"api/library_types/#arcticdb.WritePayload","title":"arcticdb.WritePayload","text":"<p>WritePayload is designed to enable batching of multiple operations with an API that mirrors the singular <code>write</code> API.</p> <p>Construction of <code>WritePayload</code> objects is only required for batch write operations.</p> <p>One instance of <code>WritePayload</code> refers to one unit that can be written through to ArcticDB.</p> METHOD DESCRIPTION <code>__init__</code> <p>Constructor.</p>"},{"location":"api/library_types/#arcticdb.WritePayload.__init__","title":"__init__","text":"<pre><code>__init__(\n    symbol: str,\n    data: Union[Any, NormalizableType],\n    metadata: Any = None,\n    index_column: Optional[str] = None,\n)\n</code></pre> <p>Constructor.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written. If data is not of NormalizableType then it will be pickled.</p> <p> TYPE: <code>Any</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>index_column</code> <p>Optional specification of timeseries index column if data is an Arrow table. Ignored if data is not an Arrow table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> See Also <p>Library.write_pickle: For information on the implications of providing data that needs to be pickled.</p>"},{"location":"api/library_types/#arcticdb.UpdatePayload","title":"arcticdb.UpdatePayload","text":"<p>UpdatePayload is designed to enable batching of multiple operations with an API that mirrors the singular <code>update</code> API.</p> <p>Construction of <code>UpdatePayload</code> objects is only required for batch update operations.</p> <p>One instance of <code>UpdatePayload</code> refers to one unit that can be written through to ArcticDB.</p> METHOD DESCRIPTION <code>__init__</code> <p>Constructor.</p>"},{"location":"api/library_types/#arcticdb.UpdatePayload.__init__","title":"__init__","text":"<pre><code>__init__(\n    symbol: str,\n    data: NormalizableType,\n    metadata: Any = None,\n    date_range: Optional[\n        Tuple[Optional[Timestamp], Optional[Timestamp]]\n    ] = None,\n    index_column: Optional[str] = None,\n)\n</code></pre> <p>Constructor.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Time-indexed data to use for the update.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the new symbol version.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>date_range</code> <p>Restricts the update to the specified range in the stored data. Leaving either bound as <code>None</code> leaves that side of the range open-ended.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]]</code> DEFAULT: <code>None</code> </p> <code>index_column</code> <p>Optional specification of timeseries index column if data is an Arrow table. Ignored if data is not an Arrow table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/library_types/#arcticdb.WriteMetadataPayload","title":"arcticdb.WriteMetadataPayload","text":"<p>WriteMetadataPayload is designed to enable batching of multiple operations with an API that mirrors the singular <code>write_metadata</code> API.</p> <p>Construction of <code>WriteMetadataPayload</code> objects is only required for batch write metadata operations.</p> <p>One instance of <code>WriteMetadataPayload</code> refers to one unit that can be written through to ArcticDB.</p> METHOD DESCRIPTION <code>__init__</code> <p>Constructor.</p>"},{"location":"api/library_types/#arcticdb.WriteMetadataPayload.__init__","title":"__init__","text":"<pre><code>__init__(symbol: str, metadata: Any)\n</code></pre> <p>Constructor.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>metadata</code> <p>metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> </p>"},{"location":"api/library_types/#arcticdb_ext.version_store.StageResult","title":"arcticdb_ext.version_store.StageResult","text":"<p>Result returned by the stage method containing information about staged segments.</p> <p>StageResult objects can be passed to finalization methods to specify which staged data to finalize. This enables selective finalization of staged data when multiple stage operations have been performed.</p> ATTRIBUTE DESCRIPTION <code>staged_segments</code> <p> TYPE: <code>List[AtomKey]</code> </p>"},{"location":"api/processing/","title":"DataFrame Processing Operations API","text":""},{"location":"api/processing/#arcticdb.LazyDataFrame","title":"arcticdb.LazyDataFrame","text":"<p>               Bases: <code>QueryBuilder</code></p> <p>Lazy dataframe implementation, allowing chains of queries to be added before the read is actually executed. Returned by <code>Library.read</code>, <code>Library.head</code>, and <code>Library.tail</code> calls when <code>lazy=True</code>.</p> See Also <p>QueryBuilder for supported querying operations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt;\n# Specify that we want version 0 of \"test\" symbol, and to only return the \"new_column\" column in the output\n&gt;&gt;&gt; lazy_df = lib.read(\"test\", as_of=0, columns=[\"new_column\"], lazy=True)\n# Perform a filtering operation\n&gt;&gt;&gt; lazy_df = lazy_df[lazy_df[\"col1\"].isin(0, 3, 6, 9)]\n# Create a new column through a projection operation\n&gt;&gt;&gt; lazy_df[\"new_col\"] = lazy_df[\"col1\"] + lazy_df[\"col2\"]\n# Actual read and processing happens here\n&gt;&gt;&gt; df = lazy_df.collect().data\n</code></pre> METHOD DESCRIPTION <code>collect</code> <p>Read the data and execute any queries applied to this object since the read call.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrame.collect","title":"collect","text":"<pre><code>collect() -&gt; VersionedItem\n</code></pre> <p>Read the data and execute any queries applied to this object since the read call.</p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Object that contains a .data and .metadata element.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection","title":"arcticdb.LazyDataFrameCollection","text":"<p>               Bases: <code>QueryBuilder</code></p> <p>Lazy dataframe implementation for batch operations. Allows the application of chains of queries to be added before the actual reads are performed. Queries applied to this object will be applied to all  the symbols being read. If per-symbol queries are required, split can be used to break this class into a list of <code>LazyDataFrame</code> objects. Returned by <code>Library.read_batch</code> calls when <code>lazy=True</code>.</p> See Also <p>QueryBuilder for supported querying operations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt;\n# Specify that we want the latest version of \"test_0\" symbol, and version 0 of \"test_1\" symbol\n&gt;&gt;&gt; lazy_dfs = lib.read_batch([\"test_0\", ReadRequest(\"test_1\", as_of=0)], lazy=True)\n# Perform a filtering operation on both the \"test_0\" and \"test_1\" symbols\n&gt;&gt;&gt; lazy_dfs = lazy_dfs[lazy_dfs[\"col1\"].isin(0, 3, 6, 9)]\n# Perform a different projection operation on each symbol\n&gt;&gt;&gt; lazy_dfs = lazy_dfs.split()\n&gt;&gt;&gt; lazy_dfs[0].apply(\"new_col\", lazy_dfs[0][\"col1\"] + 1)\n&gt;&gt;&gt; lazy_dfs[1].apply(\"new_col\", lazy_dfs[1][\"col1\"] + 2)\n# Bring together again and perform the same filter on both symbols\n&gt;&gt;&gt; lazy_dfs = LazyDataFrameCollection(lazy_dfs)\n&gt;&gt;&gt; lazy_dfs = lazy_dfs[lazy_dfs[\"new_col\"] &gt; 0]\n# Actual read and processing happens here\n&gt;&gt;&gt; res = lazy_dfs.collect()\n</code></pre> METHOD DESCRIPTION <code>__init__</code> <p>Gather a list of <code>LazyDataFrame</code>s into a single object that can be collected together.</p> <code>collect</code> <p>Read the data and execute any queries applied to this object since the read_batch call.</p> <code>split</code> <p>Separate the collection into a list of LazyDataFrames, including any queries already applied to this object.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection.__init__","title":"__init__","text":"<pre><code>__init__(\n    lazy_dataframes: List[LazyDataFrame],\n    arrow_string_format_default: Optional[\n        Union[ArrowOutputStringFormat, DataType]\n    ] = None,\n    arrow_string_format_per_column: Optional[\n        Dict[str, Union[ArrowOutputStringFormat, DataType]]\n    ] = None,\n)\n</code></pre> <p>Gather a list of <code>LazyDataFrame</code>s into a single object that can be collected together.</p> PARAMETER DESCRIPTION <code>lazy_dataframes</code> <p>Collection of <code>LazyDataFrames</code>s to gather together.</p> <p> TYPE: <code>List[LazyDataFrame]</code> </p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection.collect","title":"collect","text":"<pre><code>collect() -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Read the data and execute any queries applied to this object since the read_batch call.</p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>See documentation on <code>Library.read_batch</code>.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection.split","title":"split","text":"<pre><code>split() -&gt; List[LazyDataFrame]\n</code></pre> <p>Separate the collection into a list of LazyDataFrames, including any queries already applied to this object.</p> RETURNS DESCRIPTION <code>List[LazyDataFrame]</code>"},{"location":"api/processing/#arcticdb.concat","title":"arcticdb.concat","text":"<pre><code>concat(\n    lazy_dataframes: Union[\n        List[LazyDataFrame], LazyDataFrameCollection\n    ],\n    join: str = \"outer\",\n) -&gt; LazyDataFrameAfterJoin\n</code></pre> <p>Concatenate a list of symbols together.</p> PARAMETER DESCRIPTION <code>join</code> <p>Whether the columns of the input symbols should be inner or outer joined. Supported inputs are \"inner\" and \"outer\".</p> <ul> <li>inner - Only columns present in ALL the input symbols will be present in the returned DataFrame.</li> <li>outer - Columns present in ANY of the input symbols will be present in the returned DataFrame. Columns   that are present in some input symbols but not in others will be backfilled according to their type using   the same rules as with dynamic schema.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>\"outer\"</code> </p> RETURNS DESCRIPTION <code>LazyDataFrameAfterJoin</code> <p>Lazy DataFrame representing the joined data, to which further processing operations can be chained.</p> RAISES DESCRIPTION <code>ArcticNativeException</code> <p>The join argument is not one of \"inner\" or \"outer\"</p> <p>Examples:</p> <p>Join 2 symbols together without any pre or post processing.</p> <pre><code>&gt;&gt;&gt; df0 = pd.DataFrame(\n    {\n        \"col1\": [0.5],\n        \"col2\": [1],\n    },\n    index=[pd.Timestamp(\"2025-01-01\")],\n)\n&gt;&gt;&gt; df1 = pd.DataFrame(\n    {\n        \"col3\": [\"hello\"],\n        \"col2\": [2],\n    },\n    index=[pd.Timestamp(\"2025-01-02\")],\n)\n&gt;&gt;&gt; lib.write(\"symbol0\", df0)\n&gt;&gt;&gt; lib.write(\"symbol1\", df1)\n&gt;&gt;&gt; lazy_dfs = lib.read_batch([\"symbol0\", \"symbol1\"], lazy=True)\n&gt;&gt;&gt; adb.concat(lazy_dfs, join=\"outer\").collect().data\n</code></pre> <pre><code>                       col1     col2     col3\n2025-01-01 00:00:00     0.5        1     None\n2025-01-02 00:00:00     NaN        2  \"hello\"\n</code></pre> <p>Join 2 symbols together with both some per-symbol processing prior to the join, and some further processing after the join.</p> <pre><code>&gt;&gt;&gt; df0 = pd.DataFrame(\n    {\n        \"col\": [0, 1, 2, 3, 4],\n    },\n    index=pd.date_range(\"2025-01-01\", freq=\"min\", periods=5),\n)\n&gt;&gt;&gt; df1 = pd.DataFrame(\n    {\n        \"col\": [5, 6, 7, 8, 9],\n    },\n    index=pd.date_range(\"2025-01-01T00:05:00\", freq=\"min\" periods=5),\n)\n&gt;&gt;&gt; lib.write(\"symbol0\", df0)\n&gt;&gt;&gt; lib.write(\"symbol1\", df1)\n&gt;&gt;&gt; lazy_df0, lazy_df1 = lib.read_batch([\"symbol0\", \"symbol1\"], lazy=True).split()\n&gt;&gt;&gt; lazy_df0 = lazy_df0[lazy_df0[\"col\"] &lt;= 2]\n&gt;&gt;&gt; lazy_df1 = lazy_df1[lazy_df1[\"col\"] &lt;= 6]\n&gt;&gt;&gt; lazy_df = adb.concat([lazy_df0, lazy_df1])\n&gt;&gt;&gt; lazy_df = lazy_df.resample(\"10min\").agg({\"col\": \"sum\"})\n&gt;&gt;&gt; lazy_df.collect().data\n</code></pre> <pre><code>                        col\n2025-01-01 00:00:00      14\n</code></pre>"},{"location":"api/processing/#arcticdb.QueryBuilder","title":"arcticdb.QueryBuilder","text":"<p>Build a query to process read results with. Syntax is designed to be similar to Pandas:</p> <pre><code>q = adb.QueryBuilder()\nq = q[q[\"a\"] &lt; 5] (equivalent to q = q[q.a &lt; 5] provided the column name is also a valid Python variable name)\ndataframe = lib.read(symbol, query_builder=q).data\n</code></pre> <p>For Group By and Aggregation functionality please see the documentation for the <code>groupby</code>. For projection functionality, see the documentation for the <code>apply</code> method.</p> <p>Supported arithmetic operations when projection or filtering:</p> <ul> <li>Binary arithmetic: +, -, *, /</li> <li>Unary arithmetic: -, abs</li> </ul> <p>Supported filtering operations:</p> <ul> <li> <p>isna, isnull, notna, and notnull - return all rows where a specified column is/is not NaN or None. isna is equivalent to isnull, and notna is equivalent to notnull, i.e. no distinction is made between NaN and None values in column types that support both (e.g. strings). For example:     <pre><code>q = q[q[\"col\"].isna()]\n</code></pre></p> </li> <li> <p>Binary comparisons: &lt;, &lt;=, &gt;, &gt;=, ==, !=</p> </li> <li>Unary NOT: ~</li> <li>Binary combinators: &amp;, |, ^</li> <li>List membership: isin, isnotin (also accessible with == and !=)</li> <li>Regex match: regex_match</li> </ul> <p>isin/isnotin accept lists, sets, frozensets, 1D ndarrays, or *args unpacking. For example:</p> <pre><code>l = [1, 2, 3]\nq.isin(l)\n</code></pre> <p>is equivalent to...</p> <pre><code>q.isin(1, 2, 3)\n</code></pre> <p>regex_match, similar to pandas' contains, accepts string as pattern and can only filter string columns</p> <p>Boolean columns can be filtered on directly:</p> <pre><code>q = adb.QueryBuilder()\nq = q[q[\"boolean_column\"]]\n</code></pre> <p>and combined with other operations intuitively:</p> <pre><code>q = adb.QueryBuilder()\nq = q[(q[\"boolean_column_1\"] &amp; ~q[\"boolean_column_2\"]) &amp; (q[\"numeric_column\"] &gt; 0)]\n</code></pre> <p>Arbitrary combinations of these expressions is possible, for example:</p> <pre><code>q = q[(((q[\"a\"] * q[\"b\"]) / 5) &lt; (0.7 * q[\"c\"])) &amp; (q[\"b\"] != 12)]\n</code></pre> <p>See tests/unit/arcticdb/version_store/test_filtering.py for more example uses.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder--timestamp-filtering","title":"Timestamp filtering","text":"<p>pandas.Timestamp, datetime.datetime, pandas.Timedelta, and datetime.timedelta objects are supported. Note that internally all of these types are converted to nanoseconds (since epoch in the Timestamp/datetime cases). This means that nonsensical operations such as multiplying two times together are permitted (but not encouraged).</p>"},{"location":"api/processing/#arcticdb.QueryBuilder--restrictions","title":"Restrictions","text":"<p>String equality/inequality (and isin/isnotin) is supported for printable ASCII characters only. Although not prohibited, it is not recommended to use ==, !=, isin, or isnotin with floating point values.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder--exceptions","title":"Exceptions","text":"<p>inf or -inf values are provided for comparison Column involved in query is a Categorical Symbol is pickled Column involved in query is not present in symbol Query involves comparing strings using &lt;, &lt;=, &gt;, or &gt;= operators Query involves comparing a string to one or more numeric values, or vice versa Query involves arithmetic with a column containing strings</p> METHOD DESCRIPTION <code>apply</code> <p>Apply enables new columns to be created using supported QueryBuilder numeric operations. See the documentation for the</p> <code>concat</code> <p>Concatenate a list of symbols together. Should be the first clause in a QueryBuilder provided to either</p> <code>date_range</code> <p>DateRange to read data for.  Applicable only for Pandas data with a DateTime index. Returns only the part</p> <code>groupby</code> <p>Group symbol by column name. GroupBy operations must be followed by an aggregation operator. Currently the following five aggregation</p> <code>head</code> <p>Filter out all but the first n rows of data. If n is negative, return all rows except the last n rows.</p> <code>optimise_for_memory</code> <p>Reduce peak memory usage during the query, at the expense of some performance.</p> <code>optimise_for_speed</code> <p>Process query as fast as possible (the default behaviour)</p> <code>prepend</code> <p>Applies processing specified in other before any processing already defined for this QueryBuilder.</p> <code>resample</code> <p>Resample a symbol on the index. The symbol must be datetime indexed. Resample operations must be followed by</p> <code>row_range</code> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound.</p> <code>tail</code> <p>Filter out all but the last n rows of data. If n is negative, return all rows except the first n rows.</p> <code>then</code> <p>Applies processing specified in other after any processing already defined for this QueryBuilder.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.apply","title":"apply","text":"<pre><code>apply(name, expr)\n</code></pre> <p>Apply enables new columns to be created using supported QueryBuilder numeric operations. See the documentation for the QueryBuilder class for more information on supported expressions - any expression valid in a filter is valid when using <code>apply</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the column to be created</p> <p> </p> <code>expr</code> <p>Expression</p> <p> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"VWAP\": np.arange(0, 10, dtype=np.float64),\n        \"ASK\": np.arange(10, 20, dtype=np.uint16),\n        \"VOL_ACC\": np.arange(20, 30, dtype=np.int32),\n    },\n    index=np.arange(10),\n)\n&gt;&gt;&gt; lib.write(\"expression\", df)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.apply(\"ADJUSTED\", q[\"ASK\"] * q[\"VOL_ACC\"] + 7)\n&gt;&gt;&gt; lib.read(\"expression\", query_builder=q).data\nVOL_ACC  ASK  VWAP  ADJUSTED\n0     20   10   0.0       207\n1     21   11   1.0       238\n2     22   12   2.0       271\n3     23   13   3.0       306\n4     24   14   4.0       343\n5     25   15   5.0       382\n6     26   16   6.0       423\n7     27   17   7.0       466\n8     28   18   8.0       511\n9     29   19   9.0       558\n</code></pre> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.concat","title":"concat","text":"<pre><code>concat(join: str = 'outer')\n</code></pre> <p>Concatenate a list of symbols together. Should be the first clause in a QueryBuilder provided to either NativeVersionStore.batch_read_and_join or Library.read_batch_and_join.</p> PARAMETER DESCRIPTION <code>join</code> <p>Whether the columns of the input symbols should be inner or outer joined. Supported inputs are \"inner\" and \"outer\". * inner - Only columns present in ALL the input symbols will be present in the returned DataFrame. * outer - Columns present in ANY of the input symbols will be present in the returned DataFrame. Columns   that are present in some input symbols but not in others will be backfilled according to their type using   the same rules as with dynamic schema.</p> <p> TYPE: <code>str</code> DEFAULT: <code>\"outer\"</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p> RAISES DESCRIPTION <code>ArcticNativeException</code> <p>The join argument is not one of \"inner\" or \"outer\"</p> <p>Examples:</p> <p>Join 2 symbols together without any pre or post processing.</p> <pre><code>&gt;&gt;&gt; df0 = pd.DataFrame(\n    {\n        \"col1\": [0.5],\n        \"col2\": [1],\n    },\n    index=[pd.Timestamp(\"2025-01-01\")],\n)\n&gt;&gt;&gt; df1 = pd.DataFrame(\n    {\n        \"col3\": [\"hello\"],\n        \"col2\": [2],\n    },\n    index=[pd.Timestamp(\"2025-01-02\")],\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.concat(\"outer\")\n&gt;&gt;&gt; lib.write(\"symbol0\", df0)\n&gt;&gt;&gt; lib.write(\"symbol1\", df1)\n&gt;&gt;&gt; lib.batch_read_and_join([\"symbol0\", \"symbol1\"], query_builder=q).data\n</code></pre> <pre><code>                       col1     col2     col3\n2025-01-01 00:00:00     0.5        1     None\n2025-01-02 00:00:00     NaN        2  \"hello\"\n</code></pre> <pre><code>&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.concat(\"inner\")\n&gt;&gt;&gt; lib.batch_read_and_join([\"symbol0\", \"symbol1\"], query_builder=q).data\n</code></pre> <pre><code>                       col2\n2025-01-01 00:00:00       1\n2025-01-02 00:00:00       2\n</code></pre>"},{"location":"api/processing/#arcticdb.QueryBuilder.date_range","title":"date_range","text":"<pre><code>date_range(date_range: DateRangeInput)\n</code></pre> <p>DateRange to read data for.  Applicable only for Pandas data with a DateTime index. Returns only the part of the data that falls within the given range. If this is the only processing clause being applied, then the returned data object will use less memory than passing date_range directly as an argument to the read method, at  the cost of possibly being slightly slower.</p> PARAMETER DESCRIPTION <code>date_range</code> <p>A date range in the same format as accepted by the read method.</p> <p> TYPE: <code>DateRangeInput</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.date_range((pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2001-01-01\")))\n</code></pre> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.groupby","title":"groupby","text":"<pre><code>groupby(name: str)\n</code></pre> <p>Group symbol by column name. GroupBy operations must be followed by an aggregation operator. Currently the following five aggregation operators are supported:</p> <ul> <li>\"mean\" - compute the mean of the group</li> <li>\"sum\" - compute the sum of the group</li> <li>\"min\" - compute the min of the group</li> <li>\"max\" - compute the max of the group</li> <li>\"count\" - compute the count of group</li> </ul> <p>For usage examples, see below.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the column to group on. Note that currently GroupBy only supports single-column groupings.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <p>Average (mean) over two groups:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\", \"group_2\", \"group_2\"],\n        \"to_mean\": [1.1, 1.4, 2.5, np.nan, 2.2],\n    },\n    index=np.arange(5),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\").agg({\"to_mean\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>           to_mean\n group_1  1.666667\n group_2       2.2\n</code></pre> <p>Max over one group:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\"],\n        \"to_max\": [1, 5, 4],\n    },\n    index=np.arange(3),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\").agg({\"to_max\": \"max\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>         to_max\ngroup_1  5\n</code></pre> <p>Max and Mean:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\"],\n        \"to_mean\": [1.1, 1.4, 2.5],\n        \"to_max\": [1.1, 1.4, 2.5]\n    },\n    index=np.arange(3),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\").agg({\"to_max\": \"max\", \"to_mean\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>         to_max   to_mean\ngroup_1     2.5  1.666667\n</code></pre> <p>Min and max over one column, mean over another:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\", \"group_2\", \"group_2\"],\n        \"agg_1\": [1, 2, 3, 4, 5],\n        \"agg_2\": [1.1, 1.4, 2.5, np.nan, 2.2],\n    },\n    index=np.arange(5),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\")\n&gt;&gt;&gt; q = q.agg({\"agg_1_min\": (\"agg_1\", \"min\"), \"agg_1_max\": (\"agg_1\", \"max\"), \"agg_2\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>         agg_1_min  agg_1_max     agg_2\ngroup_1          1          3  1.666667\ngroup_2          4          5       2.2\n</code></pre> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.head","title":"head","text":"<pre><code>head(n: int = 5)\n</code></pre> <p>Filter out all but the first n rows of data. If n is negative, return all rows except the last n rows.</p> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.optimise_for_memory","title":"optimise_for_memory","text":"<pre><code>optimise_for_memory()\n</code></pre> <p>Reduce peak memory usage during the query, at the expense of some performance.</p> <p>Optimisations applied:</p> <ul> <li>Memory used by strings that are present in segments read from storage, but are not required in the final dataframe that will be presented back to the user, is reclaimed earlier in the processing pipeline.</li> </ul>"},{"location":"api/processing/#arcticdb.QueryBuilder.optimise_for_speed","title":"optimise_for_speed","text":"<pre><code>optimise_for_speed()\n</code></pre> <p>Process query as fast as possible (the default behaviour)</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.prepend","title":"prepend","text":"<pre><code>prepend(other: QueryBuilder)\n</code></pre> <p>Applies processing specified in other before any processing already defined for this QueryBuilder.</p> PARAMETER DESCRIPTION <code>other</code> <p>QueryBuilder to apply before this one in the processing pipeline.</p> <p> TYPE: <code>QueryBuilder</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.resample","title":"resample","text":"<pre><code>resample(\n    rule: Union[str, DateOffset],\n    closed: Optional[str] = None,\n    label: Optional[str] = None,\n    offset: Optional[Union[str, Timedelta]] = None,\n    origin: Union[str, Timestamp] = \"epoch\",\n)\n</code></pre> <p>Resample a symbol on the index. The symbol must be datetime indexed. Resample operations must be followed by an aggregation operator. Currently, the following 7 aggregation operators are supported:</p> <ul> <li>\"mean\" - compute the mean of the group</li> <li>\"sum\" - compute the sum of the group</li> <li>\"min\" - compute the min of the group</li> <li>\"max\" - compute the max of the group</li> <li>\"count\" - compute the count of group</li> <li>\"first\" - compute the first value in the group</li> <li>\"last\" - compute the last value in the group</li> </ul> <p>Note that not all aggregators are supported with all column types:</p> <ul> <li>Numeric columns - support all aggregators</li> <li>Bool columns - support all aggregators</li> <li>String columns - support count, first, and last aggregators</li> <li>Datetime columns - support all aggregators EXCEPT sum</li> </ul> <p>Note that time-buckets which contain no index values in the symbol will NOT be included in the returned DataFrame. This is not the same as Pandas default behaviour. Resampling is currently not supported with:</p> <ul> <li>Dynamic schema where an aggregation column is missing from one or more of the row-slices.</li> <li>Sparse data.</li> </ul> <p>The resample results match pandas resample with <code>origin=\"epoch\"</code>. We plan to add an 'origin' argument in a future release and will then change the default value to '\"start_day\"' to match the Pandas default. This will change the results in cases where the rule is not a multiple of 24 hours.</p> PARAMETER DESCRIPTION <code>rule</code> <p>The frequency at which to resample the data. Supported rule strings are ns, us, ms, s, min, h, and D, and multiples/combinations of these, such as 1h30min. pd.DataOffset objects representing frequencies from this set are also accepted.</p> <p> TYPE: <code>Union[str, DateOffset]</code> </p> <code>closed</code> <p>Which boundary of each time-bucket is closed. Must be one of 'left' or 'right'. If not provided, the default is left for all currently supported frequencies.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Which boundary of each time-bucket is used as the index value in the returned DataFrame. Must be one of 'left' or 'right'. If not provided, the default is left for all currently supported frequencies.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>offset</code> <p>Offset the start of each bucket. Supported strings are the same as in <code>pd.Timedelta</code>. If offset is larger than rule then <code>offset</code> modulo <code>rule</code> is used as an offset.</p> <p> TYPE: <code>Optional[Union[str, Timedelta]]</code> DEFAULT: <code>None</code> </p> <code>origin</code> <p>The timestamp on which to adjust the grouping. Supported string are:</p> <ul> <li>epoch: origin is 1970-01-01</li> <li>start: origin is the first value of the timeseries</li> <li>start_day: origin is the first day at midnight of the timeseries</li> <li>end: origin is the last value of the timeseries</li> <li>end_day: origin is the ceiling midnight of the last day</li> </ul> <p><code>start</code>, <code>start_day</code>, <code>end</code>, <code>end_day</code> origin values are not supported in conjunction with <code>date_range</code>.</p> <p> TYPE: <code>Union[str, Timestamp]</code> DEFAULT: <code>'epoch'</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p> RAISES DESCRIPTION <code>ArcticDbNotYetImplemented</code> <p>A frequency string or Pandas DateOffset object are provided to the rule argument outside the supported frequencies listed above.</p> <code>ArcticNativeException</code> <p>The closed or label arguments are not one of \"left\" or \"right\"</p> <code>SchemaException</code> <p>Raised on call to read if:</p> <ul> <li>If the aggregation specified is not compatible with the type of the column being aggregated as   specified above.</li> <li>The library has dynamic schema enabled, and at least one of the columns being aggregated is missing   from at least one row-slice.</li> <li>At least one of the columns being aggregated contains sparse data.</li> </ul> <code>UserInputException</code> <ul> <li><code>start</code>, <code>start_day</code>, <code>end</code>, <code>end_day</code> is used in conjunction with <code>date_range</code></li> <li><code>origin</code> is not one of <code>start</code>, <code>start_day</code>, <code>end</code>, <code>end_day</code>, <code>epoch</code> or a <code>pd.Timestamp</code></li> </ul> <p>Examples:</p> <p>Resample two hours worth of minutely data down to hourly data, summing the column 'to_sum':</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"to_sum\": np.arange(120),\n    },\n    index=pd.date_range(\"2024-01-01\", freq=\"min\", periods=120),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\").agg({\"to_sum\": \"sum\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     to_sum\n2024-01-01 00:00:00    1770\n2024-01-01 01:00:00    5370\n</code></pre> <p>As above, but specifying that the closed boundary of each time-bucket is the right hand side, and also to label the output by the right boundary:</p> <pre><code>&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\", closed=\"right\", label=\"right\").agg({\"to_sum\": \"sum\"})\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     to_sum\n2024-01-01 00:00:00       0\n2024-01-01 01:00:00    1830\n2024-01-01 02:00:00    5310\n</code></pre> <p>Nones, NaNs, and NaTs are omitted from aggregations:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"to_mean\": [1.0, np.nan, 2.0],\n    },\n    index=pd.date_range(\"2024-01-01\", freq=\"min\", periods=3),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\").agg({\"to_mean\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     to_mean\n2024-01-01 00:00:00      1.5\n</code></pre> <p>Output column names can be controlled through the format of the dict passed to agg:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"agg_1\": [1, 2, 3, 4, 5],\n        \"agg_2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n    },\n    index=pd.date_range(\"2024-01-01\", freq=\"min\", periods=5),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\")\n&gt;&gt;&gt; q = q.agg({\"agg_1_min\": (\"agg_1\", \"min\"), \"agg_1_max\": (\"agg_1\", \"max\"), \"agg_2\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     agg_1_min  agg_1_max     agg_2\n2024-01-01 00:00:00          1          5      2.75\n</code></pre>"},{"location":"api/processing/#arcticdb.QueryBuilder.row_range","title":"row_range","text":"<pre><code>row_range(row_range: Tuple[Optional[int], Optional[int]])\n</code></pre> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound. Should behave the same as df.iloc[start:end], including in the handling of negative start/end values.</p> PARAMETER DESCRIPTION <code>row_range</code> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound. Leaving either element as None leaves that side of the range open-ended. For example (5, None) would include everything from the 5th row onwards.</p> <p> TYPE: <code>Tuple[Optional[int], Optional[int]]</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.tail","title":"tail","text":"<pre><code>tail(n: int = 5)\n</code></pre> <p>Filter out all but the last n rows of data. If n is negative, return all rows except the first n rows.</p> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.then","title":"then","text":"<pre><code>then(other: QueryBuilder)\n</code></pre> <p>Applies processing specified in other after any processing already defined for this QueryBuilder.</p> PARAMETER DESCRIPTION <code>other</code> <p>QueryBuilder to apply after this one in the processing pipeline.</p> <p> TYPE: <code>QueryBuilder</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.where","title":"arcticdb.where","text":"<pre><code>where(\n    condition: Any, left: Any, right: Any\n) -&gt; ExpressionNode\n</code></pre> <p>Ternary operator choosing from the left expression where condition is true, and from the right expression where it is false. Similar to numpy.where, or the Python statement <code>left if condition else right</code>.</p> PARAMETER DESCRIPTION <code>condition</code> <p>The condition on which to choose from left or right. e.g. a boolean column, or a filtering statement such as df[\"col\"] == 0.</p> <p> TYPE: <code>Any</code> </p> <code>left</code> <p>The expression to select where condition is true. If the return value of the where function is being used as a filter, then this must be another filtering statement. If the return value is being used as a projection to create a new column, then this must be either an expression producing a column, or a value. See examples below for both use cases.</p> <p> TYPE: <code>Any</code> </p> <code>right</code> <p>The expression to select where condition is false.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>ExpressionNode</code> <p>An opaque object representing a node in the Abstract Syntax Tree of the expression being computed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n&gt;&gt;&gt;    {\n&gt;&gt;&gt;        \"col1\": [0, 0, 1, 0, 1],\n&gt;&gt;&gt;        \"col2\": [0, 1, 2, 3, 4],\n&gt;&gt;&gt;        \"col3\": [5, 6, 7, 8, 9],\n&gt;&gt;&gt;    }\n&gt;&gt;&gt;)\n&gt;&gt;&gt; lib.write(\"sym\", df)\n</code></pre> <p>Produce a new column by selecting from functions of two other columns</p> <pre><code>&gt;&gt;&gt; q = QueryBuilder()\n&gt;&gt;&gt; q.apply(\"new_col\", where(q[\"col1\"] == 0), 2 * q[\"col2\"], q[\"col3\"])\n&gt;&gt;&gt; lib.read(\"sym\", query_builder=q).data\n        col1   col2   col3   new_col\n    0      0      0      5         0\n    1      0      1      6         2\n    2      1      2      7         7\n    3      0      3      8         6\n    4      1      4      9         9\n</code></pre> <p>Produce a new column by selecting from one column and one fixed value</p> <pre><code>&gt;&gt;&gt; q = QueryBuilder()\n&gt;&gt;&gt; q.apply(\"new_col\", where(q[\"col1\"] == 0), q[\"col2\"], 10)\n&gt;&gt;&gt; lib.read(\"sym\", query_builder=q).data\n        col1   col2   col3   new_col\n    0      0      0      5         0\n    1      0      1      6         1\n    2      1      2      7        10\n    3      0      3      8         3\n    4      1      4      9        10\n</code></pre> <p>Filter based on different criteria depending on the first condition</p> <pre><code>&gt;&gt;&gt; q = QueryBuilder()\n&gt;&gt;&gt; q = q[where(q[\"col1\"] == 0, q[\"col2\"] == 0, q[\"col3\"] == 9)]\n&gt;&gt;&gt; lib.read(\"sym\", query_builder=q).data\n        col1   col2   col3\n    0      0      0      5\n    1      1      4      9\n</code></pre>"},{"location":"api/query_stats/","title":"Query Statistics API","text":"<p>This page documents the <code>arcticdb.toolbox.query_stats</code> module. This module provides utilities for collecting query statistics in ArcticDB.</p> <p>Warning: This API is unstable and not governed by ArcticDB's semantic versioning. It may change or be removed in future versions without notice.</p>"},{"location":"api/query_stats/#arcticdb.toolbox.query_stats","title":"arcticdb.toolbox.query_stats","text":"<p>Query Statistics API for ArcticDB.</p> <p>This module provides utilities for collecting query statistics.</p> Notes <p>Warning</p> <p>This API is unstable and not governed by ArcticDB's semantic versioning. It may change or be removed in future versions without notice.</p> FUNCTION DESCRIPTION <code>disable</code> <p>Disable query statistics collection.</p> <code>enable</code> <p>Enable query statistics collection.</p> <code>get_query_stats</code> <p>Get collected query statistics.</p> <code>query_stats</code> <p>Context manager for enabling query statistics collection within a specific scope.</p> <code>reset_stats</code> <p>Reset all collected query statistics.</p>"},{"location":"api/query_stats/#arcticdb.toolbox.query_stats.disable","title":"disable","text":"<pre><code>disable() -&gt; None\n</code></pre> <p>Disable query statistics collection.</p> <p>Stops collecting statistics for subsequent operations. Previously collected statistics remain available via get_query_stats().</p> Notes <p>Warning</p> <p>This API is unstable and not governed by semantic versioning.</p>"},{"location":"api/query_stats/#arcticdb.toolbox.query_stats.enable","title":"enable","text":"<pre><code>enable() -&gt; None\n</code></pre> <p>Enable query statistics collection.</p> <p>Once enabled, statistics will be collected for operations performed until disable() is called or the context manager exits.</p> Notes <p>Warning</p> <p>This API is unstable and not governed by semantic versioning.</p>"},{"location":"api/query_stats/#arcticdb.toolbox.query_stats.get_query_stats","title":"get_query_stats","text":"<pre><code>get_query_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Get collected query statistics.</p> RETURNS DESCRIPTION <code>Dict[str, Any]:</code> <p>A dictionary containing statistics organized by key type, operation group, and task type. Each task contains timing and count information. Example output: {     \"storage_operations\": {         \"S3_ListObjectsV2\": {             \"total_time_ms\": 83,             \"count\": 3         },         \"S3_GetObject\": {             \"total_time_ms\": 50,             \"count\": 3,             \"size_bytes\": 10         }     } }</p> Notes <p>Warning</p> <p>This API is unstable and not governed by semantic versioning.</p>"},{"location":"api/query_stats/#arcticdb.toolbox.query_stats.query_stats","title":"query_stats","text":"<pre><code>query_stats() -&gt; Iterator[None]\n</code></pre> <p>Context manager for enabling query statistics collection within a specific scope.</p> <p>When entering the context, query statistics collection is enabled. When exiting the context, it is automatically disabled.</p> RAISES DESCRIPTION <code>UserInputException</code> <p>If query stats is already enabled.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with query_stats():\n...     store.list_symbols()\n</code></pre> Notes <p>Warning</p> <p>This API is unstable and not governed by semantic versioning.</p>"},{"location":"api/query_stats/#arcticdb.toolbox.query_stats.reset_stats","title":"reset_stats","text":"<pre><code>reset_stats() -&gt; None\n</code></pre> <p>Reset all collected query statistics.</p> <p>This clears all statistics that have been collected since enabling the query statistics collection.</p> Notes <p>Warning</p> <p>This API is unstable and not governed by semantic versioning.</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/","title":"AWS Blockchain Notebook","text":"In\u00a0[\u00a0]: Copied! <pre># s3fs is used by pandas.read_parquet('s3://...')\n%pip install arcticdb boto3 tqdm s3fs fastparquet\n</pre> # s3fs is used by pandas.read_parquet('s3://...') %pip install arcticdb boto3 tqdm s3fs fastparquet In\u00a0[2]: Copied! <pre>import os\nfrom uuid import uuid4\nfrom datetime import timedelta, datetime\nfrom tqdm import tqdm\nimport boto3\nimport numpy as np\nimport pandas as pd\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\nimport arcticdb as adb\nfrom google.colab import drive, userdata\n</pre> import os from uuid import uuid4 from datetime import timedelta, datetime from tqdm import tqdm import boto3 import numpy as np import pandas as pd from botocore import UNSIGNED from botocore.client import Config import arcticdb as adb from google.colab import drive, userdata In\u00a0[3]: Copied! <pre># mount Google Drive for the config file to live on\ndrive.mount('/content/drive')\npath = '/content/drive/MyDrive/config/awscli.ini'\nos.environ['AWS_SHARED_CREDENTIALS_FILE'] = path\n</pre> # mount Google Drive for the config file to live on drive.mount('/content/drive') path = '/content/drive/MyDrive/config/awscli.ini' os.environ['AWS_SHARED_CREDENTIALS_FILE'] = path <pre>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n</pre> In\u00a0[4]: Copied! <pre>check = boto3.session.Session()\nno_config = check.get_credentials() is None or check.region_name is None\n\nif no_config:\n    print('*'*40)\n    print('Setup your AWS S3 credentials and region before continuing.')\n    print('https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html')\n    print('*'*40)\n</pre> check = boto3.session.Session() no_config = check.get_credentials() is None or check.region_name is None  if no_config:     print('*'*40)     print('Setup your AWS S3 credentials and region before continuing.')     print('https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html')     print('*'*40) In\u00a0[5]: Copied! <pre>aws_access_key = \"my_access_key\"\naws_secret_access_key = \"my_secret_access_key\"\nregion = \"my_region\"\n\nconfig_text = f\"\"\"\n[default]\naws_access_key_id = {aws_access_key}\naws_secret_access_key = {aws_secret_access_key}\nregion = {region}\n\"\"\"\n\nwrite_aws_config_file = False\nif write_aws_config_file:\n    with open(path, 'w') as f:\n        f.write(text)\n</pre> aws_access_key = \"my_access_key\" aws_secret_access_key = \"my_secret_access_key\" region = \"my_region\"  config_text = f\"\"\" [default] aws_access_key_id = {aws_access_key} aws_secret_access_key = {aws_secret_access_key} region = {region} \"\"\"  write_aws_config_file = False if write_aws_config_file:     with open(path, 'w') as f:         f.write(text) In\u00a0[6]: Copied! <pre>s3 = boto3.resource('s3')\nregion = boto3.session.Session().region_name\n\nbucket = [b for b in s3.buckets.all() if b.name.startswith('arcticdb-data-')]\n\nif bucket:\n    bucket_name = bucket[0].name\n    print('Bucket found:', bucket_name)\nelse:\n    bucket_name = f'arcticdb-data-{uuid4()}'\n    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint':region})\n    print('Bucket created:', bucket_name)\n</pre> s3 = boto3.resource('s3') region = boto3.session.Session().region_name  bucket = [b for b in s3.buckets.all() if b.name.startswith('arcticdb-data-')]  if bucket:     bucket_name = bucket[0].name     print('Bucket found:', bucket_name) else:     bucket_name = f'arcticdb-data-{uuid4()}'     s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint':region})     print('Bucket created:', bucket_name) <pre>Bucket found: arcticdb-data-bda6914b-2715-4acd-8b52-fa593af295bd\n</pre> In\u00a0[7]: Copied! <pre># create an arcticdb instance in the bucket\narctic = adb.Arctic(f's3://s3.{region}.amazonaws.com:{bucket_name}?aws_auth=true')\n\nif 'btc' not in arctic.list_libraries():\n    # library does not already exist\n    arctic.create_library('btc', library_options=adb.LibraryOptions(dynamic_schema=True))\nlibrary = arctic.get_library('btc')\nlibrary\n</pre> # create an arcticdb instance in the bucket arctic = adb.Arctic(f's3://s3.{region}.amazonaws.com:{bucket_name}?aws_auth=true')  if 'btc' not in arctic.list_libraries():     # library does not already exist     arctic.create_library('btc', library_options=adb.LibraryOptions(dynamic_schema=True)) library = arctic.get_library('btc') library Out[7]: <pre>Library(Arctic(config=S3(endpoint=s3.eu-north-1.amazonaws.com, bucket=arcticdb-data-bda6914b-2715-4acd-8b52-fa593af295bd)), path=btc, storage=s3_storage)</pre> In\u00a0[8]: Copied! <pre># create the list of all btc blockchain files\nbucket = s3.Bucket('aws-public-blockchain')\nobjects = bucket.objects.filter(Prefix='v1.0/btc/transactions/')\nfiles = pd.DataFrame({'path': [obj.key for obj in objects]})\n</pre> # create the list of all btc blockchain files bucket = s3.Bucket('aws-public-blockchain') objects = bucket.objects.filter(Prefix='v1.0/btc/transactions/') files = pd.DataFrame({'path': [obj.key for obj in objects]}) In\u00a0[9]: Copied! <pre># filter only the 2023-06 files to keep run time manageable\nfiles_mask = files['path'].str.contains('2023-06')\nto_load = files[files_mask]['path']\nprint(f\"Identified {len(to_load)} / {len(files)} files for processing\")\n</pre> # filter only the 2023-06 files to keep run time manageable files_mask = files['path'].str.contains('2023-06') to_load = files[files_mask]['path'] print(f\"Identified {len(to_load)} / {len(files)} files for processing\") <pre>Identified 30 / 5506 files for processing\n</pre> In\u00a0[10]: Copied! <pre>%%time\ndf_list = []\nfor path in tqdm(to_load):\n    one_day_df = pd.read_parquet('s3://aws-public-blockchain/'+path,\n                                 storage_options={\"anon\": True},\n                                 engine='fastparquet')\n    # fixup types from source data\n    one_day_df['hash'] =  one_day_df['hash'].astype(str)\n    one_day_df['block_hash'] = one_day_df['block_hash'].astype(str)\n    one_day_df['outputs'] = one_day_df['outputs'].astype(str)\n    one_day_df['date'] = pd.to_datetime(one_day_df['date'], unit='ns')\n    if 'inputs' in one_day_df.columns:\n        one_day_df['inputs'] = one_day_df['inputs'].astype(str)\n    # index on timestamp\n    one_day_df.set_index('block_timestamp', inplace=True)\n    one_day_df.sort_index(inplace=True)\n    df_list.append(one_day_df)\ndf_aws = pd.concat(df_list).sort_index()\nprint(f\"Read and assembled {len(df_aws)} transaction records from AWS\")\n# release the list to enable garbage collection\ndf_list = None\n</pre> %%time df_list = [] for path in tqdm(to_load):     one_day_df = pd.read_parquet('s3://aws-public-blockchain/'+path,                                  storage_options={\"anon\": True},                                  engine='fastparquet')     # fixup types from source data     one_day_df['hash'] =  one_day_df['hash'].astype(str)     one_day_df['block_hash'] = one_day_df['block_hash'].astype(str)     one_day_df['outputs'] = one_day_df['outputs'].astype(str)     one_day_df['date'] = pd.to_datetime(one_day_df['date'], unit='ns')     if 'inputs' in one_day_df.columns:         one_day_df['inputs'] = one_day_df['inputs'].astype(str)     # index on timestamp     one_day_df.set_index('block_timestamp', inplace=True)     one_day_df.sort_index(inplace=True)     df_list.append(one_day_df) df_aws = pd.concat(df_list).sort_index() print(f\"Read and assembled {len(df_aws)} transaction records from AWS\") # release the list to enable garbage collection df_list = None <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [02:50&lt;00:00,  5.67s/it]\n</pre> <pre>Read and assembled 12147125 transaction records from AWS\nCPU times: user 31.8 s, sys: 9.58 s, total: 41.4 s\nWall time: 2min 59s\n</pre> In\u00a0[11]: Copied! <pre>%%time\nlibrary.write('transactions', df_aws)\n</pre> %%time library.write('transactions', df_aws) <pre>CPU times: user 27.2 s, sys: 4.47 s, total: 31.6 s\nWall time: 45.3 s\n</pre> Out[11]: <pre>VersionedItem(symbol='transactions', library='btc', data=n/a, version=1487, metadata=None, host='S3(endpoint=s3.eu-north-1.amazonaws.com, bucket=arcticdb-data-bda6914b-2715-4acd-8b52-fa593af295bd)')</pre> In\u00a0[12]: Copied! <pre>%%time\nplot_start = datetime(2023, 6, 3, 0, 0)\nplot_end = plot_start + timedelta(days=25)\ndf = library.read('transactions', date_range=(plot_start, plot_end)).data\nprint(len(df))\n</pre> %%time plot_start = datetime(2023, 6, 3, 0, 0) plot_end = plot_start + timedelta(days=25) df = library.read('transactions', date_range=(plot_start, plot_end)).data print(len(df)) <pre>10097090\nCPU times: user 4.31 s, sys: 3.19 s, total: 7.51 s\nWall time: 21.1 s\n</pre> In\u00a0[13]: Copied! <pre>fees_per_day = df.groupby(pd.Grouper(freq='1D')).sum(numeric_only=True)\nt = f\"BTC Blockchain: Total fees per pay from {plot_start} to {plot_end}\"\nax = fees_per_day.plot(kind='bar', y='fee', color='red', figsize=(14, 6), title=t)\nax.figure.autofmt_xdate(rotation=60)\n</pre> fees_per_day = df.groupby(pd.Grouper(freq='1D')).sum(numeric_only=True) t = f\"BTC Blockchain: Total fees per pay from {plot_start} to {plot_end}\" ax = fees_per_day.plot(kind='bar', y='fee', color='red', figsize=(14, 6), title=t) ax.figure.autofmt_xdate(rotation=60) In\u00a0[13]: Copied! <pre>\n</pre>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#loading-aws-bitcoin-blockchain-data-into-arcticdb-using-aws-as-storage","title":"Loading AWS Bitcoin blockchain data into ArcticDB, using AWS as storage\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#in-this-demo-we-illustrate-how-to-use-aws-with-arcticdb-we-are-going-to","title":"In this demo, we illustrate how to use AWS with ArcticDB. We are going to\u00b6","text":"<ul> <li>Set up AWS access</li> <li>Initialise ArcticDB with AWS as storage</li> <li>Read a section of the Bitcoin blockchain from an AWS public dataset</li> <li>Store the data in ArcticDB</li> <li>Read the data back</li> <li>Perform a simple analysis on the data</li> </ul> <p>Note: This is set up to run on Google colab. It will need some simple changes to remove the Google drive code to run in other enviroments</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#install-arcticdb-and-s3-libraries","title":"Install ArcticDB and S3 libraries\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#read-or-create-aws-config","title":"Read or Create AWS config\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#create-a-config-file","title":"Create a config file\u00b6","text":"<ul> <li>You should only need to run this section once</li> <li>Enter your AWS details below and change <code>write_aws_config_file</code> to True</li> <li>Future runs can pick up the config file you have save in your Drive</li> </ul>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#set-up-the-aws-bucket","title":"Set up the AWS bucket\u00b6","text":"<ul> <li>First check for existing buckets to use</li> <li>Set up a new bucket if there are no suitable existing ones</li> </ul>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#initialise-arcticdb","title":"Initialise ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#mark-the-btc-blockchain-data-from-june-2023-for-processing","title":"Mark the BTC blockchain data from June 2023 for processing\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#read-the-data-from-an-aws-public-dataset-as-parquet-files","title":"Read the data from an AWS public dataset as parquet files\u00b6","text":"<p>This takes some time to run, like 5 to 10 mins</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#write-the-data-to-arcticdb","title":"Write the data to ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#read-the-data-from-arcticdb","title":"Read the data from ArcticDB\u00b6","text":"<p>This read also applies a date_range filter to get 25 days of data from 3 June</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#chart-the-transaction-fees-per-day","title":"Chart the transaction fees per day\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>We have give a simple recipe for using ArcticDB with AWS for storage</li> <li>We have demonstrated that ArcticDB is significantly faster than Parquet files</li> <li>We have shown how to read a subset of dates from a symbol of timeseries data</li> <li>If we saved a much larger section of the blockchain, it could still be stored in one symbol and subset chunks read efficiently</li> <li>Feel free to play around with the notebook to read a larger set of data</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/","title":"1 Billion Row Challenge Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[\u00a0]: Copied! <pre>from arcticdb.config import set_config_int\nset_config_int('VersionStore.NumCPUThreads', 16)\n</pre> from arcticdb.config import set_config_int set_config_int('VersionStore.NumCPUThreads', 16) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport arcticdb as adb\n</pre> import pandas as pd import numpy as np import arcticdb as adb In\u00a0[3]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_brc\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_brc\") In\u00a0[\u00a0]: Copied! <pre>sym_1brc = 'weather_stations_1brc'\nnum_cities = 10_000\nnum_rows = 1_000_000_000\naggs = {\n    'max': ('Temperature', 'max'), \n    'min': ('Temperature', 'min'),\n    'mean': ('Temperature', 'mean')\n}\nnum_blocks = 16\nblock_size = num_rows // num_blocks\nseed = 17\ncities = np.array([f\"city_{i:04d}\" for i in range(num_cities)])\nprint(f\"block_size: {block_size:,d}, total records: {block_size*num_blocks:,d}\")\n</pre> sym_1brc = 'weather_stations_1brc' num_cities = 10_000 num_rows = 1_000_000_000 aggs = {     'max': ('Temperature', 'max'),      'min': ('Temperature', 'min'),     'mean': ('Temperature', 'mean') } num_blocks = 16 block_size = num_rows // num_blocks seed = 17 cities = np.array([f\"city_{i:04d}\" for i in range(num_cities)]) print(f\"block_size: {block_size:,d}, total records: {block_size*num_blocks:,d}\") In\u00a0[17]: Copied! <pre>lib_name = 'arcticdb_brc'\n# delete the library if it already exists\narctic.delete_library(lib_name)\n# performance tuning: a large rows_per_segment value can improve performance for dataframes with a large number of rows\nlib_options = adb.LibraryOptions(rows_per_segment=10_000_000)\nlib = arctic.get_library(lib_name, create_if_missing=True, library_options=lib_options)\n</pre> lib_name = 'arcticdb_brc' # delete the library if it already exists arctic.delete_library(lib_name) # performance tuning: a large rows_per_segment value can improve performance for dataframes with a large number of rows lib_options = adb.LibraryOptions(rows_per_segment=10_000_000) lib = arctic.get_library(lib_name, create_if_missing=True, library_options=lib_options) In\u00a0[18]: Copied! <pre>def create_block_df(rng, cities, block_size):\n    random_cities = rng.choice(cities, size=block_size)\n    random_temperatures = np.round(rng.uniform(-99.9, 99.9, size=block_size), 4)\n    return pd.DataFrame({'City': random_cities, 'Temperature': random_temperatures})\n</pre> def create_block_df(rng, cities, block_size):     random_cities = rng.choice(cities, size=block_size)     random_temperatures = np.round(rng.uniform(-99.9, 99.9, size=block_size), 4)     return pd.DataFrame({'City': random_cities, 'Temperature': random_temperatures}) In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng(seed)\nprint('Writing blocks: ', end='')\nfor b in range(num_blocks):\n    block_df = create_block_df(rng, cities, block_size)\n    if b==0:\n        lib.write(sym_1brc, block_df)\n    else:\n        lib.append(sym_1brc, block_df, validate_index=False)\n    print(f'{b}, ', end='')\nprint(' Finished')\n</pre> rng = np.random.default_rng(seed) print('Writing blocks: ', end='') for b in range(num_blocks):     block_df = create_block_df(rng, cities, block_size)     if b==0:         lib.write(sym_1brc, block_df)     else:         lib.append(sym_1brc, block_df, validate_index=False)     print(f'{b}, ', end='') print(' Finished') In\u00a0[20]: Copied! <pre>%%timeit\n# this runs the query several times to get an accurate timing\nlazy_df = lib.read(sym_1brc, lazy=True)\nlazy_df.groupby('City').agg(aggs)\nlazy_df.collect()\n</pre> %%timeit # this runs the query several times to get an accurate timing lazy_df = lib.read(sym_1brc, lazy=True) lazy_df.groupby('City').agg(aggs) lazy_df.collect() <pre>7.01 s \u00b1 604 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[22]: Copied! <pre># run the query once more to see the output\nlib.read(sym_1brc, lazy=True).groupby('City').agg(aggs).collect().data.sort_index().round(1)\n</pre> # run the query once more to see the output lib.read(sym_1brc, lazy=True).groupby('City').agg(aggs).collect().data.sort_index().round(1) Out[22]: min mean max City city_0000 -99.9 0.0 99.8 city_0001 -99.9 0.2 99.9 city_0002 -99.7 0.2 99.9 city_0003 -99.9 0.1 99.9 city_0004 -99.8 -0.2 99.9 ... ... ... ... city_9995 -99.9 -0.4 99.9 city_9996 -99.9 0.1 99.9 city_9997 -99.9 0.7 99.9 city_9998 -99.9 -1.3 99.9 city_9999 -99.8 -0.2 99.9 <p>10000 rows \u00d7 3 columns</p> In\u00a0[23]: Copied! <pre># we need to aggregate sum and count to get the aggregated mean\naggs_chunked = {\n    'max': ('Temperature', 'max'), \n    'min': ('Temperature', 'min'),\n    'sum': ('Temperature', 'sum'),\n    'count': ('Temperature', 'count')\n}\n\n# define a list of ReadRequests - the chunks are based on row_ranges\nread_requests = [adb.ReadRequest(symbol=sym_1brc, \n                                 row_range=(block_size*b, block_size*(b+1)))\n                 for b in range(num_blocks)\n                ]\n</pre> # we need to aggregate sum and count to get the aggregated mean aggs_chunked = {     'max': ('Temperature', 'max'),      'min': ('Temperature', 'min'),     'sum': ('Temperature', 'sum'),     'count': ('Temperature', 'count') }  # define a list of ReadRequests - the chunks are based on row_ranges read_requests = [adb.ReadRequest(symbol=sym_1brc,                                   row_range=(block_size*b, block_size*(b+1)))                  for b in range(num_blocks)                 ] In\u00a0[24]: Copied! <pre># these functions merge the results of the chunks into one result\ndef merge_results_pair(r0, r1):\n    join_r = r0.join(r1, lsuffix='_0', rsuffix='_1')\n    return pd.DataFrame(index=join_r.index,\n                        data={\n                            'min': join_r[['min_0', 'min_1']].min(axis=1),\n                            'max': join_r[['max_0', 'max_1']].max(axis=1),\n                            'count': join_r[['count_0', 'count_1']].sum(axis=1),\n                            'sum': join_r[['sum_0', 'sum_1']].sum(axis=1),\n                        }\n                       )\n\ndef merge_results(r):\n    res = r[0].data.sort_index()\n    for b in range(1, len(r)):\n        next_res = r[b].data.sort_index()\n        res = merge_results_pair(res, next_res)\n    res['mean'] = res['sum'] / res['count']\n    res = res.drop(columns=['sum', 'count']).loc[:, ['min', 'mean', 'max']].round(1)\n    return res\n</pre> # these functions merge the results of the chunks into one result def merge_results_pair(r0, r1):     join_r = r0.join(r1, lsuffix='_0', rsuffix='_1')     return pd.DataFrame(index=join_r.index,                         data={                             'min': join_r[['min_0', 'min_1']].min(axis=1),                             'max': join_r[['max_0', 'max_1']].max(axis=1),                             'count': join_r[['count_0', 'count_1']].sum(axis=1),                             'sum': join_r[['sum_0', 'sum_1']].sum(axis=1),                         }                        )  def merge_results(r):     res = r[0].data.sort_index()     for b in range(1, len(r)):         next_res = r[b].data.sort_index()         res = merge_results_pair(res, next_res)     res['mean'] = res['sum'] / res['count']     res = res.drop(columns=['sum', 'count']).loc[:, ['min', 'mean', 'max']].round(1)     return res In\u00a0[25]: Copied! <pre>lazy_df_collection = lib.read_batch(read_requests, lazy=True)\n# Apply the same processing to each chunk\nlazy_df_collection.groupby('City').agg(aggs_chunked)\nread_results = lazy_df_collection.collect()\nresults = merge_results(read_results)\nresults\n</pre> lazy_df_collection = lib.read_batch(read_requests, lazy=True) # Apply the same processing to each chunk lazy_df_collection.groupby('City').agg(aggs_chunked) read_results = lazy_df_collection.collect() results = merge_results(read_results) results Out[25]: min mean max City city_0000 -99.9 0.0 99.8 city_0001 -99.9 0.2 99.9 city_0002 -99.7 0.2 99.9 city_0003 -99.9 0.1 99.9 city_0004 -99.8 -0.2 99.9 ... ... ... ... city_9995 -99.9 -0.4 99.9 city_9996 -99.9 0.1 99.9 city_9997 -99.9 0.7 99.9 city_9998 -99.9 -1.3 99.9 city_9999 -99.8 -0.2 99.9 <p>10000 rows \u00d7 3 columns</p>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#arcticdb-billion-row-challenge-notebook","title":"ArcticDB Billion Row Challenge Notebook\u00b6","text":""},{"location":"notebooks/ArcticDB_billion_row_challenge/#setup","title":"Setup\u00b6","text":"<ul> <li>installs</li> <li>imports</li> <li>create ArticDB object store</li> <li>define the parameters of the problem</li> <li>create an ArticDB library to hold the data</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#write-the-data-to-arcticdb","title":"Write the Data to ArcticDB\u00b6","text":"<ul> <li>Generate the data: each row has a city chosen at random from the list and a random temperature between -99.9 and 99.9</li> <li>Data is written in blocks to control memory usage</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#read-and-aggregate-the-data","title":"Read and Aggregate the Data\u00b6","text":"<ul> <li>Uses the DataFrame processing operations in ArcticDb to group and aggregate the data</li> <li>This allows the performant multi-threaded C++ layer to do the heavy lifting</li> <li>This code uses too much memory to run on the free Google colab. The chunked version below will work</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>It was easy to solve the 1 billion row challenge using ArticDB</li> <li>The code is short and easy to read</li> <li>Almost no tuning was needed to get good performance</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#bonus-chunked-read-and-aggregate","title":"Bonus: Chunked Read and Aggregate\u00b6","text":"<ul> <li>This version reads and aggregates in chunks</li> <li>It has the same end result as the simpler version above</li> <li>It performs almost as well and needs less memory</li> <li>In particular, it will run within the memory on the free version of Google colab</li> </ul>"},{"location":"notebooks/ArcticDB_demo_concat/","title":"Symbol Concat Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport arcticdb as adb\n</pre> import numpy as np import pandas as pd import arcticdb as adb In\u00a0[2]: Copied! <pre># object store\narctic = adb.Arctic(\"lmdb://arcticdb_concat\")\n</pre> # object store arctic = adb.Arctic(\"lmdb://arcticdb_concat\") In\u00a0[3]: Copied! <pre># library\nlib = arctic.get_library('concat', create_if_missing=True)\n</pre> # library lib = arctic.get_library('concat', create_if_missing=True) In\u00a0[4]: Copied! <pre># data for concatenating\nnum_symbols = 3\nrows_per_symbol = 10_000_000\ndfs = []\nfor _ in range(num_symbols):\n    int_data = np.arange(rows_per_symbol, dtype=np.int64)\n    float_data = np.round(np.random.uniform(95., 105., rows_per_symbol), 3)\n    letters = ['a','b','c','d','e','f','g']\n    dfs.append(pd.DataFrame(\n        {\n            \"int\": int_data,\n            \"float\": float_data,\n            \"string\": (letters*(rows_per_symbol//len(letters) + 1))[:rows_per_symbol]\n        }\n    ))\n</pre> # data for concatenating num_symbols = 3 rows_per_symbol = 10_000_000 dfs = [] for _ in range(num_symbols):     int_data = np.arange(rows_per_symbol, dtype=np.int64)     float_data = np.round(np.random.uniform(95., 105., rows_per_symbol), 3)     letters = ['a','b','c','d','e','f','g']     dfs.append(pd.DataFrame(         {             \"int\": int_data,             \"float\": float_data,             \"string\": (letters*(rows_per_symbol//len(letters) + 1))[:rows_per_symbol]         }     )) In\u00a0[5]: Copied! <pre># view the first 10 rows of one of the dataframes\ndfs[0].head(10)\n</pre> # view the first 10 rows of one of the dataframes dfs[0].head(10) Out[5]: int float string 0 0 104.832 a 1 1 102.837 b 2 2 99.950 c 3 3 96.403 d 4 4 97.400 e 5 5 96.681 f 6 6 103.064 g 7 7 103.962 a 8 8 96.208 b 9 9 103.464 c In\u00a0[6]: Copied! <pre>symbols = [f\"sym_{idx}\" for idx in range(num_symbols)]\nwrite_payloads = []\nfor idx in range(num_symbols):\n    write_payloads.append(adb.WritePayload(symbols[idx], dfs[idx], {\"my_metadata\": idx}))\nlib.write_batch(write_payloads)\n</pre> symbols = [f\"sym_{idx}\" for idx in range(num_symbols)] write_payloads = [] for idx in range(num_symbols):     write_payloads.append(adb.WritePayload(symbols[idx], dfs[idx], {\"my_metadata\": idx})) lib.write_batch(write_payloads) Out[6]: <pre>[VersionedItem(symbol='sym_0', library='concat', data=n/a, version=0, metadata={'my_metadata': 0}, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838164380813504),\n VersionedItem(symbol='sym_1', library='concat', data=n/a, version=0, metadata={'my_metadata': 1}, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838164360824925),\n VersionedItem(symbol='sym_2', library='concat', data=n/a, version=0, metadata={'my_metadata': 2}, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838164349127827)]</pre> In\u00a0[7]: Copied! <pre>lazy_dfs = lib.read_batch(symbols, lazy=True)\nlazy_df = adb.concat(lazy_dfs)\nresult = lazy_df.collect()\ntype(result)\n</pre> lazy_dfs = lib.read_batch(symbols, lazy=True) lazy_df = adb.concat(lazy_dfs) result = lazy_df.collect() type(result) Out[7]: <pre>arcticdb.version_store._store.VersionedItemWithJoin</pre> <p>The resulting object contains the version information and metadata associated with the symbol-version pairs that have been concatenated together</p> In\u00a0[8]: Copied! <pre>result.versions[1]\n</pre> result.versions[1] Out[8]: <pre>VersionedItem(symbol='sym_1', library='concat', data=n/a, version=0, metadata={'my_metadata': 1}, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838164360824925)</pre> <p>The <code>data</code> field of the returned object contains the concatenated dataframe (note there are 30 million rows in the output)</p> In\u00a0[9]: Copied! <pre>result.data\n</pre> result.data Out[9]: int float string 0 0 104.832 a 1 1 102.837 b 2 2 99.950 c 3 3 96.403 d 4 4 97.400 e ... ... ... ... 29999995 9999995 97.799 f 29999996 9999996 97.431 g 29999997 9999997 99.988 a 29999998 9999998 96.992 b 29999999 9999999 98.456 c <p>30000000 rows \u00d7 3 columns</p> In\u00a0[10]: Copied! <pre>lazy_dfs = lib.read_batch(symbols, lazy=True)\nlazy_dfs = lazy_dfs[lazy_dfs[\"string\"] != \"a\"]\nlazy_df = adb.concat(lazy_dfs)\nlazy_df[\"new_col\"] = lazy_df[\"float\"] * 2\nlazy_df.collect().data\n</pre> lazy_dfs = lib.read_batch(symbols, lazy=True) lazy_dfs = lazy_dfs[lazy_dfs[\"string\"] != \"a\"] lazy_df = adb.concat(lazy_dfs) lazy_df[\"new_col\"] = lazy_df[\"float\"] * 2 lazy_df.collect().data Out[10]: int float string new_col 0 1 102.837 b 205.674 1 2 99.950 c 199.900 2 3 96.403 d 192.806 3 4 97.400 e 194.800 4 5 96.681 f 193.362 ... ... ... ... ... 25714279 9999994 96.815 e 193.630 25714280 9999995 97.799 f 195.598 25714281 9999996 97.431 g 194.862 25714282 9999998 96.992 b 193.984 25714283 9999999 98.456 c 196.912 <p>25714284 rows \u00d7 4 columns</p> In\u00a0[11]: Copied! <pre>lib.write(\"inner_0\", pd.DataFrame({\"a\": [0], \"b\": [0], \"c\": [0], \"d\": [0]}))\nlib.write(\"inner_1\", pd.DataFrame({\"c\": [1], \"b\": [1], \"e\": [1]}))\nlib.write(\"inner_2\", pd.DataFrame({\"f\": [2], \"b\": [2], \"c\": [2], \"d\": [0]}))\nlazy_dfs = lib.read_batch([\"inner_0\", \"inner_1\", \"inner_2\"], lazy=True)\nadb.concat(lazy_dfs, join=\"inner\").collect().data\n</pre> lib.write(\"inner_0\", pd.DataFrame({\"a\": [0], \"b\": [0], \"c\": [0], \"d\": [0]})) lib.write(\"inner_1\", pd.DataFrame({\"c\": [1], \"b\": [1], \"e\": [1]})) lib.write(\"inner_2\", pd.DataFrame({\"f\": [2], \"b\": [2], \"c\": [2], \"d\": [0]})) lazy_dfs = lib.read_batch([\"inner_0\", \"inner_1\", \"inner_2\"], lazy=True) adb.concat(lazy_dfs, join=\"inner\").collect().data Out[11]: b c 0 0 0 1 1 1 2 2 2 In\u00a0[12]: Copied! <pre>lib.write(\"type_promotion_uint16\", pd.DataFrame({\"a\": np.arange(0, 1, dtype=np.uint16)}))\nlib.write(\"type_promotion_int16\", pd.DataFrame({\"a\": np.arange(3, 4, dtype=np.int16)}))\nlib.write(\"type_promotion_int64\", pd.DataFrame({\"a\": np.arange(4, 5, dtype=np.int64)}))\nlib.write(\"type_promotion_float64\", pd.DataFrame({\"a\": np.arange(4, 5, dtype=np.float64)}))\nlib.write(\"type_promotion_str\", pd.DataFrame({\"a\": [\"hello\"]}))\n</pre> lib.write(\"type_promotion_uint16\", pd.DataFrame({\"a\": np.arange(0, 1, dtype=np.uint16)})) lib.write(\"type_promotion_int16\", pd.DataFrame({\"a\": np.arange(3, 4, dtype=np.int16)})) lib.write(\"type_promotion_int64\", pd.DataFrame({\"a\": np.arange(4, 5, dtype=np.int64)})) lib.write(\"type_promotion_float64\", pd.DataFrame({\"a\": np.arange(4, 5, dtype=np.float64)})) lib.write(\"type_promotion_str\", pd.DataFrame({\"a\": [\"hello\"]})) Out[12]: <pre>VersionedItem(symbol='type_promotion_str', library='concat', data=n/a, version=0, metadata=None, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838174944854293)</pre> In\u00a0[13]: Copied! <pre># int32 can represent all of the values in both the uint16 and int16 dtypes\nlazy_dfs = lib.read_batch([\"type_promotion_uint16\", \"type_promotion_int16\"], lazy=True)\nadb.concat(lazy_dfs).collect().data[\"a\"].dtype\n</pre> # int32 can represent all of the values in both the uint16 and int16 dtypes lazy_dfs = lib.read_batch([\"type_promotion_uint16\", \"type_promotion_int16\"], lazy=True) adb.concat(lazy_dfs).collect().data[\"a\"].dtype Out[13]: <pre>dtype('int32')</pre> In\u00a0[14]: Copied! <pre># float64 cannot represent all of the values in the int64 dtype, but it is the best we can do\nlazy_dfs = lib.read_batch([\"type_promotion_int64\", \"type_promotion_float64\"], lazy=True)\nadb.concat(lazy_dfs).collect().data[\"a\"].dtype\n</pre> # float64 cannot represent all of the values in the int64 dtype, but it is the best we can do lazy_dfs = lib.read_batch([\"type_promotion_int64\", \"type_promotion_float64\"], lazy=True) adb.concat(lazy_dfs).collect().data[\"a\"].dtype Out[14]: <pre>dtype('float64')</pre> In\u00a0[15]: Copied! <pre># There is no sensible type we can use to combine string and numeric columns, so we throw in this case\nlazy_dfs = lib.read_batch([\"type_promotion_int64\", \"type_promotion_str\"], lazy=True)\ntry:\n    adb.concat(lazy_dfs).collect().data[\"a\"].dtype\nexcept adb.exceptions.SchemaException as e:\n    print(e)\n</pre> # There is no sensible type we can use to combine string and numeric columns, so we throw in this case lazy_dfs = lib.read_batch([\"type_promotion_int64\", \"type_promotion_str\"], lazy=True) try:     adb.concat(lazy_dfs).collect().data[\"a\"].dtype except adb.exceptions.SchemaException as e:     print(e) <pre>E_DESCRIPTOR_MISMATCH No common type between INT64 and UTF_DYNAMIC64 when joining schemas\n</pre> In\u00a0[16]: Copied! <pre>lib.write(\"missing_columns_0\", pd.DataFrame({\"string\": [\"hello\"], \"float\": [0.5], \"int\": [0]}))\nlib.write(\"missing_columns_1\", pd.DataFrame({\"another string\": [\"goodbye\"], \"another float\": [1.5], \"another int\": [1]}))\nlazy_dfs = lib.read_batch([\"missing_columns_0\", \"missing_columns_1\"], lazy=True)\nadb.concat(lazy_dfs, join=\"outer\").collect().data\n</pre> lib.write(\"missing_columns_0\", pd.DataFrame({\"string\": [\"hello\"], \"float\": [0.5], \"int\": [0]})) lib.write(\"missing_columns_1\", pd.DataFrame({\"another string\": [\"goodbye\"], \"another float\": [1.5], \"another int\": [1]})) lazy_dfs = lib.read_batch([\"missing_columns_0\", \"missing_columns_1\"], lazy=True) adb.concat(lazy_dfs, join=\"outer\").collect().data Out[16]: string float int another string another float another int 0 hello 0.5 0 None NaN 0 1 None NaN 0 goodbye 1.5 1 In\u00a0[17]: Copied! <pre>lib.write_batch([adb.WritePayload(\"range_indexed_0\", pd.DataFrame({\"col\": [0, 1]}, index=pd.RangeIndex(3, 7, 2))), adb.WritePayload(\"range_indexed_1\", pd.DataFrame({\"col\": [2, 3]}, index=pd.RangeIndex(0, 4, 2)))])\nadb.concat(lib.read_batch([\"range_indexed_0\", \"range_indexed_1\"], lazy=True)).collect().data\n</pre> lib.write_batch([adb.WritePayload(\"range_indexed_0\", pd.DataFrame({\"col\": [0, 1]}, index=pd.RangeIndex(3, 7, 2))), adb.WritePayload(\"range_indexed_1\", pd.DataFrame({\"col\": [2, 3]}, index=pd.RangeIndex(0, 4, 2)))]) adb.concat(lib.read_batch([\"range_indexed_0\", \"range_indexed_1\"], lazy=True)).collect().data Out[17]: col 3 0 5 1 7 2 9 3 <p>Otherwise, a warning is logged, and the resulting dataframe has a <code>RangeIndex</code> with start 0 and step 1:</p> In\u00a0[18]: Copied! <pre>lib.write_batch([adb.WritePayload(\"range_indexed_2\", pd.DataFrame({\"col\": [0, 1]}, index=pd.RangeIndex(3, 7, 2))), adb.WritePayload(\"range_indexed_3\", pd.DataFrame({\"col\": [2, 3]}, index=pd.RangeIndex(0, 6, 3)))])\nadb.concat(lib.read_batch([\"range_indexed_2\", \"range_indexed_3\"], lazy=True)).collect().data\n</pre> lib.write_batch([adb.WritePayload(\"range_indexed_2\", pd.DataFrame({\"col\": [0, 1]}, index=pd.RangeIndex(3, 7, 2))), adb.WritePayload(\"range_indexed_3\", pd.DataFrame({\"col\": [2, 3]}, index=pd.RangeIndex(0, 6, 3)))]) adb.concat(lib.read_batch([\"range_indexed_2\", \"range_indexed_3\"], lazy=True)).collect().data <pre>20250521 15:36:22.789909 2742407 W arcticdb | Mismatching RangeIndexes being combined, setting to start=0, step=1\n</pre> Out[18]: col 0 0 1 1 2 2 3 3 In\u00a0[19]: Copied! <pre>lib.write(\"series_0\", pd.Series([0], name=\"Name 0\"))\nlib.write(\"series_1\", pd.Series([1], name=\"Name 0\"))\nlib.write(\"series_2\", pd.Series([2], name=\"Name 2\"))\n</pre> lib.write(\"series_0\", pd.Series([0], name=\"Name 0\")) lib.write(\"series_1\", pd.Series([1], name=\"Name 0\")) lib.write(\"series_2\", pd.Series([2], name=\"Name 2\")) Out[19]: <pre>VersionedItem(symbol='series_2', library='concat', data=n/a, version=0, metadata=None, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838184226956243)</pre> In\u00a0[20]: Copied! <pre>lazy_dfs = lib.read_batch([\"series_0\", \"series_1\"], lazy=True)\nadb.concat(lazy_dfs).collect().data\n</pre> lazy_dfs = lib.read_batch([\"series_0\", \"series_1\"], lazy=True) adb.concat(lazy_dfs).collect().data Out[20]: <pre>0    0\n1    1\nName: Name 0, dtype: int64</pre> In\u00a0[21]: Copied! <pre>lazy_dfs = lib.read_batch([\"series_0\", \"series_1\", \"series_2\"], lazy=True)\nadb.concat(lazy_dfs).collect().data\n</pre> lazy_dfs = lib.read_batch([\"series_0\", \"series_1\", \"series_2\"], lazy=True) adb.concat(lazy_dfs).collect().data Out[21]: <pre>0    0\n1    1\n2    2\ndtype: int64</pre> In\u00a0[22]: Copied! <pre>lib.write(\"timeseries_0\", pd.DataFrame({\"col\": [0]}, index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01\")], name=\"timestamp\")))\nlib.write(\"timeseries_1\", pd.DataFrame({\"col\": [1]}, index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01\")], name=\"timestamp\")))\nlib.write(\"timeseries_2\", pd.DataFrame({\"col\": [2]}, index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01\")])))\n</pre> lib.write(\"timeseries_0\", pd.DataFrame({\"col\": [0]}, index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01\")], name=\"timestamp\"))) lib.write(\"timeseries_1\", pd.DataFrame({\"col\": [1]}, index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01\")], name=\"timestamp\"))) lib.write(\"timeseries_2\", pd.DataFrame({\"col\": [2]}, index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01\")]))) Out[22]: <pre>VersionedItem(symbol='timeseries_2', library='concat', data=n/a, version=0, metadata=None, host='LMDB(path=/users/is/aowens/source/man.arcticdb/arcticdb_link/docs/mkdocs/docs/notebooks/arcticdb_concat)', timestamp=1747838187224125234)</pre> In\u00a0[23]: Copied! <pre>lazy_dfs = lib.read_batch([\"timeseries_0\", \"timeseries_1\"], lazy=True)\nadb.concat(lazy_dfs).collect().data\n</pre> lazy_dfs = lib.read_batch([\"timeseries_0\", \"timeseries_1\"], lazy=True) adb.concat(lazy_dfs).collect().data Out[23]: col timestamp 2025-01-01 0 2025-01-01 1 In\u00a0[24]: Copied! <pre>lazy_dfs = lib.read_batch([\"timeseries_0\", \"timeseries_1\", \"timeseries_2\"], lazy=True)\nadb.concat(lazy_dfs).collect().data\n</pre> lazy_dfs = lib.read_batch([\"timeseries_0\", \"timeseries_1\", \"timeseries_2\"], lazy=True) adb.concat(lazy_dfs).collect().data Out[24]: col 2025-01-01 0 2025-01-01 1 2025-01-01 2"},{"location":"notebooks/ArcticDB_demo_concat/#arcticdb-concat-demo","title":"ArcticDB Concat Demo\u00b6","text":"<p>This demo notebook showcases the first symbol joining capability within ArcticDB, concatenation.</p> <p>This is what you need to know about it:</p> <ul> <li>It runs on-the-fly as part of the read</li> <li>The usage and results are similar to the Pandas concat function (although not identical, differences are highlighted below)</li> <li>Processing before and after the concat operation are parallelised like all other processing in ArcticDB, leading to potentially large performance improvements compared to concatenating and processing in Pandas</li> </ul>"},{"location":"notebooks/ArcticDB_demo_concat/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_concat/#create-some-data","title":"Create Some Data\u00b6","text":"<ul> <li>3 symbols, each with 10,000,000 rows</li> <li>int, float, string columns</li> <li>write the data into ArcticDB</li> </ul>"},{"location":"notebooks/ArcticDB_demo_concat/#1-simple-concat","title":"1. Simple Concat\u00b6","text":"<p>Concatenate the symbols together without any additional arguments or processing</p>"},{"location":"notebooks/ArcticDB_demo_concat/#2-concat-with-pre-and-post-processing","title":"2. Concat with pre and post processing\u00b6","text":"<p>Perform processing operations both before and after the concatenation. See the <code>LazyDataFrame</code> demo notebook for more explanation of the available operations and how to apply different pre-processing to each symbol</p>"},{"location":"notebooks/ArcticDB_demo_concat/#3-inner-joins","title":"3. Inner Joins\u00b6","text":"<ul> <li>In the examples above, all of the input dataframes have the same column names and dtypes, and no arguments have been provided to <code>adb.concat</code> beyond the list of lazy dataframes to be concatenated.</li> <li><code>adb.concat</code> takes an additional argument <code>join</code>, which should be one of <code>\"inner\"</code> or <code>\"outer\"</code> (default is <code>\"outer\"</code>), which dictates whether to take the intersection (inner) or union (outer) of columns from the input symbols.</li> <li>If the dtypes of columns being joined together do not match exactly, then type promotion to a common type will occur if such a type exists.</li> </ul>"},{"location":"notebooks/ArcticDB_demo_concat/#use-inner-join-to-retain-only-columns-present-in-all-input-symbols","title":"Use inner join to retain only columns present in all input symbols\u00b6","text":"<ul> <li>Only columns <code>b</code> and <code>c</code> are present in all of the input symbols</li> <li>The ordering of the output columns is taken from the first symbol (<code>inner_0</code> in this case)</li> </ul>"},{"location":"notebooks/ArcticDB_demo_concat/#types-are-promoted-to-a-valid-common-type-where-possible-or-an-exception-is-thrown-if-not","title":"Types are promoted to a valid common type where possible, or an exception is thrown if not\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_concat/#4-outer-joins","title":"4. Outer Joins\u00b6","text":"<ul> <li>Used to retain all columns present in any input symbols</li> <li>The same type promotion rules apply with outer joins as with inner joins</li> <li>The order of the columns in the output is according to when the column is first seen in the input. i.e. all of the columns from the first symbol (in the same order as in the first symbol) will be first, then any columns that appear in the second symbol but not in the first symbol (in the same order as they appear in the second symbol), and so on.</li> <li>Missing columns are backfilled with the same type-specific values as with dynamic schema:<ul> <li>strings: None</li> <li>floats: NaN</li> <li>integers: 0</li> </ul> </li> </ul> <p>Note that the behaviour with integers is different to Pandas, which changes the type of the column to <code>float64</code> and then fills in the missing values with NaNs</p>"},{"location":"notebooks/ArcticDB_demo_concat/#5-permissiveness","title":"5. Permissiveness\u00b6","text":"<p>Where possible, we allow joining symbols together whenever it is sensible to do so.</p>"},{"location":"notebooks/ArcticDB_demo_concat/#range-indexes","title":"Range indexes\u00b6","text":"<p>When concatenating two or more range indexed dataframes together, Pandas will convert to an <code>Int64Index</code> unless the <code>start/step/stop</code> values of the input dataframes all line up.</p> <p>However most data in ArcticDB with a <code>RangeIndex</code> isn't really indexed at all, it is just the default applied by Pandas if no explicit index is provided. In addition <code>Int64Index</code> is not an internally supported index type. Therefore, concatenating range indexed symbols together will always produce a range index in ArcticDB.</p> <p>Different step values will be respected if they match across all symbols being concatenated, with the start value being taken from the first symbol:</p>"},{"location":"notebooks/ArcticDB_demo_concat/#joining-series-together-will-maintain-the-name-if-it-is-the-same-between-all-of-the-input-symbols-and-return-an-unnamed-series-otherwise","title":"Joining Series together will maintain the name if it is the same between all of the input symbols, and return an unnamed Series otherwise\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_concat/#similarly-joining-timeseries-together-will-maintain-the-index-name-if-it-is-the-same-between-all-of-the-input-symbols-and-return-a-dataframe-with-an-unnamed-index-otherwise","title":"Similarly, joining timeseries together will maintain the index name if it is the same between all of the input symbols, and return a dataframe with an unnamed index otherwise\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_concat/#conclusion","title":"Conclusion\u00b6","text":"<p>We have demonstrated the following about the ArcticDB concat feature:</p> <ul> <li>Easy to use, especially if you already concat in Pandas</li> <li>Can be combined with other query functions to build processing pipelines</li> <li>When more query functions are applied post-concat, will be more performant than Pandas due to ArcticDB's parallel processing implementation</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/","title":"Equity Analytics Notebook","text":"In\u00a0[2]: Copied! <pre>!pip install arcticdb yfinance\n</pre> !pip install arcticdb yfinance In\u00a0[3]: Copied! <pre>import arcticdb as adb\nimport yfinance as yf\nimport pandas as pd\nfrom typing import List, Tuple\nfrom datetime import datetime \nimport matplotlib.pyplot as plt \n</pre> import arcticdb as adb import yfinance as yf import pandas as pd from typing import List, Tuple from datetime import datetime  import matplotlib.pyplot as plt  In\u00a0[4]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_equity\")\nlib = arctic.get_library('demo', create_if_missing=True)\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_equity\") lib = arctic.get_library('demo', create_if_missing=True) In\u00a0[24]: Copied! <pre>start = datetime(2013, 1, 1) \nend = datetime(2022, 12, 31)\nfreq = '1d'\nsymbols = {\n    '^GSPC': 'S&amp;P 500',\n    'AAPL': 'Apple',\n    'MSFT': 'Microsoft',\n    'GOOG': 'Google',\n    'AMZN': 'Amazon',\n    'NVDA': 'Nvidia',\n    'META': 'Meta',\n    'TSLA': 'Tesla',\n    'TSM': 'TSMC',\n    'TCEHY': 'Tencent',\n    '005930.KS': 'Samsung',\n    'ORCL': 'Oracle',\n    'ADBE': 'Adobe',\n    'ASML': 'ASML',\n    'CSCO': 'Cisco'\n}\n</pre> start = datetime(2013, 1, 1)  end = datetime(2022, 12, 31) freq = '1d' symbols = {     '^GSPC': 'S&amp;P 500',     'AAPL': 'Apple',     'MSFT': 'Microsoft',     'GOOG': 'Google',     'AMZN': 'Amazon',     'NVDA': 'Nvidia',     'META': 'Meta',     'TSLA': 'Tesla',     'TSM': 'TSMC',     'TCEHY': 'Tencent',     '005930.KS': 'Samsung',     'ORCL': 'Oracle',     'ADBE': 'Adobe',     'ASML': 'ASML',     'CSCO': 'Cisco' } In\u00a0[25]: Copied! <pre>hist = yf.download(list(symbols.keys()), interval=freq, start=start, end=end)\n</pre> hist = yf.download(list(symbols.keys()), interval=freq, start=start, end=end) <pre>[*********************100%%**********************]  15 of 15 completed\n</pre> In\u00a0[26]: Copied! <pre># the column levels[0] are the fields for each stock\nprint(hist.columns.levels[0])\nhist['Volume'].head(5)\n</pre> # the column levels[0] are the fields for each stock print(hist.columns.levels[0]) hist['Volume'].head(5) <pre>Index(['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')\n</pre> Out[26]: 005930.KS AAPL ADBE AMZN ASML CSCO GOOG META MSFT NVDA ORCL TCEHY TSLA TSM ^GSPC Date 2013-01-02 11449650.0 560518000.0 6483800.0 65420000.0 1824000.0 40304500.0 102033017.0 69846400.0 52899300.0 47883600.0 33758400.0 362500.0 17922000.0 10226100.0 4.202600e+09 2013-01-03 14227400.0 352965200.0 3906000.0 55018000.0 1725400.0 50603500.0 93075567.0 63140600.0 48294400.0 29888800.0 21819500.0 355000.0 11130000.0 13148600.0 3.829730e+09 2013-01-04 12999800.0 594333600.0 3809300.0 37484000.0 3170800.0 36378900.0 110954331.0 72715400.0 52521100.0 52496800.0 21687300.0 101000.0 10110000.0 7464200.0 3.424290e+09 2013-01-07 12610950.0 484156400.0 3632100.0 98200000.0 2066100.0 30790700.0 66476239.0 83781800.0 37110400.0 61073200.0 14008300.0 83000.0 6630000.0 9429900.0 3.304970e+09 2013-01-08 13822250.0 458707200.0 3080900.0 60214000.0 1182400.0 33218100.0 67295297.0 45871300.0 44703100.0 46642400.0 17408900.0 49000.0 19260000.0 8112900.0 3.601600e+09 In\u00a0[27]: Copied! <pre>def field_name_to_sym(field_name: str) -&gt; str:\n    return f\"hist/price_{field_name.replace(' ', '')}\"\n</pre> def field_name_to_sym(field_name: str) -&gt; str:     return f\"hist/price_{field_name.replace(' ', '')}\" In\u00a0[28]: Copied! <pre>for l in hist.columns.levels[0]:\n    lib.write(field_name_to_sym(l), hist[l])\n</pre> for l in hist.columns.levels[0]:     lib.write(field_name_to_sym(l), hist[l]) In\u00a0[29]: Copied! <pre># read back and check that the data round-trips precisely\nfor l in hist.columns.levels[0]:\n    hist_check_db = lib.read(field_name_to_sym(l)).data\n    if not hist[l].equals(hist_check_db):\n        print(f\"Field '{l}' does not round-trip\")\n</pre> # read back and check that the data round-trips precisely for l in hist.columns.levels[0]:     hist_check_db = lib.read(field_name_to_sym(l)).data     if not hist[l].equals(hist_check_db):         print(f\"Field '{l}' does not round-trip\") In\u00a0[30]: Copied! <pre>update_start = datetime(2022, 7, 1) \nupdate_end = datetime(2023, 12, 31)\nupdate_hist = yf.download(list(symbols.keys()), interval=freq, start=update_start, end=update_end)\n</pre> update_start = datetime(2022, 7, 1)  update_end = datetime(2023, 12, 31) update_hist = yf.download(list(symbols.keys()), interval=freq, start=update_start, end=update_end) <pre>[*********************100%%**********************]  15 of 15 completed\n</pre> In\u00a0[31]: Copied! <pre>for l in update_hist.columns.levels[0]:\n    lib.update(field_name_to_sym(l), update_hist[l])\n</pre> for l in update_hist.columns.levels[0]:     lib.update(field_name_to_sym(l), update_hist[l]) In\u00a0[32]: Copied! <pre># these are the symbols we have created\nlib.list_symbols()\n</pre> # these are the symbols we have created lib.list_symbols() Out[32]: <pre>['hist/price_High',\n 'hist/price_Low',\n 'hist/price_Volume',\n 'hist/price_Close',\n 'hist/price_AdjClose',\n 'hist/price_Open']</pre> In\u00a0[33]: Copied! <pre># each symbol contains data for one price field, with the stock tickers as columns and dates as the index\nlib.head(field_name_to_sym('Close')).data\n</pre> # each symbol contains data for one price field, with the stock tickers as columns and dates as the index lib.head(field_name_to_sym('Close')).data Out[33]: 005930.KS AAPL ADBE AMZN ASML CSCO GOOG META MSFT NVDA ORCL TCEHY TSLA TSM ^GSPC Date 2013-01-02 31520.0 19.608213 38.340000 12.8655 66.779999 20.340000 18.013729 28.000000 27.620001 3.1800 34.689999 6.720 2.357333 18.100000 1462.420044 2013-01-03 30860.0 19.360714 37.750000 12.9240 65.379997 20.450001 18.024191 27.770000 27.250000 3.1825 34.310001 6.660 2.318000 18.090000 1459.369995 2013-01-04 30500.0 18.821428 38.130001 12.9575 64.709999 20.480000 18.380356 28.760000 26.740000 3.2875 34.610001 6.694 2.293333 17.959999 1466.469971 2013-01-07 30400.0 18.710714 37.939999 13.4230 63.660000 20.290001 18.300158 29.420000 26.690001 3.1925 34.430000 6.600 2.289333 17.700001 1461.890015 2013-01-08 30000.0 18.761070 38.139999 13.3190 63.139999 20.309999 18.264042 29.059999 26.549999 3.1225 34.439999 6.570 2.245333 17.540001 1457.150024 In\u00a0[36]: Copied! <pre>hist_adj_close = lib.read(field_name_to_sym('Adj Close')).data\n</pre> hist_adj_close = lib.read(field_name_to_sym('Adj Close')).data In\u00a0[37]: Copied! <pre># ffill to remove nans (missing data)\nhist_adj_close_clean = hist_adj_close.ffill()\n# the following line will return True if there are any nans\nhist_adj_close_clean.isnull().any().any()\n</pre> # ffill to remove nans (missing data) hist_adj_close_clean = hist_adj_close.ffill() # the following line will return True if there are any nans hist_adj_close_clean.isnull().any().any() Out[37]: <pre>False</pre> In\u00a0[38]: Copied! <pre>hist_daily_returns = hist_adj_close_clean.pct_change(1).iloc[1:]\nhist_daily_returns.iloc[:5, :5]\n</pre> hist_daily_returns = hist_adj_close_clean.pct_change(1).iloc[1:] hist_daily_returns.iloc[:5, :5] Out[38]: 005930.KS AAPL ADBE AMZN ASML Date 2013-01-03 -0.020939 -0.012622 -0.015389 0.004547 -0.020964 2013-01-04 -0.011666 -0.027854 0.010066 0.002592 -0.010248 2013-01-07 -0.003278 -0.005882 -0.004983 0.035925 -0.016226 2013-01-08 -0.013158 0.002691 0.005272 -0.007748 -0.008168 2013-01-09 0.000000 -0.015629 0.013634 -0.000113 0.005702 In\u00a0[40]: Copied! <pre>returns_sym = 'hist/returns_AdjClose_clean'\nlib.write(returns_sym, hist_daily_returns)\n</pre> returns_sym = 'hist/returns_AdjClose_clean' lib.write(returns_sym, hist_daily_returns) Out[40]: <pre>VersionedItem(symbol='hist/returns_AdjClose_clean', library='demo', data=n/a, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_equity)')</pre> In\u00a0[41]: Copied! <pre>snapshot_name = f\"eod:{hist_daily_returns.iloc[-1].name.date()}\"\nprint(snapshot_name)\nif snapshot_name in lib.list_snapshots():\n    lib.delete_snapshot(snapshot_name)\nlib.snapshot(snapshot_name, metadata=\"EOD audit point\")\n</pre> snapshot_name = f\"eod:{hist_daily_returns.iloc[-1].name.date()}\" print(snapshot_name) if snapshot_name in lib.list_snapshots():     lib.delete_snapshot(snapshot_name) lib.snapshot(snapshot_name, metadata=\"EOD audit point\") <pre>eod:2023-11-17\n</pre> In\u00a0[42]: Copied! <pre>plot_start = datetime(2021, 1, 1)\nplot_end = datetime(2023, 12, 31)\nreturns_plot = lib.read(returns_sym, date_range=(plot_start, plot_end)).data\ndaily_cum_returns = ((1 + returns_plot).cumprod() - 1)\ndaily_cum_returns.rename(columns=symbols).plot(figsize=(20, 10), grid=True, linewidth=0.9, title=\"Daily Cumulative Stock Returns\")\n</pre> plot_start = datetime(2021, 1, 1) plot_end = datetime(2023, 12, 31) returns_plot = lib.read(returns_sym, date_range=(plot_start, plot_end)).data daily_cum_returns = ((1 + returns_plot).cumprod() - 1) daily_cum_returns.rename(columns=symbols).plot(figsize=(20, 10), grid=True, linewidth=0.9, title=\"Daily Cumulative Stock Returns\") Out[42]: <pre>&lt;Axes: title={'center': 'Daily Cumulative Stock Returns'}, xlabel='Date'&gt;</pre> In\u00a0[43]: Copied! <pre>index_ticker = \"^GSPC\"\nroll_days = 130\nbeta_start = datetime(2018, 1, 1)\nbeta_end = datetime(2022, 12, 31)\nbeta_returns = lib.read(returns_sym, date_range=(beta_start, beta_end)).data\nindex_returns = beta_returns[index_ticker]\nstock_returns = beta_returns.drop(columns=index_ticker)\n</pre> index_ticker = \"^GSPC\" roll_days = 130 beta_start = datetime(2018, 1, 1) beta_end = datetime(2022, 12, 31) beta_returns = lib.read(returns_sym, date_range=(beta_start, beta_end)).data index_returns = beta_returns[index_ticker] stock_returns = beta_returns.drop(columns=index_ticker) In\u00a0[44]: Copied! <pre>rolling_cov = stock_returns.rolling(roll_days).cov(index_returns).iloc[roll_days-1:]\nrolling_index_var = index_returns.rolling(roll_days).var().iloc[roll_days-1:]\nrolling_beta = rolling_cov.divide(rolling_index_var, axis='index').rename(columns=symbols)\n</pre> rolling_cov = stock_returns.rolling(roll_days).cov(index_returns).iloc[roll_days-1:] rolling_index_var = index_returns.rolling(roll_days).var().iloc[roll_days-1:] rolling_beta = rolling_cov.divide(rolling_index_var, axis='index').rename(columns=symbols) In\u00a0[45]: Copied! <pre>ax = rolling_beta.plot(figsize=(20, 10), grid=True, linewidth=0.9, title=f\"Rolling {roll_days}-day beta\")\nax.legend(loc='upper left')\n</pre> ax = rolling_beta.plot(figsize=(20, 10), grid=True, linewidth=0.9, title=f\"Rolling {roll_days}-day beta\") ax.legend(loc='upper left') Out[45]: <pre>&lt;matplotlib.legend.Legend at 0x7ef5d42b1fd0&gt;</pre>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#using-arcticdb-for-equity-analytics-a-worked-example","title":"Using ArcticDB for equity analytics: a worked example\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#a-simple-workflow-for-equity-price-timeseries","title":"A Simple Workflow for Equity Price Timeseries\u00b6","text":"<p>In this notebook we:</p> <ul> <li>Source historical tech equity and S&amp;P 500 prices from Yahoo! using the yfinance package</li> <li>Store 10 years of daily price raw data in a temporary local ArcticDB library set up in the notebook</li> <li>The data sourcing is broken into two steps: an initial load of history then an update for recent prices</li> <li>This suggests a typical system workflow: an initial backfill and daily update with recent data</li> <li>Take Adjusted Close prices, remove missing data and calculate returns. Then save this to library</li> <li>Read the clean returns and use them to calculate rolling beta against the S&amp;P 500 for the stocks</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#notice-this-notebook-sources-data-from-yahoo","title":"Notice: This notebook sources data from Yahoo!\u00b6","text":"<p>Please read Yahoo terms here before using it</p> <p>https://policies.yahoo.com/us/en/yahoo/terms/index.htm</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#installs-and-imports","title":"Installs and Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#arcticdb-setup","title":"ArcticDB Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#source-historical-prices-from-yahoo","title":"Source Historical Prices from Yahoo!\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#write-the-data-to-the-library","title":"Write the Data to the Library\u00b6","text":"<p>Currently ArcticDB cannot store multi-level column dataframes directly, so we use a symbol for each price field.</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#update-with-recent-data","title":"Update with Recent Data\u00b6","text":"<p>Here we consume data for the recent past and slightly overlap the time window with the original data.</p> <p>In a real workflow this technique can be used to apply restatements to the price dataset.</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#read-and-process-the-whole-price-dataset","title":"Read and Process the Whole Price Dataset\u00b6","text":"<ul> <li>Read the data, using the Adj Close field as our primary price source</li> <li>Remove missing data by forward filling. A simple but unsophisticated method</li> <li>Calculate daily returns from the prices</li> <li>Write the returns back to ArcticDB as another symbol</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#end-of-the-data-processing-make-a-snapshot","title":"End of the Data Processing - Make a Snapshot\u00b6","text":"<p>The snapshot is optional but it can be useful to record the state of all the data and the end of the daily update process.</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#visualise-the-returns","title":"Visualise the Returns\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#analysis-compute-rolling-betas-vs-the-sp-500","title":"Analysis: Compute Rolling Betas vs the S&amp;P 500\u00b6","text":"<p>Notice the big change in betas starting in Q2 2020 which rolls out of the betas 130 days later (the size of the rolling window).</p> <p>Possibly due to the large market moves at the start of the Covid era?</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>We have demonstrated a simple data pipeline for capture and analysis</li> <li>Although simple, this pattern scales well to much larger datasets</li> <li>In a real system, the data collection and storage would be separated from the analysis</li> <li>Throughout the ArcticDB usage is simple, clean and clear</li> <li>Researchers and Data Scientists like the simplicity - it allows them to focus on the data and their research</li> </ul> <p>For more information about equity beta see https://en.wikipedia.org/wiki/Beta_(finance)</p>"},{"location":"notebooks/ArcticDB_demo_equity_options/","title":"Equity Options Notebook","text":"A Sample Workflow for Equity Options <p>In this notebook we will:</p> <ul> <li>Download (from github) a set of market data for options on a group of tech stocks</li> <li>Store the data in ArcticDB using an incremental timeseries update workflow</li> <li>Create a range of useful option queries</li> <li>Use the queries to drive an interactive chart</li> </ul> Thank you to optiondata.org <p>We would like to express our gratitude to optiondata.org for giving us permission to use a small slice of their free data in this demo.</p> <p>This data is free and available on their website for you to download yourself.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nimport arcticdb as adb\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nimport functools\n</pre> %matplotlib inline import arcticdb as adb import pandas as pd from datetime import datetime import matplotlib.pyplot as plt import ipywidgets as widgets import functools In\u00a0[3]: Copied! <pre>branch = 'master'\nall_dates = ['2013-06-03', '2013-06-10', '2013-06-17', '2013-06-24']\ndelta_low = 0.05\ndelta_high = 0.55\n</pre> branch = 'master' all_dates = ['2013-06-03', '2013-06-10', '2013-06-17', '2013-06-24'] delta_low = 0.05 delta_high = 0.55 In\u00a0[4]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_equity_options\")\nlib = arctic.get_library('demo_options', create_if_missing=True)\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_equity_options\") lib = arctic.get_library('demo_options', create_if_missing=True) In\u00a0[12]: Copied! <pre>def gitub_url(as_of, branch):\n    return f\"https://raw.githubusercontent.com/man-group/ArcticDB/{branch}/docs/mkdocs/docs/notebooks/data/{as_of}tech-options.csv\"\n\n# read the csv from github\ndef read_github_options_file(as_of, branch='master'):\n    try:\n        raw_df = pd.read_csv(gitub_url(as_of, branch))\n    except Exception as e:\n        raise Exception(f\"Github access error: {e}\")\n    return raw_df.set_index(pd.DatetimeIndex(raw_df['quote_date'])).drop(columns=['Unnamed: 0'])\n\ndef uly_symbol(uly):\n    return f\"options/{uly}\"\n\n# query to get option expiries for an underlying\ndef options_expiries_query(lazy_df, as_of, underlying):\n    # exclude options expiring on as_of - no time value left\n    filter = (lazy_df['expiration'] != as_of)\n    lazy_df = lazy_df[filter].groupby('expiration').agg({'volume': 'sum'})\n    return lazy_df\n\ndef read_expiries(as_of, underlying):\n    read_date = pd.Timestamp(as_of)\n    sym = uly_symbol(underlying)\n    lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)\n    lazy_df = options_expiries_query(lazy_df, as_of, underlying)\n    return lazy_df.collect().data.sort_index().index.values\n\ndef read_all_underlyings():\n    # use the symbol list to get all underlyings\n    return sorted([s.split('/')[1] for s in lib.list_symbols()])\n\n# query to get all options for an expiry\n# as_of via date_range, uly via symbol\ndef options_curve_single_expiry_query(lazy_df, expiry):\n    filter = (lazy_df['expiration'] == expiry)\n    return lazy_df[filter]\n\n# query to get all options for an expiry with delta in a specified interval\n# calls have delta &gt;= 0, puts have delta &lt;= 0\ndef options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high):\n    filter = ((lazy_df['expiration'] == expiry) &amp;\n              (abs(lazy_df['delta']) &gt;= delta_low) &amp;\n              (abs(lazy_df['delta']) &lt;= delta_high)\n             )\n    return lazy_df[filter].groupby('strike').agg({'implied_volatility': 'mean'})\n\ndef read_vol_curve_single_expiry(as_of, underlying, expiry):\n    read_date = pd.Timestamp(as_of)\n    sym = uly_symbol(underlying)\n    lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)\n    lazy_df = options_curve_single_expiry_query(lazy_df, expiry)\n    return lazy_df.collect().data\n\n@functools.cache\ndef read_vol_curve_single_expiry_exclude_itm_otm(as_of, underlying, expiry, delta_low, delta_high):\n    read_date = pd.Timestamp(as_of)\n    sym = uly_symbol(underlying)\n    lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)\n    lazy_df = options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high)\n    vol_curve_raw = lazy_df.collect().data\n    return vol_curve_raw.sort_index()*100\n</pre> def gitub_url(as_of, branch):     return f\"https://raw.githubusercontent.com/man-group/ArcticDB/{branch}/docs/mkdocs/docs/notebooks/data/{as_of}tech-options.csv\"  # read the csv from github def read_github_options_file(as_of, branch='master'):     try:         raw_df = pd.read_csv(gitub_url(as_of, branch))     except Exception as e:         raise Exception(f\"Github access error: {e}\")     return raw_df.set_index(pd.DatetimeIndex(raw_df['quote_date'])).drop(columns=['Unnamed: 0'])  def uly_symbol(uly):     return f\"options/{uly}\"  # query to get option expiries for an underlying def options_expiries_query(lazy_df, as_of, underlying):     # exclude options expiring on as_of - no time value left     filter = (lazy_df['expiration'] != as_of)     lazy_df = lazy_df[filter].groupby('expiration').agg({'volume': 'sum'})     return lazy_df  def read_expiries(as_of, underlying):     read_date = pd.Timestamp(as_of)     sym = uly_symbol(underlying)     lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)     lazy_df = options_expiries_query(lazy_df, as_of, underlying)     return lazy_df.collect().data.sort_index().index.values  def read_all_underlyings():     # use the symbol list to get all underlyings     return sorted([s.split('/')[1] for s in lib.list_symbols()])  # query to get all options for an expiry # as_of via date_range, uly via symbol def options_curve_single_expiry_query(lazy_df, expiry):     filter = (lazy_df['expiration'] == expiry)     return lazy_df[filter]  # query to get all options for an expiry with delta in a specified interval # calls have delta &gt;= 0, puts have delta &lt;= 0 def options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high):     filter = ((lazy_df['expiration'] == expiry) &amp;               (abs(lazy_df['delta']) &gt;= delta_low) &amp;               (abs(lazy_df['delta']) &lt;= delta_high)              )     return lazy_df[filter].groupby('strike').agg({'implied_volatility': 'mean'})  def read_vol_curve_single_expiry(as_of, underlying, expiry):     read_date = pd.Timestamp(as_of)     sym = uly_symbol(underlying)     lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)     lazy_df = options_curve_single_expiry_query(lazy_df, expiry)     return lazy_df.collect().data  @functools.cache def read_vol_curve_single_expiry_exclude_itm_otm(as_of, underlying, expiry, delta_low, delta_high):     read_date = pd.Timestamp(as_of)     sym = uly_symbol(underlying)     lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)     lazy_df = options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high)     vol_curve_raw = lazy_df.collect().data     return vol_curve_raw.sort_index()*100 In\u00a0[13]: Copied! <pre>for d in all_dates:\n    df = read_github_options_file(d, branch)\n    underlyings = df['underlying'].unique()\n    print(f\"Date: {d} - {len(df)} records, {len(underlyings)} underlyings\")\n    for u in underlyings:\n        uly_df = df[df['underlying']==u]\n        sym = uly_symbol(u)\n        # upsert option creates the symbol if it doesn't already exist\n        lib.update(sym, uly_df, upsert=True)\n</pre> for d in all_dates:     df = read_github_options_file(d, branch)     underlyings = df['underlying'].unique()     print(f\"Date: {d} - {len(df)} records, {len(underlyings)} underlyings\")     for u in underlyings:         uly_df = df[df['underlying']==u]         sym = uly_symbol(u)         # upsert option creates the symbol if it doesn't already exist         lib.update(sym, uly_df, upsert=True) <pre>Date: 2013-06-03 - 6792 records, 11 underlyings\nDate: 2013-06-10 - 6622 records, 11 underlyings\nDate: 2013-06-17 - 6442 records, 11 underlyings\nDate: 2013-06-24 - 6134 records, 11 underlyings\n</pre> In\u00a0[14]: Copied! <pre># lets take a look at the symbols we have created - one for each underlying\nlib.list_symbols()\n</pre> # lets take a look at the symbols we have created - one for each underlying lib.list_symbols() Out[14]: <pre>['options/IBM',\n 'options/NVDA',\n 'options/MSFT',\n 'options/AAPL',\n 'options/CSCO',\n 'options/ASML',\n 'options/TSM',\n 'options/ADBE',\n 'options/AMZN',\n 'options/ORCL',\n 'options/TSLA']</pre> In\u00a0[15]: Copied! <pre>all_uly = read_all_underlyings()\nall_uly\n</pre> all_uly = read_all_underlyings() all_uly Out[15]: <pre>['AAPL',\n 'ADBE',\n 'AMZN',\n 'ASML',\n 'CSCO',\n 'IBM',\n 'MSFT',\n 'NVDA',\n 'ORCL',\n 'TSLA',\n 'TSM']</pre> In\u00a0[16]: Copied! <pre>def create_vol_curve_chart_single(ax, as_of, uly, delta_low, delta_high):\n    exp = read_expiries(as_of, uly)\n    cmap = plt.get_cmap('rainbow', len(exp))\n    format_kw = {'linewidth': 2, 'alpha': 0.85}\n    for i, e in enumerate(exp):\n        vol_curve = read_vol_curve_single_expiry_exclude_itm_otm(as_of, uly, e, delta_low, delta_high)\n        vol_curve.plot(ax=ax, y='implied_volatility', label=e, grid=True, color=cmap(i), **format_kw)\n    ax.set_title(f\"Option Volatility Curves for {uly}, as of {as_of}\")\n    ax.set_ylabel(\"implied volatility\")\n    ax.legend(loc='upper right', framealpha=0.7)\n    ax.set_facecolor('whitesmoke')\n\ndef create_vol_curve_chart(as_of1, uly1, as_of2, uly2, delta_low, delta_high):\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n    create_vol_curve_chart_single(ax1, as_of1, uly1, delta_low, delta_high)\n    create_vol_curve_chart_single(ax2, as_of2, uly2, delta_low, delta_high)\n\nclass input_widgets(object):\n    def __init__(self):\n        self.container = widgets.VBox()\n        self.create_widgets()\n        self.redraw_chart()\n\n    @property\n    def as_of1(self):\n        return self.container.children[0].children[0].value\n\n    @property\n    def uly1(self):\n        return self.container.children[0].children[1].value\n\n    @property\n    def as_of2(self):\n        return self.container.children[1].children[0].value\n\n    @property\n    def uly2(self):\n        return self.container.children[1].children[1].value\n\n    @property\n    def out(self):\n        return self.container.children[2]\n\n    def create_widgets(self):\n        self.as_of1_dd = widgets.Dropdown(\n            options=all_dates,\n            value=all_dates[0],\n            description='Date1:',\n            disabled=False,\n        )\n        self.as_of1_dd.observe(self._on_change, ['value'])\n\n        self.as_of2_dd = widgets.Dropdown(\n            options=all_dates,\n            value=all_dates[0],\n            description='Date2:',\n            disabled=False,\n        )\n        self.as_of2_dd.observe(self._on_change, ['value'])\n\n        self.uly1_dd = widgets.Dropdown(\n            options=all_uly,\n            value=all_uly[0],\n            description='Underlying1:',\n            disabled=False,\n        )\n        self.uly1_dd.observe(self._on_change, ['value'])\n\n        self.uly2_dd = widgets.Dropdown(\n            options=all_uly,\n            value=all_uly[1],\n            description='Underlying2:',\n            disabled=False,\n        )\n        self.uly2_dd.observe(self._on_change, ['value'])\n\n        self.output_widget = widgets.Output(layout=widgets.Layout(height='900px'))\n\n        self.container.children = [\n            widgets.HBox([self.as_of1_dd, self.uly1_dd]),\n            widgets.HBox([self.as_of2_dd, self.uly2_dd]),\n            self.output_widget\n        ]\n\n    def _on_change(self, _):\n        self.redraw_chart()\n\n    def redraw_chart(self):\n        with self.output_widget:\n            self.output_widget.clear_output(wait=True)\n            create_vol_curve_chart(self.as_of1, self.uly1, self.as_of2, self.uly2, delta_low, delta_high)\n            plt.show()\n</pre> def create_vol_curve_chart_single(ax, as_of, uly, delta_low, delta_high):     exp = read_expiries(as_of, uly)     cmap = plt.get_cmap('rainbow', len(exp))     format_kw = {'linewidth': 2, 'alpha': 0.85}     for i, e in enumerate(exp):         vol_curve = read_vol_curve_single_expiry_exclude_itm_otm(as_of, uly, e, delta_low, delta_high)         vol_curve.plot(ax=ax, y='implied_volatility', label=e, grid=True, color=cmap(i), **format_kw)     ax.set_title(f\"Option Volatility Curves for {uly}, as of {as_of}\")     ax.set_ylabel(\"implied volatility\")     ax.legend(loc='upper right', framealpha=0.7)     ax.set_facecolor('whitesmoke')  def create_vol_curve_chart(as_of1, uly1, as_of2, uly2, delta_low, delta_high):     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))     create_vol_curve_chart_single(ax1, as_of1, uly1, delta_low, delta_high)     create_vol_curve_chart_single(ax2, as_of2, uly2, delta_low, delta_high)  class input_widgets(object):     def __init__(self):         self.container = widgets.VBox()         self.create_widgets()         self.redraw_chart()      @property     def as_of1(self):         return self.container.children[0].children[0].value      @property     def uly1(self):         return self.container.children[0].children[1].value      @property     def as_of2(self):         return self.container.children[1].children[0].value      @property     def uly2(self):         return self.container.children[1].children[1].value      @property     def out(self):         return self.container.children[2]      def create_widgets(self):         self.as_of1_dd = widgets.Dropdown(             options=all_dates,             value=all_dates[0],             description='Date1:',             disabled=False,         )         self.as_of1_dd.observe(self._on_change, ['value'])          self.as_of2_dd = widgets.Dropdown(             options=all_dates,             value=all_dates[0],             description='Date2:',             disabled=False,         )         self.as_of2_dd.observe(self._on_change, ['value'])          self.uly1_dd = widgets.Dropdown(             options=all_uly,             value=all_uly[0],             description='Underlying1:',             disabled=False,         )         self.uly1_dd.observe(self._on_change, ['value'])          self.uly2_dd = widgets.Dropdown(             options=all_uly,             value=all_uly[1],             description='Underlying2:',             disabled=False,         )         self.uly2_dd.observe(self._on_change, ['value'])          self.output_widget = widgets.Output(layout=widgets.Layout(height='900px'))          self.container.children = [             widgets.HBox([self.as_of1_dd, self.uly1_dd]),             widgets.HBox([self.as_of2_dd, self.uly2_dd]),             self.output_widget         ]      def _on_change(self, _):         self.redraw_chart()      def redraw_chart(self):         with self.output_widget:             self.output_widget.clear_output(wait=True)             create_vol_curve_chart(self.as_of1, self.uly1, self.as_of2, self.uly2, delta_low, delta_high)             plt.show() In\u00a0[17]: Copied! <pre>w = input_widgets()\nw.container\n</pre> w = input_widgets() w.container Out[17]: <pre>VBox(children=(HBox(children=(Dropdown(description='Date1:', options=('2013-06-03', '2013-06-10', '2013-06-17'\u2026</pre> <p>For the notebook preview, the chart and widgets will look like this image</p> <p></p>"},{"location":"notebooks/ArcticDB_demo_equity_options/#using-arcticdb-for-equity-options-data-a-worked-example","title":"Using ArcticDB for equity options data: a worked example\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#installs-and-imports","title":"Installs and Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#parameters-for-the-notebook","title":"Parameters for the Notebook\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#arcticdb-setup","title":"ArcticDB Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#functions-for-reading-github-and-arcticdb-queries","title":"Functions for Reading Github and ArcticDB Queries\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#read-data-from-github-and-store-in-arcticdb","title":"Read Data from Github and Store in ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#functions-for-creating-the-charts-and-simple-interactive-controls","title":"Functions for Creating the Charts and Simple Interactive Controls\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#interactive-vol-curves-for-side-by-side-viewing-of-2-datesstocks","title":"Interactive Vol Curves for Side By Side Viewing of 2 Dates/Stocks\u00b6","text":"<ul> <li>Click the dropdowns and the charts will update</li> <li>The data is read from ArcticDB as part of the chart creation</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_options/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>We have demonstrated a simple data pipeline for capture and storage of options data</li> <li>We created a useful series of queries on the data</li> <li>These queries are used to build interactive charts</li> <li>The result is a simple visual browser for option volatility curves</li> <li>In a real system, the data collection and storage would probably be a separate process from the visualisation</li> <li>The simplicity and flexibility of ArcticDB made the data handling the easy part, compared to getting the charts and interactive widgets looking nice</li> </ul>"},{"location":"notebooks/ArcticDB_demo_lazydataframe/","title":"LazyDataFrame Notebook","text":"<p>In this demo, we will explore the DataFrame processing options available in ArcticDB using the LazyDataFrame class. We will cover various possibilities of this API, including:</p> <ul> <li>Filtering</li> <li>Projections</li> <li>Groupbys and Aggregations</li> <li>Combinations of the above features</li> </ul> <p>Why perform the processing in ArcticDB?</p> <ul> <li>Performance boost via efficient C++ implementation that uses multi-threading</li> <li>Efficient data access - only reads the data needed</li> <li>For very large data sets some queries are possible that would not fit into memory</li> </ul> <p>Note that all of the operations described here are also available using the legacy <code>QueryBuilder</code> class, but we think this API is more intuitive!</p> <p>Necessary packages installation</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb <p>Necessary libraries imports</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport arcticdb as adb\nfrom arcticdb.util.test import random_strings_of_length\n</pre> import os import numpy as np import pandas as pd import random import arcticdb as adb from arcticdb.util.test import random_strings_of_length <p>For this demo we will configure the LMDB file based backend.  ArcticDB achieves its high performance and scale when configured with an object store backend (e.g. S3).</p> In\u00a0[\u00a0]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_demo\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_demo\") <p>You can have an unlimited number of libraries, but we will just create one to start with.</p> In\u00a0[\u00a0]: Copied! <pre>if 'sample' not in arctic.list_libraries():\n    # library does not already exist\n    arctic.create_library('sample')\nlib = arctic.get_library('sample')\n</pre> if 'sample' not in arctic.list_libraries():     # library does not already exist     arctic.create_library('sample') lib = arctic.get_library('sample') <p>Run the cell to set up preliminary variables. 100,000 unique strings is a pathological case for us, as with the default row-slicing policy there are 100,000 rows per data segment, and so each unique strings will appear around once per data segment in this column.</p> In\u00a0[\u00a0]: Copied! <pre>ten_grouping_values = random_strings_of_length(10, 10, True)\none_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True)\nrng = np.random.RandomState()\n\nsym_10M = \"demo_10M\"\nsym_100M = \"demo_100M\"\nsym_1B = \"demo_1B\"\n</pre> ten_grouping_values = random_strings_of_length(10, 10, True) one_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True) rng = np.random.RandomState()  sym_10M = \"demo_10M\" sym_100M = \"demo_100M\" sym_1B = \"demo_1B\" <p>Choose which symbol you want to work with</p> <ul> <li>sym_10M: symbol with 10 million rows</li> <li>sym_100M: symbol with 100 million rows</li> <li>sym_1B: symbol with 1 billion rows</li> </ul> <p>assign the symbol you want to work with to the sym variable</p> <ul> <li>example: sym = sym_10M</li> </ul> In\u00a0[\u00a0]: Copied! <pre>sym = sym_10M\n</pre> sym = sym_10M <p>Run this cell to set up the DataFrame according to the symbol name</p> In\u00a0[\u00a0]: Copied! <pre>if sym==sym_10M:\n    num_rows = 10_000_000\nelif sym==sym_100M:\n    num_rows = 100_000_000\nelif sym==sym_1B:\n    num_rows = 1_000_000_000\ninput_df = pd.DataFrame(\n    {\n        \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),\n        \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),\n        \"numeric_column\": rng.rand((num_rows))\n    }\n)\n</pre> if sym==sym_10M:     num_rows = 10_000_000 elif sym==sym_100M:     num_rows = 100_000_000 elif sym==sym_1B:     num_rows = 1_000_000_000 input_df = pd.DataFrame(     {         \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),         \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),         \"numeric_column\": rng.rand((num_rows))     } ) In\u00a0[\u00a0]: Copied! <pre>lib.write(sym, input_df)\n</pre> lib.write(sym, input_df) <p>Show how the data has been sliced and written to disk.</p> In\u00a0[\u00a0]: Copied! <pre>lib._nvs.read_index(sym)\n</pre> lib._nvs.read_index(sym) <p>Show the first 100 rows of data as a sample.</p> In\u00a0[\u00a0]: Copied! <pre>lib.head(sym, n=100).data\n</pre> lib.head(sym, n=100).data <p>Read the symbol without any filtering.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym)\n</pre> %%time lib.read(sym) <p>Most of the time is spent allocating Python strings in the column with 100,000 unique strings, so omitting this column is much faster.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"])\n</pre> %%time lib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"]) <p>Note that all of the values in the numeric column are between 0 and 1. This query therefore does not filter out any data. This demonstrates that doing a full table scan does not significantly impact the performance. Also note that the read call is not practically instant, as no data is read until collect is called on the LazyDataFrame.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 2.0]\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 2.0] In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df.collect()\n</pre> %%time lazy_df.collect() <p>Now we are filtering down to approximately 10% of the rows in the symbol. This is faster than reading, as there are now fewer Python strings to allocate.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1]\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1] df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>Creating a new column as a funtion of existing columns and constants is approximately the same speed as a filter that doesn't reduce the amount of data displayed.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df[\"new_column\"] = lazy_df[\"numeric_column\"] * 2.0\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df[\"new_column\"] = lazy_df[\"numeric_column\"] * 2.0 df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>Equivalently, use the apply method to achieve the same results.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_df = lib.read(sym, lazy=True)\nlazy_df.apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0)\nlazy_df.collect().data\n</pre> lazy_df = lib.read(sym, lazy=True) lazy_df.apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0) lazy_df.collect().data <p>If using apply before the <code>LazyDataFrame</code> object has been created, the <code>col</code> function can be used as placeholders for columns names.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_df = lib.read(sym, lazy=True).apply(\"new_column\", adb.col(\"numeric_column\") * 2.0)\nlazy_df.collect().data\n</pre> lazy_df = lib.read(sym, lazy=True).apply(\"new_column\", adb.col(\"numeric_column\") * 2.0) lazy_df.collect().data <p>Grouping is again faster than just reading due to the reduced number of Python string allocations, even with the extra computation performed.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"})\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"}) df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>Even grouping on a pathologically large number of unique values does not significantly reduce the performance.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"})\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"}) df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>These operations can be arbitrarily combined in a seqential pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"})\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"}) df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre># Setup two symbols\nbatch_sym_1 = f'{sym}_1'\nbatch_sym_2 = f'{sym}_2'\nsyms = [batch_sym_1, batch_sym_2]\nlib.write(batch_sym_1, input_df)\nlib.write(batch_sym_2, input_df)\n</pre> # Setup two symbols batch_sym_1 = f'{sym}_1' batch_sym_2 = f'{sym}_2' syms = [batch_sym_1, batch_sym_2] lib.write(batch_sym_1, input_df) lib.write(batch_sym_2, input_df) <p><code>read_batch</code> also has a <code>lazy</code> argument, which returns a <code>LazyDataFrameCollection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lib.read_batch(syms, lazy=True)\nlazy_dfs\n</pre> lazy_dfs = lib.read_batch(syms, lazy=True) lazy_dfs <p>The same processing operations can be applied to all of the symbols being read in the batch. Note in the cell output that the pipe <code>|</code> is outside the list of <code>LazyDataFrame</code>s, so the <code>WHERE</code> clause is applied to all of the symbols.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1]\nlazy_dfs\n</pre> lazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1] lazy_dfs <p>Calling <code>collect()</code> on a <code>LazyDataFrameCollection</code> uses <code>read_batch</code> under the hood, and so is generally more performant than serialised read calls.</p> In\u00a0[\u00a0]: Copied! <pre>dfs = lazy_dfs.collect()\ndfs\n</pre> dfs = lazy_dfs.collect() dfs In\u00a0[\u00a0]: Copied! <pre>dfs[0].data.head()\n</pre> dfs[0].data.head() In\u00a0[\u00a0]: Copied! <pre>dfs[1].data.head()\n</pre> dfs[1].data.head() <p>Separate processing operations can be applied to the individual symbols in the batch if desired.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lib.read_batch(syms, lazy=True)\nlazy_dfs = lazy_dfs.split()\nlazy_dfs\n</pre> lazy_dfs = lib.read_batch(syms, lazy=True) lazy_dfs = lazy_dfs.split() lazy_dfs <p>Note in the cell output that the pipes <code>|</code> are now inside the list of <code>LazyDataFrame</code>s, so the <code>PROJECT</code> clauses are applied to individual symbols.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\"))\nlazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\"))\nlazy_dfs = adb.LazyDataFrameCollection(lazy_dfs)\nlazy_dfs\n</pre> lazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\")) lazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\")) lazy_dfs = adb.LazyDataFrameCollection(lazy_dfs) lazy_dfs In\u00a0[\u00a0]: Copied! <pre>dfs = lazy_dfs.collect()\ndfs\n</pre> dfs = lazy_dfs.collect() dfs In\u00a0[\u00a0]: Copied! <pre>dfs[0].data\n</pre> dfs[0].data In\u00a0[\u00a0]: Copied! <pre>dfs[1].data\n</pre> dfs[1].data <p>If desired, these two modes of operation can be combined in an intuitive manner.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lib.read_batch(syms, lazy=True)\nlazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1]\nlazy_dfs = lazy_dfs.split()\nlazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\"))\nlazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\"))\nlazy_dfs = adb.LazyDataFrameCollection(lazy_dfs)\nlazy_dfs = lazy_dfs[lazy_dfs[\"new_column_1\"] &lt; 0.1]\nlazy_dfs\n</pre> lazy_dfs = lib.read_batch(syms, lazy=True) lazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1] lazy_dfs = lazy_dfs.split() lazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\")) lazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\")) lazy_dfs = adb.LazyDataFrameCollection(lazy_dfs) lazy_dfs = lazy_dfs[lazy_dfs[\"new_column_1\"] &lt; 0.1] lazy_dfs In\u00a0[\u00a0]: Copied! <pre>dfs = lazy_dfs.collect()\n</pre> dfs = lazy_dfs.collect() In\u00a0[\u00a0]: Copied! <pre>dfs[0].data\n</pre> dfs[0].data In\u00a0[\u00a0]: Copied! <pre>dfs[1].data\n</pre> dfs[1].data"},{"location":"notebooks/ArcticDB_demo_lazydataframe/#arcticdb-lazydataframe-demo","title":"ArcticDB LazyDataFrame demo\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#demo-setup","title":"Demo setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#demo-start","title":"Demo Start\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#reading","title":"Reading\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#filtering","title":"Filtering\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#projections","title":"Projections\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#groupbys-and-aggregations","title":"Groupbys and Aggregations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#combinations","title":"Combinations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#batch-operations","title":"Batch Operations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/","title":"Intro Notebook","text":"<ul> <li>Namespace \u2013 Collections of libraries. Used to separate logical environments from each other. Analogous to database server.</li> <li>Library \u2013 Contains multiple symbols which are grouped in a certain way (different users, markets etc). Analogous to database.</li> <li>Symbol \u2013 Atomic unit of data storage. Identified by a string name. Data stored under a symbol strongly resembles a Pandas DataFrame. Analogous to tables.</li> <li>Version \u2013 Every modifying action (write, append, update) performed on a symbol creates a new version of that object.</li> <li>Snapshot \u2013 Data associated with all or some symbols at a particular point-in-time can be snapshotted and later retrieved via the read method.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[2]: Copied! <pre>import time\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport arcticdb as adb\n</pre> import time import numpy as np import pandas as pd from datetime import datetime import arcticdb as adb In\u00a0[3]: Copied! <pre>daily1 = pd.DataFrame(np.ones((4, 3))*1, index=pd.date_range('1/1/2023', periods=4, freq=\"D\"), columns=list('ABC'))\ndaily1\n</pre> daily1 = pd.DataFrame(np.ones((4, 3))*1, index=pd.date_range('1/1/2023', periods=4, freq=\"D\"), columns=list('ABC')) daily1 Out[3]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 In\u00a0[4]: Copied! <pre>daily2 = pd.DataFrame(np.ones((4, 3))*2, index=pd.date_range('1/5/2023', periods=4, freq=\"D\"), columns=list('ABC'))\ndaily2\n</pre> daily2 = pd.DataFrame(np.ones((4, 3))*2, index=pd.date_range('1/5/2023', periods=4, freq=\"D\"), columns=list('ABC')) daily2 Out[4]: A B C 2023-01-05 2.0 2.0 2.0 2023-01-06 2.0 2.0 2.0 2023-01-07 2.0 2.0 2.0 2023-01-08 2.0 2.0 2.0 In\u00a0[5]: Copied! <pre>daily3 = pd.DataFrame(np.ones((4, 3))*3, index=pd.date_range('1/3/2023', periods=4, freq=\"D\"), columns=list('ABC'))\ndaily3\n</pre> daily3 = pd.DataFrame(np.ones((4, 3))*3, index=pd.date_range('1/3/2023', periods=4, freq=\"D\"), columns=list('ABC')) daily3 Out[5]: A B C 2023-01-03 3.0 3.0 3.0 2023-01-04 3.0 3.0 3.0 2023-01-05 3.0 3.0 3.0 2023-01-06 3.0 3.0 3.0 In\u00a0[6]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_demo\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_demo\") <p>You can have an unlimited number of libraries, but we will just create one to start with.</p> In\u00a0[7]: Copied! <pre>lib = arctic.get_library('sample', create_if_missing=True)\n</pre> lib = arctic.get_library('sample', create_if_missing=True) <p>ArcticDB generally adheres to a philosphy of Pandas In, Pandas Out. read and write both work with Pandas DataFrames.</p> <p>Note - within a library it is common to have many thousands of symbols.</p> In\u00a0[8]: Copied! <pre>write_record = lib.write(\"DAILY\", daily1)\nwrite_record\n</pre> write_record = lib.write(\"DAILY\", daily1) write_record Out[8]: <pre>VersionedItem(symbol='DAILY', library='sample', data=n/a, version=0, metadata=None, host='LMDB(path=/content/arcticdb_demo)')</pre> In\u00a0[9]: Copied! <pre>read_record = lib.read(\"DAILY\")\nread_record\n</pre> read_record = lib.read(\"DAILY\") read_record Out[9]: <pre>VersionedItem(symbol='DAILY', library='sample', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=0, metadata=None, host='LMDB(path=/content/arcticdb_demo)')</pre> <p>NB: You can version multiple symbols/tables together with a library level Snapshot!</p> In\u00a0[10]: Copied! <pre>read_record.data\n</pre> read_record.data Out[10]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 <p>ArcticDB supports data modifications such as update and append.</p> In\u00a0[11]: Copied! <pre>lib.append(\"DAILY\", daily2)\nlib.read(\"DAILY\").data\n</pre> lib.append(\"DAILY\", daily2) lib.read(\"DAILY\").data Out[11]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 2023-01-05 2.0 2.0 2.0 2023-01-06 2.0 2.0 2.0 2023-01-07 2.0 2.0 2.0 2023-01-08 2.0 2.0 2.0 In\u00a0[12]: Copied! <pre>lib.update(\"DAILY\", daily3)\nlib.read(\"DAILY\").data\n</pre> lib.update(\"DAILY\", daily3) lib.read(\"DAILY\").data Out[12]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 3.0 3.0 3.0 2023-01-04 3.0 3.0 3.0 2023-01-05 3.0 3.0 3.0 2023-01-06 3.0 3.0 3.0 2023-01-07 2.0 2.0 2.0 2023-01-08 2.0 2.0 2.0 In\u00a0[13]: Copied! <pre># Rewind to version...\nlib.read(\"DAILY\", as_of=write_record.version).data\n</pre> # Rewind to version... lib.read(\"DAILY\", as_of=write_record.version).data Out[13]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 <p>One typical use case is to store the history of &gt;100k measures in one dataframe for easy timeseries and cross-sectional analysis.</p> <p>For this demo notebook we'll just do 10,000 rows of hourly data by 10,000 columns of measures.</p> In\u00a0[14]: Copied! <pre>n = 10_000\nlarge = pd.DataFrame(np.linspace(1, n, n)*np.linspace(1, n, n)[:,np.newaxis], columns=[f'c{i}' for i in range(n)], index=pd.date_range('1/1/2020', periods=n, freq=\"H\"))\nlarge.tail()\n</pre> n = 10_000 large = pd.DataFrame(np.linspace(1, n, n)*np.linspace(1, n, n)[:,np.newaxis], columns=[f'c{i}' for i in range(n)], index=pd.date_range('1/1/2020', periods=n, freq=\"H\")) large.tail() Out[14]: c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 ... c9990 c9991 c9992 c9993 c9994 c9995 c9996 c9997 c9998 c9999 2021-02-20 11:00:00 9996.0 19992.0 29988.0 39984.0 49980.0 59976.0 69972.0 79968.0 89964.0 99960.0 ... 99870036.0 99880032.0 99890028.0 99900024.0 99910020.0 99920016.0 99930012.0 99940008.0 99950004.0 99960000.0 2021-02-20 12:00:00 9997.0 19994.0 29991.0 39988.0 49985.0 59982.0 69979.0 79976.0 89973.0 99970.0 ... 99880027.0 99890024.0 99900021.0 99910018.0 99920015.0 99930012.0 99940009.0 99950006.0 99960003.0 99970000.0 2021-02-20 13:00:00 9998.0 19996.0 29994.0 39992.0 49990.0 59988.0 69986.0 79984.0 89982.0 99980.0 ... 99890018.0 99900016.0 99910014.0 99920012.0 99930010.0 99940008.0 99950006.0 99960004.0 99970002.0 99980000.0 2021-02-20 14:00:00 9999.0 19998.0 29997.0 39996.0 49995.0 59994.0 69993.0 79992.0 89991.0 99990.0 ... 99900009.0 99910008.0 99920007.0 99930006.0 99940005.0 99950004.0 99960003.0 99970002.0 99980001.0 99990000.0 2021-02-20 15:00:00 10000.0 20000.0 30000.0 40000.0 50000.0 60000.0 70000.0 80000.0 90000.0 100000.0 ... 99910000.0 99920000.0 99930000.0 99940000.0 99950000.0 99960000.0 99970000.0 99980000.0 99990000.0 100000000.0 <p>5 rows \u00d7 10000 columns</p> In\u00a0[15]: Copied! <pre>t1 = time.time()\nlib.write('large', large)\nt2 = time.time()\nprint(f'Wrote {n*n/(t2-t1)/1e6:.2f} million floats per second.')\n</pre> t1 = time.time() lib.write('large', large) t2 = time.time() print(f'Wrote {n*n/(t2-t1)/1e6:.2f} million floats per second.') <pre>Wrote 13.30 million floats per second.\n</pre> <p>You can select out rows and columns efficiently, necessary when the data doesn't fit into ram.</p> In\u00a0[16]: Copied! <pre>subframe = lib.read(\n    \"large\",\n    columns=[\"c0\", \"c1\", \"c5000\", \"c5001\", \"c9998\", \"c9999\"],\n    date_range=(datetime(2020, 6, 13, 8), datetime(2020, 6, 13, 13))\n).data\nsubframe\n</pre> subframe = lib.read(     \"large\",     columns=[\"c0\", \"c1\", \"c5000\", \"c5001\", \"c9998\", \"c9999\"],     date_range=(datetime(2020, 6, 13, 8), datetime(2020, 6, 13, 13)) ).data subframe Out[16]: c0 c1 c5000 c5001 c9998 c9999 2020-06-13 08:00:00 3945.0 7890.0 19728945.0 19732890.0 39446055.0 39450000.0 2020-06-13 09:00:00 3946.0 7892.0 19733946.0 19737892.0 39456054.0 39460000.0 2020-06-13 10:00:00 3947.0 7894.0 19738947.0 19742894.0 39466053.0 39470000.0 2020-06-13 11:00:00 3948.0 7896.0 19743948.0 19747896.0 39476052.0 39480000.0 2020-06-13 12:00:00 3949.0 7898.0 19748949.0 19752898.0 39486051.0 39490000.0 2020-06-13 13:00:00 3950.0 7900.0 19753950.0 19757900.0 39496050.0 39500000.0 In\u00a0[17]: Copied! <pre>n = 100_000_000\nlong = pd.DataFrame(np.linspace(1, n, n), columns=['Price'], index=pd.date_range('1/1/2020', periods=n, freq=\"S\"))\nlong.tail()\n</pre> n = 100_000_000 long = pd.DataFrame(np.linspace(1, n, n), columns=['Price'], index=pd.date_range('1/1/2020', periods=n, freq=\"S\")) long.tail() Out[17]: Price 2023-03-03 09:46:35 99999996.0 2023-03-03 09:46:36 99999997.0 2023-03-03 09:46:37 99999998.0 2023-03-03 09:46:38 99999999.0 2023-03-03 09:46:39 100000000.0 In\u00a0[18]: Copied! <pre>t1 = time.time()\nlib.write('long', long)\nt2 = time.time()\nprint(f'Wrote {n/(t2-t1)/1e6:.2f} million floats per second.')\n</pre> t1 = time.time() lib.write('long', long) t2 = time.time() print(f'Wrote {n/(t2-t1)/1e6:.2f} million floats per second.') <pre>Wrote 12.20 million floats per second.\n</pre> In\u00a0[19]: Copied! <pre>%%time\nlazy_df = lib.read(\"long\", lazy=True)\nlazy_df = lazy_df[(lazy_df[\"Price\"] &gt; 49e6) &amp; (lazy_df[\"Price\"] &lt; 51e6)]\nfiltered = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(\"long\", lazy=True) lazy_df = lazy_df[(lazy_df[\"Price\"] &gt; 49e6) &amp; (lazy_df[\"Price\"] &lt; 51e6)] filtered = lazy_df.collect().data <pre>CPU times: user 3.07 s, sys: 525 ms, total: 3.6 s\nWall time: 2.3 s\n</pre> In\u00a0[20]: Copied! <pre>len(filtered)\n</pre> len(filtered) Out[20]: <pre>1999999</pre> In\u00a0[21]: Copied! <pre>filtered.tail()\n</pre> filtered.tail() Out[21]: Price 2021-08-13 06:39:54 50999995.0 2021-08-13 06:39:55 50999996.0 2021-08-13 06:39:56 50999997.0 2021-08-13 06:39:57 50999998.0 2021-08-13 06:39:58 50999999.0"},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-concepts-and-terminology","title":"ArcticDB Concepts and Terminology\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-is-designed-for-time-series-data","title":"ArcticDB is designed for Time Series data\u00b6","text":"<p>Let's create a few small dataframes of daily data so we can see what's happening.</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#library-management","title":"Library Management\u00b6","text":"<p>For this demo we will configure the LMDB file based backend.  ArcticDB achieves its high performance and scale when configured with an object store backend (e.g. S3).</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#reading-writing-data","title":"Reading &amp; writing data\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#read-a-pandas-dataframe-from-source-and-write-it-to-the-target","title":"Read a pandas dataframe from source, and write it to the target\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#modifying-data","title":"Modifying Data\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-is-bitemporal","title":"ArcticDB is bitemporal\u00b6","text":"<p>All ArcticDB operations are versioned - rewind through time to understand historical revisions and enable point-in-time analysis of data!</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-supports-extremely-large-dataframes","title":"ArcticDB supports extremely large DataFrames\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-supports-extremely-long-dataframes","title":"ArcticDB supports extremely long DataFrames\u00b6","text":"<p>Another typical use case is high frequency data with billions of rows.</p> <p>For this demo notebook we will just try a modest 100 million rows of second frequency data.</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#you-can-query-the-data-with-with-the-familiarity-of-pandas-and-the-efficiency-of-c","title":"You can query the data with with the familiarity of Pandas and the efficiency of C++\u00b6","text":"<p>For more information please check out our LazyDataFrame and QueryBuilder docs.</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#where-to-go-from-here","title":"Where to go from here?\u00b6","text":"<ol> <li>Read the docs</li> <li>Signup for Slack via our website</li> <li>Checkout the code on Github</li> </ol>"},{"location":"notebooks/ArcticDB_demo_querybuilder/","title":"Querybuilder Notebook","text":"<p>Note that all of the functionality demonstrated here is available in the more intuitive LazyDataFrame API, which we recommend is used in preference to the QueryBuilder.</p> <p>In this demo, we will explore the different functionalities of the QueryBuilder for ArcticDB. We will cover various possibilities of this API, including:</p> <ul> <li>Filtering</li> <li>Projections</li> <li>Groupbys and Aggregations</li> <li>Combinations of the above features</li> </ul> <p>Why use QueryBuilder?</p> <ul> <li>Performance boost via efficient C++ implementation that uses multi-threading</li> <li>Efficient data access - only reads the data needed</li> <li>For very large data sets some queries are possible that would not fit into memory</li> </ul> <p>Necessary packages installation</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb <p>Necessary libraries imports</p> In\u00a0[3]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport arcticdb as adb\nfrom arcticdb.util.test import random_strings_of_length\n</pre> import os import numpy as np import pandas as pd import random import arcticdb as adb from arcticdb.util.test import random_strings_of_length <p>For this demo we will configure the LMDB file based backend.  ArcticDB achieves its high performance and scale when configured with an object store backend (e.g. S3).</p> In\u00a0[4]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_demo\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_demo\") <p>You can have an unlimited number of libraries, but we will just create one to start with.</p> In\u00a0[5]: Copied! <pre>if 'sample' not in arctic.list_libraries():\n    # library does not already exist\n    arctic.create_library('sample')\nlib = arctic.get_library('sample')\n</pre> if 'sample' not in arctic.list_libraries():     # library does not already exist     arctic.create_library('sample') lib = arctic.get_library('sample') <p>Run the cell to set up preliminary variables. 100,000 unique strings is a pathological case for us, as with the default row-slicing policy there are 100,000 rows per data segment, and so each unique strings will appear around once per data segment in this column.</p> In\u00a0[6]: Copied! <pre>ten_grouping_values = random_strings_of_length(10, 10, True)\none_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True)\nrng = np.random.RandomState()\n\nsym_10M = \"demo_10M\"\nsym_100M = \"demo_100M\"\nsym_1B = \"demo_1B\"\n</pre> ten_grouping_values = random_strings_of_length(10, 10, True) one_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True) rng = np.random.RandomState()  sym_10M = \"demo_10M\" sym_100M = \"demo_100M\" sym_1B = \"demo_1B\" <p>Choose which symbol you want to work with</p> <ul> <li>sym_10M: symbol with 10 million rows</li> <li>sym_100M: symbol with 100 million rows</li> <li>sym_1B: symbol with 1 billion rows</li> </ul> <p>assign the symbol you want to work with to the sym variable</p> <ul> <li>example: sym = sym_10M</li> </ul> In\u00a0[7]: Copied! <pre>sym = sym_10M\n</pre> sym = sym_10M <p>Run this cell to set up the DataFrame according to the symbol name</p> In\u00a0[8]: Copied! <pre>if sym==sym_10M:\n    num_rows = 10_000_000\nelif sym==sym_100M:\n    num_rows = 100_000_000\nelif sym==sym_1B:\n    num_rows = 1_000_000_000\ndf = pd.DataFrame(\n    {\n        \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),\n        \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),\n        \"numeric_column\": rng.rand((num_rows))\n    }\n)\n</pre> if sym==sym_10M:     num_rows = 10_000_000 elif sym==sym_100M:     num_rows = 100_000_000 elif sym==sym_1B:     num_rows = 1_000_000_000 df = pd.DataFrame(     {         \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),         \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),         \"numeric_column\": rng.rand((num_rows))     } ) In\u00a0[\u00a0]: Copied! <pre>lib.write(sym, df)\n</pre> lib.write(sym, df) <p>Show how the data has been sliced and written to disk.</p> In\u00a0[\u00a0]: Copied! <pre>lib._nvs.read_index(sym)\n</pre> lib._nvs.read_index(sym) <p>Show the first 100 rows of data as a sample.</p> In\u00a0[\u00a0]: Copied! <pre>lib.head(sym, n=100).data\n</pre> lib.head(sym, n=100).data <p>Read the symbol without any filtering.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym)\n</pre> %%time lib.read(sym) <p>Most of the time is spent allocating Python strings in the column with 100,000 unique strings, so omitting this column is much faster.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"])\n</pre> %%time lib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"]) <p>Note that all of the values in the numeric column are between 0 and 1. Thisquery therefore does not filter out any data. This demonstrates that doing a full table scan does not significantly impact the performance.</p> In\u00a0[16]: Copied! <pre>q = adb.QueryBuilder()\nq = q[q[\"numeric_column\"] &lt; 2.0]\n</pre> q = adb.QueryBuilder() q = q[q[\"numeric_column\"] &lt; 2.0] In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) <p>Now we are filtering down to approximately 10% of the rows in the symbol. This is faster than reading, as there are now fewer Python strings to allocate.</p> In\u00a0[18]: Copied! <pre>q = adb.QueryBuilder()\nq = q[q[\"numeric_column\"] &lt; 0.1]\n</pre> q = adb.QueryBuilder() q = q[q[\"numeric_column\"] &lt; 0.1] In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q).data\n</pre> %%time lib.read(sym, query_builder=q).data In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>Creating a new column as a funtion of existing columns and constants is approximately the same speed as a filter that doesn't reduce the amount of data displayed.</p> In\u00a0[21]: Copied! <pre>q = adb.QueryBuilder()\nq = q.apply(\"new_column\", q[\"numeric_column\"] * 2.0)\n</pre> q = adb.QueryBuilder() q = q.apply(\"new_column\", q[\"numeric_column\"] * 2.0) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>Grouping is again faster than just reading due to the reduced number of Python string allocations, even with the extra computation performed.</p> In\u00a0[24]: Copied! <pre>q = adb.QueryBuilder()\nq = q.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"})\n</pre> q = adb.QueryBuilder() q = q.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"}) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>Even grouping on a pathologically large number of unique values does not significantly reduce the performance.</p> In\u00a0[27]: Copied! <pre>q = adb.QueryBuilder()\nq = q.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"})\n</pre> q = adb.QueryBuilder() q = q.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"}) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>These operations can be arbitrarily combined in a seqential pipeline.</p> In\u00a0[30]: Copied! <pre>q = adb.QueryBuilder()\nq = q[q[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", q[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"})\n</pre> q = adb.QueryBuilder() q = q[q[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", q[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"}) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data"},{"location":"notebooks/ArcticDB_demo_querybuilder/#arcticdb-query-builder-demo","title":"ArcticDB Query Builder demo\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#demo-setup","title":"Demo setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#demo-start","title":"Demo Start\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#reading","title":"Reading\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#filtering","title":"Filtering\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#projections","title":"Projections\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#groupbys-and-aggregations","title":"Groupbys and Aggregations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#combinations","title":"Combinations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/","title":"Arrow Notebook","text":"<p>This notebook demonstrates ArcticDB's Arrow-based output formats: PyArrow and Polars. These formats provide performance improvements for string-heavy workloads, and allow better integrations with modern Arrow-based table processing libraries like <code>polars</code> and <code>duckdb</code>.</p> <p>Key benefits:</p> <ul> <li>Better performance: Particularly for string columns (no GIL required)</li> <li>Zero-copy integration with Arrow: Zero-copy pass dataframes from <code>arctidb</code> to third party libraries like <code>duckdb</code></li> </ul> <p>Notebook structure:</p> <ol> <li>Setup and basic usage</li> <li>Polars output format</li> <li>PyArrow output format and record batch structure</li> <li>Configurable string formats</li> <li>Pandas interoperability and column naming</li> <li>Performance benchmarks</li> </ol> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nfrom arcticdb import Arctic, LibraryOptions, OutputFormat, ArrowOutputStringFormat, where\n</pre> import numpy as np import pandas as pd import polars as pl import pyarrow as pa from arcticdb import Arctic, LibraryOptions, OutputFormat, ArrowOutputStringFormat, where In\u00a0[2]: Copied! <pre>ac = Arctic(\"lmdb://tmp/arrow_reads_demo\")\n</pre> ac = Arctic(\"lmdb://tmp/arrow_reads_demo\") In\u00a0[3]: Copied! <pre>ac.delete_library(\"arrow\")\nlib = ac.create_library(\"arrow\")\n</pre> ac.delete_library(\"arrow\") lib = ac.create_library(\"arrow\") In\u00a0[4]: Copied! <pre>sym = \"test\"\n</pre> sym = \"test\" In\u00a0[5]: Copied! <pre>## Basic Usage\n\n# Write some data with Pandas (writing Polars/Arrow directly is not yet supported)\ndf = pd.DataFrame({\n    \"col_int\": np.arange(10, dtype=np.int64),\n    \"col_float\": np.arange(10, 20, dtype=np.float64),\n    \"col_str\": [f\"value_{i}\" for i in range(10)]\n})\nlib.write(\"demo\", df)\ndf\n</pre> ## Basic Usage  # Write some data with Pandas (writing Polars/Arrow directly is not yet supported) df = pd.DataFrame({     \"col_int\": np.arange(10, dtype=np.int64),     \"col_float\": np.arange(10, 20, dtype=np.float64),     \"col_str\": [f\"value_{i}\" for i in range(10)] }) lib.write(\"demo\", df) df Out[5]: col_int col_float col_str 0 0 10.0 value_0 1 1 11.0 value_1 2 2 12.0 value_2 3 3 13.0 value_3 4 4 14.0 value_4 5 5 15.0 value_5 6 6 16.0 value_6 7 7 17.0 value_7 8 8 18.0 value_8 9 9 19.0 value_9 <p>Read back as a Polars DataFrame:</p> In\u00a0[6]: Copied! <pre>polars_df = lib.read(\"demo\", output_format=OutputFormat.POLARS).data\nprint(f\"Type: {type(polars_df)}\")\nprint(f\"Dtypes: {polars_df.dtypes}\")\npolars_df\n</pre> polars_df = lib.read(\"demo\", output_format=OutputFormat.POLARS).data print(f\"Type: {type(polars_df)}\") print(f\"Dtypes: {polars_df.dtypes}\") polars_df <pre>Type: &lt;class 'polars.dataframe.frame.DataFrame'&gt;\nDtypes: [Int64, Float64, String]\n</pre> Out[6]: shape: (10, 3)col_intcol_floatcol_stri64f64str010.0\"value_0\"111.0\"value_1\"212.0\"value_2\"313.0\"value_3\"414.0\"value_4\"515.0\"value_5\"616.0\"value_6\"717.0\"value_7\"818.0\"value_8\"919.0\"value_9\" In\u00a0[7]: Copied! <pre># Per-read (most granular)\npolars_df = lib.read(\"demo\", output_format=OutputFormat.POLARS).data\n\n# Case-insensitive strings also work\npolars_df = lib.head(\"demo\", output_format=\"polars\").data\npolars_df\n</pre> # Per-read (most granular) polars_df = lib.read(\"demo\", output_format=OutputFormat.POLARS).data  # Case-insensitive strings also work polars_df = lib.head(\"demo\", output_format=\"polars\").data polars_df Out[7]: shape: (5, 3)col_intcol_floatcol_stri64f64str010.0\"value_0\"111.0\"value_1\"212.0\"value_2\"313.0\"value_3\"414.0\"value_4\" <p>Set at library level:</p> In\u00a0[8]: Copied! <pre>lib_polars = ac.get_library(\"arrow\", output_format=OutputFormat.POLARS)\n# Now all reads default to Polars\nlib_polars.head(\"demo\").data\n</pre> lib_polars = ac.get_library(\"arrow\", output_format=OutputFormat.POLARS) # Now all reads default to Polars lib_polars.head(\"demo\").data Out[8]: shape: (5, 3)col_intcol_floatcol_stri64f64str010.0\"value_0\"111.0\"value_1\"212.0\"value_2\"313.0\"value_3\"414.0\"value_4\" In\u00a0[9]: Copied! <pre>polars_ac = Arctic(\"lmdb://tmp/arrow_reads_demo_2\", output_format=OutputFormat.POLARS)\n\n# Now all libraries will have a default polars output format\npolars_ac.delete_library(\"arrow\")\nlib = polars_ac.create_library(\"arrow\")\nlib.write(\"demo\", df)\nlib.head(\"demo\").data\n</pre> polars_ac = Arctic(\"lmdb://tmp/arrow_reads_demo_2\", output_format=OutputFormat.POLARS)  # Now all libraries will have a default polars output format polars_ac.delete_library(\"arrow\") lib = polars_ac.create_library(\"arrow\") lib.write(\"demo\", df) lib.head(\"demo\").data Out[9]: shape: (5, 3)col_intcol_floatcol_stri64f64str010.0\"value_0\"111.0\"value_1\"212.0\"value_2\"313.0\"value_3\"414.0\"value_4\" <p>You can always override back to Pandas for individual reads:</p> In\u00a0[10]: Copied! <pre>pandas_df = lib.read(\"demo\", output_format=OutputFormat.PANDAS).data\ntype(pandas_df)\n</pre> pandas_df = lib.read(\"demo\", output_format=OutputFormat.PANDAS).data type(pandas_df) Out[10]: <pre>pandas.core.frame.DataFrame</pre> In\u00a0[11]: Copied! <pre>df = pd.DataFrame({\n    \"col_int\": np.arange(10, dtype=np.int64),\n    \"col_float\": np.arange(10, 20, dtype=np.float64)\n}, index=pd.date_range(\"2025-01-01\", periods=10))\ndf.index.name = \"timestamp\"\nlib.write(\"timeseries\", df)\ndf\n</pre> df = pd.DataFrame({     \"col_int\": np.arange(10, dtype=np.int64),     \"col_float\": np.arange(10, 20, dtype=np.float64) }, index=pd.date_range(\"2025-01-01\", periods=10)) df.index.name = \"timestamp\" lib.write(\"timeseries\", df) df Out[11]: col_int col_float timestamp 2025-01-01 0 10.0 2025-01-02 1 11.0 2025-01-03 2 12.0 2025-01-04 3 13.0 2025-01-05 4 14.0 2025-01-06 5 15.0 2025-01-07 6 16.0 2025-01-08 7 17.0 2025-01-09 8 18.0 2025-01-10 9 19.0 In\u00a0[12]: Copied! <pre>polars_df = lib.read(\"timeseries\").data\nprint(f\"Columns: {polars_df.columns}\")\nprint(f\"Dtypes: {polars_df.dtypes}\")\npolars_df\n</pre> polars_df = lib.read(\"timeseries\").data print(f\"Columns: {polars_df.columns}\") print(f\"Dtypes: {polars_df.dtypes}\") polars_df <pre>Columns: ['timestamp', 'col_int', 'col_float']\nDtypes: [Datetime(time_unit='ns', time_zone=None), Int64, Float64]\n</pre> Out[12]: shape: (10, 3)timestampcol_intcol_floatdatetime[ns]i64f642025-01-01 00:00:00010.02025-01-02 00:00:00111.02025-01-03 00:00:00212.02025-01-04 00:00:00313.02025-01-05 00:00:00414.02025-01-06 00:00:00515.02025-01-07 00:00:00616.02025-01-08 00:00:00717.02025-01-09 00:00:00818.02025-01-10 00:00:00919.0 <p>Timezones are preserved:</p> In\u00a0[13]: Copied! <pre>df_tz = pd.DataFrame({\n    \"value\": np.arange(5)\n}, index=pd.date_range(\"2025-01-01\", periods=5, tz=\"America/New_York\"))\ndf_tz.index.name = \"timestamp\"\nlib.write(\"tz_data\", df_tz)\ndf_tz\n</pre> df_tz = pd.DataFrame({     \"value\": np.arange(5) }, index=pd.date_range(\"2025-01-01\", periods=5, tz=\"America/New_York\")) df_tz.index.name = \"timestamp\" lib.write(\"tz_data\", df_tz) df_tz Out[13]: value timestamp 2025-01-01 00:00:00-05:00 0 2025-01-02 00:00:00-05:00 1 2025-01-03 00:00:00-05:00 2 2025-01-04 00:00:00-05:00 3 2025-01-05 00:00:00-05:00 4 In\u00a0[14]: Copied! <pre>polars_df = lib.read(\"tz_data\").data\nprint(f\"Timezone preserved: {polars_df['timestamp'].dtype}\")\npolars_df\n</pre> polars_df = lib.read(\"tz_data\").data print(f\"Timezone preserved: {polars_df['timestamp'].dtype}\") polars_df <pre>Timezone preserved: Datetime(time_unit='ns', time_zone='America/New_York')\n</pre> Out[14]: shape: (5, 2)timestampvaluedatetime[ns, America/New_York]i642025-01-01 00:00:00 EST02025-01-02 00:00:00 EST12025-01-03 00:00:00 EST22025-01-04 00:00:00 EST32025-01-05 00:00:00 EST4 In\u00a0[15]: Copied! <pre>polars_df = lib.read(\n    \"timeseries\",\n    date_range=(pd.Timestamp(\"2025-01-03\"), pd.Timestamp(\"2025-01-06\")),\n    columns=[\"col_int\"]\n).data\npolars_df\n</pre> polars_df = lib.read(     \"timeseries\",     date_range=(pd.Timestamp(\"2025-01-03\"), pd.Timestamp(\"2025-01-06\")),     columns=[\"col_int\"] ).data polars_df Out[15]: shape: (4, 2)timestampcol_intdatetime[ns]i642025-01-03 00:00:0022025-01-04 00:00:0032025-01-05 00:00:0042025-01-06 00:00:005 In\u00a0[16]: Copied! <pre>arrow_table = lib.read(\"demo\", output_format=OutputFormat.PYARROW).data\nprint(f\"Type: {type(arrow_table)}\")\nprint(f\"Schema: {arrow_table.schema}\")\narrow_table\n</pre> arrow_table = lib.read(\"demo\", output_format=OutputFormat.PYARROW).data print(f\"Type: {type(arrow_table)}\") print(f\"Schema: {arrow_table.schema}\") arrow_table <pre>Type: &lt;class 'pyarrow.lib.Table'&gt;\nSchema: col_int: int64\ncol_float: double\ncol_str: large_string\n-- schema metadata --\npandas: '{\"index_columns\": [{\"name\": null, \"start\": 0, \"step\": 1, \"stop\":' + 500\n</pre> Out[16]: <pre>pyarrow.Table\ncol_int: int64\ncol_float: double\ncol_str: large_string\n----\ncol_int: [[0,1,2,3,4,5,6,7,8,9]]\ncol_float: [[10,11,12,13,14,15,16,17,18,19]]\ncol_str: [[\"value_0\",\"value_1\",\"value_2\",\"value_3\",\"value_4\",\"value_5\",\"value_6\",\"value_7\",\"value_8\",\"value_9\"]]</pre> In\u00a0[17]: Copied! <pre># PyArrow to Polars (zero-copy)\npolars_from_arrow = pl.from_arrow(arrow_table)\nprint(f\"Converted to Polars: {type(polars_from_arrow)}\")\n\n# Polars to PyArrow (zero-copy)\narrow_from_polars = polars_from_arrow.to_arrow()\nprint(f\"Converted back to Arrow: {type(arrow_from_polars)}\")\nprint(f\"Tables equal: {arrow_table.equals(arrow_from_polars)}\")\n</pre> # PyArrow to Polars (zero-copy) polars_from_arrow = pl.from_arrow(arrow_table) print(f\"Converted to Polars: {type(polars_from_arrow)}\")  # Polars to PyArrow (zero-copy) arrow_from_polars = polars_from_arrow.to_arrow() print(f\"Converted back to Arrow: {type(arrow_from_polars)}\") print(f\"Tables equal: {arrow_table.equals(arrow_from_polars)}\") <pre>Converted to Polars: &lt;class 'polars.dataframe.frame.DataFrame'&gt;\nConverted back to Arrow: &lt;class 'pyarrow.lib.Table'&gt;\nTables equal: True\n</pre> In\u00a0[18]: Copied! <pre>import duckdb\n\n# Load data from ArcticDB as PyArrow table\narrow_table = lib.read(\"timeseries\", output_format=OutputFormat.PYARROW).data\n\n# Query directly with DuckDB (zero-copy)\nresult = duckdb.query(\"\"\"\n    SELECT \n        timestamp,\n        col_int,\n        col_float,\n        col_int * col_float AS product\n    FROM arrow_table\n    WHERE col_int &gt; 5\n    ORDER BY timestamp\n\"\"\").to_arrow_table()\n\nprint(f\"Result type: {type(result)}\")\n# Converting to polars for easier viewing\npl.from_arrow(result)\n</pre> import duckdb  # Load data from ArcticDB as PyArrow table arrow_table = lib.read(\"timeseries\", output_format=OutputFormat.PYARROW).data  # Query directly with DuckDB (zero-copy) result = duckdb.query(\"\"\"     SELECT          timestamp,         col_int,         col_float,         col_int * col_float AS product     FROM arrow_table     WHERE col_int &gt; 5     ORDER BY timestamp \"\"\").to_arrow_table()  print(f\"Result type: {type(result)}\") # Converting to polars for easier viewing pl.from_arrow(result) <pre>Result type: &lt;class 'pyarrow.lib.Table'&gt;\n</pre> Out[18]: shape: (4, 4)timestampcol_intcol_floatproductdatetime[ns]i64f64f642025-01-07 00:00:00616.096.02025-01-08 00:00:00717.0119.02025-01-09 00:00:00818.0144.02025-01-10 00:00:00919.0171.0 In\u00a0[19]: Copied! <pre>df_0 = pd.DataFrame({\"col1\": np.arange(2, dtype=np.int64)})\ndf_1 = pd.DataFrame({\"col1\": np.arange(2, 4, dtype=np.int64)})\nlib.write(\"multi_batch\", df_0)\nlib.append(\"multi_batch\", df_1)\n\narrow_table = lib.read(\"multi_batch\", output_format=OutputFormat.PYARROW).data\nprint(f\"Number of record batches: {len(arrow_table.to_batches())}\")\nprint(f\"Table structure: {arrow_table}\")\narrow_table\n</pre> df_0 = pd.DataFrame({\"col1\": np.arange(2, dtype=np.int64)}) df_1 = pd.DataFrame({\"col1\": np.arange(2, 4, dtype=np.int64)}) lib.write(\"multi_batch\", df_0) lib.append(\"multi_batch\", df_1)  arrow_table = lib.read(\"multi_batch\", output_format=OutputFormat.PYARROW).data print(f\"Number of record batches: {len(arrow_table.to_batches())}\") print(f\"Table structure: {arrow_table}\") arrow_table <pre>Number of record batches: 2\nTable structure: pyarrow.Table\ncol1: int64\n----\ncol1: [[0,1],[2,3]]\n</pre> Out[19]: <pre>pyarrow.Table\ncol1: int64\n----\ncol1: [[0,1],[2,3]]</pre> <p>You can combine chunks into a single contiguous table (involves memory allocation and copying):</p> In\u00a0[20]: Copied! <pre>contiguous_table = arrow_table.combine_chunks()\nprint(f\"Combined equals original: {contiguous_table.equals(arrow_table)}\")\ncontiguous_table\n</pre> contiguous_table = arrow_table.combine_chunks() print(f\"Combined equals original: {contiguous_table.equals(arrow_table)}\") contiguous_table <pre>Combined equals original: True\n</pre> Out[20]: <pre>pyarrow.Table\ncol1: int64\n----\ncol1: [[0,1,2,3]]</pre> <p>Chunks can also be combined with polars output format with <code>rechunk</code>.</p> <p>After rechunking some computations on the polars frame will be faster.</p> In\u00a0[21]: Copied! <pre>polars_df = lib.read(\"multi_batch\", output_format=OutputFormat.POLARS).data\nprint(\"Number of chunks in Polars DataFrame:\", polars_df.n_chunks())\npolars_df = polars_df.rechunk()\nprint(\"Number of chunks after rechunking:\", polars_df.n_chunks())\npolars_df\n</pre> polars_df = lib.read(\"multi_batch\", output_format=OutputFormat.POLARS).data print(\"Number of chunks in Polars DataFrame:\", polars_df.n_chunks()) polars_df = polars_df.rechunk() print(\"Number of chunks after rechunking:\", polars_df.n_chunks()) polars_df <pre>Number of chunks in Polars DataFrame: 2\nNumber of chunks after rechunking: 1\n</pre> Out[21]: shape: (4, 1)col1i640123 In\u00a0[22]: Copied! <pre>df_strings = pd.DataFrame({\n    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"] * 2,  # Low cardinality\n    \"description\": [f\"Long description text {i}\" for i in range(10)]  # High cardinality\n})\nlib.write(\"strings\", df_strings)\n\n# Default behavior (LARGE_STRING)\npolars_df = lib.read(\"strings\", output_format=OutputFormat.POLARS).data\nprint(f\"Dtypes: {polars_df.dtypes}\")\npolars_df\n</pre> df_strings = pd.DataFrame({     \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\"] * 2,  # Low cardinality     \"description\": [f\"Long description text {i}\" for i in range(10)]  # High cardinality }) lib.write(\"strings\", df_strings)  # Default behavior (LARGE_STRING) polars_df = lib.read(\"strings\", output_format=OutputFormat.POLARS).data print(f\"Dtypes: {polars_df.dtypes}\") polars_df <pre>Dtypes: [String, String]\n</pre> Out[22]: shape: (10, 2)categorydescriptionstrstr\"A\"\"Long description text 0\"\"B\"\"Long description text 1\"\"A\"\"Long description text 2\"\"B\"\"Long description text 3\"\"A\"\"Long description text 4\"\"A\"\"Long description text 5\"\"B\"\"Long description text 6\"\"A\"\"Long description text 7\"\"B\"\"Long description text 8\"\"A\"\"Long description text 9\" In\u00a0[23]: Copied! <pre># Set categorical for all string columns\npolars_df_cat = lib.read(\n    \"strings\",\n    output_format=OutputFormat.POLARS,\n    arrow_string_format_default=ArrowOutputStringFormat.CATEGORICAL\n).data\nprint(f\"Dtypes: {polars_df_cat.dtypes}\")\npolars_df_cat\n</pre> # Set categorical for all string columns polars_df_cat = lib.read(     \"strings\",     output_format=OutputFormat.POLARS,     arrow_string_format_default=ArrowOutputStringFormat.CATEGORICAL ).data print(f\"Dtypes: {polars_df_cat.dtypes}\") polars_df_cat <pre>Dtypes: [Categorical, Categorical]\n</pre> Out[23]: shape: (10, 2)categorydescriptioncatcat\"A\"\"Long description text 0\"\"B\"\"Long description text 1\"\"A\"\"Long description text 2\"\"B\"\"Long description text 3\"\"A\"\"Long description text 4\"\"A\"\"Long description text 5\"\"B\"\"Long description text 6\"\"A\"\"Long description text 7\"\"B\"\"Long description text 8\"\"A\"\"Long description text 9\" In\u00a0[24]: Copied! <pre># Categorical for low cardinality, LARGE_STRING for high cardinality\npolars_df_mixed = lib.read(\n    \"strings\",\n    output_format=OutputFormat.POLARS,\n    arrow_string_format_per_column={\n        \"category\": ArrowOutputStringFormat.CATEGORICAL,\n        \"description\": ArrowOutputStringFormat.LARGE_STRING\n    }\n).data\nprint(f\"Dtypes: {polars_df_mixed.dtypes}\")\nprint(f\"Category is Categorical: {polars_df_mixed['category'].dtype == pl.Categorical}\")\nprint(f\"Description is String: {polars_df_mixed['description'].dtype == pl.String}\")\n</pre> # Categorical for low cardinality, LARGE_STRING for high cardinality polars_df_mixed = lib.read(     \"strings\",     output_format=OutputFormat.POLARS,     arrow_string_format_per_column={         \"category\": ArrowOutputStringFormat.CATEGORICAL,         \"description\": ArrowOutputStringFormat.LARGE_STRING     } ).data print(f\"Dtypes: {polars_df_mixed.dtypes}\") print(f\"Category is Categorical: {polars_df_mixed['category'].dtype == pl.Categorical}\") print(f\"Description is String: {polars_df_mixed['description'].dtype == pl.String}\") <pre>Dtypes: [Categorical, String]\nCategory is Categorical: True\nDescription is String: True\n</pre> In\u00a0[25]: Copied! <pre># DataFrame with unnamed DatetimeIndex\ndf_unnamed_idx = pd.DataFrame(\n    {\"value\": [1, 2, 3]},\n    index=pd.date_range(\"2025-01-01\", periods=3)\n)\nlib.write(\"unnamed_idx\", df_unnamed_idx)\n\npolars_df = lib.read(\"unnamed_idx\", output_format=OutputFormat.POLARS).data\nprint(f\"Columns: {polars_df.columns}\")\nprint(f\"First column name: '{polars_df.columns[0]}'\")\npolars_df\n</pre> # DataFrame with unnamed DatetimeIndex df_unnamed_idx = pd.DataFrame(     {\"value\": [1, 2, 3]},     index=pd.date_range(\"2025-01-01\", periods=3) ) lib.write(\"unnamed_idx\", df_unnamed_idx)  polars_df = lib.read(\"unnamed_idx\", output_format=OutputFormat.POLARS).data print(f\"Columns: {polars_df.columns}\") print(f\"First column name: '{polars_df.columns[0]}'\") polars_df <pre>Columns: ['__index__', 'value']\nFirst column name: '__index__'\n</pre> Out[25]: shape: (3, 2)__index__valuedatetime[ns]i642025-01-01 00:00:0012025-01-02 00:00:0022025-01-03 00:00:003 In\u00a0[26]: Copied! <pre># We already wrote \"timeseries\" with a named index \"timestamp\"\npolars_df = lib.read(\"timeseries\", output_format=OutputFormat.POLARS).data\nprint(f\"First column: '{polars_df.columns[0]}' (the named index)\")\npolars_df.head(3)\n</pre> # We already wrote \"timeseries\" with a named index \"timestamp\" polars_df = lib.read(\"timeseries\", output_format=OutputFormat.POLARS).data print(f\"First column: '{polars_df.columns[0]}' (the named index)\") polars_df.head(3) <pre>First column: 'timestamp' (the named index)\n</pre> Out[26]: shape: (3, 3)timestampcol_intcol_floatdatetime[ns]i64f642025-01-01 00:00:00010.02025-01-02 00:00:00111.02025-01-03 00:00:00212.0 In\u00a0[27]: Copied! <pre>df_multi = pd.DataFrame({\n    \"price\": [100, 101, 102, 103]\n}, index=pd.MultiIndex.from_product([[\"2025-01-01\", \"2025-01-02\"], [\"AAPL\", \"GOOGL\"]], names=[\"date\", \"ticker\"]))\nlib.write(\"multiindex\", df_multi)\n\npolars_df = lib.read(\"multiindex\", output_format=OutputFormat.POLARS).data\nprint(f\"Columns: {polars_df.columns}\")\nprint(\"MultiIndex levels became regular columns\")\npolars_df\n</pre> df_multi = pd.DataFrame({     \"price\": [100, 101, 102, 103] }, index=pd.MultiIndex.from_product([[\"2025-01-01\", \"2025-01-02\"], [\"AAPL\", \"GOOGL\"]], names=[\"date\", \"ticker\"])) lib.write(\"multiindex\", df_multi)  polars_df = lib.read(\"multiindex\", output_format=OutputFormat.POLARS).data print(f\"Columns: {polars_df.columns}\") print(\"MultiIndex levels became regular columns\") polars_df <pre>Columns: ['date', 'ticker', 'price']\nMultiIndex levels became regular columns\n</pre> Out[27]: shape: (4, 3)datetickerpricestrstri64\"2025-01-01\"\"AAPL\"100\"2025-01-01\"\"GOOGL\"101\"2025-01-02\"\"AAPL\"102\"2025-01-02\"\"GOOGL\"103 <p>Unnamed MultiIndex columns get displayed as <code>\"__index_level_0__\"</code>, <code>\"__index_level_1__\"</code>, etc.</p> In\u00a0[28]: Copied! <pre>df_multi = pd.DataFrame({\n    \"price\": [100, 101, 102, 103]\n}, index=pd.MultiIndex.from_product([[\"2025-01-01\", \"2025-01-02\"], [\"AAPL\", \"GOOGL\"]]))\nlib.write(\"multiindex\", df_multi)\n\npolars_df = lib.read(\"multiindex\", output_format=OutputFormat.POLARS).data\nprint(f\"Columns: {polars_df.columns}\")\nprint(\"MultiIndex levels became regular columns and use special names when unnamed\")\npolars_df\n</pre> df_multi = pd.DataFrame({     \"price\": [100, 101, 102, 103] }, index=pd.MultiIndex.from_product([[\"2025-01-01\", \"2025-01-02\"], [\"AAPL\", \"GOOGL\"]])) lib.write(\"multiindex\", df_multi)  polars_df = lib.read(\"multiindex\", output_format=OutputFormat.POLARS).data print(f\"Columns: {polars_df.columns}\") print(\"MultiIndex levels became regular columns and use special names when unnamed\") polars_df <pre>Columns: ['__index_level_0__', '__index_level_1__', 'price']\nMultiIndex levels became regular columns and use special names when unnamed\n</pre> Out[28]: shape: (4, 3)__index_level_0____index_level_1__pricestrstri64\"2025-01-01\"\"AAPL\"100\"2025-01-01\"\"GOOGL\"101\"2025-01-02\"\"AAPL\"102\"2025-01-02\"\"GOOGL\"103 In\u00a0[29]: Copied! <pre># Default RangeIndex is restored\ndf_simple = pd.DataFrame({\"col1\": np.arange(5)})\nlib.write(\"simple_pandas\", df_simple)\n\narrow_table = lib.read(\"simple_pandas\", output_format=OutputFormat.PYARROW).data\npandas_restored = arrow_table.to_pandas()\nprint(f\"Index restored: {type(pandas_restored.index)}\")\npandas_restored\n</pre> # Default RangeIndex is restored df_simple = pd.DataFrame({\"col1\": np.arange(5)}) lib.write(\"simple_pandas\", df_simple)  arrow_table = lib.read(\"simple_pandas\", output_format=OutputFormat.PYARROW).data pandas_restored = arrow_table.to_pandas() print(f\"Index restored: {type(pandas_restored.index)}\") pandas_restored <pre>Index restored: &lt;class 'pandas.core.indexes.range.RangeIndex'&gt;\n</pre> Out[29]: col1 0 0 1 1 2 2 3 3 4 4 In\u00a0[30]: Copied! <pre># MultiIndex is also restored correctly\narrow_table = lib.read(\"multiindex\", output_format=OutputFormat.PYARROW).data\npandas_restored = arrow_table.to_pandas()\nprint(f\"MultiIndex restored: {type(pandas_restored.index)}\")\nprint(f\"Index names: {pandas_restored.index.names}\")\npandas_restored\n</pre> # MultiIndex is also restored correctly arrow_table = lib.read(\"multiindex\", output_format=OutputFormat.PYARROW).data pandas_restored = arrow_table.to_pandas() print(f\"MultiIndex restored: {type(pandas_restored.index)}\") print(f\"Index names: {pandas_restored.index.names}\") pandas_restored <pre>MultiIndex restored: &lt;class 'pandas.core.indexes.multi.MultiIndex'&gt;\nIndex names: [None, None]\n</pre> Out[30]: price 2025-01-01 AAPL 100 GOOGL 101 2025-01-02 AAPL 102 GOOGL 103 In\u00a0[31]: Copied! <pre>polars_df = lib.read(\"timeseries\", output_format=OutputFormat.POLARS).data\npandas_from_polars = polars_df.to_pandas()\nprint(f\"Type: {type(pandas_from_polars)}\")\nprint(\"Note: Index information is lost when converting from Polars to Pandas\")\npandas_from_polars.head()\n</pre> polars_df = lib.read(\"timeseries\", output_format=OutputFormat.POLARS).data pandas_from_polars = polars_df.to_pandas() print(f\"Type: {type(pandas_from_polars)}\") print(\"Note: Index information is lost when converting from Polars to Pandas\") pandas_from_polars.head() <pre>Type: &lt;class 'pandas.core.frame.DataFrame'&gt;\nNote: Index information is lost when converting from Polars to Pandas\n</pre> Out[31]: timestamp col_int col_float 0 2025-01-01 0 10.0 1 2025-01-02 1 11.0 2 2025-01-03 2 12.0 3 2025-01-04 3 13.0 4 2025-01-05 4 14.0 In\u00a0[32]: Copied! <pre># Create a dynamic schema library\nac.delete_library(\"arrow_dynamic\")\nlib_dyn = ac.create_library(\"arrow_dynamic\", LibraryOptions(dynamic_schema=True))\n\n## Type Promotion\n\ndf_0 = pd.DataFrame({\"col1\": np.arange(3, dtype=np.uint8)})\ndf_1 = pd.DataFrame({\"col1\": np.arange(3, 6, dtype=np.int16)})\nlib_dyn.write(\"type_promo\", df_0)\nlib_dyn.append(\"type_promo\", df_1)\n\npolars_df = lib_dyn.read(\"type_promo\", output_format=OutputFormat.POLARS).data\nprint(f\"Promoted dtype: {polars_df['col1'].dtype}\")\npolars_df\n</pre> # Create a dynamic schema library ac.delete_library(\"arrow_dynamic\") lib_dyn = ac.create_library(\"arrow_dynamic\", LibraryOptions(dynamic_schema=True))  ## Type Promotion  df_0 = pd.DataFrame({\"col1\": np.arange(3, dtype=np.uint8)}) df_1 = pd.DataFrame({\"col1\": np.arange(3, 6, dtype=np.int16)}) lib_dyn.write(\"type_promo\", df_0) lib_dyn.append(\"type_promo\", df_1)  polars_df = lib_dyn.read(\"type_promo\", output_format=OutputFormat.POLARS).data print(f\"Promoted dtype: {polars_df['col1'].dtype}\") polars_df <pre>Promoted dtype: Int16\n</pre> Out[32]: shape: (6, 1)col1i16012345 In\u00a0[33]: Copied! <pre>df_0 = pd.DataFrame({\"col_a\": [1, 2]})\ndf_1 = pd.DataFrame({\"col_b\": [\"x\", \"y\"]})\nlib_dyn.write(\"missing_cols\", df_0)\nlib_dyn.append(\"missing_cols\", df_1)\n\npolars_df = lib_dyn.read(\"missing_cols\", output_format=OutputFormat.POLARS).data\nprint(\"Missing columns filled with nulls:\")\npolars_df\n</pre> df_0 = pd.DataFrame({\"col_a\": [1, 2]}) df_1 = pd.DataFrame({\"col_b\": [\"x\", \"y\"]}) lib_dyn.write(\"missing_cols\", df_0) lib_dyn.append(\"missing_cols\", df_1)  polars_df = lib_dyn.read(\"missing_cols\", output_format=OutputFormat.POLARS).data print(\"Missing columns filled with nulls:\") polars_df <pre>Missing columns filled with nulls:\n</pre> Out[33]: shape: (4, 2)col_acol_bi64str1null2nullnull\"x\"null\"y\" In\u00a0[34]: Copied! <pre>df_proc_1 = pd.DataFrame({\"value\": [-1, 2, 10] * 3, \"float_1\": np.arange(9, dtype=np.float64)})\ndf_proc_2 = pd.DataFrame({\"value\": [-5, 8, 20] * 3, \"float_2\": np.arange(9, dtype=np.float64)})\nlib_dyn.write(\"missing_cols\", df_proc_1)\nlib_dyn.append(\"missing_cols\", df_proc_2)\n\n# Use lazy processing with Polars output\nlazy_df = lib_dyn.read(\"missing_cols\", lazy=True, output_format=OutputFormat.POLARS)\nlazy_df = lazy_df[lazy_df[\"value\"] &lt; 10] # Filter based on column values\nlazy_df[\"combined_float\"] = where(lazy_df[\"value\"] &lt; 0, lazy_df[\"float_1\"], lazy_df[\"float_2\"]) # Project a column with interleaved nulls\nresult = lazy_df.collect().data\n\nprint(f\"Result type: {type(result)}\")\nprint(f\"Filtered rows: {len(result)}\")\nresult\n</pre> df_proc_1 = pd.DataFrame({\"value\": [-1, 2, 10] * 3, \"float_1\": np.arange(9, dtype=np.float64)}) df_proc_2 = pd.DataFrame({\"value\": [-5, 8, 20] * 3, \"float_2\": np.arange(9, dtype=np.float64)}) lib_dyn.write(\"missing_cols\", df_proc_1) lib_dyn.append(\"missing_cols\", df_proc_2)  # Use lazy processing with Polars output lazy_df = lib_dyn.read(\"missing_cols\", lazy=True, output_format=OutputFormat.POLARS) lazy_df = lazy_df[lazy_df[\"value\"] &lt; 10] # Filter based on column values lazy_df[\"combined_float\"] = where(lazy_df[\"value\"] &lt; 0, lazy_df[\"float_1\"], lazy_df[\"float_2\"]) # Project a column with interleaved nulls result = lazy_df.collect().data  print(f\"Result type: {type(result)}\") print(f\"Filtered rows: {len(result)}\") result <pre>Result type: &lt;class 'polars.dataframe.frame.DataFrame'&gt;\nFiltered rows: 12\n</pre> Out[34]: shape: (12, 4)valuefloat_1float_2combined_floati64f64f64f64-10.0null0.021.0nullnull-13.0null3.024.0nullnull-16.0null6.0\u2026\u2026\u2026\u20268null1.01.0-5null3.0null8null4.04.0-5null6.0null8null7.07.0 In\u00a0[35]: Copied! <pre>import timeit\n\n# Create numeric dataset\ndf_numeric = pd.DataFrame({\n    f\"col_{i}\": np.random.randn(10_000_000) for i in range(10)\n})\nlib.write(\"bench_numeric\", df_numeric)\n\n# Benchmark\npandas_time = timeit.timeit(\n    lambda: lib.read(\"bench_numeric\", output_format=OutputFormat.PANDAS).data,\n    number=10\n) / 10\n\npolars_time = timeit.timeit(\n    lambda: lib.read(\"bench_numeric\", output_format=OutputFormat.POLARS).data,\n    number=10\n) / 10\n\nprint(f\"Pandas: {pandas_time*1000:.2f} ms\")\nprint(f\"Polars: {polars_time*1000:.2f} ms\")\nprint(f\"Speedup: {pandas_time/polars_time:.2f}x\")\n</pre> import timeit  # Create numeric dataset df_numeric = pd.DataFrame({     f\"col_{i}\": np.random.randn(10_000_000) for i in range(10) }) lib.write(\"bench_numeric\", df_numeric)  # Benchmark pandas_time = timeit.timeit(     lambda: lib.read(\"bench_numeric\", output_format=OutputFormat.PANDAS).data,     number=10 ) / 10  polars_time = timeit.timeit(     lambda: lib.read(\"bench_numeric\", output_format=OutputFormat.POLARS).data,     number=10 ) / 10  print(f\"Pandas: {pandas_time*1000:.2f} ms\") print(f\"Polars: {polars_time*1000:.2f} ms\") print(f\"Speedup: {pandas_time/polars_time:.2f}x\") <pre>Pandas: 466.19 ms\nPolars: 441.10 ms\nSpeedup: 1.06x\n</pre> In\u00a0[36]: Copied! <pre>df_strings = pd.DataFrame({\n    f\"col_{i}\": np.random.randn(1_000_000).astype(str) for i in range(10)\n})\nlib.write(\"bench_strings\", df_strings)\n\n# Benchmark\npandas_time = timeit.timeit(\n    lambda: lib.read(\"bench_strings\", output_format=OutputFormat.PANDAS).data,\n    number=10\n) / 10\n\npolars_time = timeit.timeit(\n    lambda: lib.read(\"bench_strings\", output_format=OutputFormat.POLARS).data,\n    number=10\n) / 10\n\nprint(f\"Pandas: {pandas_time*1000:.2f} ms\")\nprint(f\"Polars: {polars_time*1000:.2f} ms\")\nprint(f\"Speedup: {pandas_time/polars_time:.2f}x\")\n</pre>  df_strings = pd.DataFrame({     f\"col_{i}\": np.random.randn(1_000_000).astype(str) for i in range(10) }) lib.write(\"bench_strings\", df_strings)  # Benchmark pandas_time = timeit.timeit(     lambda: lib.read(\"bench_strings\", output_format=OutputFormat.PANDAS).data,     number=10 ) / 10  polars_time = timeit.timeit(     lambda: lib.read(\"bench_strings\", output_format=OutputFormat.POLARS).data,     number=10 ) / 10  print(f\"Pandas: {pandas_time*1000:.2f} ms\") print(f\"Polars: {polars_time*1000:.2f} ms\") print(f\"Speedup: {pandas_time/polars_time:.2f}x\") <pre>Pandas: 1662.48 ms\nPolars: 694.08 ms\nSpeedup: 2.40x\n</pre>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#arcticdb-read-as-arrow-demo","title":"ArcticDB Read as Arrow demo\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#polars-output-format","title":"Polars Output Format\u00b6","text":"<p>All ArcticDB operations returning dataframes (e.g. <code>read</code>, <code>head</code>, <code>tail</code>, <code>read_batch</code>, <code>read_batch_and_join</code>) accept an <code>output_format</code> which controls in which format will the data be returned. The default output format is still <code>OutputFormat.PANDAS</code> which return <code>pd.DataFrame</code>s as before.</p> <p>The Apache Arrow memory layout based output formats are <code>POLARS</code> which returns <code>polars.DataFrame</code> objects and <code>PYARROW</code> which returns <code>pyarrow.Table</code> objects.</p> <p>Let's see several examples of using <code>POLARS</code> output format</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#index-handling","title":"Index Handling\u00b6","text":"<p>Note that default <code>RangeIndex</code> is dropped (Polars has no concept of row indexes):</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#configuration-levels","title":"Configuration Levels\u00b6","text":"<p>Output format can be set at three levels (Arctic instance, Library, or per-read):</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#or-on-the-entire-arctic-instance-so-all-libraries-fetched-from-this-instance-use-arrow-as-the-default-return-type","title":"Or on the entire <code>Arctic</code> instance, so all libraries fetched from this instance use Arrow as the default return type\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#working-with-timeseries-indices","title":"Working with timeseries indices\u00b6","text":"<p>Timeseries indices appear as the first column:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#filtering-and-column-selection","title":"Filtering and Column Selection\u00b6","text":"<p>Standard ArcticDB operations work with Arrow output formats:</p> <p>Note that index column is fetched even if not specified in the <code>columns</code> list.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#pyarrow-output-format","title":"PyArrow Output Format\u00b6","text":"<p>The PyArrow output format returns <code>pyarrow.Table</code> objects. While we recommend Polars for most arrow use cases, PyArrow is useful when you want to integrate with other Arrow-based tools which expect a <code>pyarrow.Table</code>.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#basic-pyarrow-usage","title":"Basic PyArrow Usage\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#zero-copy-conversion-between-arrow-based-libraries","title":"Zero-Copy Conversion Between Arrow based libraries\u00b6","text":"<p>Converting between PyArrow and Polars is zero-copy since they share the same memory layout:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#record-batch-structure","title":"Record Batch Structure\u00b6","text":"<p>Arrow tables returned from ArcticDB are split in record batches. Each record batch corresponds to a row-slice in ArcticDB's storage.</p> <p>ArcticDB will slice data by rows if it was written in multiple operations (e.g. with append in example below) or if a large (&gt;100k rows) dataframe is written in a single operation.</p> <p>This applies to both Pyarrow and Polars. With pyarrow we can see the record batch separation directly.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#configurable-string-formats","title":"Configurable String Formats\u00b6","text":"<p>Arrow-based output formats support configurable string encoding to optimize for your data characteristics. Three formats are available: <code>LARGE_STRING</code> (default), <code>SMALL_STRING</code> (only for PyArrow), and <code>CATEGORICAL</code>.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#string-format-options","title":"String Format Options\u00b6","text":"<ul> <li><p><code>LARGE_STRING</code> (default): 64-bit variable-size encoding, best for general use</p> <ul> <li>PyArrow: <code>pa.large_string()</code>, Polars: <code>pl.String</code></li> </ul> </li> <li><p><code>SMALL_STRING</code>: 32-bit variable-size encoding, slightly more memory efficient for smaller data</p> <ul> <li>PyArrow: <code>pa.string()</code>, Polars: Not supported</li> <li>Polars can only use large_strings or categoricals</li> </ul> </li> <li><p><code>CATEGORICAL</code>: Dictionary-encoded, best for low cardinality (few unique values)</p> <ul> <li>PyArrow: <code>pa.dictionary(pa.int32(), pa.large_string())</code>, Polars: <code>pl.Categorical</code></li> </ul> </li> </ul>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#example-default-large_string-format","title":"Example: Default LARGE_STRING Format\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#example-categorical-format-for-low-cardinality","title":"Example: CATEGORICAL Format for Low Cardinality\u00b6","text":"<p>Use <code>CATEGORICAL</code> when you have few unique values repeated many times:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#per-column-string-format-configuration","title":"Per-Column String Format Configuration\u00b6","text":"<p>Optimize each column individually:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#pandas-interoperability-and-column-naming","title":"Pandas Interoperability and Column Naming\u00b6","text":"<p>Since ArcticDB currently only supports writing Pandas DataFrames, understanding how Pandas metadata translates to Arrow/Polars is important. ArcticDB attaches normalization metadata to enable seamless round-trip conversion.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#index-column-naming-__index__","title":"Index Column Naming: <code>__index__</code>\u00b6","text":"<p>When Pandas DataFrames have unnamed indexes, PyArrow/Polars output formats use the special column name <code>__index__</code>:</p> <p>Note that if there are duplicate column names (e.g. column <code>__index__</code> already exists) all duplicates are resolved by adding extra underscores to beginning and end of column name</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#named-indexes-preserved","title":"Named Indexes Preserved\u00b6","text":"<p>Named indexes retain their names as column names:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#multiindex-handling","title":"MultiIndex Handling\u00b6","text":"<p>MultiIndex levels become separate columns:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#round-trip-conversion","title":"Round-Trip Conversion\u00b6","text":"<p>Converting to PyArrow and back preserves Pandas metadata:</p> <p>Note that Polars does not have a concept of Pandas metadata and can't be round tripped to pandas without loosing index metadata.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#working-with-dynamic-schema","title":"Working with Dynamic Schema\u00b6","text":"<p>Dynamic schema libraries work seamlessly with Arrow output formats, including processing operations.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#missing-columns","title":"Missing Columns\u00b6","text":"<p>Columns missing from some segments are backfilled with nulls:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#processing-with-dynamic-schema","title":"Processing with Dynamic Schema\u00b6","text":"<p>Processing operations work with both static and dynamic schema libraries. All missing values from processing are filled with nulls:</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#performance-benchmarks","title":"Performance Benchmarks\u00b6","text":"<p>Arrow-based formats provide performance improvements for string-heavy data (because with Arrow we don't need to take the GIL).</p> <p>For numeric data arrow based formats are in line with pandas performance.</p>"},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#benchmark-numeric-data","title":"Benchmark: Numeric Data\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_read_as_arrow/#benchmark-string-data","title":"Benchmark: String Data\u00b6","text":"<p>String performance shows big improvements (no GIL required for Arrow):</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/","title":"Recursive normalizer Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport arcticdb as adb\nfrom arcticdb.util.test import equals\n</pre> import numpy as np import pandas as pd import arcticdb as adb from arcticdb.util.test import equals In\u00a0[3]: Copied! <pre># Create Arctic instance with LMDB backend\narctic = adb.Arctic(\"lmdb://recursive_normalizers_demo\")\n\nlib = arctic.get_library(\"demo\", create_if_missing=True)\n</pre> # Create Arctic instance with LMDB backend arctic = adb.Arctic(\"lmdb://recursive_normalizers_demo\")  lib = arctic.get_library(\"demo\", create_if_missing=True) In\u00a0[4]: Copied! <pre># Create sample data\ndf1 = pd.DataFrame({\"col1\": [1, 2, 3], \"col2\": [\"a\", \"b\", \"c\"]})\ndf2 = pd.DataFrame({\"value\": [10, 20, 30]})\narray = np.arange(5)\n\n# Create nested structure\nnested_data = {\n    \"dataframe1\": df1,\n    \"dataframe2\": df2,\n    \"array\": array,\n    \"metadata\": {\"description\": \"Sample nested data\"}\n}\n\nprint(\"Original nested data:\")\nprint(nested_data)\n</pre> # Create sample data df1 = pd.DataFrame({\"col1\": [1, 2, 3], \"col2\": [\"a\", \"b\", \"c\"]}) df2 = pd.DataFrame({\"value\": [10, 20, 30]}) array = np.arange(5)  # Create nested structure nested_data = {     \"dataframe1\": df1,     \"dataframe2\": df2,     \"array\": array,     \"metadata\": {\"description\": \"Sample nested data\"} }  print(\"Original nested data:\") print(nested_data) <pre>Original nested data:\n{'dataframe1':    col1 col2\n0     1    a\n1     2    b\n2     3    c, 'dataframe2':    value\n0     10\n1     20\n2     30, 'array': array([0, 1, 2, 3, 4]), 'metadata': {'description': 'Sample nested data'}}\n</pre> In\u00a0[5]: Copied! <pre># This will raise an exception, recursive_normalizers is False by default\ntry:\n    lib.write(\"nested_data_fail\", nested_data)\nexcept Exception as e:\n    print(f\"Exception thrown: {e}\")\n</pre> # This will raise an exception, recursive_normalizers is False by default try:     lib.write(\"nested_data_fail\", nested_data) except Exception as e:     print(f\"Exception thrown: {e}\") <pre>Exception thrown: Data is of a type that cannot be normalized. Consider using write_pickle instead. type(data)=[&lt;class 'dict'&gt;]\n</pre> In\u00a0[6]: Copied! <pre># Write with recursive normalizers enabled\nlib.write(\"nested_data_success\", nested_data, recursive_normalizers=True)\nprint(\"Successfully written!\")\n\n# Read it back\nresult = lib.read(\"nested_data_success\").data\n\n# Verify the data matches the original\nequals(nested_data, result)\nprint(\"\\nAssertion passed: Read data matches original nested_data!\")\n</pre> # Write with recursive normalizers enabled lib.write(\"nested_data_success\", nested_data, recursive_normalizers=True) print(\"Successfully written!\")  # Read it back result = lib.read(\"nested_data_success\").data  # Verify the data matches the original equals(nested_data, result) print(\"\\nAssertion passed: Read data matches original nested_data!\") <pre>Successfully written!\n\nAssertion passed: Read data matches original nested_data!\n</pre> In\u00a0[7]: Copied! <pre># Create list of DataFrames\nlist_data = [\n    pd.DataFrame({\"a\": [1, 2, 3]}),\n    pd.DataFrame({\"b\": [4, 5, 6]}),\n    np.array([7, 8, 9])\n]\n\nlib.write(\"list_data\", list_data, recursive_normalizers=True)\nresult = lib.read(\"list_data\").data\n\n# Verify the data matches\nequals(list_data, result)\nprint(\"List data successfully written and verified!\")\n</pre> # Create list of DataFrames list_data = [     pd.DataFrame({\"a\": [1, 2, 3]}),     pd.DataFrame({\"b\": [4, 5, 6]}),     np.array([7, 8, 9]) ]  lib.write(\"list_data\", list_data, recursive_normalizers=True) result = lib.read(\"list_data\").data  # Verify the data matches equals(list_data, result) print(\"List data successfully written and verified!\") <pre>List data successfully written and verified!\n</pre> In\u00a0[8]: Copied! <pre># Create tuple of mixed data\ntuple_data = (\n    np.arange(5),\n    pd.DataFrame({\"col\": [1, 2, 3]}),\n    {\"nested\": np.arange(3)}\n)\n\nlib.write(\"tuple_data\", tuple_data, recursive_normalizers=True)\nresult = lib.read(\"tuple_data\").data\n\n# Verify the data matches\nequals(tuple_data, result)\nprint(\"Tuple data successfully written and verified!\")\n</pre> # Create tuple of mixed data tuple_data = (     np.arange(5),     pd.DataFrame({\"col\": [1, 2, 3]}),     {\"nested\": np.arange(3)} )  lib.write(\"tuple_data\", tuple_data, recursive_normalizers=True) result = lib.read(\"tuple_data\").data  # Verify the data matches equals(tuple_data, result) print(\"Tuple data successfully written and verified!\") <pre>Tuple data successfully written and verified!\n</pre> In\u00a0[9]: Copied! <pre># Create nested structure\nnested = {\n    \"level1\": {\n        \"level2\": {\n            \"level3\": {\n                \"dataframe\": pd.DataFrame({\"x\": [1, 2, 3]}),\n                \"array\": np.arange(10)\n            }\n        }\n    },\n    \"another_branch\": [\n        pd.DataFrame({\"y\": [4, 5, 6]}),\n        {\"nested_dict\": np.array([7, 8, 9])}\n    ]\n}\n\nlib.write(\"nested\", nested, recursive_normalizers=True)\nresult = lib.read(\"nested\").data\n\n# Verify the data matches\nequals(nested, result)\nprint(\"Nested structure successfully written and verified!\")\n</pre> # Create nested structure nested = {     \"level1\": {         \"level2\": {             \"level3\": {                 \"dataframe\": pd.DataFrame({\"x\": [1, 2, 3]}),                 \"array\": np.arange(10)             }         }     },     \"another_branch\": [         pd.DataFrame({\"y\": [4, 5, 6]}),         {\"nested_dict\": np.array([7, 8, 9])}     ] }  lib.write(\"nested\", nested, recursive_normalizers=True) result = lib.read(\"nested\").data  # Verify the data matches equals(nested, result) print(\"Nested structure successfully written and verified!\") <pre>Nested structure successfully written and verified!\n</pre> In\u00a0[10]: Copied! <pre># Enable recursive normalizers for existing library\nfrom arcticdb_ext.storage import ModifiableLibraryOption\narctic.modify_library_option(lib, ModifiableLibraryOption.RECURSIVE_NORMALIZERS, True)\n\n# Now writes will enable recursive normalizers by default\ndata = {\"df\": pd.DataFrame({\"b\": [4, 5, 6]}), \"arr\": np.arange(3)}\nlib.write(\"modified_lib_write\", data)\n\nresult = lib.read(\"modified_lib_write\").data\nequals(data, result)\nprint(\"Data written successfully after modifying library option and verified!\")\n</pre> # Enable recursive normalizers for existing library from arcticdb_ext.storage import ModifiableLibraryOption arctic.modify_library_option(lib, ModifiableLibraryOption.RECURSIVE_NORMALIZERS, True)  # Now writes will enable recursive normalizers by default data = {\"df\": pd.DataFrame({\"b\": [4, 5, 6]}), \"arr\": np.arange(3)} lib.write(\"modified_lib_write\", data)  result = lib.read(\"modified_lib_write\").data equals(data, result) print(\"Data written successfully after modifying library option and verified!\") <pre>2025-11-21 13:07:07,890 INFO  [arcticdb.arctic] Set option=[ModifiableLibraryOption.RECURSIVE_NORMALIZERS] to value=[True] for Arctic=[Arctic(config=LMDB(path=/recursive_normalizers_demo))] Library=[Library(Arctic(config=LMDB(path=/recursive_normalizers_demo)), path=demo, storage=lmdb_storage)]\n</pre> <pre>Data written successfully after modifying library option and verified!\n</pre> In\u00a0[11]: Copied! <pre># Create a new library with recursive normalizers enabled by default\nlib_recursive = arctic.create_library(\"demo_recursive\", library_options=adb.LibraryOptions(recursive_normalizers=True))\n\n# Now you can write without specifying recursive_normalizers=True\ndata = {\"df\": pd.DataFrame({\"a\": [1, 2, 3]}), \"arr\": np.arange(5)}\nlib_recursive.write(\"auto_recursive\", data)\n\nresult = lib_recursive.read(\"auto_recursive\").data\nequals(data, result)\nprint(\"Data written successfully with library option enabled and verified!\")\n</pre> # Create a new library with recursive normalizers enabled by default lib_recursive = arctic.create_library(\"demo_recursive\", library_options=adb.LibraryOptions(recursive_normalizers=True))  # Now you can write without specifying recursive_normalizers=True data = {\"df\": pd.DataFrame({\"a\": [1, 2, 3]}), \"arr\": np.arange(5)} lib_recursive.write(\"auto_recursive\", data)  result = lib_recursive.read(\"auto_recursive\").data equals(data, result) print(\"Data written successfully with library option enabled and verified!\") <pre>Data written successfully with library option enabled and verified!\n</pre> In\u00a0[12]: Copied! <pre># Custom class that cannot be natively normalized\nclass CustomClass:\n    def __init__(self, value):\n        self.value = value\n    \n    def __eq__(self, other):\n        return isinstance(other, CustomClass) and self.value == other.value\n\n# Data with custom class\nmixed_data = {\n    \"dataframe\": pd.DataFrame({\"a\": [1, 2, 3]}),\n    \"array\": np.arange(5),\n    \"custom\": CustomClass(42)\n}\n\n# This will fail with regular write\ntry:\n    lib.write(\"mixed_fail\", mixed_data, recursive_normalizers=True)\nexcept Exception as e:\n    print(f\"Error with write: {e}\")\n\n# But works with write_pickle\nlib.write_pickle(\"mixed_success\", mixed_data, recursive_normalizers=True)\nresult = lib.read(\"mixed_success\").data\n\n# Verify the data matches\nequals(mixed_data, result)\nprint(\"Data written with write_pickle and verified! Data has been partially-pickled\")\n</pre> # Custom class that cannot be natively normalized class CustomClass:     def __init__(self, value):         self.value = value          def __eq__(self, other):         return isinstance(other, CustomClass) and self.value == other.value  # Data with custom class mixed_data = {     \"dataframe\": pd.DataFrame({\"a\": [1, 2, 3]}),     \"array\": np.arange(5),     \"custom\": CustomClass(42) }  # This will fail with regular write try:     lib.write(\"mixed_fail\", mixed_data, recursive_normalizers=True) except Exception as e:     print(f\"Error with write: {e}\")  # But works with write_pickle lib.write_pickle(\"mixed_success\", mixed_data, recursive_normalizers=True) result = lib.read(\"mixed_success\").data  # Verify the data matches equals(mixed_data, result) print(\"Data written with write_pickle and verified! Data has been partially-pickled\") <pre>Error with write: Error while normalizing symbol=mixed_fail__custom, Normalizing data by pickling has been disabled.\nData written with write_pickle and verified! Data has been partially-pickled\n</pre> In\u00a0[13]: Copied! <pre>from arcticdb.version_store.library import ArcticUnsupportedDataTypeException\n\ndata = {\"df\": pd.DataFrame({\"a\": [1, 2, 3]})}\n\ntry:\n    lib.write(\"exception_test1\", data, recursive_normalizers=False)\nexcept ArcticUnsupportedDataTypeException as e:\n    print(\"Caught ArcticUnsupportedDataTypeException:\")\n    print(f\"Message: {str(e)}\")\n    print(\"\\nSolution: Use recursive_normalizers=True or write_pickle\")\n</pre> from arcticdb.version_store.library import ArcticUnsupportedDataTypeException  data = {\"df\": pd.DataFrame({\"a\": [1, 2, 3]})}  try:     lib.write(\"exception_test1\", data, recursive_normalizers=False) except ArcticUnsupportedDataTypeException as e:     print(\"Caught ArcticUnsupportedDataTypeException:\")     print(f\"Message: {str(e)}\")     print(\"\\nSolution: Use recursive_normalizers=True or write_pickle\") <pre>Caught ArcticUnsupportedDataTypeException:\nMessage: Data is of a type that cannot be normalized. Consider using write_pickle instead. type(data)=[&lt;class 'dict'&gt;]\n\nSolution: Use recursive_normalizers=True or write_pickle\n</pre> In\u00a0[14]: Copied! <pre>from arcticdb.exceptions import DataTooNestedException\n\n# Create a structure that's too deeply nested (256 levels)\ndef create_deep_nest(depth):\n    if depth == 0:\n        return pd.DataFrame({\"a\": [1, 2, 3]})\n    return {\"nested\": create_deep_nest(depth - 1)}\n\ntry:\n    too_deep = create_deep_nest(256)\n    lib.write(\"exception_test3\", too_deep, recursive_normalizers=True)\nexcept DataTooNestedException as e:\n    print(\"Caught DataTooNestedException:\")\n    print(f\"Message: {str(e)}\")\n    print(\"\\nSolution: Reduce nesting depth to 255 levels or less\")\n</pre> from arcticdb.exceptions import DataTooNestedException  # Create a structure that's too deeply nested (256 levels) def create_deep_nest(depth):     if depth == 0:         return pd.DataFrame({\"a\": [1, 2, 3]})     return {\"nested\": create_deep_nest(depth - 1)}  try:     too_deep = create_deep_nest(256)     lib.write(\"exception_test3\", too_deep, recursive_normalizers=True) except DataTooNestedException as e:     print(\"Caught DataTooNestedException:\")     print(f\"Message: {str(e)}\")     print(\"\\nSolution: Reduce nesting depth to 255 levels or less\") <pre>Caught DataTooNestedException:\nMessage: Symbol exception_test3 cannot be recursively normalized as it contains more than 255 levels of nested dictionaries. This is a limitation of the msgpack serializer.\n\nSolution: Reduce nesting depth to 255 levels or less\n</pre> In\u00a0[16]: Copied! <pre>from arcticdb.exceptions import SchemaException\nfrom arcticdb.version_store.processing import QueryBuilder\n\n# Try to filter recursively normalized data - this will fail\ntry:\n    q = QueryBuilder()\n    q = q[q[\"col\"] == 0]\n    lib.read(\"nested\", query_builder=q)\nexcept SchemaException as e:\n    print(\"Caught SchemaException\")\n    print(f\"Message: {str(e)}\")\n    print(\"\\nSolution: Recursive normalized data cannot be filtered. Read the full data without filtering.\")\n\ntry:\n    lib.head(\"nested\")\nexcept SchemaException as e:\n    print(\"Caught SchemaException again\")\n</pre> from arcticdb.exceptions import SchemaException from arcticdb.version_store.processing import QueryBuilder  # Try to filter recursively normalized data - this will fail try:     q = QueryBuilder()     q = q[q[\"col\"] == 0]     lib.read(\"nested\", query_builder=q) except SchemaException as e:     print(\"Caught SchemaException\")     print(f\"Message: {str(e)}\")     print(\"\\nSolution: Recursive normalized data cannot be filtered. Read the full data without filtering.\")  try:     lib.head(\"nested\") except SchemaException as e:     print(\"Caught SchemaException again\")  <pre>Caught SchemaException\nMessage: E_OPERATION_NOT_SUPPORTED_WITH_RECURSIVE_NORMALIZED_DATA Cannot filter recursively normalized data\n\nSolution: Recursive normalized data cannot be filtered. Read the full data without filtering.\nCaught SchemaException again\n</pre>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#arcticdb-recursive-normalizers-demo","title":"ArcticDB Recursive Normalizers Demo\u00b6","text":"<p>This notebook demonstrates the recursive normalizers feature in ArcticDB, which allows you to write nested data structures without having to pickle the entire object.</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#what-are-recursive-normalizers","title":"What are Recursive Normalizers?\u00b6","text":"<p>Recursive normalizers enable ArcticDB to:</p> <ul> <li>Write and read nested data structures (dicts, lists, tuples) containing DataFrames and arrays</li> <li>Store each component efficiently without pickling the entire structure</li> </ul>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#meta-structure-v2","title":"Meta structure V2\u00b6","text":"<p>Meta structure V2 for recursive normalizer is introduced in v6.7.0. It has removed the dependency on pickle for normalizing the meta structure. Please consider enabling V2 as V1 will be deprecated in future v7.0.0 release. To enabld it, you need to set environment variable ARCTICDB_VersionStore_RecursiveNormalizerMetastructure_int to 2.</p> <p>Reader of the data is needed to be &gt;= v6.7.0 or KeyError will be raised while reading V2 meta structure</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#basic-example-writing-dict-data","title":"Basic Example: Writing Dict Data\u00b6","text":"<p>Let's start with a simple example of writing a dictionary containing DataFrames and arrays.</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#writing-without-recursive-normalizers-will-fail","title":"Writing Without Recursive Normalizers (Will Fail)\u00b6","text":"<p>By default, ArcticDB cannot write nested structures without pickling:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#writing-with-recursive-normalizers-success","title":"Writing With Recursive Normalizers (Success)\u00b6","text":"<p>Enable recursive normalizers to write the nested structure:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#example-lists-and-tuples","title":"Example: Lists and Tuples\u00b6","text":"<p>Recursive normalizers also work with lists and tuples:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#example-nested-structures","title":"Example: Nested Structures\u00b6","text":"<p>Recursive normalizers can handle nested structures:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#enable-recursive-normalizer-in-library-configuration","title":"Enable Recursive Normalizer in Library Configuration\u00b6","text":"<p>You can configure recursive normalizers at the library configuration instead of specifying it for each write:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#modifying-library-options","title":"Modifying Library Options\u00b6","text":"<p>You can modify an existing library's configuration:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#new-library","title":"New Library\u00b6","text":"<p>You can also create a new library with recursive normalizers enabled by default</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#using-write_pickle-with-recursive-normalizers","title":"Using write_pickle with Recursive Normalizers\u00b6","text":"<p>The <code>write_pickle</code> method can also use recursive normalizers. This is useful when you have nested structures with some components that cannot be natively normalized:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#exception-handling-examples","title":"Exception Handling Examples\u00b6","text":"<p>Let's explore various exceptions that can occur when using recursive normalizers:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#1-arcticunsupporteddatatypeexception","title":"1. ArcticUnsupportedDataTypeException\u00b6","text":"<p>This exception occurs when trying to write nested data without enabling recursive normalizers:</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#2-datatoonestedexception","title":"2. DataTooNestedException\u00b6","text":"<p>There's a limit to how deeply nested structures can be (255 levels):</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#3-schemaexception","title":"3. SchemaException\u00b6","text":"<p>Recursive normalized data cannot be filtered. Attempting to filter such data will raise a SchemaException.</p>"},{"location":"notebooks/ArcticDB_demo_recursive_normalizers/#best-practices","title":"Best Practices\u00b6","text":"<ol> <li>Use recursive normalizers for nested structures: When you have dictionaries or lists containing DataFrames and arrays</li> <li>Configure at library level: If you frequently write nested data, enable it at the library level</li> <li>Limit nesting depth: Keep nesting under 255 levels</li> <li>Use write_pickle for mixed data: When you have custom objects that can't be normalized</li> </ol>"},{"location":"notebooks/ArcticDB_demo_resample/","title":"Resample Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport arcticdb as adb\n</pre> import numpy as np import pandas as pd import arcticdb as adb In\u00a0[3]: Copied! <pre># object store\narctic = adb.Arctic(\"lmdb://arcticdb_resample\")\n</pre> # object store arctic = adb.Arctic(\"lmdb://arcticdb_resample\") In\u00a0[4]: Copied! <pre># library\nlib = arctic.get_library('resample', create_if_missing=True)\n</pre> # library lib = arctic.get_library('resample', create_if_missing=True) In\u00a0[5]: Copied! <pre># data for resampling\nindex = pd.date_range(\"1990-01-01\", periods=12_000_000, freq=\"s\")\nint_data = np.arange(len(index), dtype=np.uint64)\nfloat_data = np.round(np.random.uniform(95., 105., len(index)), 3)\nletters = ['a','b','c','d','e','f','g']\nmkt_data = pd.DataFrame(\n    index=index,\n    data={\n        \"id\": int_data,\n        \"price\": float_data,\n        \"category\": (letters*(len(index)//len(letters) + 1))[:len(index)]\n    }\n)\n</pre> # data for resampling index = pd.date_range(\"1990-01-01\", periods=12_000_000, freq=\"s\") int_data = np.arange(len(index), dtype=np.uint64) float_data = np.round(np.random.uniform(95., 105., len(index)), 3) letters = ['a','b','c','d','e','f','g'] mkt_data = pd.DataFrame(     index=index,     data={         \"id\": int_data,         \"price\": float_data,         \"category\": (letters*(len(index)//len(letters) + 1))[:len(index)]     } ) In\u00a0[6]: Copied! <pre># view the first 10 rows of the data\nmkt_data.head(10)\n</pre> # view the first 10 rows of the data mkt_data.head(10) Out[6]: id price category 1990-01-01 00:00:00 0 95.176 a 1990-01-01 00:00:01 1 97.872 b 1990-01-01 00:00:02 2 104.930 c 1990-01-01 00:00:03 3 103.573 d 1990-01-01 00:00:04 4 97.052 e 1990-01-01 00:00:05 5 103.435 f 1990-01-01 00:00:06 6 99.339 g 1990-01-01 00:00:07 7 103.358 a 1990-01-01 00:00:08 8 104.301 b 1990-01-01 00:00:09 9 104.651 c In\u00a0[7]: Copied! <pre># write the data into ArcticDB\nsym = 'market_data'\nlib.write(sym, mkt_data)\n</pre> # write the data into ArcticDB sym = 'market_data' lib.write(sym, mkt_data) Out[7]: <pre>VersionedItem(symbol='market_data', library='resample', data=n/a, version=0, metadata=None, host='LMDB(path=~/arcticdb_resample)', timestamp=1718958796318913629)</pre> In\u00a0[8]: Copied! <pre># frequency and aggregator params\nfreq1 = '1min'\naggs1 = {'id': 'max', 'price': 'last', 'category': 'count'}\n</pre> # frequency and aggregator params freq1 = '1min' aggs1 = {'id': 'max', 'price': 'last', 'category': 'count'} In\u00a0[9]: Copied! <pre>%%time\n# create the resample query and apply it on the read\nmarket_data_1min_df = lib.read(sym, lazy=True).resample(freq1).agg(aggs1).collect().data\nprint(len(market_data_1min_df))\nmarket_data_1min_df.tail()\n</pre> %%time # create the resample query and apply it on the read market_data_1min_df = lib.read(sym, lazy=True).resample(freq1).agg(aggs1).collect().data print(len(market_data_1min_df)) market_data_1min_df.tail() <pre>200000\nCPU times: user 684 ms, sys: 251 ms, total: 935 ms\nWall time: 171 ms\n</pre> Out[9]: id price category 1990-05-19 21:15:00 11999759 104.106 60 1990-05-19 21:16:00 11999819 104.456 60 1990-05-19 21:17:00 11999879 95.570 60 1990-05-19 21:18:00 11999939 103.967 60 1990-05-19 21:19:00 11999999 97.899 60 In\u00a0[10]: Copied! <pre>%%time\n# read the full data set and resample in Pandas\nfull_df = lib.read(sym).data\nmarket_data_1min_pd_df = full_df.resample(freq1).agg(aggs1)\nprint(len(market_data_1min_pd_df))\nmarket_data_1min_pd_df.tail()\n</pre> %%time # read the full data set and resample in Pandas full_df = lib.read(sym).data market_data_1min_pd_df = full_df.resample(freq1).agg(aggs1) print(len(market_data_1min_pd_df)) market_data_1min_pd_df.tail() <pre>200000\nCPU times: user 1.6 s, sys: 401 ms, total: 2 s\nWall time: 1.15 s\n</pre> Out[10]: id price category 1990-05-19 21:15:00 11999759 104.106 60 1990-05-19 21:16:00 11999819 104.456 60 1990-05-19 21:17:00 11999879 95.570 60 1990-05-19 21:18:00 11999939 103.967 60 1990-05-19 21:19:00 11999999 97.899 60 In\u00a0[11]: Copied! <pre>freq2 = '5min'\naggs2 = {'id': 'max', 'price_last': ('price' ,'last'), 'price_count': ('price' ,'count'), 'category': 'first'}\n</pre> freq2 = '5min' aggs2 = {'id': 'max', 'price_last': ('price' ,'last'), 'price_count': ('price' ,'count'), 'category': 'first'} In\u00a0[12]: Copied! <pre>%%time\nlib.read(sym, lazy=True).resample(freq2).agg(aggs2).collect().data\n</pre> %%time lib.read(sym, lazy=True).resample(freq2).agg(aggs2).collect().data <pre>CPU times: user 1.07 s, sys: 415 ms, total: 1.49 s\nWall time: 151 ms\n</pre> Out[12]: id category price_count price_last 1990-01-01 00:00:00 299 a 300 102.172 1990-01-01 00:05:00 599 g 300 101.450 1990-01-01 00:10:00 899 f 300 96.718 1990-01-01 00:15:00 1199 e 300 96.345 1990-01-01 00:20:00 1499 d 300 98.955 ... ... ... ... ... 1990-05-19 20:55:00 11998799 d 300 100.277 1990-05-19 21:00:00 11999099 c 300 103.596 1990-05-19 21:05:00 11999399 b 300 96.182 1990-05-19 21:10:00 11999699 a 300 99.911 1990-05-19 21:15:00 11999999 g 300 97.899 <p>40000 rows \u00d7 4 columns</p> In\u00a0[14]: Copied! <pre>%%time\nlib.read(sym, lazy=True).resample('2min30s').agg({'id': 'min', 'category': 'first'}).groupby('category').agg({'id': 'mean'}).collect().data\n</pre> %%time lib.read(sym, lazy=True).resample('2min30s').agg({'id': 'min', 'category': 'first'}).groupby('category').agg({'id': 'mean'}).collect().data <pre>CPU times: user 1.12 s, sys: 309 ms, total: 1.43 s\nWall time: 183 ms\n</pre> Out[14]: id category a 5999700.0 b 5999925.0 d 5999850.0 e 6000075.0 f 5999775.0 g 6000000.0 c 6000150.0 In\u00a0[15]: Copied! <pre>freq_ohlc = '5min'\nagg_ohlc = {\n    'open': ('price', 'first'),\n    'high': ('price', 'max'),\n    'low': ('price', 'min'),\n    'close': ('price', 'last')\n}\n</pre> freq_ohlc = '5min' agg_ohlc = {     'open': ('price', 'first'),     'high': ('price', 'max'),     'low': ('price', 'min'),     'close': ('price', 'last') } In\u00a0[16]: Copied! <pre>%%time\nohlc_5min_bars = lib.read(sym, lazy=True).resample(freq_ohlc).agg(agg_ohlc).collect().data\n</pre> %%time ohlc_5min_bars = lib.read(sym, lazy=True).resample(freq_ohlc).agg(agg_ohlc).collect().data <pre>CPU times: user 1.26 s, sys: 492 ms, total: 1.75 s\nWall time: 118 ms\n</pre> In\u00a0[17]: Copied! <pre>ohlc_5min_bars.head()\n</pre> ohlc_5min_bars.head() Out[17]: close low high open 1990-01-01 00:00:00 102.172 95.076 104.992 95.176 1990-01-01 00:05:00 101.450 95.008 104.999 98.520 1990-01-01 00:10:00 96.718 95.053 104.990 103.959 1990-01-01 00:15:00 96.345 95.070 104.969 95.878 1990-01-01 00:20:00 98.955 95.011 104.983 103.538"},{"location":"notebooks/ArcticDB_demo_resample/#arcticdb-resample-demo","title":"ArcticDB Resample Demo\u00b6","text":"<p>This demo notebook showcases the high-performance resample capability of ArcticDB.</p> <p>This is what you need to know about it:</p> <ul> <li>It runs on-the-fly as part of the read</li> <li>This makes it much more efficient than Pandas on large datasets</li> <li>The usage is similar to the Pandas resample function</li> <li>You can apply multiple aggregators to each column</li> <li>It can be used for downsampling high frequency data and generating \"bar\" data (see example 4)</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_resample/#create-some-data","title":"Create Some Data\u00b6","text":"<ul> <li>timeseries with 12,000,000 rows and a 1-second index</li> <li>int, float, string columns</li> <li>write the data into ArcticDB</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#1-simple-resample","title":"1. Simple Resample\u00b6","text":"<ul> <li>Resample to 1-minute</li> <li>Use different aggregators</li> <li>Resample can be thought of as a time-based groupby</li> <li>The groups are all the rows within a time interval</li> <li>Run also in Pandas to compare performance and results</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#2-multiple-aggregators-per-column","title":"2. Multiple Aggregators per Column\u00b6","text":"<ul> <li>Similar to NamedAgg in Pandas</li> <li>Downsample to 5-minute frequency</li> <li>Apply both\u00a0max and last aggregators to the price column</li> <li>For multiple aggregators, the syntax is\u00a0<code>output_column_name: (input_column_name: aggregator)</code></li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#3-processing-pipeline-chaining-operations","title":"3. Processing Pipeline: Chaining Operations\u00b6","text":"<ul> <li>Downsample to 2.5-minutes frequency</li> <li>Group the resampled data by the category column</li> <li>Aggregate the category groups using mean</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#4-example-ohlc-open-high-low-close-bars","title":"4. Example: OHLC (Open High Low Close) Bars\u00b6","text":"<ul> <li>Downsample to 5-minute frequency</li> <li>Use multiple aggregators on the price column</li> <li>This is a simple example of how to convert tick data to OHLC bar data</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#conclusion","title":"Conclusion\u00b6","text":"<p>We have demonstrated the following about the ArcticDB resample feature:</p> <ul> <li>Easy to use, especially if you already resample in Pandas</li> <li>Very high performance - in particular much faster than reading all the data then resampling in Pandas</li> <li>Can be combined with other query functions to build processing pipelines</li> <li>Can be used to generate timeseries bars</li> </ul>"},{"location":"notebooks/ArcticDB_demo_snapshots/","title":"Snapshots Notebook","text":"In\u00a0[1]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[2]: Copied! <pre>import pandas as pd\nimport logging\nimport arcticdb as adb\n</pre> import pandas as pd import logging import arcticdb as adb In\u00a0[3]: Copied! <pre>lib_name = 'demo'\narctic = adb.Arctic(\"lmdb://arcticdb_snapshot_demo\")\nif lib_name in arctic.list_libraries():\n    arctic.delete_library(lib_name)\nlib = arctic.get_library('demo', create_if_missing=True)\n</pre> lib_name = 'demo' arctic = adb.Arctic(\"lmdb://arcticdb_snapshot_demo\") if lib_name in arctic.list_libraries():     arctic.delete_library(lib_name) lib = arctic.get_library('demo', create_if_missing=True) In\u00a0[4]: Copied! <pre>num_symbols = 4\nsymbols = [f\"sym_{idx}\" for idx in range(num_symbols)]\nhalf_symbols = symbols[:num_symbols // 2]\nprint(symbols)\nprint(half_symbols)\n</pre> num_symbols = 4 symbols = [f\"sym_{idx}\" for idx in range(num_symbols)] half_symbols = symbols[:num_symbols // 2] print(symbols) print(half_symbols) <pre>['sym_0', 'sym_1', 'sym_2', 'sym_3']\n['sym_0', 'sym_1']\n</pre> In\u00a0[5]: Copied! <pre># write data for each symbol\nfor idx, symbol in enumerate(symbols):\n    lib.write(symbol, pd.DataFrame({\"col\": [idx]}))\n</pre> # write data for each symbol for idx, symbol in enumerate(symbols):     lib.write(symbol, pd.DataFrame({\"col\": [idx]})) In\u00a0[6]: Copied! <pre># write data only for the first half of the symbols\nfor idx, symbol in enumerate(half_symbols):\n    lib.write(symbol, pd.DataFrame({\"col\": [idx+10]}))\n</pre> # write data only for the first half of the symbols for idx, symbol in enumerate(half_symbols):     lib.write(symbol, pd.DataFrame({\"col\": [idx+10]})) In\u00a0[7]: Copied! <pre>lib.snapshot(\"snapshot_0\", metadata=\"this is the core of the demo\")\n</pre> lib.snapshot(\"snapshot_0\", metadata=\"this is the core of the demo\") In\u00a0[8]: Copied! <pre># list all snapshots\nlib.list_snapshots()\n</pre> # list all snapshots lib.list_snapshots() Out[8]: <pre>{'snapshot_0': 'this is the core of the demo'}</pre> In\u00a0[9]: Copied! <pre># list the symbols in a snapshot\nlib.list_symbols(snapshot_name=\"snapshot_0\")\n</pre> # list the symbols in a snapshot lib.list_symbols(snapshot_name=\"snapshot_0\") Out[9]: <pre>['sym_2', 'sym_1', 'sym_0', 'sym_3']</pre> In\u00a0[10]: Copied! <pre># list the versions in a snapshot\nlib.list_versions(snapshot=\"snapshot_0\")\n</pre> # list the versions in a snapshot lib.list_versions(snapshot=\"snapshot_0\") Out[10]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_0']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_0']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_0']),\n sym_0_v1: (date=2023-11-20 10:24:45.413203317+00:00, snapshots=['snapshot_0'])}</pre> In\u00a0[11]: Copied! <pre># list all versions in the library, with associated snapshots\nlib.list_versions()\n</pre> # list all versions in the library, with associated snapshots lib.list_versions() Out[11]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_0']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_0']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_0']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00),\n sym_0_v1: (date=2023-11-20 10:24:45.413203317+00:00, snapshots=['snapshot_0']),\n sym_0_v0: (date=2023-11-20 10:24:45.041944641+00:00)}</pre> In\u00a0[12]: Copied! <pre>vit = lib.read(\"sym_0\", as_of=\"snapshot_0\")\nprint(vit)\nprint(vit.data)\n</pre> vit = lib.read(\"sym_0\", as_of=\"snapshot_0\") print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_0', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0   10\n</pre> In\u00a0[13]: Copied! <pre>vit = lib.read(\"sym_3\", as_of=\"snapshot_0\")\nprint(vit)\nprint(vit.data)\n</pre> vit = lib.read(\"sym_3\", as_of=\"snapshot_0\") print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_3', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=0, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0    3\n</pre> In\u00a0[14]: Copied! <pre># delete the symbol sym_0\nlib.delete(\"sym_0\")\n</pre> # delete the symbol sym_0 lib.delete(\"sym_0\") In\u00a0[15]: Copied! <pre># show that sym_0 has been deleted\nlib.list_symbols()\n</pre> # show that sym_0 has been deleted lib.list_symbols() Out[15]: <pre>['sym_2', 'sym_1', 'sym_3']</pre> In\u00a0[16]: Copied! <pre># sym_0 does not appear in the current library versions\nlib.list_versions()\n</pre> # sym_0 does not appear in the current library versions lib.list_versions() Out[16]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_0']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_0']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_0']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00)}</pre> In\u00a0[17]: Copied! <pre># however we can still read the version of sym_0 that was recorded in the snapshot\nvit = lib.read(\"sym_0\", as_of=\"snapshot_0\")\nprint(vit)\nprint(vit.data)\n</pre> # however we can still read the version of sym_0 that was recorded in the snapshot vit = lib.read(\"sym_0\", as_of=\"snapshot_0\") print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_0', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0   10\n</pre> In\u00a0[20]: Copied! <pre>lib.delete_snapshot(\"snapshot_0\")\n</pre> lib.delete_snapshot(\"snapshot_0\") In\u00a0[21]: Copied! <pre>lib.list_snapshots()\n</pre> lib.list_snapshots() Out[21]: <pre>{}</pre> In\u00a0[22]: Copied! <pre># version 1, which was kept as part of the snapshot, has now been deleted\ntry:\n    vit = lib.read(\"sym_0\", as_of=1)\n    print(vit)\n    print(vit.data)\nexcept adb.exceptions.NoSuchVersionException:\n    logging.error(\"Version not found\")\n</pre> # version 1, which was kept as part of the snapshot, has now been deleted try:     vit = lib.read(\"sym_0\", as_of=1)     print(vit)     print(vit.data) except adb.exceptions.NoSuchVersionException:     logging.error(\"Version not found\") <pre>ERROR:root:Version not found\n</pre> In\u00a0[23]: Copied! <pre>lib.list_versions()\n</pre> lib.list_versions() Out[23]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00)}</pre> In\u00a0[24]: Copied! <pre>lib.snapshot(\"snapshot_1\", metadata=\"demo snapshot names need to be unique\")\n</pre> lib.snapshot(\"snapshot_1\", metadata=\"demo snapshot names need to be unique\") In\u00a0[25]: Copied! <pre>try:\n    lib.snapshot(\"snapshot_1\")\nexcept Exception as e:\n    logging.error(e)\n</pre> try:     lib.snapshot(\"snapshot_1\") except Exception as e:     logging.error(e) <pre>ERROR:root:E_ASSERTION_FAILURE Snapshot with name snapshot_1 already exists\n</pre> In\u00a0[26]: Copied! <pre>lib.list_snapshots()\n</pre> lib.list_snapshots() Out[26]: <pre>{'snapshot_1': 'demo snapshot names need to be unique'}</pre> In\u00a0[27]: Copied! <pre># exclude sym_1 from snapshot\nlib.snapshot(\"snapshot_2\", skip_symbols=[\"sym_1\"], metadata=\"demo skip_symbols\")\n</pre> # exclude sym_1 from snapshot lib.snapshot(\"snapshot_2\", skip_symbols=[\"sym_1\"], metadata=\"demo skip_symbols\") In\u00a0[28]: Copied! <pre>lib.list_versions()\n</pre> lib.list_versions() Out[28]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_1', 'snapshot_2']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_1', 'snapshot_2']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_1']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00)}</pre> In\u00a0[29]: Copied! <pre># include specific versions of sym_1 and sym_2 from snapshot\nlib.snapshot(\"snapshot_3\", versions={\"sym_1\": 0, \"sym_2\": 0}, metadata=\"demo versions\")\n</pre> # include specific versions of sym_1 and sym_2 from snapshot lib.snapshot(\"snapshot_3\", versions={\"sym_1\": 0, \"sym_2\": 0}, metadata=\"demo versions\") In\u00a0[30]: Copied! <pre>lib.list_versions(snapshot=\"snapshot_3\")\n</pre> lib.list_versions(snapshot=\"snapshot_3\") Out[30]: <pre>{sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_1', 'snapshot_2', 'snapshot_3']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00, snapshots=['snapshot_3'])}</pre> In\u00a0[31]: Copied! <pre>lib.list_snapshots()\n</pre> lib.list_snapshots() Out[31]: <pre>{'snapshot_1': 'demo snapshot names need to be unique',\n 'snapshot_2': 'demo skip_symbols',\n 'snapshot_3': 'demo versions'}</pre>"},{"location":"notebooks/ArcticDB_demo_snapshots/#snapshots-how-to-use-them-and-why-they-are-useful","title":"Snapshots: how to use them and why they are useful\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#an-introduction-to-snapshots","title":"An Introduction to Snapshots\u00b6","text":"<p>In order to understand snapshots we first need to be clear about versions.</p> <p>In ArcticDB, every time a change is made to a symbol a new version is created. So each symbol has a sequence of versions through time.</p> <p>In a library there will typically be many symbols with each having many versions.</p> <p>Suppose we reach a point where we wish to record the current state of the data in the library. This is exactly the purpose of a snapshot.</p> <p>A snapshot records the current versions of all the symbols in the library (or a custom set of versions, see below)</p> <p>The data recorded in the snapshot can then be read back using the <code>as_of</code> parameter in the read.</p> <p>Versions that are part of a snapshot are protected from deletion, even if their symbol is deleted.</p> <p>Below is a simple example that demonstrates snapshots in action.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#installs-and-imports","title":"Installs and Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#set-up-articdb","title":"Set up ArticDB\u00b6","text":"<p>Note: In this example we delete the library if it exists. That is not normal but we want to make sure we have a clean library in this case.</p> <p>Don't copy those lines unless you are sure that is what you need.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#create-some-symbols","title":"Create some symbols\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#create-the-snapshot","title":"Create the snapshot\u00b6","text":"<p>The metadata is optional</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#functions-to-discover-and-inspect-snapshots","title":"Functions to discover and inspect snapshots\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#reading-a-snapshot-version-of-a-symbol","title":"Reading a snapshot version of a symbol\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#demonstration-that-snapshot-versions-are-protected-from-deletion","title":"Demonstration that snapshot versions are protected from deletion\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#deleting-a-snapshot","title":"Deleting a snapshot\u00b6","text":"<p>When we delete a snapshot, any versions that are only referenced by that snapshot will be deleted.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#snapshot-names-must-be-unique","title":"Snapshot names must be unique\u00b6","text":"<p>Creating a snapshot with a name that already has a snapshot causes an error.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#modifiers-for-snapshot-creation-exclude-or-include-symbols","title":"Modifiers for snapshot creation: exclude or include symbols\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#snapshots-why-and-why-not-to-use-them","title":"Snapshots: why and why not to use them\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#why","title":"Why\u00b6","text":"<ul> <li>Snapshots record the current state of the library</li> <li>They can be thought of as recoverable checkpoints in the evolution of the data</li> <li>Snapshots can create an audit trail</li> <li>Snapshots protect their data from deletion by other activity in the library</li> </ul>"},{"location":"notebooks/ArcticDB_demo_snapshots/#why-not","title":"Why Not\u00b6","text":"<ul> <li>Generally we encourage the use of snapshots</li> <li>However if many snapshots are created they can impose a slight performance penalty on some operations due to the deletion protection</li> <li>Snapshots can also increase the storage used by ArcticDB, through protecting older versions that would otherwise be deleted</li> <li>Use snapshots in a considered fashion and delete them when they are no longer needed</li> </ul>"},{"location":"notebooks/ArcticDB_demo_snapshots/#further-info-extras","title":"Further Info / Extras\u00b6","text":"<p>For full descriptions of the functions used above, please see the ArcticDb documentation:</p> <ul> <li><code>snapshot()</code> https://docs.arcticdb.io/latest/api/library/#arcticdb.version_store.library.Library.snapshot</li> <li><code>list_snapshots()</code> https://docs.arcticdb.io/latest/api/library/#arcticdb.version_store.library.Library.list_snapshots</li> <li><code>list_versions()</code> https://docs.arcticdb.io/latest/api/library/#arcticdb.version_store.library.Library.list_versions</li> </ul>"},{"location":"notebooks/ArcticDB_merge/","title":"Merge Notebook","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport arcticdb\nimport numpy as np\nfrom IPython.display import display\nfrom arcticdb.version_store.library import MergeStrategy, MergeAction\n</pre> import pandas as pd import arcticdb import numpy as np from IPython.display import display from arcticdb.version_store.library import MergeStrategy, MergeAction <p>Next, create an ArcticDB instance and a library.</p> In\u00a0[2]: Copied! <pre>ac = arcticdb.Arctic(\"lmdb://merge_example\")\nlib = ac.get_library(\"prices\", create_if_missing=True)\n</pre> ac = arcticdb.Arctic(\"lmdb://merge_example\") lib = ac.get_library(\"prices\", create_if_missing=True) <p>Create an example prices DataFrame and store it in ArcticDB.</p> In\u00a0[3]: Copied! <pre># Create example data\ndaily_prices = pd.DataFrame(\n    data={\n        \"High\": [102.5, 103.6, 101.7, 103.2, 104.8, 106.0, 108.9, 107.6, 109.3, 108.7, \n                 111.1, 113.5, 113.2, 115.0, 112.3, 115.9, 117.3, 118.6, 119.8, 119.1],\n        \"Low\": [100.5, 101.8, 99.7, 100.9, 101.7, 103.4, 106.2, 104.8, 107.5, 106.3, \n                109.1, 110.8, 110.2, 111.7, 109.6, 112.7, 114.0, 115.4, 116.5, 116.2],\n        \"Volume\": [1200, 1500, 900, 1400, 1600, 1900, 2500, 2300, 1800, 2100, \n                   2000, 2700, 2600, 3000, 2200, 3100, 2800, 3200, 3500, 3400],\n    },\n    index=pd.date_range(start=\"2023-01-01\", periods=20, freq=\"D\")\n)\ndaily_prices\n</pre> # Create example data daily_prices = pd.DataFrame(     data={         \"High\": [102.5, 103.6, 101.7, 103.2, 104.8, 106.0, 108.9, 107.6, 109.3, 108.7,                   111.1, 113.5, 113.2, 115.0, 112.3, 115.9, 117.3, 118.6, 119.8, 119.1],         \"Low\": [100.5, 101.8, 99.7, 100.9, 101.7, 103.4, 106.2, 104.8, 107.5, 106.3,                  109.1, 110.8, 110.2, 111.7, 109.6, 112.7, 114.0, 115.4, 116.5, 116.2],         \"Volume\": [1200, 1500, 900, 1400, 1600, 1900, 2500, 2300, 1800, 2100,                     2000, 2700, 2600, 3000, 2200, 3100, 2800, 3200, 3500, 3400],     },     index=pd.date_range(start=\"2023-01-01\", periods=20, freq=\"D\") ) daily_prices Out[3]: High Low Volume 2023-01-01 102.5 100.5 1200 2023-01-02 103.6 101.8 1500 2023-01-03 101.7 99.7 900 2023-01-04 103.2 100.9 1400 2023-01-05 104.8 101.7 1600 2023-01-06 106.0 103.4 1900 2023-01-07 108.9 106.2 2500 2023-01-08 107.6 104.8 2300 2023-01-09 109.3 107.5 1800 2023-01-10 108.7 106.3 2100 2023-01-11 111.1 109.1 2000 2023-01-12 113.5 110.8 2700 2023-01-13 113.2 110.2 2600 2023-01-14 115.0 111.7 3000 2023-01-15 112.3 109.6 2200 2023-01-16 115.9 112.7 3100 2023-01-17 117.3 114.0 2800 2023-01-18 118.6 115.4 3200 2023-01-19 119.8 116.5 3500 2023-01-20 119.1 116.2 3400 In\u00a0[4]: Copied! <pre>lib.write(\"daily_prices\", daily_prices)\n</pre> lib.write(\"daily_prices\", daily_prices) Out[4]: <pre>VersionedItem(symbol='daily_prices', library='prices', data=n/a, version=0, metadata=None, host='LMDB(path=/home/vasil/Documents/source/ArcticDB/docs/mkdocs/docs/notebooks/merge_example)', timestamp=1769761430529539749)</pre> <p>Create the correction DataFrame.</p> In\u00a0[7]: Copied! <pre>daily_prices_correction = pd.DataFrame(\n    data={\n        \"High\":  [3000.0, 4000.0, 5000.0],\n        \"Low\":   [1000.0, 2000.0, 3000.0],\n        \"Volume\":[10000, 20000, 30000]\n    },\n    index=pd.DatetimeIndex([pd.Timestamp(\"2023-01-05\"), pd.Timestamp(\"2023-01-07\"), pd.Timestamp(\"2023-01-19\")])\n)\ndaily_prices_correction\n</pre> daily_prices_correction = pd.DataFrame(     data={         \"High\":  [3000.0, 4000.0, 5000.0],         \"Low\":   [1000.0, 2000.0, 3000.0],         \"Volume\":[10000, 20000, 30000]     },     index=pd.DatetimeIndex([pd.Timestamp(\"2023-01-05\"), pd.Timestamp(\"2023-01-07\"), pd.Timestamp(\"2023-01-19\")]) ) daily_prices_correction Out[7]: High Low Volume 2023-01-05 3000.0 1000.0 10000 2023-01-07 4000.0 2000.0 20000 2023-01-19 5000.0 3000.0 30000 <p>Perform the merge update using the correction data. Ensure that both daily_prices_correction and the stored data are sorted. By default, two rows in DataFrames with a datetime index are considered matching if their indexes are equal.</p> In\u00a0[8]: Copied! <pre>lib.merge_experimental(\"daily_prices\", daily_prices_correction, strategy=MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\"))\nprint(\"Merged result\")\ndisplay(lib.read(\"daily_prices\").data)\nprint(\"Diff between merged and original data\")\ndaily_prices.compare(lib.read(\"daily_prices\").data, keep_equal=True)\n</pre> lib.merge_experimental(\"daily_prices\", daily_prices_correction, strategy=MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\")) print(\"Merged result\") display(lib.read(\"daily_prices\").data) print(\"Diff between merged and original data\") daily_prices.compare(lib.read(\"daily_prices\").data, keep_equal=True) <pre>Merged result\n</pre> High Low Volume 2023-01-01 102.5 100.5 1200 2023-01-02 103.6 101.8 1500 2023-01-03 101.7 99.7 900 2023-01-04 103.2 100.9 1400 2023-01-05 3000.0 1000.0 10000 2023-01-06 106.0 103.4 1900 2023-01-07 4000.0 2000.0 20000 2023-01-08 107.6 104.8 2300 2023-01-09 109.3 107.5 1800 2023-01-10 108.7 106.3 2100 2023-01-11 111.1 109.1 2000 2023-01-12 113.5 110.8 2700 2023-01-13 113.2 110.2 2600 2023-01-14 115.0 111.7 3000 2023-01-15 112.3 109.6 2200 2023-01-16 115.9 112.7 3100 2023-01-17 117.3 114.0 2800 2023-01-18 118.6 115.4 3200 2023-01-19 5000.0 3000.0 30000 2023-01-20 119.1 116.2 3400 <pre>Diff between merged and original data\n</pre> Out[8]: High Low Volume self other self other self other 2023-01-05 104.8 3000.0 101.7 1000.0 1600 10000 2023-01-07 108.9 4000.0 106.2 2000.0 2500 20000 2023-01-19 119.8 5000.0 116.5 3000.0 3500 30000 In\u00a0[9]: Copied! <pre>lib.write(\"prices_update_example\", daily_prices)\nlib.update(\"prices_update_example\", daily_prices_correction)\nlib.read(\"prices_update_example\").data\n</pre> lib.write(\"prices_update_example\", daily_prices) lib.update(\"prices_update_example\", daily_prices_correction) lib.read(\"prices_update_example\").data Out[9]: High Low Volume 2023-01-01 102.5 100.5 1200 2023-01-02 103.6 101.8 1500 2023-01-03 101.7 99.7 900 2023-01-04 103.2 100.9 1400 2023-01-05 3000.0 1000.0 10000 2023-01-07 4000.0 2000.0 20000 2023-01-19 5000.0 3000.0 30000 2023-01-20 119.1 116.2 3400 <p>In the example above, the input to <code>update</code> included only rows for <code>2023-01-05</code>, <code>2023-01-07</code>, and <code>2023-01-19</code>. As a result, any data between <code>2023-01-05</code> and <code>2023-01-19</code> that was not present in the <code>update</code> input is omitted from the final result.</p> In\u00a0[10]: Copied! <pre>data_with_duplicates = pd.DataFrame(\n    {\"Bid\": [100, 101, 102], \"Ask\": [101.1, 101, 102.5]},\n    index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 09:00:00\")])\n)\nlib.write(\"merge_update_with_duplicates\", data_with_duplicates)\nprint(\"Original data\")\ndisplay(lib.read(\"merge_update_with_duplicates\").data)\nlib.merge_experimental(\n    \"merge_update_with_duplicates\",\n    pd.DataFrame(\n        {\"Bid\": [105, 102], \"Ask\": [105.3, 102]},\n        index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-02-01 08:00:00\")])\n    ),\n    MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\")\n)\nprint(\"Data after merge\")\ndisplay(lib.read(\"merge_update_with_duplicates\").data)\n</pre> data_with_duplicates = pd.DataFrame(     {\"Bid\": [100, 101, 102], \"Ask\": [101.1, 101, 102.5]},     index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 09:00:00\")]) ) lib.write(\"merge_update_with_duplicates\", data_with_duplicates) print(\"Original data\") display(lib.read(\"merge_update_with_duplicates\").data) lib.merge_experimental(     \"merge_update_with_duplicates\",     pd.DataFrame(         {\"Bid\": [105, 102], \"Ask\": [105.3, 102]},         index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-02-01 08:00:00\")])     ),     MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\") ) print(\"Data after merge\") display(lib.read(\"merge_update_with_duplicates\").data) <pre>Original data\n</pre> Bid Ask 2025-01-01 08:00:00 100 101.1 2025-01-01 08:00:00 101 101.0 2025-01-01 09:00:00 102 102.5 <pre>Data after merge\n</pre> Bid Ask 2025-01-01 08:00:00 105 105.3 2025-01-01 08:00:00 105 105.3 2025-01-01 09:00:00 102 102.5 <p>In the example above, there is a row in <code>source</code> with the index value <code>2025-01-01 08:00:00</code> that matches two rows in <code>target</code>. Both matching rows in <code>target</code> are updated. A row in <code>source</code> that does not match any row in <code>target</code> is not inserted. Rows in <code>target</code> that do not match any row in <code>source</code> remain unchanged.</p> <p>The elements of <code>MergeStrategy</code> can also be values of the <code>MergeAction</code> enum.</p> In\u00a0[11]: Copied! <pre>data_with_duplicates = pd.DataFrame(\n    {\"Bid\": [100, 101, 102], \"Ask\": [101.1, 101, 102.5]},\n    index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 09:00:00\")])\n)\nlib.write(\"merge_update_with_duplicates\", data_with_duplicates)\nprint(\"Original data\")\ndisplay(lib.read(\"merge_update_with_duplicates\").data)\nlib.merge_experimental(\n    \"merge_update_with_duplicates\",\n    pd.DataFrame(\n        {\"Bid\": [105, 102], \"Ask\": [105.3, 102]},\n        index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-02-01 08:00:00\")])\n    ),\n    MergeStrategy(MergeAction.UPDATE, not_matched_by_target=MergeAction.DO_NOTHING)\n)\nprint(\"Data after merge\")\ndisplay(lib.read(\"merge_update_with_duplicates\").data)\n</pre> data_with_duplicates = pd.DataFrame(     {\"Bid\": [100, 101, 102], \"Ask\": [101.1, 101, 102.5]},     index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-01-01 09:00:00\")]) ) lib.write(\"merge_update_with_duplicates\", data_with_duplicates) print(\"Original data\") display(lib.read(\"merge_update_with_duplicates\").data) lib.merge_experimental(     \"merge_update_with_duplicates\",     pd.DataFrame(         {\"Bid\": [105, 102], \"Ask\": [105.3, 102]},         index=pd.DatetimeIndex([pd.Timestamp(\"2025-01-01 08:00:00\"), pd.Timestamp(\"2025-02-01 08:00:00\")])     ),     MergeStrategy(MergeAction.UPDATE, not_matched_by_target=MergeAction.DO_NOTHING) ) print(\"Data after merge\") display(lib.read(\"merge_update_with_duplicates\").data) <pre>Original data\n</pre> Bid Ask 2025-01-01 08:00:00 100 101.1 2025-01-01 08:00:00 101 101.0 2025-01-01 09:00:00 102 102.5 <pre>Data after merge\n</pre> Bid Ask 2025-01-01 08:00:00 105 105.3 2025-01-01 08:00:00 105 105.3 2025-01-01 09:00:00 102 102.5 <ul> <li><code>MergeStrategy(matched=\"do_nothing\", not_matched_by_target=\"insert\")</code> inserts rows from <code>source</code> that do not match the selected columns.</li> <li><code>MergeStrategy(matched=\"update\", not_matched_by_target=\"insert\")</code> updates <code>target</code> by modifying matching rows and inserting non-matching rows from <code>source</code>.</li> </ul> <p>When the strategy involves updating on match, a row in <code>target</code> must not be matched by more than one row in <code>source</code>, as this would create ambiguity about which values to use.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/ArcticDB_merge/#arcticdb-merge","title":"ArcticDB Merge\u00b6","text":"\u26a0\ufe0f Warning: The merge API is under development and subject to change. Additional features will be introduced in the future. The API is not subject to semver and can change in minor or patch releases. Stay tuned for updates! The API call at the moment is named merge_experimental but it will be changed to merge when the API becomes stable."},{"location":"notebooks/ArcticDB_merge/#motivation","title":"Motivation\u00b6","text":"<p>The merge API offers a straightforward way for users to modify or insert specific rows in their data. Data providers often issue corrections for particular entries, and the merge functionality can efficiently apply these updates.</p>"},{"location":"notebooks/ArcticDB_merge/#example","title":"Example\u00b6","text":"<p>Let's start with a quick example before exploring the details.</p> <p>In this example, we have an ArcticDB library called <code>prices</code> that stores price data, and a symbol named <code>daily_prices</code> for daily prices. At some point, the data provider issued corrections for <code>2023-01-05</code>, <code>2023-01-07</code>, and <code>2023-01-19</code>. We want to apply these corrections, modifying only the affected rows.</p> <p>Begin by importing the necessary modules.</p>"},{"location":"notebooks/ArcticDB_merge/#merge-vs-update","title":"Merge vs. Update\u00b6","text":"<p>ArcticDB also provides an <code>update</code> method, which requires sorted input. The key difference is that <code>update</code> overwrites all data between the start and end of the input, potentially removing any rows that fall within those bounds and are not in the input.</p>"},{"location":"notebooks/ArcticDB_merge/#merge-semantics-and-behavior","title":"Merge: Semantics and Behavior\u00b6","text":"<p>In this context, the data stored in ArcticDB is referred to as the <code>target</code>, while the input to <code>merge</code> is called the <code>source</code>.</p> <p>Merge works by performing a join between <code>target</code> and <code>source</code> on a subset of columns, and updating <code>target</code> based on the <code>strategy</code> parameter. If <code>target</code> is a time series, the index will always be included among the join columns. This is done to ensure that ordered date-time indexes stay ordered after performing a merge. Otherwise the following would be possible</p> <pre>lib.write(\"sym\", pd.DataFrame({\"a\": [1, 2, 3]}, index=pd.DatetimeIndex([pd.Timestamp(1), pd.Timestamp(2), pd.Timestamp(3)])))\nlib.merge_experimental(\"sym\", pd.DataFrame({\"a\": [2]}, index=pd.DatetimeIndex([pd.Timestamp(10)])), on=[\"a\"])\nprint(lib.read(\"sym\").data)\n                               a\n1970-01-01 00:00:00.000000001  1\n1970-01-01 00:00:00.000000010  2\n1970-01-01 00:00:00.000000003  3\n</pre>"},{"location":"notebooks/ArcticDB_merge/#strategies","title":"Strategies\u00b6","text":"<p>The strategy is a named tuple that defines how <code>target</code> will be modified. Its members can be either case-insensitive strings (<code>update</code>, <code>insert</code>, or <code>do_nothing</code>) or members of the <code>MergeAction</code> enum.</p> <ul> <li><code>matched</code> specifies what to do when a row in <code>source</code> matches a row in <code>target</code>. Acceptable values are <code>update</code> and <code>do_nothing</code>.</li> <li><code>not_matched_by_target</code> specifies what to do when a row in <code>source</code> does not match any row in <code>target</code>. Acceptable values are <code>insert</code> and <code>do_nothing</code>.</li> </ul>"},{"location":"notebooks/ArcticDB_merge/#acceptable-combinations","title":"Acceptable combinations\u00b6","text":"<ul> <li><code>matched=\"update\"</code>, <code>not_matched_by_target=\"do_nothing\"</code></li> <li><code>matched=\"update\"</code>, <code>not_matched_by_target=\"insert\"</code></li> <li><code>matched=\"do_nothing\"</code>, <code>not_matched_by_target=\"insert\"</code></li> </ul>"},{"location":"notebooks/ArcticDB_merge/#unacceptable-combinations","title":"Unacceptable combinations\u00b6","text":"<ul> <li><code>matched=\"do_nothing\"</code>, <code>not_matched_by_target=\"do_nothing\"</code> - nothing is going to happen</li> <li><code>matched=\"do_nothing\"|\"insert\"|\"update\"</code>, <code>not_matched_by_target=\"update\"</code> - cannot update a row that is not existing in the target</li> <li><code>matched=\"insert\"</code>, <code>not_matched_by_target=\"do_nothing\"|\"insert\"|\"update\"</code> - while technically possible it doesn't make sense to insert duplicates</li> </ul>"},{"location":"notebooks/ArcticDB_merge/#examples","title":"Examples\u00b6","text":"<ul> <li><code>MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\")</code> updates only the rows in <code>target</code> that match the selected columns. Rows that do not match remain unchanged, and new rows are not inserted. If a row in <code>source</code> matches multiple rows in <code>target</code>, all matching rows in <code>target</code> will be updated.</li> </ul>"},{"location":"notebooks/ArcticDB_merge/#limitations","title":"Limitations\u00b6","text":"<p>Current implementation has a few limitations</p> <ol> <li>Both <code>target</code> and <code>source</code> must have a sorted <code>pd.DatetimeIndex</code>.</li> <li>The library must use a static schema, and both <code>source</code> and <code>target</code> must share the same schema.</li> <li>The only supported strategy is <code>MergeStrategy(matched=\"update\", not_matched_by_target=\"do_nothing\")</code>.</li> <li>Matching on columns other than the index is not yet implemented.</li> </ol>"},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/","title":"Pythagorean Won Loss Formula Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>try:\n    import google.colab\n    import requests\n    exec(requests.get(f\"https://raw.githubusercontent.com/man-group/ArcticDB/master/docs/mkdocs/docs/notebooks/styling.py\").text)\n    dataFile = f'https://raw.githubusercontent.com/man-group/ArcticDB/master/docs/mkdocs/docs/notebooks/data/2025-08-26-sports.csv'\nexcept ImportError:\n    from styling import *\n    dataFile = \"data/2025-08-26-sports.csv\"\n</pre> try:     import google.colab     import requests     exec(requests.get(f\"https://raw.githubusercontent.com/man-group/ArcticDB/master/docs/mkdocs/docs/notebooks/styling.py\").text)     dataFile = f'https://raw.githubusercontent.com/man-group/ArcticDB/master/docs/mkdocs/docs/notebooks/data/2025-08-26-sports.csv' except ImportError:     from styling import *     dataFile = \"data/2025-08-26-sports.csv\"  In\u00a0[2]: Copied! <pre>%pip -q install arcticdb pandas numpy statsmodels plotly scikit-learn\nimport arcticdb as adb\nadb.__version__\n</pre> %pip -q install arcticdb pandas numpy statsmodels plotly scikit-learn import arcticdb as adb adb.__version__ <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> Out[2]: <pre>'6.1.0'</pre> In\u00a0[3]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ncolors = setup_plotly_theme()\n</pre> import numpy as np import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf  colors = setup_plotly_theme() In\u00a0[4]: Copied! <pre>df = pd.read_csv(dataFile)\n</pre> df = pd.read_csv(dataFile) In\u00a0[5]: Copied! <pre>df = df.drop(columns=\"Unnamed: 0\")\ndf = df.rename(columns={'FREQ_RUNS_S': 'PS',\n                        'FREQ_RUNS_A': 'PA' })\n\ndf = df.sort_values(['LEAGUE','YEAR_ID','TEAM_ID'])\nstyled = style_table(df.head())\nexport_table(styled, \"basic_data\")\nstyled\n</pre> df = df.drop(columns=\"Unnamed: 0\") df = df.rename(columns={'FREQ_RUNS_S': 'PS',                         'FREQ_RUNS_A': 'PA' })  df = df.sort_values(['LEAGUE','YEAR_ID','TEAM_ID']) styled = style_table(df.head()) export_table(styled, \"basic_data\") styled <pre>Exported: basic_data.csv\n</pre> Out[5]: YEAR_ID TEAM_ID OPP_TEAM_ID PS PA date home wi LEAGUE 1897 Carlton Fitzroy 16 49 1897-05-08 0 0.000000 AFL 1897 Carlton South Melbourne 36 40 1897-05-15 0 0.000000 AFL 1897 Carlton Essendon 41 78 1897-05-24 0 0.000000 AFL 1897 Carlton Geelong 22 44 1897-05-29 0 0.000000 AFL 1897 Carlton Melbourne 26 107 1897-06-05 0 0.000000 AFL In\u00a0[6]: Copied! <pre># Create a head/tail display of the full dataset\nstyled_head_tail = style_head_tail_table(df, head_rows=10, tail_rows=10)\nexport_table(styled_head_tail, \"head_tail_data\", \"Sports Data - First 10 and Last 10 Rows\")\nstyled_head_tail\n</pre> # Create a head/tail display of the full dataset styled_head_tail = style_head_tail_table(df, head_rows=10, tail_rows=10) export_table(styled_head_tail, \"head_tail_data\", \"Sports Data - First 10 and Last 10 Rows\") styled_head_tail <pre>Exported: head_tail_data.csv\n</pre> Out[6]: 853152 rows \u00d7 9 columns YEAR_ID TEAM_ID OPP_TEAM_ID PS PA date home wi LEAGUE 1897 Carlton Fitzroy 16 49 1897-05-08 0 0.000000 AFL 1897 Carlton South Melbourne 36 40 1897-05-15 0 0.000000 AFL 1897 Carlton Essendon 41 78 1897-05-24 0 0.000000 AFL 1897 Carlton Geelong 22 44 1897-05-29 0 0.000000 AFL 1897 Carlton Melbourne 26 107 1897-06-05 0 0.000000 AFL 1897 Carlton St Kilda 35 24 1897-06-19 0 1.000000 AFL 1897 Carlton Collingwood 35 41 1897-06-22 1 0.000000 AFL 1897 Carlton Fitzroy 9 47 1897-06-26 1 0.000000 AFL 1897 Carlton South Melbourne 28 74 1897-07-03 1 0.000000 AFL 1897 Carlton Essendon 13 42 1897-07-10 1 0.000000 AFL ... ... ... ... ... ... ... ... ... 2019 Waratahs Blues 29 32 2019-04-06 0 0.000000 SUP 2019 Waratahs Rebels 23 20 2019-04-20 1 1.000000 SUP 2019 Waratahs Sharks 15 23 2019-04-27 1 0.000000 SUP 2019 Waratahs Bulls 21 28 2019-05-04 0 0.000000 SUP 2019 Waratahs Lions 28 29 2019-05-11 0 0.000000 SUP 2019 Waratahs Reds 40 32 2019-05-18 0 1.000000 SUP 2019 Waratahs Jaguares 15 23 2019-05-25 1 0.000000 SUP 2019 Waratahs Rebels 20 15 2019-05-31 0 1.000000 SUP 2019 Waratahs Brumbies 24 35 2019-06-08 1 0.000000 SUP 2019 Waratahs Highlanders 12 49 2019-06-14 0 0.000000 SUP In\u00a0[7]: Copied! <pre>arctic = adb.Arctic('lmdb://arcticdb_sports')\nlib = arctic.get_library('leagues', create_if_missing=True)\nfor league in df['LEAGUE'].unique():\n    lib.write(league, df[df['LEAGUE']==league])\nlib.list_symbols()\n</pre> arctic = adb.Arctic('lmdb://arcticdb_sports') lib = arctic.get_library('leagues', create_if_missing=True) for league in df['LEAGUE'].unique():     lib.write(league, df[df['LEAGUE']==league]) lib.list_symbols() Out[7]: <pre>['MLB', 'NFL', 'AFL', 'EPL', 'SUP', 'NHL', 'LAX', 'IPL', 'NBA']</pre> In\u00a0[8]: Copied! <pre>style_head_tail_table(lib.read('LAX').data, head_rows=5, tail_rows=5)\n</pre> style_head_tail_table(lib.read('LAX').data, head_rows=5, tail_rows=5) Out[8]: 10006 rows \u00d7 9 columns YEAR_ID TEAM_ID OPP_TEAM_ID PS PA date home wi LEAGUE 2010 Air Force St. John's (NY) 7 10 2010-02-28 1 0.000000 LAX 2010 Air Force Lehigh 13 14 2010-03-06 1 0.000000 LAX 2010 Air Force Penn 7 8 2010-03-07 1 0.000000 LAX 2010 Air Force Army West Point 8 7 2010-03-13 1 1.000000 LAX 2010 Air Force Loyola Maryland 3 18 2010-03-20 0 0.000000 LAX ... ... ... ... ... ... ... ... ... 2019 Yale Penn 11 12 2019-05-05 1 0.000000 LAX 2019 Yale Georgetown 19 16 2019-05-11 1 1.000000 LAX 2019 Yale Penn 19 18 2019-05-19 1 1.000000 LAX 2019 Yale Penn St. 21 17 2019-05-25 1 1.000000 LAX 2019 Yale Virginia 9 13 2019-05-27 1 0.000000 LAX In\u00a0[9]: Copied! <pre>def ols_coef (df, xcol, ycol):\n    \"\"\"simple OLS function used for lambda calcs -- assumes univariate regression\"\"\"\n    model=sm.OLS(df[ycol],sm.add_constant(df[xcol])).fit()    \n\n    outdf = pd.DataFrame({'adj.'    : model.params.iloc[0],\n                          'adj. se' : model.bse.iloc[0],\n                          '\u03bb'    : model.params.iloc[1],\n                          '\u03bb se' : model.bse.iloc[1],\n                          'nobs' : model.nobs,\n                          'adj. $R^2$': model.rsquared_adj}, index=[0])    \n\n    return outdf\n</pre> def ols_coef (df, xcol, ycol):     \"\"\"simple OLS function used for lambda calcs -- assumes univariate regression\"\"\"     model=sm.OLS(df[ycol],sm.add_constant(df[xcol])).fit()          outdf = pd.DataFrame({'adj.'    : model.params.iloc[0],                           'adj. se' : model.bse.iloc[0],                           '\u03bb'    : model.params.iloc[1],                           '\u03bb se' : model.bse.iloc[1],                           'nobs' : model.nobs,                           'adj. $R^2$': model.rsquared_adj}, index=[0])          return outdf <p>Our study period is from 2010 to 2019</p> In\u00a0[10]: Copied! <pre>st_year_ = 2010\nend_year_ = 2019\n</pre> st_year_ = 2010 end_year_ = 2019 In\u00a0[11]: Copied! <pre>league_data = arctic.get_library('leagues')\nleagues = league_data.list_symbols()\nleagues\n</pre> league_data = arctic.get_library('leagues') leagues = league_data.list_symbols() leagues Out[11]: <pre>['MLB', 'NFL', 'AFL', 'EPL', 'SUP', 'NHL', 'LAX', 'IPL', 'NBA']</pre> In\u00a0[\u00a0]: Copied! <pre># Get points scored data for all games, 2010-2019 seasons  \nlazy_dfs = league_data.read_batch(leagues, lazy=True)\nlazy_dfs = lazy_dfs[(lazy_dfs['YEAR_ID']&gt;=st_year_) &amp; (lazy_dfs['YEAR_ID']&lt;=end_year_)]\nlazy_dfs = adb.concat(lazy_dfs)\npoints_data = lazy_dfs.collect().data[['LEAGUE', 'PS']]\n\n# Define league order to match Figure 1 layout (3x3 grid, alphabetical)\nleague_order = ['AFL', 'EPL', 'IPL', 'LAX', 'MLB', 'NBA', 'NFL', 'NHL', 'SUP']\n\n# Create and display interactive Plotly version with adaptive binning\nfig = create_plotly_histplot(\n    data=points_data,\n    x_col='PS',\n    col_col='LEAGUE',\n    col_order=league_order,  # Same order as seaborn\n    col_wrap=3,  # 3x3 grid layout\n    adaptive_bins=True  # Use adaptive binning to fix EPL/NHL gaps\n)\n\n# Display interactive plot in notebook\nfig.show()\n\n# Export both HTML and JSON versions\nexport_plot(fig, \"figure1_points_distributions\", \"Figure 1 - Points per game distributions\")\n</pre> # Get points scored data for all games, 2010-2019 seasons   lazy_dfs = league_data.read_batch(leagues, lazy=True) lazy_dfs = lazy_dfs[(lazy_dfs['YEAR_ID']&gt;=st_year_) &amp; (lazy_dfs['YEAR_ID']&lt;=end_year_)] lazy_dfs = adb.concat(lazy_dfs) points_data = lazy_dfs.collect().data[['LEAGUE', 'PS']]  # Define league order to match Figure 1 layout (3x3 grid, alphabetical) league_order = ['AFL', 'EPL', 'IPL', 'LAX', 'MLB', 'NBA', 'NFL', 'NHL', 'SUP']  # Create and display interactive Plotly version with adaptive binning fig = create_plotly_histplot(     data=points_data,     x_col='PS',     col_col='LEAGUE',     col_order=league_order,  # Same order as seaborn     col_wrap=3,  # 3x3 grid layout     adaptive_bins=True  # Use adaptive binning to fix EPL/NHL gaps )  # Display interactive plot in notebook fig.show()  # Export both HTML and JSON versions export_plot(fig, \"figure1_points_distributions\", \"Figure 1 - Points per game distributions\") <pre>Exported: exports/figure1_points_distributions.json\n</pre> In\u00a0[13]: Copied! <pre># points scored for all games\nlazy_dfs = league_data.read_batch(leagues, lazy=True)\nlazy_dfs = lazy_dfs[(lazy_dfs['YEAR_ID']&gt;=st_year_) &amp; (lazy_dfs['YEAR_ID']&lt;=end_year_)]\nlazy_dfs = adb.concat(lazy_dfs)\npoints_scored = lazy_dfs.collect().data[['LEAGUE', 'PS']]\nstyle_table(points_scored)\n</pre> # points scored for all games lazy_dfs = league_data.read_batch(leagues, lazy=True) lazy_dfs = lazy_dfs[(lazy_dfs['YEAR_ID']&gt;=st_year_) &amp; (lazy_dfs['YEAR_ID']&lt;=end_year_)] lazy_dfs = adb.concat(lazy_dfs) points_scored = lazy_dfs.collect().data[['LEAGUE', 'PS']] style_table(points_scored) Out[13]: LEAGUE PS MLB 6 MLB 3 MLB 2 MLB 1 MLB 4 MLB 4 MLB 4 MLB 5 MLB 5 MLB 2 MLB 7 MLB 6 MLB 3 MLB 2 MLB 6 MLB 3 MLB 4 MLB 6 MLB 1 MLB 8 In\u00a0[14]: Copied! <pre>point_stats = points_scored.groupby(['LEAGUE']).agg(Mean=('PS', 'mean'),\n                                             Std=('PS', 'std'),\n                                             Skew=('PS', 'skew'),\n                                             nobs=('PS', 'count'))\npoint_stats['IR']=point_stats['Mean']/point_stats['Std']\npoint_stats = point_stats.sort_values(['IR'])\nstyle_table(point_stats.round(2), hide_index=False)\n</pre> point_stats = points_scored.groupby(['LEAGUE']).agg(Mean=('PS', 'mean'),                                              Std=('PS', 'std'),                                              Skew=('PS', 'skew'),                                              nobs=('PS', 'count')) point_stats['IR']=point_stats['Mean']/point_stats['Std'] point_stats = point_stats.sort_values(['IR']) style_table(point_stats.round(2), hide_index=False) Out[14]: Mean Std Skew nobs IR LEAGUE EPL 1.380000 1.260000 1.030000 7600 1.090000 MLB 4.390000 3.100000 1.000000 48594 1.420000 NHL 2.810000 1.640000 0.420000 23744 1.710000 SUP 25.100000 11.970000 0.880000 2402 2.100000 NFL 22.640000 10.180000 0.290000 5120 2.230000 LAX 10.260000 3.980000 0.500000 10006 2.580000 AFL 88.300000 27.750000 0.480000 3892 3.180000 IPL 163.270000 31.420000 0.160000 1274 5.200000 NBA 102.240000 12.660000 0.180000 24118 8.080000 In\u00a0[\u00a0]: Copied! <pre># Create and display interactive Plotly version with same layout\nfig = create_plotly_catplot(\n    data=point_stats.reset_index().melt(['LEAGUE']),\n    x_col=\"LEAGUE\",\n    y_col=\"value\",\n    hue_col=\"LEAGUE\",\n    col_col=\"variable\",\n    col_wrap=3,  # Same as seaborn version\n    sharey=False\n)\n\n# Display interactive plot in notebook\nfig.show()\n\n# Export both HTML and JSON versions\nexport_plot(fig, \"point_stats_catplot\", \"Points Scored Statistics\")\n</pre> # Create and display interactive Plotly version with same layout fig = create_plotly_catplot(     data=point_stats.reset_index().melt(['LEAGUE']),     x_col=\"LEAGUE\",     y_col=\"value\",     hue_col=\"LEAGUE\",     col_col=\"variable\",     col_wrap=3,  # Same as seaborn version     sharey=False )  # Display interactive plot in notebook fig.show()  # Export both HTML and JSON versions export_plot(fig, \"point_stats_catplot\", \"Points Scored Statistics\") <pre>Exported: exports/point_stats_catplot.json\n</pre> In\u00a0[16]: Copied! <pre>analysis = arctic.get_library('analysis', create_if_missing=True)\nanalysis.write('point_stats', point_stats)\n</pre> analysis = arctic.get_library('analysis', create_if_missing=True) analysis.write('point_stats', point_stats) Out[16]: <pre>VersionedItem(symbol='point_stats', library='analysis', data=n/a, version=0, metadata=None, host='LMDB(path=/home/nick/source/customers/man-group/ArcticDB/docs/mkdocs/docs/notebooks/arcticdb_sports)', timestamp=1756225340782814206)</pre> In\u00a0[17]: Copied! <pre>lazy_dfs = league_data.read_batch(leagues, lazy=True)\ndf = adb.concat(lazy_dfs).collect().data[['LEAGUE','YEAR_ID','TEAM_ID','PS','PA','wi']]\ndf_team = df.groupby(['LEAGUE','YEAR_ID','TEAM_ID']).agg(PS=('PS', 'mean'),\n                                                         PA=('PA', 'mean'),\n                                                         wi=('wi', 'mean'),\n                                                         nobs=('wi', 'count'))\nstyle_table(df_team.round(2).head(),hide_index=False)\n</pre> lazy_dfs = league_data.read_batch(leagues, lazy=True) df = adb.concat(lazy_dfs).collect().data[['LEAGUE','YEAR_ID','TEAM_ID','PS','PA','wi']] df_team = df.groupby(['LEAGUE','YEAR_ID','TEAM_ID']).agg(PS=('PS', 'mean'),                                                          PA=('PA', 'mean'),                                                          wi=('wi', 'mean'),                                                          nobs=('wi', 'count')) style_table(df_team.round(2).head(),hide_index=False) Out[17]: PS PA wi nobs LEAGUE YEAR_ID TEAM_ID AFL 1897 Carlton 26.860000 52.640000 0.140000 14 Collingwood 36.360000 31.790000 0.640000 14 Essendon 50.430000 31.790000 0.790000 14 Fitzroy 36.360000 34.640000 0.320000 14 Geelong 50.290000 27.360000 0.790000 14 In\u00a0[\u00a0]: Copied! <pre># prepare data for regression\ndf_reg = df_team.copy()\ndf_reg = df_reg.loc[(df_reg['PS']&gt;0) &amp; (df_reg['PA']&gt;0) &amp; (df_reg['wi']&gt;0) &amp; (df_reg['wi']&lt;1)]\ndf_reg['lnwi']=np.log(df_reg['wi']/(1-df_reg['wi']))\ndf_reg['lnPSPA']=np.log(df_reg['PS']/df_reg['PA'])\ndf_reg = df_reg.reset_index()\n\n# Create static seaborn plot\nNBA_and_EPL = df_reg[df_reg['LEAGUE'].isin(['NBA', 'EPL'])]\n\n# Create and display interactive Plotly version\nfig = create_plotly_scatterplot(\n    data=NBA_and_EPL,\n    x_col='lnPSPA',\n    y_col='lnwi', \n    hue_col='LEAGUE',\n    show_regression=True\n)\n\n# Display interactive plot in notebook\nfig.show()\n\n# Export both HTML and JSON versions\nexport_plot(fig, \"team_season_plot\", \"Team-Season Analysis\")\n</pre> # prepare data for regression df_reg = df_team.copy() df_reg = df_reg.loc[(df_reg['PS']&gt;0) &amp; (df_reg['PA']&gt;0) &amp; (df_reg['wi']&gt;0) &amp; (df_reg['wi']&lt;1)] df_reg['lnwi']=np.log(df_reg['wi']/(1-df_reg['wi'])) df_reg['lnPSPA']=np.log(df_reg['PS']/df_reg['PA']) df_reg = df_reg.reset_index()  # Create static seaborn plot NBA_and_EPL = df_reg[df_reg['LEAGUE'].isin(['NBA', 'EPL'])]  # Create and display interactive Plotly version fig = create_plotly_scatterplot(     data=NBA_and_EPL,     x_col='lnPSPA',     y_col='lnwi',      hue_col='LEAGUE',     show_regression=True )  # Display interactive plot in notebook fig.show()  # Export both HTML and JSON versions export_plot(fig, \"team_season_plot\", \"Team-Season Analysis\") <pre>Exported: exports/team_season_plot.json\n</pre> In\u00a0[19]: Copied! <pre># Compute standard regression results by team-season for 2010-19 period\ndf_ols = df_reg.loc[(df_reg['YEAR_ID']&gt;=st_year_) &amp; (df_reg['YEAR_ID']&lt;=end_year_)]\nols=df_ols.groupby(['LEAGUE']).apply(ols_coef,xcol=['lnPSPA'],ycol=['lnwi'])\nols = ols.droplevel(1).sort_values(['\u03bb']).reset_index().round(2)\nstyle_table(ols, hide_index=False)\n</pre> # Compute standard regression results by team-season for 2010-19 period df_ols = df_reg.loc[(df_reg['YEAR_ID']&gt;=st_year_) &amp; (df_reg['YEAR_ID']&lt;=end_year_)] ols=df_ols.groupby(['LEAGUE']).apply(ols_coef,xcol=['lnPSPA'],ycol=['lnwi']) ols = ols.droplevel(1).sort_values(['\u03bb']).reset_index().round(2) style_table(ols, hide_index=False) <pre>/tmp/ipykernel_42612/1625533100.py:3: FutureWarning:\n\nDataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n\n</pre> Out[19]: LEAGUE adj. adj. se \u03bb \u03bb se nobs adj. $R^2$ 0 EPL -0.150000 0.010000 1.240000 0.020000 200.000000 0.940000 1 MLB -0.000000 0.010000 1.780000 0.040000 300.000000 0.890000 2 NHL 0.240000 0.010000 2.060000 0.040000 302.000000 0.910000 3 SUP -0.030000 0.030000 2.600000 0.090000 154.000000 0.840000 4 NFL 0.020000 0.020000 2.840000 0.080000 319.000000 0.810000 5 LAX -0.020000 0.010000 3.170000 0.050000 655.000000 0.840000 6 AFL -0.020000 0.020000 3.570000 0.090000 177.000000 0.900000 7 IPL -0.020000 0.040000 6.790000 0.650000 84.000000 0.570000 8 NBA -0.000000 0.010000 14.320000 0.210000 300.000000 0.940000 In\u00a0[20]: Copied! <pre>analysis.write('ols', ols);\n</pre> analysis.write('ols', ols); In\u00a0[\u00a0]: Copied! <pre># Prepare data for interactive Plotly version with error bars\nols_melted = ols.melt(['LEAGUE'])\n# Add error column - only \u03bb has error bars (2 * standard error)\nols_melted['error'] = ols_melted.apply(lambda row: \n    ols.loc[ols['LEAGUE'] == row['LEAGUE'], '\u03bb se'].iloc[0] * 2 \n    if row['variable'] == '\u03bb' else None, axis=1)\n\n# Create and display interactive Plotly version\nfig = create_plotly_catplot(\n    data=ols_melted,\n    x_col=\"LEAGUE\",\n    y_col=\"value\",\n    hue_col=\"LEAGUE\",\n    col_col=\"variable\",\n    col_order=[\"\u03bb\", \"nobs\", \"adj. $R^2$\"],\n    sharey=False,\n    error_col=\"error\",\n    add_value_labels=True  # Show values on R^2 bars like the seaborn version\n)\n\n# Display interactive plot in notebook\nfig.show()\n\n# Export both HTML and JSON versions\nexport_plot(fig, \"ols_results_catplot\", \"OLS Results by League\")\n</pre> # Prepare data for interactive Plotly version with error bars ols_melted = ols.melt(['LEAGUE']) # Add error column - only \u03bb has error bars (2 * standard error) ols_melted['error'] = ols_melted.apply(lambda row:      ols.loc[ols['LEAGUE'] == row['LEAGUE'], '\u03bb se'].iloc[0] * 2      if row['variable'] == '\u03bb' else None, axis=1)  # Create and display interactive Plotly version fig = create_plotly_catplot(     data=ols_melted,     x_col=\"LEAGUE\",     y_col=\"value\",     hue_col=\"LEAGUE\",     col_col=\"variable\",     col_order=[\"\u03bb\", \"nobs\", \"adj. $R^2$\"],     sharey=False,     error_col=\"error\",     add_value_labels=True  # Show values on R^2 bars like the seaborn version )  # Display interactive plot in notebook fig.show()  # Export both HTML and JSON versions export_plot(fig, \"ols_results_catplot\", \"OLS Results by League\") <pre>Exported: exports/ols_results_catplot.json\n</pre> In\u00a0[\u00a0]: Copied! <pre># Create and display interactive Plotly versions\n\n# 1. Lambda estimates with error bars\nfig1 = create_plotly_bar_chart(\n    data=ols,\n    x_col='LEAGUE',\n    y_col='\u03bb',\n    title='Figure 3a. Estimates of \u03bb by League, 2010-2019 Seasons',\n    error_col='\u03bb se',  # Error bars (will be multiplied by 2 in the function)\n    add_value_labels=False\n)\n# Multiply error bars by 2 to match matplotlib version\nfig1.data[0].error_y.array = fig1.data[0].error_y.array * 2\nfig1.show()\nexport_plot(fig1, \"lambda_estimates\", \"Lambda Estimates by League\")\n\n# 2. Number of observations\nfig2 = create_plotly_bar_chart(\n    data=ols,\n    x_col='LEAGUE',\n    y_col='nobs',\n    title='Figure 3a. Number of Obs by League, 2010-2019 Seasons',\n    add_value_labels=False\n)\n# Use secondary color\nfig2.data[0].marker.color = colors['secondary_color']\nfig2.show()\nexport_plot(fig2, \"number_observations\", \"Number of Observations by League\")\n\n# 3. Adjusted R-squared with value labels\nfig3 = create_plotly_bar_chart(\n    data=ols,\n    x_col='LEAGUE',\n    y_col='adj. $R^2$',\n    add_value_labels=True\n)\n# Use accent color\nfig3.data[0].marker.color = colors['accent_color']\nfig3.show()\nexport_plot(fig3, \"adjusted_rsquared\", \"Adjusted R-squared by League\")\n</pre>  # Create and display interactive Plotly versions  # 1. Lambda estimates with error bars fig1 = create_plotly_bar_chart(     data=ols,     x_col='LEAGUE',     y_col='\u03bb',     title='Figure 3a. Estimates of \u03bb by League, 2010-2019 Seasons',     error_col='\u03bb se',  # Error bars (will be multiplied by 2 in the function)     add_value_labels=False ) # Multiply error bars by 2 to match matplotlib version fig1.data[0].error_y.array = fig1.data[0].error_y.array * 2 fig1.show() export_plot(fig1, \"lambda_estimates\", \"Lambda Estimates by League\")  # 2. Number of observations fig2 = create_plotly_bar_chart(     data=ols,     x_col='LEAGUE',     y_col='nobs',     title='Figure 3a. Number of Obs by League, 2010-2019 Seasons',     add_value_labels=False ) # Use secondary color fig2.data[0].marker.color = colors['secondary_color'] fig2.show() export_plot(fig2, \"number_observations\", \"Number of Observations by League\")  # 3. Adjusted R-squared with value labels fig3 = create_plotly_bar_chart(     data=ols,     x_col='LEAGUE',     y_col='adj. $R^2$',     add_value_labels=True ) # Use accent color fig3.data[0].marker.color = colors['accent_color'] fig3.show() export_plot(fig3, \"adjusted_rsquared\", \"Adjusted R-squared by League\") <pre>Exported: exports/lambda_estimates.json\n</pre> <pre>Exported: exports/number_observations.json\n</pre> <pre>Exported: exports/adjusted_rsquared.json\n</pre> In\u00a0[\u00a0]: Copied! <pre># Figure 3a - Single Lambda Plot with Error Bars and N Values\n# Sort by lambda values (ascending order)\nols_sorted = ols.sort_values('\u03bb').copy()\n\n\n# Create interactive Plotly version\nfig_plotly = create_plotly_bar_chart(\n    data=ols_sorted,\n    x_col='LEAGUE',\n    y_col='\u03bb',\n    error_col='\u03bb se',\n    add_value_labels=True\n)\n\n# Multiply error bars by 2 to show \u00b12 standard errors\nfig_plotly.data[0].error_y.array = fig_plotly.data[0].error_y.array * 2\n\n# Update x-axis labels to include N values\nx_labels_plotly = [f\"{row['LEAGUE']}&lt;br&gt;N = {int(row['nobs'])}\" for _, row in ols_sorted.iterrows()]\nfig_plotly.update_xaxes(\n    tickmode='array',\n    tickvals=list(range(len(ols_sorted))),\n    ticktext=x_labels_plotly\n)\n\n# Update layout for better spacing\nfig_plotly.update_layout(\n    yaxis_title=\"Fitted \u03bb\",\n    height=600,\n    margin=dict(b=100)  # More bottom margin for N values\n)\n\nfig_plotly.show()\n\n# Export the plot\nexport_plot(fig_plotly, \"lambda_estimates_complete\", \"Lambda Estimates with Error Bars\")\n</pre> # Figure 3a - Single Lambda Plot with Error Bars and N Values # Sort by lambda values (ascending order) ols_sorted = ols.sort_values('\u03bb').copy()   # Create interactive Plotly version fig_plotly = create_plotly_bar_chart(     data=ols_sorted,     x_col='LEAGUE',     y_col='\u03bb',     error_col='\u03bb se',     add_value_labels=True )  # Multiply error bars by 2 to show \u00b12 standard errors fig_plotly.data[0].error_y.array = fig_plotly.data[0].error_y.array * 2  # Update x-axis labels to include N values x_labels_plotly = [f\"{row['LEAGUE']}N = {int(row['nobs'])}\" for _, row in ols_sorted.iterrows()] fig_plotly.update_xaxes(     tickmode='array',     tickvals=list(range(len(ols_sorted))),     ticktext=x_labels_plotly )  # Update layout for better spacing fig_plotly.update_layout(     yaxis_title=\"Fitted \u03bb\",     height=600,     margin=dict(b=100)  # More bottom margin for N values )  fig_plotly.show()  # Export the plot export_plot(fig_plotly, \"lambda_estimates_complete\", \"Lambda Estimates with Error Bars\") <pre>Exported: exports/lambda_estimates_complete.json\n</pre> <p>Source: Various (listed in Appendix) as at January 2025.</p> <p>Key: Australian Football League (AFL), English Premier League (EPL), Indian Premier League (IPL), NCAA Men's Division I Lacrosse (LAX), Major League Baseball (MLB), National Basketball Association (NBA), National Football League (NFL), National Hockey League (NHL), Super Rugby Pacific (SUP)</p> <p>Note: Error bars represent \u00b12 standard errors around the coefficient estimate. N is the number of team-seasons used to estimate the model.</p> <p>The \u03bb estimates and errors are shown in Figure 3a. As we've noted, the EPL and NBA sit at the extremes. The model fits all sports well with statistically significant coefficients. The confidence bands around our estimate for the IPL is wider (also adjusted R\u00b2 is 0.57 for the IPL compared to a range of 0.81 to 0.94 for the other leagues). For those that are interested, the poorer fit for the IPL we believe is due to the limited number of teams each season (8\u201310) and the truncated game effect in Twenty20 Cricket, discussed in Appendix A, which we can only partially adjust for.</p> In\u00a0[\u00a0]: Copied! <pre># calculate mean league IRs group by league-season\ndf_leagueM = df[['LEAGUE','YEAR_ID','PS','PA','wi']].groupby(['LEAGUE','YEAR_ID']).mean()\ndf_leagueS = df[['LEAGUE','YEAR_ID','PS','PA','wi']].groupby(['LEAGUE','YEAR_ID']).std()\ndf_leagueIR = df_leagueM['PS']/df_leagueS['PS']\ndf_leagueIR = df_leagueIR.reset_index().rename(columns={'PS': 'IR'})\n\n# calc 10 year average for 2010-2019\ndf_leagueIR10 = df_leagueIR.loc[(df_leagueIR['YEAR_ID']&gt;=st_year_) &amp; (df_leagueIR['YEAR_ID']&lt;=end_year_)]\ndf_leagueIR10 = df_leagueIR10[['LEAGUE','IR']].groupby(['LEAGUE']).mean()\ndf_leagueIR10 = df_leagueIR10.reset_index().rename(columns={'IR': 'IR10'})\ndf_leagueIR10 = df_leagueIR10.merge(ols, how='left', on=['LEAGUE'])\n\n\n# Create and display interactive Plotly version\nfig = create_plotly_scatterplot(\n    data=df_leagueIR10,\n    x_col='IR10',\n    y_col='\u03bb',\n    show_regression=True,\n    text_col='LEAGUE',  # Add league labels to data points\n    xlim=(0, 9),        # Match matplotlib axis limits\n    ylim=(0, 16)\n)\n\n# Display interactive plot in notebook\nfig.show()\n\n# Export both HTML and JSON versions\nexport_plot(fig, \"lambda_vs_information_ratio\", \"Lambda vs Information Ratio\")\n</pre> # calculate mean league IRs group by league-season df_leagueM = df[['LEAGUE','YEAR_ID','PS','PA','wi']].groupby(['LEAGUE','YEAR_ID']).mean() df_leagueS = df[['LEAGUE','YEAR_ID','PS','PA','wi']].groupby(['LEAGUE','YEAR_ID']).std() df_leagueIR = df_leagueM['PS']/df_leagueS['PS'] df_leagueIR = df_leagueIR.reset_index().rename(columns={'PS': 'IR'})  # calc 10 year average for 2010-2019 df_leagueIR10 = df_leagueIR.loc[(df_leagueIR['YEAR_ID']&gt;=st_year_) &amp; (df_leagueIR['YEAR_ID']&lt;=end_year_)] df_leagueIR10 = df_leagueIR10[['LEAGUE','IR']].groupby(['LEAGUE']).mean() df_leagueIR10 = df_leagueIR10.reset_index().rename(columns={'IR': 'IR10'}) df_leagueIR10 = df_leagueIR10.merge(ols, how='left', on=['LEAGUE'])   # Create and display interactive Plotly version fig = create_plotly_scatterplot(     data=df_leagueIR10,     x_col='IR10',     y_col='\u03bb',     show_regression=True,     text_col='LEAGUE',  # Add league labels to data points     xlim=(0, 9),        # Match matplotlib axis limits     ylim=(0, 16) )  # Display interactive plot in notebook fig.show()  # Export both HTML and JSON versions export_plot(fig, \"lambda_vs_information_ratio\", \"Lambda vs Information Ratio\") <pre>Exported: exports/lambda_vs_information_ratio.json\n</pre> In\u00a0[25]: Copied! <pre># Rolling 10-Year Lambda Analysis\n# First, let's check what columns are available in the league data\nprint(\"Checking available columns in league data...\")\nsample_league = league_data.read('MLB').data\nprint(f\"Available columns: {list(sample_league.columns)}\")\nprint(f\"Sample data shape: {sample_league.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(sample_league.head())\n\ndef calculate_rolling_lambdas(league_name, window_years=10, min_observations=50):\n    \"\"\"\n    Calculate rolling lambda estimates for a specific league.\n    \n    Args:\n        league_name: Name of the league\n        window_years: Size of rolling window in years\n        min_observations: Minimum observations required for regression\n    \n    Returns:\n        DataFrame with year, lambda, std_error, adj_r2, nobs\n    \"\"\"\n    # Get league data from ArcticDB\n    league_df = league_data.read(league_name).data\n    \n    # Check if the columns exist, if not use the renamed columns from processed data\n    if 'FREQ_RUNS_S' in league_df.columns:\n        ps_col = 'FREQ_RUNS_S'\n        pa_col = 'FREQ_RUNS_A'\n    else:\n        ps_col = 'PS'\n        pa_col = 'PA'\n    \n    # Filter for team-season data (same processing as before)\n    team_season = league_df.groupby(['YEAR_ID','TEAM_ID']).agg(\n        PS=(ps_col, 'mean'),\n        PA=(pa_col, 'mean'),\n        wi=('wi', 'mean'),\n        nobs=('wi', 'count')\n    ).reset_index()\n    \n    # Filter valid observations\n    team_season = team_season.loc[\n        (team_season['PS'] &gt; 0) &amp; \n        (team_season['PA'] &gt; 0) &amp; \n        (team_season['wi'] &gt; 0) &amp; \n        (team_season['wi'] &lt; 1)\n    ]\n    \n    # Create log variables\n    team_season['lnwi'] = np.log(team_season['wi'] / (1 - team_season['wi']))\n    team_season['lnPSPA'] = np.log(team_season['PS'] / team_season['PA'])\n    \n    # Get year range\n    years = sorted(team_season['YEAR_ID'].unique())\n    start_year = min(years) + window_years - 1  # First complete window\n    end_year = max(years)\n    \n    results = []\n    \n    for year in range(start_year, end_year + 1):\n        # Define window\n        window_start = year - window_years + 1\n        window_data = team_season[\n            (team_season['YEAR_ID'] &gt;= window_start) &amp; \n            (team_season['YEAR_ID'] &lt;= year)\n        ]\n        \n        if len(window_data) &gt;= min_observations:\n            try:\n                # Run OLS regression\n                model = sm.OLS(window_data['lnwi'], \n                              sm.add_constant(window_data['lnPSPA'])).fit()\n                \n                results.append({\n                    'year': year,\n                    'lambda': model.params.iloc[1],\n                    'lambda_se': model.bse.iloc[1],\n                    'adj_r2': model.rsquared_adj,\n                    'nobs': len(window_data)\n                })\n            except:\n                # Skip if regression fails\n                continue\n    \n    return pd.DataFrame(results)\n\n# Calculate rolling lambdas for leagues shown in the reference chart\ntarget_leagues = ['AFL', 'EPL', 'MLB', 'NBA', 'NFL', 'NHL']  # Matching the 2x3 layout\nrolling_results = {}\n\nprint(\"\\nCalculating rolling 10-year lambda estimates...\")\nfor league in target_leagues:\n    print(f\"Processing {league}...\")\n    rolling_results[league] = calculate_rolling_lambdas(league)\n    print(f\"  {league}: {len(rolling_results[league])} data points\")\n\nprint(\"Rolling lambda calculations complete!\")\n</pre> # Rolling 10-Year Lambda Analysis # First, let's check what columns are available in the league data print(\"Checking available columns in league data...\") sample_league = league_data.read('MLB').data print(f\"Available columns: {list(sample_league.columns)}\") print(f\"Sample data shape: {sample_league.shape}\") print(\"\\nFirst few rows:\") print(sample_league.head())  def calculate_rolling_lambdas(league_name, window_years=10, min_observations=50):     \"\"\"     Calculate rolling lambda estimates for a specific league.          Args:         league_name: Name of the league         window_years: Size of rolling window in years         min_observations: Minimum observations required for regression          Returns:         DataFrame with year, lambda, std_error, adj_r2, nobs     \"\"\"     # Get league data from ArcticDB     league_df = league_data.read(league_name).data          # Check if the columns exist, if not use the renamed columns from processed data     if 'FREQ_RUNS_S' in league_df.columns:         ps_col = 'FREQ_RUNS_S'         pa_col = 'FREQ_RUNS_A'     else:         ps_col = 'PS'         pa_col = 'PA'          # Filter for team-season data (same processing as before)     team_season = league_df.groupby(['YEAR_ID','TEAM_ID']).agg(         PS=(ps_col, 'mean'),         PA=(pa_col, 'mean'),         wi=('wi', 'mean'),         nobs=('wi', 'count')     ).reset_index()          # Filter valid observations     team_season = team_season.loc[         (team_season['PS'] &gt; 0) &amp;          (team_season['PA'] &gt; 0) &amp;          (team_season['wi'] &gt; 0) &amp;          (team_season['wi'] &lt; 1)     ]          # Create log variables     team_season['lnwi'] = np.log(team_season['wi'] / (1 - team_season['wi']))     team_season['lnPSPA'] = np.log(team_season['PS'] / team_season['PA'])          # Get year range     years = sorted(team_season['YEAR_ID'].unique())     start_year = min(years) + window_years - 1  # First complete window     end_year = max(years)          results = []          for year in range(start_year, end_year + 1):         # Define window         window_start = year - window_years + 1         window_data = team_season[             (team_season['YEAR_ID'] &gt;= window_start) &amp;              (team_season['YEAR_ID'] &lt;= year)         ]                  if len(window_data) &gt;= min_observations:             try:                 # Run OLS regression                 model = sm.OLS(window_data['lnwi'],                                sm.add_constant(window_data['lnPSPA'])).fit()                                  results.append({                     'year': year,                     'lambda': model.params.iloc[1],                     'lambda_se': model.bse.iloc[1],                     'adj_r2': model.rsquared_adj,                     'nobs': len(window_data)                 })             except:                 # Skip if regression fails                 continue          return pd.DataFrame(results)  # Calculate rolling lambdas for leagues shown in the reference chart target_leagues = ['AFL', 'EPL', 'MLB', 'NBA', 'NFL', 'NHL']  # Matching the 2x3 layout rolling_results = {}  print(\"\\nCalculating rolling 10-year lambda estimates...\") for league in target_leagues:     print(f\"Processing {league}...\")     rolling_results[league] = calculate_rolling_lambdas(league)     print(f\"  {league}: {len(rolling_results[league])} data points\")  print(\"Rolling lambda calculations complete!\") <pre>Checking available columns in league data...\nAvailable columns: ['YEAR_ID', 'TEAM_ID', 'OPP_TEAM_ID', 'PS', 'PA', 'date', 'home', 'wi', 'LEAGUE']\nSample data shape: (436786, 9)\n\nFirst few rows:\n        YEAR_ID TEAM_ID OPP_TEAM_ID  PS  PA        date  home   wi LEAGUE\n138750     1871     ATL         WS3  20  18  1871-05-05     0  1.0    MLB\n138751     1871     ATL         TRO   9   5  1871-05-09     0  1.0    MLB\n138752     1871     ATL         TRO  14  29  1871-05-16     1  0.0    MLB\n138753     1871     ATL         PH1  11   8  1871-05-20     1  1.0    MLB\n138754     1871     ATL         WS3   4   4  1871-05-24     1  0.5    MLB\n\nCalculating rolling 10-year lambda estimates...\nProcessing AFL...\n  AFL: 114 data points\nProcessing EPL...\n  EPL: 122 data points\nProcessing MLB...\n  MLB: 140 data points\nProcessing NBA...\n  NBA: 64 data points\nProcessing NFL...\n  NFL: 89 data points\nProcessing NHL...\n  NHL: 93 data points\nRolling lambda calculations complete!\n</pre> In\u00a0[\u00a0]: Copied! <pre># Define the order to match the reference chart layout\nleague_layout = ['AFL', 'EPL', 'MLB', 'NBA', 'NFL', 'NHL']\n\n# Create interactive Plotly version\nfrom plotly.subplots import make_subplots\n\n# Set up Plotly subplot structure\nfig_plotly = make_subplots(\n    rows=2, cols=3,\n    subplot_titles=league_layout,\n    horizontal_spacing=0.08,\n    vertical_spacing=0.12\n)\n\nfor i, league in enumerate(league_layout):\n    row = (i // 3) + 1\n    col = (i % 3) + 1\n    \n    if league in rolling_results and len(rolling_results[league]) &gt; 0:\n        data = rolling_results[league]\n        \n        # Add confidence band\n        upper_bound = data['lambda'] + 2 * data['lambda_se']\n        lower_bound = data['lambda'] - 2 * data['lambda_se']\n        \n        # Create filled area for confidence band\n        years_extended = list(data['year']) + list(data['year'][::-1])\n        bounds_extended = list(upper_bound) + list(lower_bound[::-1])\n        \n        fig_plotly.add_trace(\n            dict(\n                type='scatter',\n                x=years_extended,\n                y=bounds_extended,\n                fill='toself',\n                fillcolor='rgba(128,128,128,0.3)',\n                line=dict(color='rgba(255,255,255,0)'),\n                name='\u00b12 SE',\n                showlegend=(i == 0),\n                hoverinfo='skip'\n            ),\n            row=row, col=col\n        )\n        \n        # Add lambda line\n        fig_plotly.add_trace(\n            dict(\n                type='scatter',\n                x=data['year'],\n                y=data['lambda'],\n                mode='lines',\n                line=dict(color=colors['primary_color'], width=2),\n                name='\u03bb estimate',\n                showlegend=(i == 0),\n                hovertemplate=f'{league}&lt;br&gt;Year: %{{x}}&lt;br&gt;\u03bb: %{{y:.2f}}&lt;extra&gt;&lt;/extra&gt;'\n            ),\n            row=row, col=col\n        )\n\n# Update layout\nfig_plotly.update_layout(\n    height=700,\n    template='custom_theme',\n    showlegend=True,\n    margin=dict(t=100, b=50)\n)\n\n# Update individual subplot y-axes to match reference chart ranges\ny_ranges = {\n    'AFL': [2.5, 5.0],\n    'EPL': [0.9, 1.3], \n    'MLB': [1.6, 2.1],\n    'NBA': [12, 16],\n    'NFL': [1.0, 3.0],\n    'NHL': [1.5, 2.3]\n}\n\nfor i, league in enumerate(league_layout):\n    row = (i // 3) + 1\n    col = (i % 3) + 1\n    if league in y_ranges:\n        fig_plotly.update_yaxes(range=y_ranges[league], row=row, col=col)\n\nfig_plotly.show()\n\n# Export the plot\nexport_plot(fig_plotly, \"rolling_lambda_time_series\", \"Rolling 10-Year Lambda Estimates\")\n</pre> # Define the order to match the reference chart layout league_layout = ['AFL', 'EPL', 'MLB', 'NBA', 'NFL', 'NHL']  # Create interactive Plotly version from plotly.subplots import make_subplots  # Set up Plotly subplot structure fig_plotly = make_subplots(     rows=2, cols=3,     subplot_titles=league_layout,     horizontal_spacing=0.08,     vertical_spacing=0.12 )  for i, league in enumerate(league_layout):     row = (i // 3) + 1     col = (i % 3) + 1          if league in rolling_results and len(rolling_results[league]) &gt; 0:         data = rolling_results[league]                  # Add confidence band         upper_bound = data['lambda'] + 2 * data['lambda_se']         lower_bound = data['lambda'] - 2 * data['lambda_se']                  # Create filled area for confidence band         years_extended = list(data['year']) + list(data['year'][::-1])         bounds_extended = list(upper_bound) + list(lower_bound[::-1])                  fig_plotly.add_trace(             dict(                 type='scatter',                 x=years_extended,                 y=bounds_extended,                 fill='toself',                 fillcolor='rgba(128,128,128,0.3)',                 line=dict(color='rgba(255,255,255,0)'),                 name='\u00b12 SE',                 showlegend=(i == 0),                 hoverinfo='skip'             ),             row=row, col=col         )                  # Add lambda line         fig_plotly.add_trace(             dict(                 type='scatter',                 x=data['year'],                 y=data['lambda'],                 mode='lines',                 line=dict(color=colors['primary_color'], width=2),                 name='\u03bb estimate',                 showlegend=(i == 0),                 hovertemplate=f'{league}Year: %{{x}}\u03bb: %{{y:.2f}}'             ),             row=row, col=col         )  # Update layout fig_plotly.update_layout(     height=700,     template='custom_theme',     showlegend=True,     margin=dict(t=100, b=50) )  # Update individual subplot y-axes to match reference chart ranges y_ranges = {     'AFL': [2.5, 5.0],     'EPL': [0.9, 1.3],      'MLB': [1.6, 2.1],     'NBA': [12, 16],     'NFL': [1.0, 3.0],     'NHL': [1.5, 2.3] }  for i, league in enumerate(league_layout):     row = (i // 3) + 1     col = (i % 3) + 1     if league in y_ranges:         fig_plotly.update_yaxes(range=y_ranges[league], row=row, col=col)  fig_plotly.show()  # Export the plot export_plot(fig_plotly, \"rolling_lambda_time_series\", \"Rolling 10-Year Lambda Estimates\") <pre>Exported: exports/rolling_lambda_time_series.json\n</pre> <p>Source: Various (listed in Appendix) as at January 2025.</p> <p>Key: Australian Football League (AFL), English Premier League (EPL), Major League Baseball (MLB), National Basketball Association (NBA), National Football League (NFL), National Hockey League (NHL)</p> <p>Note: Shaded regions represent \u00b12 standard errors around the coefficient estimate.</p> <p>The rolling 10-year lambda estimates shown in Figure 4 reveal the temporal evolution of the Pythagorean exponent across different sports leagues. Each panel shows how \u03bb has changed over time, providing insights into the historical development of competitive balance and scoring dynamics within each league. The confidence bands help assess the statistical precision of these estimates over different time periods.</p>"},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#pythagorean-won-loss-formula-notebook","title":"Pythagorean Won Loss Formula Notebook\u00b6","text":"<p>Initial setup</p>"},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#connect-to-arcticdb-and-get-list-of-leagues","title":"Connect to ArcticDB and get list of leagues\u00b6","text":""},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#figure-1-calculate-points-scored-stats","title":"Figure 1 calculate points-scored stats\u00b6","text":"<ul> <li>check positive skew comment in text</li> <li>note need help to plot via python like r panel plot</li> </ul>"},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#save-into-arcticdb","title":"Save into ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#figure-2-nba-and-epl","title":"Figure 2 -- NBA and EPL\u00b6","text":""},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#save-into-arcticdb","title":"Save into ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_pythagorean_won_loss_formula_notebook/#figure-3a-2010-to-2019","title":"Figure 3a -- 2010 to 2019\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/","title":"Staged Data Notebook","text":"<p>Staged data is data written to storage but not yet ready to be read. Use <code>stage</code> to write data in an \"incomplete\" state, then <code>finalize_staged_data</code> to make it readable. This enables:</p> <ul> <li>Parallel writes - multiple workers writing staged data at the same time to the same symbol, with one process able to later finalize all staged data rendering the data readable by clients.</li> </ul> <p><code>stage</code> returns a <code>StageResult</code> that lets you finalize only specific data using <code>finalize_stage_data(stage_results=)</code>. This enables:</p> <ul> <li>Faster finalization.</li> <li>Staging while finalizing - since the user specifies the data to be finalized other staging writers can continue working.</li> <li>Reliable finalize - writers can retry failed stages and then finalizers can process only successful ones.</li> </ul> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport time\nfrom multiprocessing import Process, Queue\n\nfrom arcticdb.exceptions import NoSuchVersionException, UnsortedDataException\nfrom arcticdb import Arctic, LibraryOptions\n\ndef make_df(rows: int, cols: int, start_date):\n    index = pd.date_range(start=pd.to_datetime(start_date), periods=rows, freq=\"D\")\n    data = np.arange(1, rows * cols + 1).reshape(rows, cols, order=\"F\")\n    return pd.DataFrame(data, index=index, columns=[f\"col_{i}\" for i in range(cols)])\n\n# Below library is configured to have only 2 rows per segment. This is to make the example more readable as the default is 100 000 rows per segment.    \ndef get_lib():\n    return Arctic(\"lmdb://tmp/stage_tokens_demo\").get_library(\"stage_tokens_demo\", library_options=LibraryOptions(rows_per_segment=2), create_if_missing=True)\n\ndef clear_lib():\n    get_lib()._nvs.version_store.clear()\n\nsym = \"sym\"\nclear_lib()\nlib = get_lib()\n</pre> import pandas as pd import numpy as np import time from multiprocessing import Process, Queue  from arcticdb.exceptions import NoSuchVersionException, UnsortedDataException from arcticdb import Arctic, LibraryOptions  def make_df(rows: int, cols: int, start_date):     index = pd.date_range(start=pd.to_datetime(start_date), periods=rows, freq=\"D\")     data = np.arange(1, rows * cols + 1).reshape(rows, cols, order=\"F\")     return pd.DataFrame(data, index=index, columns=[f\"col_{i}\" for i in range(cols)])  # Below library is configured to have only 2 rows per segment. This is to make the example more readable as the default is 100 000 rows per segment.     def get_lib():     return Arctic(\"lmdb://tmp/stage_tokens_demo\").get_library(\"stage_tokens_demo\", library_options=LibraryOptions(rows_per_segment=2), create_if_missing=True)  def clear_lib():     get_lib()._nvs.version_store.clear()  sym = \"sym\" clear_lib() lib = get_lib() In\u00a0[2]: Copied! <pre>df1 = make_df(rows=1, cols=3, start_date=\"2025-01-01\")\ndf2 = make_df(rows=1, cols=3, start_date=\"2025-01-02\")\ndf3 = make_df(rows=1, cols=3, start_date=\"2025-01-03\")\n</pre> df1 = make_df(rows=1, cols=3, start_date=\"2025-01-01\") df2 = make_df(rows=1, cols=3, start_date=\"2025-01-02\") df3 = make_df(rows=1, cols=3, start_date=\"2025-01-03\") In\u00a0[3]: Copied! <pre>df1\n</pre> df1 Out[3]: col_0 col_1 col_2 2025-01-01 1 2 3 In\u00a0[4]: Copied! <pre>df2\n</pre> df2 Out[4]: col_0 col_1 col_2 2025-01-02 1 2 3 In\u00a0[5]: Copied! <pre>df3\n</pre> df3 Out[5]: col_0 col_1 col_2 2025-01-03 1 2 3 <p>Staging can be done both sequentially and in parallel.</p> In\u00a0[6]: Copied! <pre># writer1\nlib.stage(sym, df1)\n</pre> # writer1 lib.stage(sym, df1) Out[6]: <pre>&lt;arcticdb_ext.version_store.StageResult at 0x77c441112cb0&gt;</pre> In\u00a0[7]: Copied! <pre># writer2\nlib.stage(sym, df2)\n</pre> # writer2 lib.stage(sym, df2) Out[7]: <pre>&lt;arcticdb_ext.version_store.StageResult at 0x77c440979730&gt;</pre> In\u00a0[8]: Copied! <pre># writer3\nlib.stage(sym, df3)\n</pre> # writer3 lib.stage(sym, df3) Out[8]: <pre>&lt;arcticdb_ext.version_store.StageResult at 0x77c441124c30&gt;</pre> In\u00a0[9]: Copied! <pre>try: \n    lib.read(sym)\nexcept NoSuchVersionException:\n    print(\"No verioned data\")\n</pre> try:      lib.read(sym) except NoSuchVersionException:     print(\"No verioned data\") <pre>No verioned data\n</pre> In\u00a0[10]: Copied! <pre># worker finalizing all staged data - must be called after all staging workers have finished.\nlib.finalize_staged_data(sym)\n</pre> # worker finalizing all staged data - must be called after all staging workers have finished. lib.finalize_staged_data(sym) Out[10]: <pre>VersionedItem(symbol='sym', library='stage_tokens_demo', data=n/a, version=0, metadata=None, host='LMDB(path=/data/team/data/arctic_native/examples/arctcdb_staged_data_with_tokens/tmp/stage_tokens_demo)', timestamp=1759235711434788266)</pre> In\u00a0[11]: Copied! <pre>lib.read(sym).data\n</pre> lib.read(sym).data Out[11]: col_0 col_1 col_2 2025-01-01 1 2 3 2025-01-02 1 2 3 2025-01-03 1 2 3 In\u00a0[12]: Copied! <pre>clear_lib()\n</pre> clear_lib() In\u00a0[13]: Copied! <pre># worker4\nstage_result_df1 = lib.stage(sym, df1)\n</pre> # worker4 stage_result_df1 = lib.stage(sym, df1) In\u00a0[14]: Copied! <pre># worker5\nstage_result_df2 = lib.stage(sym, df2)\n</pre> # worker5 stage_result_df2 = lib.stage(sym, df2) In\u00a0[15]: Copied! <pre># worker6\nstage_result_df3 = lib.stage(sym, df3)\n</pre> # worker6 stage_result_df3 = lib.stage(sym, df3) In\u00a0[16]: Copied! <pre># worker finalizing only some data - only corresponding stagers (worker4 and worker6) are required to have finished before running this finalize. worker5 and any other worker may continue staging while this runs.\nlib.finalize_staged_data(sym, stage_results=[stage_result_df1, stage_result_df3])\n</pre> # worker finalizing only some data - only corresponding stagers (worker4 and worker6) are required to have finished before running this finalize. worker5 and any other worker may continue staging while this runs. lib.finalize_staged_data(sym, stage_results=[stage_result_df1, stage_result_df3]) Out[16]: <pre>VersionedItem(symbol='sym', library='stage_tokens_demo', data=n/a, version=1, metadata=None, host='LMDB(path=/data/team/data/arctic_native/examples/arctcdb_staged_data_with_tokens/tmp/stage_tokens_demo)', timestamp=1759235712142698498)</pre> In\u00a0[17]: Copied! <pre>lib.read(sym).data\n</pre> lib.read(sym).data Out[17]: col_0 col_1 col_2 2025-01-01 1 2 3 2025-01-03 1 2 3 In\u00a0[18]: Copied! <pre>lib.get_staged_symbols()\n</pre> lib.get_staged_symbols() Out[18]: <pre>['sym']</pre> In\u00a0[19]: Copied! <pre>lib.delete_staged_data(sym)\n</pre> lib.delete_staged_data(sym) In\u00a0[20]: Copied! <pre>lib.get_staged_symbols()\n</pre> lib.get_staged_symbols() Out[20]: <pre>[]</pre>"},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#arcticdb-staged-data-demo","title":"ArcticDB Staged Data Demo\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#introduction","title":"Introduction\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#demo-setup","title":"Demo setup\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#api","title":"API\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#finalizing-all-staged-data","title":"Finalizing all staged data\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#finalizing-only-some-data-stage-returns-a-stageresult-which-can-be-used-to-control-which-segments-to-finalize","title":"Finalizing only some data: <code>stage</code> returns a <code>StageResult</code> which can be used to control which segments to finalize\u00b6","text":""},{"location":"notebooks/ArcticDB_staged_data_with_tokens/#df2-is-still-in-the-staged-index-while-df1-and-df3-are-not-clearing-all-the-unfinalized-staged-data-can-be-done-with-delete_staged_datasym","title":"df2 is still in the staged index, while df1 and df3 are not. Clearing all the unfinalized staged data can be done with <code>delete_staged_data(sym)</code>.\u00b6","text":""},{"location":"technical/architecture/","title":"Architecture","text":""},{"location":"technical/architecture/#overview","title":"Overview","text":"<p>ArcticDB is deployed as a shared library, using PyBind for operability between CPython and the core database engine which is written in C++.</p> <p>Users interact with the C++ storage engine, and therefore the storage itself, via the Python/C++ bindings. The engine transforms Python objects which are typically DataFrames, Series or numpy arrays to and from its internal columnar structure. The data is then tiled, indexed, compressed, and written to storage. The storage format is tailor-designed for the storage and retrieval of dense and sparse timeseries data. </p> <p>Please note that there is no required server component.</p> <p></p>"},{"location":"technical/architecture/#arcticdb-dataflow","title":"ArcticDB DataFlow","text":"<p>The below diagram visualises the flow of data through ArcticDB from source to storage and back again:</p> <p></p> <p>For more information on the storage format, please see On-Disk Storage.</p>"},{"location":"technical/contributing/","title":"Contribution Licensing","text":"<p>Since this project is distributed under the terms of the BSL license, contributions that you make are licensed under the same terms. For us to be able to accept your contributions, we will need explicit confirmation from you that you are able and willing to provide them under these terms, and the mechanism we use to do this is the ArcticDB Individual Contributor License Agreement.</p> <p>Individuals - To participate under these terms, please include the following line as the last line of the commit message for each commit in your contribution. You must use your real name (no pseudonyms, and no anonymous contributions).</p> <pre><code>Signed-Off By: Random J. Developer &lt;random@developer.example.org&gt;. By including this sign-off line I agree to the terms of the Contributor License Agreement.\n</code></pre> <p>Corporations - For corporations who wish to make contributions to ArcticDB, please contact info@arcticdb.io and we will arrange for the CLA to be sent to the signing authority within your corporation.</p>"},{"location":"technical/contributing/#docker-quickstart","title":"Docker Quickstart","text":"<p>This quickstart builds a release using build dependencies from vcpkg. ArcticDB releases on PyPi use vcpkg dependencies in the manner as described below.</p> <p>Note the below instructions will build a Linux X86_64 release.</p>"},{"location":"technical/contributing/#1-start-the-arcticdb-build-docker-image","title":"1) Start the ArcticDB build docker image","text":"<p>Run in a Linux terminal:</p> <pre><code>docker pull ghcr.io/man-group/cibuildwheel_manylinux:2.12.1-3a897\ndocker run -it ghcr.io/man-group/cibuildwheel_manylinux:2.12.1-3a897\n</code></pre> <p>:warning: The below instructions do not have to be run in the provided docker image. They can be run against any Python installation on any Linux distribution as long as the basic build dependencies are available.</p> <p>If running outside the provided docker image, please change <code>/opt/python/cp39-cp39/bin/python3</code> in the examples below to an appropriate path for Python.</p>"},{"location":"technical/contributing/#2-check-out-arcticdb-including-submodules","title":"2) Check out ArcticDB including submodules","text":"<pre><code>cd\ngit clone https://github.com/man-group/ArcticDB.git\ncd ArcticDB\ngit submodule init &amp;&amp; git submodule update\n</code></pre>"},{"location":"technical/contributing/#3-kick-off-the-build","title":"3) Kick off the build","text":"<pre><code>MY_PYTHON=/opt/python/cp39-cp39/bin/python3  # change if outside docker container/building against a different python\n$MY_PYTHON -m pip install -U pip setuptools wheel grpcio-tools\nARCTIC_CMAKE_PRESET=skip $MY_PYTHON setup.py develop\n# Change the below Python_EXECUTABLE value to build against a different Python version\ncmake -DPython_EXECUTABLE=$MY_PYTHON -DTEST=off --preset linux-debug cpp\npushd cpp\ncmake --build --preset linux-debug\npopd\n</code></pre>"},{"location":"technical/contributing/#4-run-arcticdb","title":"4) Run ArcticDB","text":"<p>Ensure the below is run in the Git project root:</p> <pre><code># PYTHONPATH first part = Python module, second part compiled C++ binary\nPYTHONPATH=`pwd`/python:`pwd`/cpp/out/linux-debug-build/arcticdb/ $MY_PYTHON\n</code></pre> <p>Now, inside the Python shell:</p> <pre><code>from arcticdb import Arctic\n</code></pre> <p>Rather than setting the <code>PYTHONPATH</code> environment variable, you could install the appropriate paths into your Python environment by running (note that this will invoke the build tooling so will compile any changed files since the last compilation):</p> <pre><code>$MY_PYTHON -m pip install -ve .\n</code></pre> <p>Note that as this will copy the binary to your Python installation this will have to be run after each and every change of a C++ file.</p>"},{"location":"technical/contributing/#building-using-mamba-and-conda-forge","title":"Building using mamba and conda-forge","text":"<p>This section uses build dependencies from conda-forge. It is a pre-requisite for releasing ArcticDB on conda-forge.</p> <p>\u26a0\ufe0f At the time of writing, installing ArcticDB with this setup under Windows is not possible due to a linkage problems with libprotobuf. See: https://github.com/man-group/ArcticDB/pull/449</p> <ul> <li>Install <code>mamba</code></li> <li>Create the <code>arcticdb</code> environment from its specification (<code>environment-dev.yml</code>):</li> </ul> <pre><code>mamba env create -f environment-dev.yml\n</code></pre> <ul> <li>Activate the <code>arcticdb</code> environment (you will need to do this for every new shell session):</li> </ul> <pre><code>mamba activate arcticdb\n</code></pre>"},{"location":"technical/contributing/#building-cmake-targets","title":"Building CMake targets","text":"<p>Several CMake presets are defined for build types, OS's, and build system.</p> <p>For instance:</p> <ul> <li>for debug build on Linux with mamba and conda-forge, use:</li> </ul> <pre><code>export ARCTICDB_USING_CONDA=1\ncmake -DTEST=off --preset linux-conda-debug cpp\ncd cpp\n\n# You might need to use fewer threads than what's possible on your machine\n# to not have it swap and freeze (e.g. we use 1 here).\ncmake --build --preset linux-conda-debug -j 1\n</code></pre> <ul> <li>for release build on MacOS with mamba and conda-forge, use:</li> </ul> <p><pre><code>export ARCTICDB_USING_CONDA=1\ncmake -DTEST=off --preset macos-conda-release cpp\ncd cpp\n\n# You might need to use fewer threads than what's possible your machine\n# not to have it swap and freeze (e.g. we use 1 here).\ncmake --build --preset linux-conda-debug -j 1\n</code></pre> Note: If you don't use presets, you may want to set some useful environment variables:</p> <pre><code># To define the number of threads to use\nexport CMAKE_BUILD_PARALLEL_LEVEL=1\n# To enable C++ tests\nexport ARCTICDB_BUILD_CPP_TESTS=1\n</code></pre>"},{"location":"technical/contributing/#building-and-installing-the-python-package","title":"Building and installing the Python Package","text":"<ul> <li>Build and install ArcticDB in the <code>arcticdb</code> environment using dependencies installed in this environment.    We recommend using the <code>editable</code> installation for development:</li> </ul> <pre><code>export ARCTICDB_USING_CONDA=1\n# Adapt the CMake preset to your setup.\nexport ARCTIC_CMAKE_PRESET=linux-conda-debug\npython -m pip install --no-build-isolation --no-deps --verbose --editable .\n</code></pre> <ul> <li>Use ArcticDB from Python:</li> </ul> <pre><code>from arcticdb import Arctic\n</code></pre>"},{"location":"technical/contributing/#faq","title":"FAQ","text":""},{"location":"technical/contributing/#how-do-i-build-against-different-python-versions","title":"How do I build against different Python versions?","text":"<p>Run <code>cmake</code> (configure, not build) with either:</p> <ol> <li>A different version of Python as the first version of Python on your PATH or...</li> <li>Point the <code>Python_EXECUTABLE</code> CMake variable to a different Python binary</li> </ol> <p>Note that to build the ArcticDB C++ tests you must have the Python static library available in your installation!</p>"},{"location":"technical/contributing/#how-do-i-run-the-python-tests","title":"How do I run the Python tests?","text":"<p>See running tests below.</p>"},{"location":"technical/contributing/#how-do-i-run-the-c-tests","title":"How do I run the C++ tests?","text":"<p>See running tests below.</p>"},{"location":"technical/contributing/#how-do-i-specify-how-many-cores-to-build-using","title":"How do I specify how many cores to build using?","text":"<p>This is determined auto-magically by CMake at build time, but can be manually set by passing in <code>--parallel &lt;num cores&gt;</code> into the build command.</p>"},{"location":"technical/contributing/#detailed-build-information","title":"Detailed Build Information","text":""},{"location":"technical/contributing/#docker-image-construction","title":"Docker Image Construction","text":"<p>The above docker image is built from ManyLinux. Build script is located here.</p> <p>GitHub output here.</p> <p>We recommend you use this image for compilation and testing!</p>"},{"location":"technical/contributing/#setting-up-linux","title":"Setting up Linux","text":"<p>The codebase and build system can work with any reasonably recent Linux distribution with at least GCC 11 and CMake 3.12 (these instructions assume 3.21+).</p> <p>A development install of Python 3.6+ (with <code>libpython.a</code> or <code>.so</code> and full headers) is also necessary. See pybind11 configuration.</p> <p>We require a Mongo executable for a couple of Python tests on Linux. You can check whether you have it with <code>mongod --version</code>.</p> <p>Search the internet for \"mongo installation Linux\" for instructions for your distro if you do not already have <code>mongod</code> available.</p>"},{"location":"technical/contributing/#dependencies-by-distro","title":"Dependencies by distro","text":"Distro Versions reported to work Packages Ubuntu 20.04, 22.04 build-essential g++-11 libpcre3-dev libsasl2-dev libsodium-dev libkrb5-dev libcurl4-openssl-dev python3-dev Centos 7 devtoolset-11-gcc-c++ openssl-devel cyrus-sasl-devel devtoolset-11-libatomic-devel libcurl-devel python3-devel"},{"location":"technical/contributing/#setting-up-windows","title":"Setting up Windows","text":"<p>We recommend using Visual Studio 2022 (or later) to install the compiler (MSVC v142 or newer) and tools (Windows SDK, CMake, Python).</p> <p>The Python that comes with Visual Studio is sufficient for creating release builds, but for debug builds, you will have to separately download from Python.org.</p> <p>After building <code>arcticdb_ext</code>, you need to symlink to the <code>.pyd</code> for the Python tests to run against it:</p> <pre><code># From the root of the ArcticDB git root checkout, in an administrator powershell session\n# Change Python version as appropriate - below is for Python 3.11.\nNew-Item -Path .\\python\\arcticdb_ext.cp311-win_amd64.pyd -ItemType SymbolicLink -Value .\\cpp\\out\\windows-cl-debug-build\\arcticdb\\arcticdb_ext.cp311-win_amd64.pyd\n</code></pre>"},{"location":"technical/contributing/#running-python-tests","title":"Running Python tests","text":"<p>With <code>python</code> pointing to a Python interpeter with <code>ArcticDB</code> installed/on the <code>PYTHON_PATH</code>:</p> <pre><code>python -m pip install arcticdb[Testing]\npython -m pytest python/tests\n</code></pre>"},{"location":"technical/contributing/#running-c-tests","title":"Running C++ tests","text":"<p>Configure ArcticDB with TEST=ON (default is OFF):</p> <pre><code>cmake -DPython_EXECUTABLE=&lt;path to python&gt; -DTEST=ON --preset linux-debug cpp\n</code></pre> <p>Or you can set the following environment variable:</p> <pre><code>export ARCTICDB_BUILD_CPP_TESTS=1\n</code></pre> <p>Note that <code>&lt;path to python&gt;</code> must point to a Python that is compatible with Development.Embed. This will probably be the result of installing <code>python3-devel</code> from your dependency manager.</p> <p>Inside the provided docker image, <code>python3-devel</code> resolves to Python 3.6 installed at <code>/usr/bin/python3</code>, so the resulting command will be:</p> <pre><code>cmake -DPython_EXECUTABLE=/usr/bin/python3 -DTEST=ON --preset linux-debug cpp\n</code></pre> <p>Then build and run the tests:</p> <pre><code>cd cpp/out/linux-debug-build\nmake -j 1 arcticdb_rapidcheck_tests\nmake -j 1 test_unit_arcticdb\nmake test\n</code></pre>"},{"location":"technical/contributing/#cibuildwheel","title":"CIBuildWheel","text":"<p>Our source repo works with CIBuildWheel which runs the compilation and tests against all supported Python versions in isolated environments. Please follow their documentation.</p>"},{"location":"technical/contributing/#configurations","title":"Configurations","text":""},{"location":"technical/contributing/#cmake-presets","title":"CMake presets","text":"<p>To make it easier to set and share all the environment variables, config and commands, we recommend using the CMake presets feature.</p> <p>Recent versions of some popular C++ IDEs support reading/importing these presets: * Visual Studio &amp; Code * CLion</p> <p>And it's equally easy to use on the command line.</p> <p>We already ship a <code>CMakePresets.json</code> in the cpp directory, which is used by our builds. You can add a <code>CMakeUserPresets.json</code> in the same directory for local overrides. Inheritance is supported.</p> <p>If you're working on Linux but not using our Docker image, you may want to create a preset with these <code>cacheVariables</code>: * <code>CMAKE_MAKE_PROGRAM</code> - <code>make</code> or <code>ninja</code> should work * <code>CMAKE_C_COMPILER</code> and <code>CMAKE_CXX_COMPILER</code> - If your preferred compiler is not <code>cc</code> and <code>cxx</code></p> <p>More examples:</p> Windows Preset to specify a Python version <pre><code>{\n  \"version\": 3,\n  \"configurePresets\": [\n    {\n      \"name\": \"alt-vcpkg-debug:py3.10\",\n      \"inherits\": \"windows-cl-debug\",\n      \"cacheVariables\": {\n        \"Python_ROOT_DIR\": \"C:\\\\Program Files\\\\Python310\"\n      },\n      \"environment\": {\n        \"PATH\": \"C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\Scripts;C:\\\\Program Files\\\\Python310;$penv{PATH}\",\n        \"PYTHONPATH\": \"C:\\\\Program Files\\\\Python310\\\\Lib;C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\"\n      }\n    }\n  ],\n  \"buildPresets\": [\n    {\n      \"name\": \"alt-vcpkg-debug:py3.10\",\n      \"configurePreset\": \"alt-vcpkg-debug:py3.10\",\n      \"inheritConfigureEnvironment\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"technical/contributing/#vcpkg-caching","title":"vcpkg caching","text":"<p>We use vcpkg to manage the C++ dependencies.</p> <p>Compiling the dependencies uses a lot of disk space. Once CMake configuration is done, you can remove the <code>cpp\\vcpkg\\buildtrees</code> folder.</p> <p>You may also want to configure some caches: * Binary caching * Asset caching</p>"},{"location":"technical/contributing/#pybind11-configuration","title":"pybind11 configuration","text":"<p>We augmented pybind11's Python discovery with our own PythonUtils to improve diagnostics. Please pay attention to warning messages from <code>PythonUtils.cmake</code> in the CMake output which highlights any configuration issues with Python.</p> <p>We compile against the first <code>python</code> on the <code>PATH</code> by default.</p> <p>To override that, use one of the following CMake variables*:</p> <ul> <li> <p><code>Python_ROOT_DIR</code> - The common path \"prefix\" for a particular Python install.   Usually, the <code>python</code> executable is in the same directory or the <code>bin</code> subdirectory.   This directory should also contain the <code>include</code> and <code>lib(rary)</code> subdirectories.   E.g. <code>/usr</code> for a system-wide install on *nix;   <code>/opt/pythonXX</code> for a locally-managed Python install;   <code>/home/user/my_virtualenv</code> for a virtual env;   <code>C:\\Program Files\\PythonXX</code> for a Windows Python install</p> </li> <li> <p><code>Python_EXECUTABLE</code> - The path to the Python executable. (CMake 3.15+)   CMake will try to extract the <code>include</code> and <code>library</code> paths by running this program.   This differs from the default behaviour of FindPython.</p> </li> </ul> <p>(* Note CMake variables are set with <code>-D</code> on the CMake command line or with the <code>cacheVariables</code> key in <code>CMake*Presets.json</code>.    The names are case-sensitive.</p> <p>(Only) <code>Python_ROOT_DIR</code> can also be set as an environment variable.    Setting the others in the environment might have no effect.)</p>"},{"location":"technical/contributing/#development-guidelines","title":"Development Guidelines","text":"<p>ArcticDB has a lot of write-time options to configure how the data is stored on disk. When adding new features, it is important to test not only the simplest case, but also how the new feature interacts with these various options. This section serves as a guide as to what should be considered when adding a new feature.</p>"},{"location":"technical/contributing/#backwards-compatibility","title":"Backwards compatibility","text":""},{"location":"technical/contributing/#data-stored-on-disk","title":"Data stored on disk","text":"<p>Due to ArcticDB being a purely client-side database, it is possible for data to be written by clients that are of later versions than clients that need to read the same data. If the change is breaking, such that older clients will not be able to read the data, then this should be clearly documented both in the PR and in the online documentation. If possible, a version number should also be added such that future changes in the same area will display a more clear error message.</p>"},{"location":"technical/contributing/#api","title":"API","text":"<p>Any changes to the API (including when exceptions are thrown and the type of the exception) must be weighed up against the change breaking behaviour for existing users. Please make this as clear to reviewers as possible by ensuring API changes are clearly described in the PR description.  If the change is breaking, please also ensure that that is appropriately highlighted in the PR description as well. This is particularly true of the <code>NativeVersionStore</code> API, as this has many users inside Man Group.</p>"},{"location":"technical/contributing/#snapshots","title":"Snapshots","text":"<p>Symbols are almost completely decoupled from one another in ArcticDB. The major feature for which this is not the case is snapshots. Whenever a change involves deleting any keys from the storage, care must be taken that those keys are not in fact still needed by a snapshot. Similarly, any operation that modifies a snapshot, including deleting it, must ensure that any keys for which this snapshot was the last reference are also deleted. See the tutorial and API documentation for more details.</p>"},{"location":"technical/contributing/#batch-methods","title":"Batch methods","text":"<p>Almost all of the major reading and writing methods have batch equivalents. If a feature is added for the non-batch version, it should almost always work with the batch version as well. We should also strive for consistency of behaviour across the batch methods in circumstances that could be encountered by any of them. e.g. <code>read_batch</code> and <code>read_metadata_batch</code> should have the same behaviour if the specified version does not exist.</p>"},{"location":"technical/contributing/#dynamic-schema","title":"Dynamic schema","text":"<p>Static schema is much simpler than dynamic schema, and so features added and only tested with static schema may not \"just work\" with dynamic schema. In particular, the following cases should be considered:</p> <ul> <li>A column that is not present in an initial call to <code>write</code> is present in a subsequent call to <code>append</code>.</li> <li>A column that is present in an initial call to <code>write</code> is not present in a subsequent call to <code>append</code>.</li> <li>A numeric column with a \"small\" type (e.g. <code>uint8</code>) is appended to with a numeric column of a larger type (e.g. int16), and vice versa.</li> </ul>"},{"location":"technical/contributing/#segmentation","title":"Segmentation","text":"<p>Most tests are written with small amounts of data to keep the runtime down as low as possible. However, many bugs are only exposed when data being written is column-sliced and/or row-sliced. Using test fixtures configured to slice into small data segments (e.g. 2x2) can help to catch these issues early.</p>"},{"location":"technical/contributing/#data-sub-selection","title":"Data sub-selection","text":"<p>Many use cases of ArcticDB involve writing huge symbols with hundreds of thousands of columns or billions of rows. In these cases, methods for reading data will almost never be called without some parameters to cut down the amount of data returned to the user. The most commonly used are:</p> <ul> <li>The stand-alone methods <code>head</code> and <code>tail</code>*</li> <li><code>columns</code> - select only a subset of the columns</li> <li><code>date_range</code> and <code>row_range</code> - select a contiguous range of rows</li> <li>More advanced filtering - see section on processing pipeline</li> </ul>"},{"location":"technical/contributing/#processing-pipeline","title":"Processing pipeline","text":"<p>Reading data using only <code>columns</code>, <code>date_range</code>, and <code>row_range</code> arguments is heavily optimised to avoid copying data in memory unnecessarily.  Any read-like method provided with the <code>query_builder</code> optional argument, or equivalently, lazy reads with further processing applied, takes quite a different code path, as we do not know the shape of the output data a priori.  While it is not necessary to exhaustively test all new features against every possible analytics operations, it is generally worth having a test using a simple filter that excludes some of the data that would otherwise be returned to the user.</p>"},{"location":"technical/contributing/#sparse-columns","title":"Sparse columns","text":"<p>The majority of data written into ArcticDB uses dense columns, meaning that within a given row-slice, every row has an associated value in every column (even if that value is <code>NaN</code>). The concept of sparse columns is implemented as well, where columns may have values missing for none, some, or all rows within a row-slice. Any read-like method should be able to handle both types of stored column.</p>"},{"location":"technical/contributing/#pickling","title":"Pickling","text":"<p>When data that cannot be normalized to a supported data type in ArcticDB, it can still be stored using <code>write_pickle</code> and similar. There are many operations that cannot be performed on pickled data, such as <code>append</code>, <code>update</code>, <code>date_range</code> search, and many more. It is important that if a user attempts an operation that is not supported with pickled data, that they receive a helpful error message.</p>"},{"location":"technical/migration/","title":"Migrating from Arctic","text":""},{"location":"technical/migration/#arcticdb-vs-arctic","title":"ArcticDB vs Arctic","text":"<p>ArcticDB is API-compatible with Arctic. Please note however that it is not data compatible. You cannot use ArcticDB to read Arctic data, nor use Arctic to write data that ArcticDB can read.</p>"},{"location":"technical/migration/#how-can-i-migrate-from-arctic-to-arcticdb","title":"How can I migrate from Arctic to ArcticDB","text":"<p>There are two ways that you can migrate data from Arctic to ArcticDB.</p> <ul> <li>Manual migration: The easiest way to migrate relatively small datasets is to read the data out of Arctic, and then write it using ArcticDB. As the ArcticDB is API compatible, this should be relatively simple.</li> <li>Data Conversion: Large data estates can be converted from Arctic to ArcticDB by using automated tooling that we can provide. For more information, please contact our commercial team via our website or email us at info@arcticdb.io. </li> </ul>"},{"location":"technical/on_disk_storage/","title":"ArcticDB On-Disk Storage Format","text":"<p>ArcticDB uses a custom storage format that differs from Parquet, HDF5 or other similar well-known columnar storage formats. This page provides a high-level description of this format.</p> <p>The ArcticDB storage engine is designed to work with any key-value storage backend and currently has full optimised support for the following backends:</p> <ol> <li>S3</li> <li>LMDB</li> </ol> <p>The structure of the stored data is optimised for that storage - for example, when using S3 all related paths (or keys) share a common prefix to optimise for <code>ListObjects</code> calls.</p> <p>Despite this storage specific optimisation, the hierarchy and segment format remain identical regardless of the backend storage. These components are described below.</p>"},{"location":"technical/on_disk_storage/#structural-overview","title":"Structural overview","text":"<p>The above diagram provides an overview of this format, illustrating how ArcticDB manages the symbol metadata, version history and index layout of the data it stores. Not shown is the binary format used to store ArcticDB segments.</p> <p>Note</p> <p>Please note that the key formatting illustrated on the left hand side of the above diagram is specific to S3. The formatting may differ depending on the underlying storage and as such the key paths might not match  when using another storage engine such as LMDB.</p> <p>The ArcticDB storage format is comprised of 4 layers; the Reference Layer, the Version Layer, the Index Layer and finally, the Data Layer. For a full definition of a key, please see the above diagram. </p>"},{"location":"technical/on_disk_storage/#reference-layer","title":"Reference Layer","text":"<p>The reference layer maintains an active pointer to the head of the version layer linked list, which enables fast retrieval of the latest version of a symbol. This pointer is stored in the Data Segment as illustrated in the above diagram. As a result, the reference layer is the only mutable part of ArcticDB's storage format with the value of each reference-layer-key able to be overwritten (hence using a Reference Key, rather than an Atom Key)</p> <p>The reference layer primarily stores keys of type Version Reference, as documented below.</p>"},{"location":"technical/on_disk_storage/#version-layer","title":"Version Layer","text":"<p>The version layer contains a linked list of immutable atom keys/values. Each element of the linked list contains two pointers in the data segment; one points to the next entry in the linked list, and the other points to an index key, providing a route through to the index layer of the storage structure. As a result, traversing the version layer linked list for a symbol allows you to travel backwards through time to retrieve data as it were at a previous version/point in time. The version layer also contains information about which versions have been deleted, and which are still \"live\".</p> <p>This means that for symbols with a lot of versions, this linked list can get quite long, and so reading old versions (using the <code>as_of</code> argument) can become slower as lots of tiny object reads are required in order to find the relevant index key. A method will soon be added to the <code>Library</code> API allowing users to \"compact\" this linked list into a few larger objects.</p> <p>The version layer primarily stores keys of type Version, as documented below.</p>"},{"location":"technical/on_disk_storage/#index-layer","title":"Index Layer","text":"<p>The index layer is an immutable layer that provides a B-Tree index over the data layer. Much like the reference and version layer, this utilises data segments containing data pointers. Each pointer is simply a key that that contains a data segment. </p> <p>For more information on the data stored in this layer, see the Structural Overview diagram.</p> <p>The index layer primarily stores keys of type Table Index, as documented below.</p>"},{"location":"technical/on_disk_storage/#data-layer","title":"Data Layer","text":"<p>The data layer is an immutable layer that contains compressed data segments. Dataframes provided by the user are sliced by both columns and rows, in order to facilitate rapid date-range and column searching during read operations. See the documentation for the <code>rows_per_segment</code> and <code>columns_per_segment</code> library configuration options for more details.</p> <p>The data layer primarily stores keys of type Table Data, as documented below.</p>"},{"location":"technical/on_disk_storage/#arcticdb-key-types","title":"ArcticDB Key Types","text":""},{"location":"technical/on_disk_storage/#what-is-an-arcticdb-key","title":"What is an ArcticDB key?","text":"<p>ArcticDB stores data in either LMDB or object storage (S3). Regardless of the backend though, ArcticDB stores data in well-defined key types where a key type defines the purpose of the associated value and the structure of the name/path of the key. Examples of how the paths differ for S3 are shown below.</p> <p>Note that where this documentation refers to a Key Type key (for example a version reference key), it refers to both the S3/LMDB key and the associated S3/LMDB value.</p>"},{"location":"technical/on_disk_storage/#key-types","title":"Key Types","text":"Key Type Atom/Reference S3 Prefix Purpose Version Reference Reference vref Maintains the top level pointer to the latest version of a symbol Version Atom ver Maintains pointers to index keys for one or more versions of a symbol Table Index Atom tindex Maintains an index structure over the data Table Data Atom tdata Maintains the data for a table Symbol List Atom sl Caches symbol addition/removals <p>Note that Atom keys are immutable. Reference keys are not immutable and therefore the associated value can be updated. </p> <p>Version Ref</p> <p>In this documentation, <code>Version Reference key</code> is sometimes shortened to <code>Version Ref key</code>. </p>"},{"location":"technical/on_disk_storage/#example-paths","title":"Example paths","text":"<p>The paths below are examples of S3-based paths. LMDB ArcticDB libraries may have slightly different paths. </p> Key Type Example Path Version Ref {bucket}/{library prefix}/{library storage id}/vref/sUtMYSYMBOL Symbol List {bucket}/{library prefix}/{library storage id}/*sSt*__add__*6*1632469542244313925*8986477046003934300*ORJDSG8GU4_6*ORJDSG8GU4_6"},{"location":"technical/on_disk_storage/#additional-information","title":"Additional Information","text":""},{"location":"technical/on_disk_storage/#data-layer-fragmentation","title":"Data Layer Fragmentation","text":"<p>As the example given in the Structural Overview diagram demonstrates, Atom Keys in the data layer from older versions are re-used (i.e. pointed to) by the new version's index layer if the data in them is still needed. This is always trivially true for <code>append</code> operations, and this is conceptually simpler, so the example we will work with here is based on <code>append</code>. It is worth noting that all of the same arguments also apply to <code>update</code> as well though.</p> <p>By re-using data layer keys from the previous version, calls to <code>append</code> are as efficient as possible, as they do not need to read any of the data layer keys from the previous version. However, this can result in the data layer becoming fragmented, in the sense that data in the data layer is spread over a lot of small objects in storage, and this can have a negative performance impact on <code>read</code> operations.</p> <p>Consider a use case where a symbol with 10 columns, all of 8 byte numeric types, is appended to once per minute. This means that each day, 1,440 data layer segments are written, each with just 80 bytes of information in. Calling <code>read</code> for a days worth of data therefore requires 1,440 reads of these tiny data layer objects, which will be much less efficient than reading a single object of 115,200 bytes.</p> <p>We will soon be adding an API to perform exactly this operation, re-slicing data such that subsequent reads can be as efficient as possible, without harming the efficiency of existing <code>append</code> or <code>update</code> operations.</p>"},{"location":"technical/on_disk_storage/#symbol-list-caching","title":"Symbol List Caching","text":"<p>Speeding up listing symbols</p> <p>The below documentation details the architecture for the symbol list cache.</p> <p>It explains that to speed up <code>list_symbols</code>, simply run <code>list_symbols</code> through to completion frequently.  The cache is built on first run and compacted afterwards.  This will speed up <code>list_symbols</code> for all accessors of the library - not just the user that runs <code>list_symbols</code>. </p> <p><code>list_symbols</code> is a common operation to perform on an ArcticDB library. As this returns a list of \"live\" symbols (those for which at least one version has not been deleted), using the data structures described above, this involves:</p> <ul> <li>Loading a list of version reference keys in the library.</li> <li>For each version reference key:<ul> <li>Traverse the version key linked-list until either a live version is found, or it is established that all versions have been deleted.</li> </ul> </li> </ul> <p>For many libraries, these operations will be quick enough. However, if there are millions of symbols, or if the first live version to be found for symbols is very far back in the version key linked-list, then this method can be prohibitively expensive.</p> <p>Therefore, ArcticDB maintains a cache of live symbols, such that the <code>list_symbols</code> call should always return quickly. This works by writing special atom keys into the storage that track when symbols are created or deleted, and the time that the operation happened. These Symbol List atom keys are of the form:</p> <ul> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*&lt;timestamp 1&gt;*&lt;content hash&gt;*&lt;symbol name&gt;*&lt;symbol name&gt;</code> - signifies that <code>&lt;symbol name&gt;</code> was created at <code>&lt;timestamp 1&gt;</code></li> <li><code>&lt;library prefix&gt;/sl/*sSt*__delete__*0*&lt;timestamp 2&gt;*&lt;content hash&gt;*&lt;symbol name&gt;*&lt;symbol name&gt;</code> - signifies that <code>&lt;symbol name&gt;</code> was deleted at <code>&lt;timestamp 2&gt;</code></li> </ul> <p>The operation to list symbols then involves:</p> <ul> <li>Read all of the symbol list atom keys for the library (this resolves to a <code>ListObjects</code> call with S3 storage).</li> <li>Separate the keys by the symbol they refer to being created/deleted.</li> <li>For each symbol:<ul> <li>Check if the most recent operation was a creation or a deletion.</li> </ul> </li> </ul> <p>Symbols for which the latest operation is a creation are then returned to the caller of <code>list_symbols</code>.</p> <p>Without any housekeeping, this process could lead to unbounded growth in the number of symbol list atom keys in the library. Even worse, many of these keys would contain redundant information, as we only care about the latest operation for each symbol. Therefore, whenever <code>list_symbols</code> is called by a client with write permissions on the library, if there are too many (500 by default, see the Runtime Configuration page for details on how to configure) symbol list atom keys, all of the information from these keys is compacted into a single symbol list atom key, and the old keys are deleted. For example, if there were 4 symbol list atom keys:</p> <ul> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t0*&lt;content hash&gt;*symbol1*symbol1</code> Create \"symbol1\" at t0</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__delete__*0*t1*&lt;content hash&gt;*symbol1*symbol1</code> Delete \"symbol1\" at t1</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t2*&lt;content hash&gt;*symbol12*symbol2</code> Create \"symbol2\" at t2</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t3*&lt;content hash&gt;*symbol1*symbol1</code> Create \"symbol1\" at t3</li> </ul> <p>They would be compacted into a single object stating that \"symbol1\" and \"symbol2\" were both alive at time t3. The key for this object is of the form <code>&lt;library prefix&gt;/sl/*sSt*__symbols__*0*t3*&lt;content hash&gt;*0*0</code>.</p> <p>Astute observers will correctly take issue with basing logical decisions on timestamps in a purely client-side database with no synchronisation between clocks of different clients enforced. As such, this cache can diverge from reality, if two different clients create and delete the same symbol at about the same time, and this is the most likely cause of odd behaviour such as <code>lib.read(symbol)</code> working, but <code>symbol in lib.list_symbols()</code> returning <code>False</code>. If this happens, <code>lib.reload_symbol_list()</code> should resolve the issue.</p>"},{"location":"technical/upgrade_storage/","title":"Upgrade storage config","text":"<p>You only need to follow this guide when pointed to it by an error message when accessing your library or when asked to during an upgrade.</p> <p>This indicates that the stored config for your library is unsupported by this version of ArcticDB.</p> <p>The rest of this guide explains how to update the stored config across all libraries in an Arctic instance.</p> <p>Since this requires write access on the storage, this should be performed by a suitably permissioned user.</p>"},{"location":"technical/upgrade_storage/#upgrade-script","title":"Upgrade script","text":""},{"location":"technical/upgrade_storage/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>Ensure that all users are on at least version 3.0.0 of ArcticDB</li> <li>Install latest ArcticDB <code>pip install -U arcticdb</code> or <code>conda install -c conda-forge arcticdb</code></li> <li>You must have write credentials on the storage you are using as your ArcticDB backend (eg S3 bucket / Azure blob storage)</li> <li>Create a <code>uri</code> suitable to use with an <code>Arctic</code> instance for your backend, with write credentials. See docs</li> <li>Run <code>arcticdb_update_storage --uri \"&lt;uri&gt;\"</code> where <code>&lt;uri&gt;</code> is that created in the step above. This will not modify anything, but will log the affected libraries with \"Config NOT OK for \""},{"location":"technical/upgrade_storage/#run-script","title":"Run Script","text":"<p>If no libraries were shown as affected after following the steps above, you can stop now. You do not need to do any more.</p> <p>Warning</p> <p>Running this script will break access for clients on less than version 3.0.0 of ArcticDB for affected libraries. The affected libraries were shown in the step above. Ensure users have upgraded to at least version <code>arcticdb==3.0.0</code> first.</p> <ul> <li>Run <code>arcticdb_update_storage --uri \"&lt;uri&gt;\" --run</code> where <code>&lt;uri&gt;</code> is the same as the one above.</li> </ul>"},{"location":"technical/upgrade_storage/#release-history","title":"Release History","text":"ArcticDB Version Upgrade Github Issue 3.0.0 Removes credentials from stored config. #802"},{"location":"tutorials/data_organisation/","title":"Guidelines for Organising Your Data","text":""},{"location":"tutorials/data_organisation/#introduction","title":"Introduction","text":"<p>We are often asked about how best to organise data in ArcticDB. There is no single answer to this question - it will depend on many factors, but especially on the data itself and how it will be used.</p> <p>ArcticDB offers a high degree of flexibility in how data is arranged. Although this is a strength of the system, sometimes the choices are so broad that it is hard to know where to start.</p> <p>In this guide we aim to outline some design principles and rules of thumb to help you decide the best scheme for your data, system and users.</p> <p>We begin by taking a design view of the way ArcticDB structures data and the system design concerns, then from those consider how to assess and balance the trade-offs of some typical organisation strategies.</p>"},{"location":"tutorials/data_organisation/#arcticdbs-data-hierarchy","title":"ArcticDB's Data Hierarchy","text":"<p>Let's revisit the structures that ArcticDB provides for organising data, illustrated below</p> <p></p>"},{"location":"tutorials/data_organisation/#object-store","title":"Object Store","text":"<p>The object stores available will typically be decided by external factors such as</p> <ul> <li>Environment: prod/uat/research</li> <li>Permissions: enviroments that grant/deny read/write permission to different groups of users/systems</li> <li>Accounting: different cloud buckets may be charged to different internal accounts</li> <li>Storage Quotas: different amounts of storage may be allocated for different purposes</li> <li>Storage Performance: faster/slower storage for different applications according to requirements and cost</li> </ul>"},{"location":"tutorials/data_organisation/#library","title":"Library","text":"<p>ArcticDB can handle a large number of libraries per object store - up to 50,000 is perfectly reasonable.</p> <p>A library groups together a set of related data. Typically its primary purpose is to help catalog the data - well named/documented libraries help people find the data they need.</p>"},{"location":"tutorials/data_organisation/#symbol","title":"Symbol","text":"<p>The symbol is the base unit of storage in ArcticDB. Each symbol is a DataFrame and has optimised data access on the columns and index of the DataFrame.</p> <p>So the choice of how to use the (symbol, columns, index) is important to how the system will perform. It is also the full key to access a specific single data item from a DataFrame (including the version, when needed).</p>"},{"location":"tutorials/data_organisation/#versions","title":"Versions","text":"<p>Each symbol gets a new version every time it is modified. A consideration of how versions will be treated is a key design choice - in particular it will have a big impact how much storage is used. The most common choices are:</p> <ul> <li>Keep only the latest versions. Delete older versions either with <code>prune_previous_version</code> or in a background process (see Enterprise Features).</li> <li>Keep key versions only. Often implemented using snapshots.</li> <li>Keep all versions. This can use large amounts of storage if the data modifications are frequent but is very powerful for fully archiving the data.</li> </ul>"},{"location":"tutorials/data_organisation/#system-design-concerns","title":"System Design Concerns","text":"<p>The following topics are necessary concerns when running a system. To that extent the data organisation plan must bear them in mind.</p>"},{"location":"tutorials/data_organisation/#initial-data-population","title":"Initial Data Population","text":"<p>This is usually a one-off process to get a system up and running. Ideally the same scripts can be used for initial data population and regular updates, with different parameters.</p>"},{"location":"tutorials/data_organisation/#data-update-process","title":"Data Update Process","text":"<p>Regular updates to the data are the norm for most systems. We must consider in our design</p> <ul> <li>Delivery format (the format that the data source uses) vs storage format of the data. The trade-offs are<ul> <li>Storing data in a format close to delivery format makes it easier to reconcile with the source</li> <li>The delivery format is often not the best format for downstream processes</li> </ul> </li> <li>Frequency of data updates</li> <li>Storage growth characteristics over time</li> <li>System performance over time</li> </ul>"},{"location":"tutorials/data_organisation/#efficient-downstream-access","title":"Efficient Downstream Access","text":"<p>Often the data is accessed by downstream users and systems much more frequently than it is updated. If that is the case, it is worth organising data to optimise downstream access performance.</p>"},{"location":"tutorials/data_organisation/#data-problems-investigation-and-remediation","title":"Data Problems: Investigation and Remediation","text":"<p>System problems are often caused by suspected bad data. It is worth thinking about how to investigate and remediate such problems. For example</p> <ul> <li>Tooling to find and examine suspected bad data</li> <li>Consider how to reconcile suspected bad data vs the original data source</li> <li>Manual overrides to fix bad data to allow the system to run effectively while data is re-sourced</li> <li>Versioning and snaphsots makes it easy to return to a last known good state</li> </ul>"},{"location":"tutorials/data_organisation/#maintenance","title":"Maintenance","text":"<p>Some routine maintenance is required to keep systems running efficiently and deal with problems, some of which may have causes from outside the system.</p> <p>For ArcticDB specifically, the following issues are worth consideration:</p> <ul> <li>Version management (see versions)</li> <li>Defragmentation</li> <li>Replication / Backup of data</li> </ul> <p>There are Enterprise Features to help with these issues and more.</p>"},{"location":"tutorials/data_organisation/#general-performance-guidelines-for-arcticdb","title":"General Performance Guidelines for ArcticDB","text":"<p>In this section we want to highlight some general rules of thumb that will help you get the best performance from ArcticDB.</p>"},{"location":"tutorials/data_organisation/#properties-of-arcticdb","title":"Properties of ArcticDB","text":"<p>It is worth understanding and bearing in mind these properties of the system</p>"},{"location":"tutorials/data_organisation/#indices","title":"Indices","text":"<ul> <li>The primary and performant index is the first DataFrame row index.</li> <li>The columns names are also a performant index.</li> <li>There are no secondary row or column indices.</li> <li>The symbol-list is a good index when compacted.</li> <li>The version-list is a good index when compacted.</li> </ul>"},{"location":"tutorials/data_organisation/#data","title":"Data","text":"<ul> <li>DataFrame data is always stored columnar and compressed. ArcticDB is optimised to read and write these.</li> <li>DataFrame data is tiled and stored in objects (default is 100,000 rows x 127 cols)</li> <li>Data in different DataFrames is stored in different objects</li> </ul>"},{"location":"tutorials/data_organisation/#arcticdb-is-optimised-for-large-dataframes","title":"ArcticDB is Optimised for Large DataFrames","text":"<ul> <li>Prefer a smaller number of symbols that have a large amount of data each</li> <li>Accessing date ranges and subsets of columns is very efficient</li> <li>Collecting together data from a large number of symbols can be slower</li> </ul>"},{"location":"tutorials/data_organisation/#plan-your-version-management","title":"Plan your Version Management","text":"<ul> <li>In particular plan which versions to keep and how to delete the others</li> <li>Snapshots are useful for version management</li> <li>There are interactions between snapshots and versions: snapshots keep versions from being deleted</li> </ul> <p>Please see the snapshots documentation and snapshots notebook for more details.</p>"},{"location":"tutorials/data_organisation/#lots-of-small-appends-or-updates-can-fragment-your-data","title":"Lots of Small Appends or Updates Can Fragment Your Data","text":"<p>Append and Update are efficient because they always add new chunks of data rather than reorganising the existing data (actually update will reorganise but only where necessary which is typically only a small amount).</p> <p>This means that lots of small appends or updates can result in lots of small data chunks, which makes reads slower.</p> <p>You can defragment a symbol manually using the defragment_symbol_data library function. Altnernatively the Enterprise Features offer background processes that will take care of defragmentation for you.</p>"},{"location":"tutorials/data_organisation/#examples-based-on-market-data","title":"Examples Based on Market Data","text":""},{"location":"tutorials/data_organisation/#seperate-symbol-for-each-security","title":"Seperate Symbol for Each Security","text":"<p>The data for a single security would be a timeseries of market data. In this set of sample data this would be all the price data for AAPL.</p> <p></p> <p>Every security has its own symbol with the same data shape.</p>"},{"location":"tutorials/data_organisation/#pros","title":"Pros","text":"<ul> <li>Ideal for single security analysis</li> <li>The delivery format for the data is often per security</li> <li>The update process is simple</li> </ul>"},{"location":"tutorials/data_organisation/#cons","title":"Cons","text":"<ul> <li>Analysis involving many securities requires reading many symbols</li> <li>Time index for raw data may not match between securities.</li> </ul> <p>The data is easier to use with time index alignment.</p>"},{"location":"tutorials/data_organisation/#single-symbol-for-all-securities","title":"Single Symbol for all Securities","text":"<p>The data for all securities is merged together into a single symbol.</p> <p></p> <p>The security identifier is included as an extra index column. Note that DataFrames with a Pandas MultiIndex will round trip correctly, which is useful. However the high performance ArcticDB indexing is only on the primary index level.</p>"},{"location":"tutorials/data_organisation/#pros_1","title":"Pros","text":"<ul> <li>Good for analysis involving many securities at once eg. portfolio analysis</li> <li>A single large DataFrame tends to give very good read performance</li> </ul>"},{"location":"tutorials/data_organisation/#cons_1","title":"Cons","text":"<ul> <li>The update process is more complicated. Probably needs a read, modify, write sequence of operations (although see Future Features)</li> <li>Time index for raw data may not match between securities.</li> </ul> <p>The data is easier to use with time index alignment.</p>"},{"location":"tutorials/data_organisation/#single-symbol-for-each-data-item","title":"Single Symbol for each Data Item","text":"<p>A symbol holds the timeseries for all securities for a single field. In the example below are the close prices over time for all securities.</p> <p></p>"},{"location":"tutorials/data_organisation/#pros_2","title":"Pros","text":"<ul> <li>Good for analysis involving many securities at once eg. portfolio analysis</li> <li>A single large DataFrame tends to give very good read performance</li> <li>Update process is reasonably simple.</li> </ul>"},{"location":"tutorials/data_organisation/#cons_2","title":"Cons","text":"<ul> <li>Time index for raw data may not match between securities. </li> </ul> <p>The data is easier to use with time index alignment.</p>"},{"location":"tutorials/data_organisation/#time-index-alignment","title":"Time Index Alignment","text":"<p>Raw data may not be time-aligned</p> <ul> <li>The underlying tick data arrives at arbitrary times</li> <li>Different products have different trading hours</li> <li>Some securities may trade less frequently</li> </ul> <p>For many purposes time-aligned data is either necessary or makes the task much simpler.</p> <p>Resampling is one way to align data. There is a Pandas resample function and a faster function planned in ArcticDB (see Future Features).</p>"},{"location":"tutorials/data_organisation/#enterprise-features","title":"Enterprise Features","text":"<p>Many of the housekeeping and maintenance procedures recommended in this guide are taken care of by processes available in the ArcticDB Enterprise package. Please contact us if you would like to explore further. Click the Get ArcticDB button on our website to contact us.</p>"},{"location":"tutorials/data_organisation/#planned-future-features-and-improvements","title":"Planned Future Features and Improvements","text":"<p>We are always improving and adding new features to ArcticDB. Below are a few relevant new features that are in our plans</p> <ul> <li>Multi-Index: support for more flexible data updates for multi-index symbols.</li> <li>Arrow: support for Arrow-backed Pandas and Polars DataFrames.</li> </ul> <p>If you have a suggestion for a new feature, please raise an issue on our github. Please include as much detail as possible.</p>"},{"location":"tutorials/fundamentals/","title":"ArcticDB Fundamentals","text":"<p>This tutorial will walk through the fundamentals of ArcticDB:</p> <ol> <li>Accessing libraries</li> <li>Writing data</li> <li>Reading data</li> <li>Modifying data</li> </ol> <p>To start, let's import <code>arcticdb</code>:</p> <pre><code>import arcticdb as adb\n</code></pre>"},{"location":"tutorials/fundamentals/#accessing-libraries","title":"Accessing libraries","text":"<p>Connect to your storage:</p> <pre><code># Connect using defined keys\nac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arctic-test-aws?access=&lt;access key&gt;&amp;secret=&lt;secret key&gt;')\n# Leave AWS SDK to work out auth details \nac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arctic-test-aws?aws_auth=true)\n</code></pre> <p>For more information on how the AWS SDK configures authentication without utilising defined keys, please see the AWS documentation.</p> <p>Access a library using either the <code>[library_name]</code> notation or the <code>get_library</code> method:</p> <pre><code>lib = ac['library']\n# ...equivalent to...\nlib = ac.get_library['library']\n</code></pre> <p>Let's see what data is already present:</p> <pre><code>&gt;&gt;&gt; lib.list_symbols()\n['sym_2', 'sym_1', 'symbol']\n</code></pre>"},{"location":"tutorials/fundamentals/#arcticdb-api","title":"ArcticDB API","text":"<p>ArcticDB's API is built around four main primitives that each operate over a single symbol.</p> <ol> <li>write: Creates a new version consisting solely of the item passed in.</li> <li>append: Creates a new version consisting of the item appended to the previously-written data.</li> <li>update: Creates a new version consisting the previous data patched with the provided item.</li> <li>read: Retrieves the given version (if no version is provided the latest version is used).</li> </ol> <p>These primitives aren't exhaustive, but cover most use cases. We'll show the usage of these primitives.</p>"},{"location":"tutorials/fundamentals/#writing-data","title":"Writing data","text":"<p>Let's start by generating some data. The below snippet generates some random data with a datetime index:</p> <pre><code>import numpy as np\nimport pandas as pd\nNUM_COLUMNS=10\nNUM_ROWS=100_000\ndf = pd.DataFrame(np.random.randint(0,100,size=(NUM_ROWS, NUM_COLUMNS)), columns=[f\"COL_{i}\" for i in range(NUM_COLUMNS)], index=pd.date_range('2000', periods=NUM_ROWS, freq='h'))\n</code></pre> <p>Let's take this data and write it to ArcticDB:</p> <pre><code>&gt;&gt;&gt; lib.write(\"my_data\", df)\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'NoneType'&gt;,version=0,metadata=None,host=local)\n</code></pre>"},{"location":"tutorials/fundamentals/#reading-data","title":"Reading data","text":"<p>To read the data, simply use the <code>read</code> primitive:</p> <pre><code>&gt;&gt;&gt; data = lib.read(\"my_data\")\n&gt;&gt;&gt; data\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'pandas.core.frame.DataFrame'&gt;,version=0,metadata=None,host=local)\n</code></pre> <p>Note that you get back a <code>VersionedItem</code> - it allows us to retrieve the version of the written data:</p> <pre><code>&gt;&gt;&gt; data.version\n0\n</code></pre> <p>As this is the first write to this symbol, the version is <code>0</code>. To retrieve the data:</p> <pre><code>&gt;&gt;&gt; data.data\n</code></pre>"},{"location":"tutorials/fundamentals/#slicing-and-filtering","title":"Slicing and filtering","text":"<p>See the getting started guide for more information.</p>"},{"location":"tutorials/fundamentals/#modifying-data","title":"Modifying data","text":"<p>Let's append some data. First, note that the data we've written ends in 2011:</p> <pre><code>&gt;&gt;&gt; data.data.tail()\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  COL_8  COL_9\n2011-05-29 11:00:00     44     94     70     32     91      4     35     19     74     53\n2011-05-29 12:00:00     79     51     67     48      8     83     46     54     86     38\n2011-05-29 13:00:00     60     98     74      4     81     86     64     78     13     32\n2011-05-29 14:00:00     27     24     16      6     84     99     11     94     29      4\n2011-05-29 15:00:00     81     76     52     93     31     91     64      2     26     78\n</code></pre> <p>That's simply because the data we generated started on January 1st, 2000 at 00:00 at consists of 100,000 rows, incrementing one hour at a time. When <code>append</code>-ing data, the data you are appending must begin  after the existing data ends. As a result, let's generate some data that begins in 2012:</p> <pre><code>df_to_append = pd.DataFrame(np.random.randint(0,100,size=(NUM_ROWS, NUM_COLUMNS)), columns=[f\"COL_{i}\" for i in range(NUM_COLUMNS)], index=pd.date_range('2012', periods=NUM_ROWS, freq='h'))\n</code></pre> <p>Now let's append!</p> <pre><code>&gt;&gt;&gt; lib.append(\"my_data\", df_to_append)\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'NoneType'&gt;,version=1,metadata=None,host=local)\n</code></pre> <p><code>append</code> has created a new version of the data. When reading version 0, the data will end in 2011. When reading version 1, the data will end in 2023.</p>"},{"location":"tutorials/fundamentals/#update","title":"Update","text":"<p>If append can only append date that begins after the existing data ends, then it begs the question - how do we mutate data?</p> <p>The answer is that we use the <code>update</code> primitive. <code>update</code> overwrites (creating a new version - nothing is lost!) existing symbol data with the data that is passed in.  Note that the entire range between the first and last index entry in the existing data is replaced in its entirety with the data that is passed in, adding additional index entries if required. This means <code>update</code> is a contiguous operation - see the documentation of <code>update</code> for more information.</p>"},{"location":"tutorials/fundamentals/#time-travel","title":"Time travel!","text":"<p>ArcticDB is bitemporal - all new versions are timestamped! Let's pull in the first version of the data, prior to the <code>append</code>:</p> <pre><code>&gt;&gt;&gt; lib.read(\"my_date\", as_of=0).data.tail()\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  COL_8  COL_9\n2011-05-29 11:00:00     44     94     70     32     91      4     35     19     74     53\n2011-05-29 12:00:00     79     51     67     48      8     83     46     54     86     38\n2011-05-29 13:00:00     60     98     74      4     81     86     64     78     13     32\n2011-05-29 14:00:00     27     24     16      6     84     99     11     94     29      4\n2011-05-29 15:00:00     81     76     52     93     31     91     64      2     26     78\n</code></pre> <p>Note that it ends in 2011 - it's like the <code>append</code> never happened. <code>as_of</code> can take a timestamp (<code>datatime.datetime</code> or <code>pd.Timestamp</code>) as well.</p>"},{"location":"tutorials/library_sizes/","title":"Library Sizes","text":"<p>ArcticDB includes some tools to analyze the amount of storage used by your libraries.</p> <p>The API documentation for these features is here.</p> <p>We break this down by internal key types, which are documented in <code>arcticdb.KeyType</code>. To get the total space used by your library, just sum across the key types.</p> <p>To use these tools, you can run code like,</p> <pre><code>from arcticdb import Arctic, KeyType\nfrom arcticdb.version_store.admin_tools import sum_sizes\n\nlib = Arctic(\"&lt;your URI&gt;\").get_library(\"&lt;your library&gt;\")\nadmin_tools = lib.admin_tools()\n\nsizes = admin_tools.get_sizes()  # scan all the sizes in the library, can be slow\nsum_sizes(sizes.values())  # total size of the library\nsizes[KeyType.TABLE_DATA].bytes_compressed  # how much storage is consumed by data segments?\nsizes[KeyType.TABLE_DATA].count  # how many data segments are in your library?\n\nby_symbol = admin_tools.get_sizes_by_symbol()  # scan all the sizes in the library, grouped by symbol\nsize_for_sym = by_symbol[\"sym\"]\nsum_sizes(size_for_sym.values())  # total size of the symbol\nsize_for_sym[KeyType.TABLE_INDEX].bytes_compressed  # how much storage is consumed by index structures?\nsize_for_sym[KeyType.TABLE_INDEX].count  # how many indexes does this symbol have?\n\nfor_symbol = admin_tools.get_sizes_for_symbol(\"&lt;your symbol&gt;\")  # scan sizes for one particular symbol, faster than the APIs above\nsum_sizes(for_symbol.values())  # total size of the symbol\nfor_symbol[KeyType.VERSION].bytes_compressed  # how much storage is consumed by our versioning metadata layer?\nfor_symbol[KeyType.VERSION].count  # how many version keys are in the library?\n</code></pre> <p>Most of the space used by a library should be in its <code>TABLE_DATA</code> keys since this is where your data is actually kept. The other key types are metadata tracked by ArcticDB, primarily to index and version your data. More information about our data layout is available here.</p>"},{"location":"tutorials/lmdb_and_in_memory/","title":"In-Memory Storage Backends","text":"<p>ArcticDB can use a file-based LMDB backend, or a RAM-based in-memory storage, as alternatives to object storage such as S3 and Azure.</p> <p>For temporary in-memory solutions, LMDB can be set up to write to tmpfs. As this guide will explore, with this solution multiple writers can access the database concurrently, and additionally benefit from increased performance when compared to LMDB writing to a permanent on-disk filesystem.</p> <p>On Linux, the steps to set up a tmpfs filesystem are:</p> <pre><code>$ mkdir ./tmpfs_mount_point\n$ sudo mount -t tmpfs -o size=1g tmpfs tmpfs_mount_point\n</code></pre> <p>And we can inspect that it is there with:</p> <pre><code>$ df -h\nFilesystem               Size  Used Avail Use% Mounted on\ntmpfs                    1.0G     0  1.0G   0% /somedir/tmpfs_mount_point\n</code></pre> <p>From ArcticDB you can connect to this filesystem with LMDB as you would usually:</p> <pre><code>import arcticdb as adb\nimport pandas as pd\n\nac_lmdb = adb.Arctic('lmdb:///somedir/tmpfs_mount_point')\nlib = ac_lmdb.get_library('lib', create_if_missing=True)\nlib.write('symbol', pd.DataFrame({'a': [1, 2, 3]})\nprint(lib.read('symbol').data)\n</code></pre> <p>which gives:</p> <pre><code>   a\n0  1\n1  2\n2  3\n</code></pre> <p>The equivalent instantiation for an in-memory store uses the URI <code>'mem://'</code> as in:</p> <pre><code>ac_mem = adb.Arctic('mem://')\n</code></pre> <p>The <code>ac_mem</code> instance owns the storage, which lives only for the lifetime of the <code>ac_mem</code> Python object. Behind the scenes, a Folly Concurrent Hash Map is used as the key/value store. For test cases and experimentation, the in-memory backend is a good option.</p> <p>If multiple processes want concurrent access to the same data, then we recommend using LMDB over a tmpfs.</p>"},{"location":"tutorials/lmdb_and_in_memory/#how-to-handle-concurrent-writers","title":"How to handle concurrent writers?","text":"<p>Again, it should be noted that ArcticDB achieves its highest performance and scale when configured with an object storage backed (e.g. S3). Nevertheless, applications may want to concurrently write to LMDB stores. In-memory stores are not appropriate for this use case.</p> <p>The following Python code uses <code>multiprocessing</code> to spawn 50 processes that concurrently write to different symbols. (Non-staged parallel writes to the same symbol are not supported).</p> <pre><code># Code tested on Linux\nimport arcticdb as adb\nimport pandas as pd\nfrom multiprocessing import Process, Queue\nimport numpy as np\n\nlmdb_dir = 'data.lmdb'\nnum_processes = 50\ndata_KB = 1000\nncols = 10\n\nnrows = int(data_KB * 1e3 / ncols / np.dtype(float).itemsize)\n\nac = adb.Arctic(f'lmdb://{lmdb_dir}')\nac.create_library('lib')\nlib = ac['lib']\n\ntimings = Queue()\ndef connect_and_write_symbol(symbol, lmdb_dir, timings_):\n    ac = adb.Arctic(f'lmdb://{lmdb_dir}')\n    lib = ac['lib']\n    start = pd.to_datetime('now')\n    lib.write(\n        symbol,\n        pd.DataFrame(\n            np.random.randn(nrows, ncols),\n            columns=[f'c{i}' for i in range(ncols)]\n        )\n    )\n    timings_.put((start, pd.to_datetime('now')))\n\nsymbol_names = {f'symbol_{i}' for i in range(num_processes)}\nconcurrent_writers = {\n    Process(target=connect_and_write_symbol, args=(symbol, lmdb_dir, timings))\n    for symbol in symbol_names\n}\n\n# Start all processes\nfor proc in concurrent_writers:\n    proc.start()\n# Wait for them to complete\nfor proc in concurrent_writers:\n    proc.join()\n\nassert set(lib.list_symbols()) == symbol_names\n\ntimings_list = []\nwhile not timings.empty():\n    timings_list.append(timings.get())\n\npd.DataFrame(timings_list, columns=['start', 'end']).to_csv('timings.csv')\n</code></pre> <p>Plotting the lifetimes of each process with matplotlib we get:</p> <p></p> <p>Explanation of graph: Each line segment represents the execution of a process writing to the shared LMDB backend. File locks are repeatedly obtained and released by LMDB throughout the calls to <code>lib.write(..)</code>.</p> <p>The LMDB documentation homepage states that multi-threaded concurrency is also possible. However as explained on this page we should not call <code>mdb_env_open()</code> multiple times from a single process. Hence, since this is called in the <code>Arctic</code> instantiation, the above code could not be transferred to a multi-threaded application.</p>"},{"location":"tutorials/lmdb_and_in_memory/#is-lmdb-on-tmpfs-any-faster-than-lmdb-on-disk","title":"Is LMDB on tmpfs any faster than LMDB on disk?","text":"<p>See the timing results below and the Appendix for the code to generate this data. The differences are not huge with tmpfs out-performing disk only for symbol writing operations. Nevertheless, tmpfs is clearly a better option for an ephemeral LMDB backend. We can also see that the in-memory store is significantly faster across the board as would be expected.</p> <p></p> <p>Note: the ranges are 95% confidence intervals based on five repeats for each data size. The hardware limits are for writing to disk (not tmpfs) using the <code>np.save</code> and <code>np.load</code> functions. No data was sought for the RAM's hardware limit.</p>"},{"location":"tutorials/lmdb_and_in_memory/#appendix-profiling-script","title":"Appendix: Profiling script","text":"<p>The profiling script used to generate the above graphs, and discussion of determining theoretical maximum read and write speeds using <code>fio</code> have been moved to this Wiki page.</p>"},{"location":"tutorials/metadata/","title":"Metadata","text":"<p>ArcticDB enables you to store arbitrary binary-blobs alongside symbols and versions.</p> <p>The below example shows a basic example of writing and reading metadata (in this case a Python dictionary):</p> <pre><code>import arcticdb as adb\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\nlibrary = \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nmetadata = {\n    \"This\": \"is\",\n    \"a\": \"Python\",\n    \"Dictionary\": \"!\"\n}\n\nlib.write(\"meta\", data=pd.DataFrame(), metadata=metadata)  # or write_metadata can be used - will still create a new version, but doesn't require `data` to be passed in\n\nassert lib.read(\"meta\").metadata == metadata\nassert lib.read_metadata(\"meta\").metadata == metadata  # Same as read, but doesn't return data from storage\n</code></pre> <p>New versions of symbols do not \"inherit\" the metadata of a previous version. Metadata needs to be specified explicitly each time that you create a new version of the symbol:</p> <pre><code>lib.write(\"new_sym\", data=pd.DataFrame(), metadata=metadata)\nlib.write(\"new_sym\", data=pd.DataFrame())\n\nassert lib.read(\"new_sym\").metadata is None\nassert lib.read(\"new_sym\", as_of=0).metadata == metadata\n</code></pre>"},{"location":"tutorials/metadata/#serialization-format","title":"Serialization Format","text":"<p>We use <code>msgpack</code> serialization for metadata when possible. We support the built-in <code>msgpack</code> types and also:</p> <ul> <li>Pandas timestamps <code>pd.Timestamp</code></li> <li>Python datetime <code>datetime.datetime</code></li> <li>Python timedelta <code>datetime.timedelta</code></li> </ul> <p>Documentation of supported <code>msgpack</code> structures is available here. Arrays and maps correspond to Python lists and dicts.</p> <p>When this <code>msgpack</code> serialization of the metadata fails due to unsupported types we fall back to pickling the metadata. Pickling can have serious downsides as it may not be possible to unpickle data written with one set of library versions from a client with a different set of library versions.</p> <p>Because of this, we log a warning when metadata gets pickled. You can disable the warning by setting an environment variable <code>ARCTICDB_PickledMetadata_loglevel_str</code> to <code>DEBUG</code>. The log message looks like:</p> <pre><code>Pickling metadata - may not be readable by other clients\n</code></pre> <p>The metadata may be up to 4GB in size.</p>"},{"location":"tutorials/metadata/#practical-example-using-metadata-to-track-vendor-timelines","title":"Practical example - using metadata to track vendor timelines","text":"<p>One common example for metadata is to store the vendor-provided date alongside the version. For example, let's say we are processing three files - <code>data-2004-01-01.csv</code>, <code>data-2004-01-02.csv</code> and <code>data-2004-01-03.csv</code>. Each file name contains a date which we'd like to be able to store along side the version information in ArcticDB.</p> <p>We can do this using the following code:</p> <pre><code>import glob\nimport datetime\nimport arcticdb as adb\n\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\nlibrary = \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nfile_names = glob.glob('*.csv')  # returns ['data-2004-01-01.csv', 'data-2004-01-02.csv', 'data-2004-01-03.csv']\n\nfor i, name in enumerate(file_names):\n    data = pd.read_csv('name')\n    date = datetime.datetime.strptime(name[5:][:-4], '%Y-%m-%d')\n\n    if i == 0:\n        lib.write(\"symbol\", data, metadata={\"vendor_date\": date})\n    else:\n        lib.append(\"symbol\", data, metadata={\"vendor_date\": date})\n</code></pre> <p>We'll now use this to read the data along the vendor-provided timeline - that is, we'll retrieve the data as if we had written each file on the day it was generated. We'll read all metadata entries for all versions of the symbol and select the date that is most recent with respect to a given date (in this case 2004-01-02):</p> <pre><code># Continuing from the previous code snippet\nmetadata = [\n    (v[\"version\"], lib.read_metadata(\"symbol\", as_of=v[\"version\"]).metadata.get(\"vendor_date\"))\n    for v in lib.list_versions(\"symbol\")\n]\nsorted_metadata = sorted(metadata, key=lambda x: x[1])\n\nversion_to_read_index = bisect_right([x[1] for x in sorted_metadata], datetime.datetime(2004, 1, 2))\nlib.read(\"symbol\", as_of=sorted_metadata[version_to_read_index - 1][0])\n</code></pre> <p>Note that if the data is written across multiple symbols, then ArcticDB Snapshots can be used to achieve the same result. </p>"},{"location":"tutorials/parallel_writes/","title":"Parallel Writes","text":"<p>As mentioned, ArcticDB fundamentally does not support concurrent writers to a single symbol - unless the data is concurrently written as staged data!</p> <p>Staged data is not available to read, and requires a process to finalize all staged data prior to it being available for reading. Each unit of staged data must not overlap with any other unit of staged data -  as a result staged data must be timeseries indexed. The below code uses Spark to concurrently write to a single symbol in parallel, before finalizing the data:</p> <pre><code>import pyspark\nimport arcticdb as adb\n\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\n\ndef _load(work):\n    # This method is run in parallel via Spark.\n    host, bucket, access, secret, symbol, library, file_path = work\n    ac = adb.Arctic(f\"s3://{host}:{bucket}?access={access}&amp;secret={secret}\")\n\n    library = ac[library]\n\n    df = pd.read_csv(file_path)\n    df = df.set_index(df.columns[0])\n    df.index = df.index.to_datetime()\n\n    # When staged, the written data is not available to read until finalized.\n    library.write(symbol, df, staged=True)\n\nsymbol = \"my_data\"\nlibrary = \"my_library\"\n\nconf = SparkConf().setAppName('appName').setMaster('local')\nsc = SparkContext(conf=conf)\n\n# Assumes there are a set of CSV files in the current directory to load from\ndata = [(host, bucket, access, secret, symbol, library, f) for f in glob.glob(\"*.csv\")]\ndist_data = sc.parallelize(data)\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nret = dist_data.map(_load)\nret.collect()\n\nlibrary.finalize_staged_data(symbol)\n\ndata = library.read(symbol)\n</code></pre>"},{"location":"tutorials/query_stats/","title":"Query Statistics","text":"<p>Warning: The Query Statistics API is unstable and not governed by ArcticDB's semantic versioning. It may change or be removed in future versions without notice.</p> <p>ArcticDB provides a Query Statistics API that allows you to collect and analyze performance metrics for operations performed on your data stores. This can be useful for debugging code issues and optimizing performance in applications.</p> <p>Currently only the S3 and in-memory backends are supported.</p>"},{"location":"tutorials/query_stats/#basic-usage","title":"Basic Usage","text":"<p>There are two ways to enable query statistics collection:</p>"},{"location":"tutorials/query_stats/#using-the-context-manager","title":"Using the Context Manager","text":"<p>The context manager automatically enables statistics at the beginning of a block and disables them at the end. Please note that nested context managers is not supported:</p> <pre><code>import arcticdb as adb\nimport arcticdb.toolbox.query_stats as qs\n\narctic = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET}\")\nlib = arctic[\"library_name\"]\n\n# Collect statistics for specific operations\nwith qs.query_stats():\n    lib.list_symbols()\n\n# Get the collected statistics\nstats = qs.get_query_stats()\nprint(stats)\n</code></pre>"},{"location":"tutorials/query_stats/#using-enabledisable-explicitly","title":"Using Enable/Disable Explicitly","text":"<p>For more control, you can manually enable and disable statistics collection:</p> <pre><code>import arcticdb as adb\nimport arcticdb.toolbox.query_stats as qs\n\narctic = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET}\")\nlib = arctic[\"library_name\"]\n\n# Enable statistics collection\nqs.enable()\n\n# Perform operations you want to measure\nlib.write(\"symbol\", data)\nlib.read(\"symbol\")\n\n# Get the collected statistics\nstats = qs.get_query_stats()\nprint(stats)\n\n# Optionally, reset the statistics\nqs.reset_stats()\n\n# Continue with more operations\nlib.list_symbols()\n\n# Get new statistics\nstats = qs.get_query_stats()\nprint(stats)\n\n# Disable statistics collection when done\nqs.disable()\n</code></pre>"},{"location":"tutorials/query_stats/#output-structure","title":"Output Structure","text":"<p>The statistics are returned as a nested dictionary organized by: - Operation group (currently <code>storage_operations</code> only) - Task type (e.g., <code>S3_ListObjectsV2</code>, <code>S3_PutObject</code>) - Key type (e.g., <code>SYMBOL_LIST</code>, <code>TABLE_DATA</code>, <code>VERSION_REF</code>)</p> <p>Each task contains measurements like: - <code>count</code>: Number of times the operation was performed - <code>total_time_ms</code>: Total execution time in milliseconds - For data upload and download, additional metric <code>size_bytes</code> for the size of compressed data being transferred</p> <p>Example output:</p> <pre><code>{\n    \"storage_operations\": {\n        \"S3_DeleteObjects\": {\n            \"LOCK\": {\n                \"count\": 1,\n                \"size_bytes\": 0,\n                \"total_time_ms\": 14\n            },\n            \"SYMBOL_LIST\": {\n                \"count\": 1,\n                \"size_bytes\": 0,\n                \"total_time_ms\": 17\n            }\n        },\n        \"S3_GetObject\": {\n            \"LOCK\": {\n                \"count\": 2,\n                \"size_bytes\": 206,\n                \"total_time_ms\": 31\n            }\n        },\n        \"S3_HeadObject\": {\n            \"LOCK\": {\n                \"count\": 1,\n                \"size_bytes\": 0,\n                \"total_time_ms\": 4\n            }\n        },\n        \"S3_ListObjectsV2\": {\n            \"SYMBOL_LIST\": {\n                \"count\": 2,\n                \"size_bytes\": 0,\n                \"total_time_ms\": 35\n            },\n            \"VERSION_REF\": {\n                \"count\": 1,\n                \"size_bytes\": 0,\n                \"total_time_ms\": 15\n            }\n        },\n        \"S3_PutObject\": {\n            \"LOCK\": {\n                \"count\": 1,\n                \"size_bytes\": 103,\n                \"total_time_ms\": 15\n            },\n            \"SYMBOL_LIST\": {\n                \"count\": 1,\n                \"size_bytes\": 308,\n                \"total_time_ms\": 15\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"tutorials/query_stats/#common-use-cases","title":"Common Use Cases","text":""},{"location":"tutorials/query_stats/#measure-how-many-ios-have-been-made-in-one-read","title":"Measure how many IOs have been made in one read","text":"<pre><code>import arcticdb as adb\nimport arcticdb.toolbox.query_stats as qs\nimport pandas as pd\n\narctic = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET}\")\nlib = arctic[\"library_name\"]\n\n# Measure read performance\nlib.read(\"test_symbol\")\nread_stats = qs.get_query_stats()\n</code></pre>"},{"location":"tutorials/query_stats/#resetting-statistics","title":"Resetting Statistics","text":"<p>You can clear all collected statistics using <code>reset_stats()</code>:</p> <pre><code>qs.reset_stats()\n</code></pre> <p>This is useful when you want to isolate statistics for specific operations or when you're done with one phase of analysis and want to start fresh.</p>"},{"location":"tutorials/query_stats/#note","title":"Note","text":"<ol> <li> <p>The Query Statistics feature uses a global container to store all measurements. This means statistics from all Python threads in your application will be collected in the same structure.  For this reason, enabling or disabling Query Statistics in multiple threads can lead to unpredictable results and should be avoided. It's best to treat Query Statistics as an application-wide setting rather than a thread-specific one.</p> </li> <li> <p>Running an enormous number of operations with Query Statistics enabled risks overflowing the internal counters. To avoid this issue, please reset statistics periodically or keep sessions with Query Statistics enabled relatively short. ```</p> </li> </ol>"},{"location":"tutorials/snapshots/","title":"Snapshots","text":"<p>ArcticDB enables multi-symbol snapshotting. Snapshots enable multiple symbols to be versioned together via a human readable string name. </p> <p>In practise this is useful to tie derived data with the source data.</p> <pre><code>import arcticdb as adb\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\n\nlibrary= \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\n# Assumes there are CSV files containing pricing data and factor data. Each time we've written ALL new factor files\n# to their symbol, we'll take a snapshot across all symbols.\nfor i, f in enumerate(sorted(glob.glob('*.csv'), key=lambda f: f.split('_')[1].split('.')[0])):\n    df = pd.read_csv(f)\n    if 'FACTORS' in f:\n        library.write(f, df)\n        # SNAP_{i} will forever point to all symbols that exist at this time at their current latest version\n        library.snapshot(f\"SNAP_{i}\")\n    else:\n        df = df.set_index(df.columns[0])\n        df.index = df.index.to_datetime()\n\n        library.append(pricing_symbol, df, write_if_missing=True)\n\nsnapshots = library.list_snapshots()\nsymbols = library.list_symbols(snapshot_name=list(snapshots.keys())[0])\n</code></pre> <p>To generate this data, the following code can be used:</p> <pre><code>import argparse\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef run(num_files, num_symbols):\n    starting_date = datetime.today() - timedelta(weeks=num_files)\n    starting_date = datetime(starting_date.year, starting_date.month, starting_date.day)\n\n    for file_num in range(num_files):\n        index_size = 7 * 24\n        this_file_starting_date = starting_date + timedelta(weeks=file_num)\n\n        df = pd.DataFrame(np.random.randint(0,index_size,size=(index_size, num_symbols)), columns=['SYM_%d' % i for i in range(num_symbols)])\n\n        df.index = pd.date_range(this_file_starting_date, periods=index_size, freq=\"H\")\n\n        df.to_csv(f\"PRICING_{this_file_starting_date}.csv\")\n        if file_num % 3 == 0:\n            df = pd.DataFrame(np.random.randint(0, 5,size=(5, num_symbols)), columns=['SYM_%d' % i for i in range(num_symbols)])\n            df['FACTORS'] = ['FACTOR_1', 'FACTOR_2', 'FACTOR_3', 'FACTOR_4', 'FACTOR_5']\n            df.to_csv(f\"FACTORS_{this_file_starting_date}.csv\")\n\n        print(f\"Written {file_num + 1} / {num_files}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--num-files', type=int, default=15)\n    parser.add_argument('--symbols-per-file', type=int, default=500)\n\n    args = parser.parse_args()\n\n    run(args.num_files, args.symbols_per_file)\n</code></pre>"}]}