name: __benchmark_commits
on:
  workflow_call:
    inputs:
      run_all_benchmarks: {required: true, type: boolean, description: Run all benchmarks or just the one for the given commit}
      commit:             {required: true, type: string, description: commit hash that will be benchmarked}
      run_on_pr_head:     {required: false, default: false, type: boolean, description: Specifies if the benchmark should run on PR head branch}
      dev_image_tag:      {required: false, default: 'latest', type: string, description: Tag of the ArcticDB development image}
      suite_to_run:       {required: true, type: string, default: 'LMDB', description: Default benchmark on 'LMDB' storage (or 'REAL' storage ond 'ALL' for both)}
      suite_overwrite:       {required: false, type: string, default: '', description: User defined tests to run}
jobs:
  start_ec2_runner:
    uses: ./.github/workflows/ec2_runner_jobs.yml
    secrets: inherit
    with:
      job_type: start

  benchmark_commit:
    timeout-minutes: 1200
    needs: [start_ec2_runner]
    if: |
      always() &&
      !cancelled()
    runs-on: ${{ needs.start_ec2_runner.outputs.label }}
    container: ghcr.io/man-group/arcticdb-dev:${{ inputs.dev_image_tag }}
    env:
      # this is potentially overflowing the cache, so should be looked into after we address issue #1057
      # 0 - uses S3 Cache, 1 - uses GHA cache
      # this way the external PRs can use the GHA cache
      SCCACHE_GHA_VERSION: ${{secrets.AWS_S3_ACCESS_KEY == null}}
      SCCACHE_BUCKET: arcticdb-ci-sccache-bucket
      SCCACHE_ENDPOINT: http://s3.eu-west-1.amazonaws.com
      SCCACHE_REGION: eu-west-1
      SCCACHE_S3_USE_SSL: false
      AWS_ACCESS_KEY_ID: ${{secrets.AWS_S3_ACCESS_KEY}}
      AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_S3_SECRET_KEY}}
      VCPKG_NUGET_USER: ${{secrets.VCPKG_NUGET_USER || github.repository_owner}}
      VCPKG_NUGET_TOKEN: ${{secrets.VCPKG_NUGET_TOKEN || secrets.GITHUB_TOKEN}}
      VCPKG_MAN_NUGET_USER: ${{secrets.VCPKG_MAN_NUGET_USER}} # For forks to download pre-compiled dependencies from the Man repo
      VCPKG_MAN_NUGET_TOKEN: ${{secrets.VCPKG_MAN_NUGET_TOKEN}}
      CMAKE_C_COMPILER_LAUNCHER: sccache
      CMAKE_CXX_COMPILER_LAUNCHER: sccache
      ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true
    defaults:
      run: {shell: bash}
    steps:  
      - uses: actions/checkout@v3.3.0
        with:
          lfs: 'true'
          fetch-depth: 0
          submodules: recursive
          token: ${{ secrets.ARCTICDB_TEST_PAT }}
          ref: ${{ inputs.run_on_pr_head && github.event.pull_request.head.sha || '' }}  # Note: This is dangerous if we run automatic CI on external PRs

      - name: Configure sccache
        uses: mozilla-actions/sccache-action@v0.0.9
        with:
          disable_annotations: 'true'  # supress noisy report that pollutes the summary page

      - name: Extra envs
        shell: bash -l {0}
        run: |
          . build_tooling/vcpkg_caching.sh # Linux follower needs another call in CIBW
          echo -e "VCPKG_BINARY_SOURCES=$VCPKG_BINARY_SOURCES
          VCPKG_ROOT=$PLATFORM_VCPKG_ROOT" | tee -a $GITHUB_ENV
          cmake -P cpp/CMake/CpuCount.cmake | sed 's/^-- //' | tee -a $GITHUB_ENV
        env:
          CMAKE_BUILD_PARALLEL_LEVEL: ${{vars.CMAKE_BUILD_PARALLEL_LEVEL}}

      - name: Set persistent storage variables
        uses: ./.github/actions/set_persistent_storage_env_vars
        with:
          bucket: "arcticdb-ci-benchmark-results"
          aws_access_key: "${{ secrets.AWS_S3_ACCESS_KEY }}"
          aws_secret_key: "${{ secrets.AWS_S3_SECRET_KEY }}"
          strategy_branch: "${{ inputs.commit }}"
          shared_storage_prefix: "_github_runner_"

      # Workaround for https://github.com/airspeed-velocity/asv/issues/1465
      - name: Setup micromamba
        uses: mamba-org/setup-micromamba@v2
        with:
          micromamba-version: 2.1.0-0

      - name: Install libmambapy
        shell: bash -el {0}
        run: |
          micromamba install -y -c conda-forge "libmambapy<2"

      - name: Install ASV
        shell: bash -el {0}
        run: |
          git config --global --add safe.directory .
          python -m pip install --upgrade pip
          pip install asv virtualenv
          python -m asv machine -v --yes --machine ArcticDB-Medium-Runner
          
          # Make a separate venv to run our build_tooling/ scripts, else ASV's venv creation wastes time uninstalling from the "python" installation
          python -m venv ./tooling_venv
          ./tooling_venv/bin/pip install "ArcticDB[Testing]"

      - name: Configure what suite or tests to execute
        shell: bash -el {0}
        run: | 
            if [ "${{ inputs.suite_to_run }}" == "REAL" ]; then
              # ASV tests with real storage starts with 'real_' in the test name
              SUITE='^(real_).*'
            elif [ "${{ inputs.suite_to_run }}" == "ALL" ]; then
              # Select all tests
              SUITE='^.*'
            else
              # LMDB tests are other tests not starting with 'real_'
              SUITE='^(?!real_).*'
            fi
            echo "selection of suite SUITE=$SUITE"
            SUITE_OVERWRITE=${{ github.event.inputs.suite_overwrite }}
            echo "selection of SUITE_OVERWRITE=$SUITE_OVERWRITE"
            # Remove leading and trailing whitespaces using parameter expansion
            SUITE_OVERWRITE="${SUITE_OVERWRITE#"${SUITE_OVERWRITE%%[![:space:]]*}"}"
            SUITE_OVERWRITE="${SUITE_OVERWRITE%"${SUITE_OVERWRITE##*[![:space:]]}"}"
            if [[ -n "${SUITE_OVERWRITE// /}" ]]; then
              SUITE=$SUITE_OVERWRITE
            fi
            echo "FINAL selection to execute SUITE=$SUITE"
            echo "SUITE=$SUITE" >> $GITHUB_ENV
            # Now lets reduce logging
            echo "ARCTICDB_WARN_ON_WRITING_EMPTY_DATAFRAME=0" >> $GITHUB_ENV 

      - name: Benchmark given commit
        if: github.event_name != 'pull_request_target' || inputs.run_all_benchmarks == true
        shell: bash -l {0}
        run: | 
          git config --global --add safe.directory .
          python -m asv run --show-stderr --durations all --bench $SUITE ${{ inputs.commit }}^!
          exit_code=$?
          python build_tooling/summarize_asv_run.py --commit-hash ${{ inputs.commit }}
          exit $exit_code

      - name: Benchmark against master
        if: github.event_name == 'pull_request_target' && inputs.run_all_benchmarks == false
        shell: bash -l {0}
        run: |
          python -m asv continuous --show-stderr --bench $SUITE origin/master HEAD -f 1.15
          exit_code=$?
          python build_tooling/summarize_asv_run.py --commit-hash $(git rev-parse HEAD)
          exit $exit_code

      - name: Add results to ArcticDB database
        shell: bash -el {0}
        run: |
          pip install arcticdb[Testing] "protobuf<6"
          python build_tooling/transform_asv_results.py --mode save ${{ github.ref != 'refs/heads/master' && format('--arcticdb_library {0}_asv_results', github.ref_name) || ''}}

          python build_tooling/transform_asv_results.py --mode analyze \
          ${{ github.ref != 'refs/heads/master' && format('--arcticdb_library {0}_asv_results', github.ref_name) || ''}} \
          --hash $(git rev-parse --short=8 ${{inputs.commit}})

  stop-ec2-runner:
    needs: [start_ec2_runner, benchmark_commit]
    if: |
      always()
    uses: ./.github/workflows/ec2_runner_jobs.yml
    secrets: inherit
    with:
      job_type: stop
      label: ${{ needs.start_ec2_runner.outputs.label }}
      ec2-instance-id: ${{ needs.start_ec2_runner.outputs.ec2-instance-id }}