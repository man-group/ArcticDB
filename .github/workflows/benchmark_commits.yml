name: __benchmark_commits
on:
  workflow_call:
    inputs:
      benchmark_all_tags: {required: true, type: boolean, description: Run benchmarks for all tags or just the given commit}
      commit:             {required: true, type: string, description: commit hash that will be benchmarked}
      dev_image_tag:      {required: false, default: 'latest', type: string, description: Tag of the ArcticDB development image}
      storage:            {required: true, type: string, default: 'LMDB', description: Default benchmark on 'LMDB' storage (or 'REAL' storage ond 'ALL' for both)}
      suite_overwrite:    {required: false, type: string, default: '', description: User defined tests to run}
jobs:
  start_ec2_runner:
    uses: ./.github/workflows/ec2_runner_jobs.yml
    secrets: inherit
    with:
      job_type: start

  benchmark_commit:
    timeout-minutes: 1200
    needs: [start_ec2_runner]
    if: |
      always() &&
      !cancelled()
    runs-on: ${{ needs.start_ec2_runner.outputs.label }}
    container: ghcr.io/man-group/arcticdb-dev:${{ inputs.dev_image_tag }}
    env:
      # this is potentially overflowing the cache, so should be looked into after we address issue #1057
      # 0 - uses S3 Cache, 1 - uses GHA cache
      # this way the external PRs can use the GHA cache
      SCCACHE_GHA_VERSION: ${{secrets.AWS_S3_ACCESS_KEY == null}}
      SCCACHE_BUCKET: arcticdb-ci-sccache-bucket
      SCCACHE_ENDPOINT: http://s3.eu-west-1.amazonaws.com
      SCCACHE_REGION: eu-west-1
      SCCACHE_S3_USE_SSL: false
      AWS_ACCESS_KEY_ID: ${{secrets.AWS_S3_ACCESS_KEY}}
      AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_S3_SECRET_KEY}}
      ARCTICDB_REAL_S3_ACCESS_KEY: ${{secrets.AWS_S3_ACCESS_KEY}}
      ARCTICDB_REAL_S3_SECRET_KEY: ${{secrets.AWS_S3_SECRET_KEY}}
      ARCTICDB_REAL_S3_REGION: eu-west-1
      ARCTICDB_REAL_S3_ENDPOINT: s3.eu-west-1.amazonaws.com
      VCPKG_NUGET_USER: ${{secrets.VCPKG_NUGET_USER || github.repository_owner}}
      VCPKG_NUGET_TOKEN: ${{secrets.VCPKG_NUGET_TOKEN || secrets.GITHUB_TOKEN}}
      VCPKG_MAN_NUGET_USER: ${{secrets.VCPKG_MAN_NUGET_USER}} # For forks to download pre-compiled dependencies from the Man repo
      VCPKG_MAN_NUGET_TOKEN: ${{secrets.VCPKG_MAN_NUGET_TOKEN}}
      CMAKE_C_COMPILER_LAUNCHER: sccache
      CMAKE_CXX_COMPILER_LAUNCHER: sccache
      ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true
    defaults:
      run: {shell: bash}
    steps:  
      - uses: actions/checkout@v3.3.0
        with:
          lfs: 'true'
          fetch-depth: 0
          submodules: recursive
          token: ${{ secrets.ARCTICDB_TEST_PAT }}

      - name: Configure sccache
        uses: mozilla-actions/sccache-action@v0.0.9
        with:
          disable_annotations: 'true'  # supress noisy report that pollutes the summary page

      - name: Extra envs
        shell: bash -l {0}
        run: |
          . build_tooling/vcpkg_caching.sh # Linux follower needs another call in CIBW
          echo -e "VCPKG_BINARY_SOURCES=$VCPKG_BINARY_SOURCES
          VCPKG_ROOT=$PLATFORM_VCPKG_ROOT" | tee -a $GITHUB_ENV
          cmake -P cpp/CMake/CpuCount.cmake | sed 's/^-- //' | tee -a $GITHUB_ENV
        env:
          CMAKE_BUILD_PARALLEL_LEVEL: ${{vars.CMAKE_BUILD_PARALLEL_LEVEL}}

      - name: Create test bucket
        shell: bash -el {0}
        run: |
          random_id=$(openssl rand -hex 4)
          timestamp=$(date +%s)
          bucket_name="arcticdb-asv-data-${timestamp}-${random_id}"
          echo "Bucket name: ${bucket_name}"
          aws s3 mb "s3://${bucket_name}" --endpoint-url "${ARCTICDB_REAL_S3_ENDPOINT}" --region "${ARCTICDB_REAL_S3_REGION}"
          echo "Created bucket s3://${bucket_name}"
          echo "ARCTICDB_REAL_S3_BUCKET=${bucket_name}" >> $GITHUB_ENV

      # Workaround for https://github.com/airspeed-velocity/asv/issues/1465
      - name: Setup micromamba
        uses: mamba-org/setup-micromamba@v2
        with:
          micromamba-version: 2.1.0-0

      - name: Install libmambapy
        shell: bash -el {0}
        run: |
          micromamba install -y -c conda-forge "libmambapy<2"

      - name: Install deps
        shell: bash -el {0}
        run: |
          git config --global --add safe.directory .
          python -m pip install --upgrade pip
          pip install asv virtualenv
          python -m asv machine -v --yes --machine ArcticDB-Medium-Runner
          
          # Make a separate venv to run our build_tooling/ scripts, else ASV's venv creation wastes time uninstalling from the "python" installation
          python -m venv ./tooling_venv
          ./tooling_venv/bin/pip install "ArcticDB[Testing]"

      - name: Configure what suite or tests to execute
        shell: bash -el {0}
        run: | 
            SUITE='^.*'
            if [ "${{ inputs.storage }}" == "REAL" ]; then
              echo "ARCTICDB_STORAGE_AWS_S3=1" >> $GITHUB_ENV
              echo "ARCTICDB_STORAGE_LMDB=0" >> $GITHUB_ENV
            elif [ "${{ inputs.storage }}" == "ALL" ]; then
              echo "ARCTICDB_STORAGE_AWS_S3=1" >> $GITHUB_ENV
            fi
            
            SUITE_OVERWRITE=${{ github.event.inputs.suite_overwrite }}
            echo "selection of SUITE_OVERWRITE=$SUITE_OVERWRITE"
            # Remove leading and trailing whitespaces using parameter expansion
            SUITE_OVERWRITE="${SUITE_OVERWRITE#"${SUITE_OVERWRITE%%[![:space:]]*}"}"
            SUITE_OVERWRITE="${SUITE_OVERWRITE%"${SUITE_OVERWRITE##*[![:space:]]}"}"
            if [[ -n "${SUITE_OVERWRITE// /}" ]]; then
              SUITE=$SUITE_OVERWRITE
            fi
            echo "FINAL selection to execute SUITE=$SUITE"
            echo "SUITE=$SUITE" >> $GITHUB_ENV
            # Now lets reduce logging
            echo "ARCTICDB_WARN_ON_WRITING_EMPTY_DATAFRAME=0" >> $GITHUB_ENV 

      - name: Benchmark given commit
        if: github.event_name != 'pull_request' || inputs.benchmark_all_tags == true
        shell: bash -l {0}
        run: | 
          git config --global --add safe.directory .
          python -m asv run --show-stderr --durations all --bench $SUITE ${{ inputs.commit }}^!
          exit_code=$?
          ./tooling_venv/bin/python build_tooling/summarize_asv_run.py --commit-hash ${{ inputs.commit }}
          exit $exit_code

      - name: Benchmark against master
        if: github.event_name == 'pull_request'
        shell: bash -l {0}
        run: |
          # Look up the most recent master commit stored in our ASV database and compare against it.
          # asv compare exits with code 0 even if there are regressions.
          ./tooling_venv/bin/python build_tooling/transform_asv_results.py --mode extract-recent || exit 1
          python -m asv run --show-stderr --durations all --bench $SUITE HEAD^! || exit 1
          python -m asv compare -s -f 1.15 $(cat master_commit_hash.txt) HEAD > results.txt || exit 1
          
          # Get rid of the master results so we don't re-add them to the database.
          rm python/.asv/results/**/$(cat master_commit_hash.txt)*.json || exit 1
          
          grep -q "Benchmarks that have got worse" results.txt
          grep_exit_code=$?
          any_worse=$(( ! grep_exit_code ))

          cat results.txt

          ./tooling_venv/bin/python build_tooling/summarize_asv_run.py --commit-hash $(git rev-parse HEAD)
          exit $any_worse

      - name: Clean up the test bucket
        if: always()
        shell: bash -el {0}
        run: |
          if [[ $ARCTICDB_REAL_S3_BUCKET == arcticdb-asv-data* ]]; then
            echo "Removing ${ARCTICDB_REAL_S3_BUCKET}"
            # --force since there will be files in the bucket
            aws s3 rb "s3://${ARCTICDB_REAL_S3_BUCKET}" --force --endpoint-url "${ARCTICDB_REAL_S3_ENDPOINT}" --region "${ARCTICDB_REAL_S3_REGION}"
          else
            echo "Unexpected bucket to clean up! ${ARCTICDB_REAL_S3_BUCKET}"
            echo "This indicates a bug in the workflow. Not deleting anything and exiting..."
            exit 1
          fi

      - name: Add results to ArcticDB database
        if: always()
        shell: bash -el {0}
        run: |
          ./tooling_venv/bin/python build_tooling/transform_asv_results.py --mode save ${{ github.ref != 'refs/heads/master' && format('--arcticdb_library {0}_asv_results', github.ref_name) || ''}}

          ./tooling_venv/bin/python build_tooling/transform_asv_results.py --mode analyze \
          ${{ github.ref != 'refs/heads/master' && format('--arcticdb_library {0}_asv_results', github.ref_name) || ''}} \
          --hash $(git rev-parse --short=8 ${{inputs.commit}})
        env:
          ARCTICDB_REAL_S3_BUCKET: arcticdb-ci-benchmark-results

  stop-ec2-runner:
    needs: [start_ec2_runner, benchmark_commit]
    if: |
      always()
    uses: ./.github/workflows/ec2_runner_jobs.yml
    secrets: inherit
    with:
      job_type: stop
      label: ${{ needs.start_ec2_runner.outputs.label }}
      ec2-instance-id: ${{ needs.start_ec2_runner.outputs.ec2-instance-id }}