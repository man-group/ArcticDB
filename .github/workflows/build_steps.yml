name: __build_steps
on:
  workflow_call:
    inputs:
      job_type:          {required: true, type: string, description: Selects the steps to enable}
      cmake_preset_type: {required: true, type: string, description: release/debug}
      matrix:            {required: true, type: string, description: JSON string to feed into the matrix}
      cibw_image_tag:    {required: false, type: string, description: Linux only. As built by cibw_docker_image.yml workflow}
      cibw_version:      {required: false, type: string, description: build-python-wheels only. Must match the cibw_image_tag}
      python_deps_ids:   {default: '[""]', type: string, description: build-python-wheels test matrix parameter. JSON string.}
      python3:           {default: -1, type: number, description: Python 3 minor version}
      persistent_storage:      {default: "no", type: string, description: Specifies whether the python tests should tests against real storages e.g. AWS S3  }
      pytest_xdist_mode: {default: "", type: string, description: additional argument to pass for pytest-xdist}
      pytest_args:       {default: "", type: string, description: a way to rewrite the pytest args to change what tests are being run}
      version_cache_full_test: {default: false, type: boolean, description: if true - tests will run with both version cache 0 and 2000000000 otherwise only 2000000000}
jobs:
  compile:
    strategy:
      matrix:
        # Declaring the dummy fields here to aid the Github Actions linting in VSCode and to provide documentation
        os: [0] # Decouples the steps from any distro version changes
        cmake_preset_prefix: [0]
        cibw_build_suffix: [0]
        envs: [0]
        build_dir: [0] # Must be an absolute path
        vcpkg_installed_dir: [0]
        vcpkg_packages_dir: [0]
        symbols: [0] # Glob for symbol symbol files. Used for including in build-python-wheels builds and exclusion on others.
        do_not_archive: [0]
        test_services: [0] # Github service containers to spin up for the pytest run
        container: [0]
        exclude:
            - os: 0
        include:
            - ${{fromJSON(inputs.matrix)[0]}} # The items after 0 are for tests only

    runs-on: ${{ matrix.distro}}
    container: ${{ (matrix.os == 'linux' && inputs.job_type != 'build-python-wheels') && matrix.container || null}}
    env:
      # 0 - uses S3 Cache, 1 - uses GHA cache
      # this way the external PRs can use the GHA cache
      SCCACHE_GHA_VERSION: ${{secrets.AWS_S3_ACCESS_KEY && 0 || 1}}
      SCCACHE_BUCKET: arcticdb-ci-sccache-bucket
      SCCACHE_ENDPOINT: http://s3.eu-west-1.amazonaws.com
      SCCACHE_REGION: eu-west-1
      SCCACHE_S3_USE_SSL: false
      AWS_ACCESS_KEY_ID: ${{secrets.AWS_S3_ACCESS_KEY}}
      AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_S3_SECRET_KEY}}
      VCPKG_NUGET_USER: ${{secrets.VCPKG_NUGET_USER || github.repository_owner}}
      VCPKG_NUGET_TOKEN: ${{secrets.VCPKG_NUGET_TOKEN || secrets.GITHUB_TOKEN}}
      VCPKG_MAN_NUGET_USER: ${{secrets.VCPKG_MAN_NUGET_USER}} # For forks to download pre-compiled dependencies from the Man repo
      VCPKG_MAN_NUGET_TOKEN: ${{secrets.VCPKG_MAN_NUGET_TOKEN}}
      CMAKE_C_COMPILER_LAUNCHER: sccache
      CMAKE_CXX_COMPILER_LAUNCHER: sccache
      ARCTIC_CMAKE_PRESET: ${{matrix.cmake_preset_prefix}}-${{inputs.cmake_preset_type}}
      ARCTICDB_BUILD_DIR: ${{matrix.build_dir}}
      ARCTICDB_VCPKG_INSTALLED_DIR: ${{matrix.vcpkg_installed_dir}}
      ARCTICDB_VCPKG_PACKAGES_DIR: ${{matrix.vcpkg_packages_dir}}
      MACOSX_DEPLOYMENT_TARGET: "15.0"
      CMAKE_OSX_DEPLOYMENT_TARGET: "15.0"
      CIBW_ENVIRONMENT_PASS_LINUX: SCCACHE_GHA_VERSION ACTIONS_CACHE_URL ACTIONS_RUNTIME_TOKEN VCPKG_INSTALLATION_ROOT
        VCPKG_BINARY_SOURCES VCPKG_NUGET_USER VCPKG_NUGET_TOKEN VCPKG_MAN_NUGET_USER VCPKG_MAN_NUGET_TOKEN
        CMAKE_C_COMPILER_LAUNCHER CMAKE_CXX_COMPILER_LAUNCHER CMAKE_BUILD_PARALLEL_LEVEL ARCTIC_CMAKE_PRESET
        ARCTICDB_BUILD_DIR TEST_OUTPUT_DIR ARCTICDB_VCPKG_INSTALLED_DIR ARCTICDB_VCPKG_PACKAGES_DIR
        SCCACHE_BUCKET SCCACHE_ENDPOINT AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY SCCACHE_REGION SCCACHE_S3_USE_SSL
      ARCTICDB_DEBUG_FIND_PYTHON: ${{vars.ARCTICDB_DEBUG_FIND_PYTHON}}
      python_impl_name: ${{inputs.python3 > 0 && format('cp3{0}', inputs.python3) || 'default'}}
      CIBW_BUILD: ${{format('cp3{0}-{1}', inputs.python3, matrix.cibw_build_suffix)}}
      ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true
    defaults:
      run: {shell: bash}
    steps:
      - name: Checkout
        uses: actions/checkout@v3.3.0
        with:
          submodules: recursive
          fetch-depth: 0

      - name: Convert to Full Clone
        run: |
          git config --global --add safe.directory /__w/ArcticDB/ArcticDB
          git submodule update --init --recursive
          cd cpp/vcpkg
          git submodule update --init --recursive
          cd ../..

      - name: Configure sccache
        uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.10.0"
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup macOS build environment
        if: matrix.os == 'macos'
        run: |
          brew install sccache
          brew install mono
          brew install bison   # required by thrift pulled via vcpkg
          brew install openssl@3
          which openssl
          echo "SCCACHE_DIR=/tmp/sccache" >> $GITHUB_ENV
          mkdir -p /tmp/sccache
          # Start sccache server
          sccache --start-server
          # Show sccache stats
          sccache --show-stats

      - name: Windows Pagefile
        if: matrix.os == 'windows'
        uses: al-cheb/configure-pagefile-action@v1.3
        with:
          minimum-size: 2GB
          maximum-size: 6GB
          disk-root: "D:"  # This is also the checkout directory. Total size 12GB.
        continue-on-error: true

      - name: Extra envs
        # This has to come after msvc-dev-cmd to overwrite the bad VCPKG_ROOT it sets
        run: |
          HOME=~ . build_tooling/vcpkg_caching.sh # Linux build-python-wheels needs another call in CIBW
          echo -e "VCPKG_BINARY_SOURCES=$VCPKG_BINARY_SOURCES
          VCPKG_ROOT=$PLATFORM_VCPKG_ROOT
          ${{matrix.envs || ''}}" | tee -a $GITHUB_ENV
          cmake -P cpp/CMake/CpuCount.cmake | sed 's/^-- //' | tee -a $GITHUB_ENV
        env:
          CMAKE_BUILD_PARALLEL_LEVEL: ${{vars.CMAKE_BUILD_PARALLEL_LEVEL}}

      # ========================= Leader steps =========================
      - name: Remove GitHub default packages (manylinux) # To save space
        if: inputs.job_type == 'cpp-tests' && matrix.os == 'linux'
        run: |
          du -m /mnt/usr/local/lib/ | sort -n | tail -n 50
          nohup rm -rf /mnt/usr/local/lib/android &

      - name: Find and remove ccache # See PR: #945
        if: matrix.os == 'windows'
        run: rm $(which ccache) || true

      - name: Prepare C++ compilation env
        if: inputs.job_type != 'build-python-wheels'
        run: . build_tooling/prep_cpp_build.sh # Also applies to Windows

        # When a GitHub Windows image gets update the MSVC compiler also can get updated. New compilers can have compilation errors in Arctic or in the VCPKG dependencies.
        # We needd to pin a particular MSVC so that runner updates don't affect us.
        # When the MSVC version is update custom-triplets/x64-windows-static-msvc.cmake must also be updated with the correct toolsed version.
      - name: Install Required MSVC
        if: matrix.os == 'windows'
        run: |
          choco install -y -f visualstudio2022buildtools --version=117.11.4 --params "--add Microsoft.VisualStudio.Component.VC.Tools.x86.x64 --installChannelUri https://aka.ms/vs/17/release/390666095_1317821361/channel"
          choco install -y ninja

      - name: Enable Windows compiler commands
        if: matrix.os == 'windows'
        uses: TheMrMilchmann/setup-msvc-dev@v3
        with:
          arch: x64
          toolset: 14.41
          vs-path: 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools'

      - name: Setup cmake
        uses: jwlawson/actions-setup-cmake@v2
        with:
          cmake-version: '3.31.6'

      - name: Use cmake
        run: cmake --version

      - name: CMake compile
        if: inputs.job_type != 'build-python-wheels'
        # We are pinning the version to 10.6 because >= 10.7, use node20 which is not supported in the container
        uses: lukka/run-cmake@v10.6
        with:
          cmakeListsTxtPath: ${{github.workspace}}/cpp/CMakeLists.txt
          configurePreset: ${{env.ARCTIC_CMAKE_PRESET}}
          configurePresetAdditionalArgs: "['-DVCPKG_INSTALL_OPTIONS=--clean-after-build']"
          buildPreset: ${{env.ARCTIC_CMAKE_PRESET}}
        env:
          SCCACHE_DIR: ${{env.SCCACHE_DIR || ''}}

      - name: Compile C++ tests
        if: inputs.job_type == 'cpp-tests'
        run: cd cpp; cmake --build --preset $ARCTIC_CMAKE_PRESET --target install

      - name: C++ Rapidcheck
        if: inputs.job_type == 'cpp-tests'
        run: cpp/out/install/arcticdb_rapidcheck_tests

      - name: Collect arcticdb_rapidcheck_tests binary on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: arcticdb_rapidcheck_tests_${{matrix.os}}
          path: cpp/out/install/arcticdb_rapidcheck_tests*

      - name: C++ unit tests
        if: inputs.job_type == 'cpp-tests'
        run: |
          cd cpp/out
          install/test_unit_arcticdb --gtest_output=json:test_unit_arcticdb.json \
            --gtest_filter=-TestNfsBackedStorage.*:TestS3Storage.* || true
          [[ $(jq '.tests' test_unit_arcticdb.json) -gt 0 ]]
          [[ $(jq '.failures' test_unit_arcticdb.json) -eq 0 ]]
          [[ $(jq '.errors' test_unit_arcticdb.json) -eq 0 ]]
        env:
          ARCTICDB_memory_loglevel: INFO

      - name: Collect test_unit_arcticdb binary on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test_unit_arcticdb_${{matrix.os}}
          path: cpp/out/install/test_unit_arcticdb*

      # We don't do anything with the benchmarks automatically yet, but check that they at least compile and run.
      - name: Compile C++ Benchmarks
        if: inputs.job_type == 'cpp-tests'
        run: cd cpp; cmake --build --preset $ARCTIC_CMAKE_PRESET --target benchmarks -j $CMAKE_BUILD_PARALLEL_LEVEL

      - name: Run C++ Benchmarks
        if: inputs.job_type == 'cpp-tests'
        # We set a benchmark_min_time=20x because running until a time limit is reached can take a lot of time on
        # benchmarks which pause the timer.
        run: cpp/out/${ARCTIC_CMAKE_PRESET}-build/arcticdb/benchmarks --benchmark_time_unit=ms --benchmark_min_time=20x

      # ========================= build-python-wheels (CIBW) steps =========================
      - name: Get CIBuildWheel image & metadata
        if: inputs.job_type == 'build-python-wheels' && matrix.os == 'linux'
        run: |
            docker login ghcr.io -u token -p "${{secrets.GITHUB_TOKEN}}"
            docker pull "${{inputs.cibw_image_tag}}"
            docker inspect --type=image "${{inputs.cibw_image_tag}}" \
              --format='manylinux_image={{index .Config.Labels "io.arcticdb.base"}}' | tee -a $GITHUB_ENV

      - name: Build wheel
        if: inputs.job_type == 'build-python-wheels'
        run: pipx run cibuildwheel==${{inputs.cibw_version}}
        env:
          CIBW_MANYLINUX_X86_64_IMAGE: ${{inputs.cibw_image_tag}}
          CIBW_MACOSX_DEPLOYMENT_TARGET: "15.0"
          CIBW_ARCHS_MACOS: "arm64"
          SCCACHE_DIR: ${{env.SCCACHE_DIR || ''}}

      - name: Store wheel artifact
        if: inputs.job_type == 'build-python-wheels'
        uses: actions/upload-artifact@v4
        with:
          name: wheel-${{env.CIBW_BUILD}}
          path: wheelhouse/*.whl

      - name: Discover test directory names
        if: inputs.job_type == 'build-python-wheels'
        # We only run the compat tests on for newer python versions.
        # There are so few nonreg tests, run them in the hypothesis runner.
        run: |
          if [ ${{inputs.python3}} -gt 8 ]
          then
            find python/tests/* -maxdepth 0 -type d -not \( -name "__pycache__" -o -name "util" -o -name "nonreg" -o -name "scripts" \) -exec basename {} \; | tr '\n' ',' |
              sed 's/^/test_dirs=["/ ; s/,$/"]/ ; s/,/","/g ; s/"hypothesis"/"{hypothesis,nonreg,scripts}"/' | tee -a $GITHUB_ENV
          else
            find python/tests/* -maxdepth 0 -type d -not \( -name "__pycache__" -o -name "util" -o -name "nonreg" -o -name "scripts" -o -name "compat" \) -exec basename {} \; | tr '\n' ',' |
              sed 's/^/test_dirs=["/ ; s/,$/"]/ ; s/,/","/g ; s/"hypothesis"/"{hypothesis,nonreg,scripts}"/' | tee -a $GITHUB_ENV
          fi

      # ========================= Common =========================
      - name: Disk usage
        if: always()
        run: du -m . ${{matrix.build_dir}} ${{matrix.vcpkg_packages_dir}} | sort -n | tail -n 50 || true; df -h
        continue-on-error: true

      - name: Make build directory readable for archiving
        if: inputs.job_type == 'build-python-wheels' && matrix.os == 'linux' && always()
        run: sudo chown -R $UID ${{matrix.build_dir}}

      - name: Archive build metadata
        uses: actions/upload-artifact@v4
        if: always()
        env:
          _exclusion: "\n!${{matrix.build_dir}}/**/"
        with:
          name: build-metadata-${{inputs.job_type}}-${{matrix.os}}-${{env.python_impl_name}}
          retention-days: ${{inputs.job_type == 'cpp-tests' && 7 || 90}}
          # On Windows, exclusions like "!**/*.ext" are prefixed with a drive letter (D:\) of the current working dir
          # before matching. This breaks since we moved the build_dir to C:. Work around by templating exclusions:
          path: ${{matrix.build_dir}}/*-build
            ${{env._exclusion}}${{inputs.job_type == 'build-python-wheels' && 'nofile' || matrix.symbols}}
            ${{env._exclusion}}${{join(matrix.do_not_archive, env._exclusion)}}

    outputs:
      manylinux_image: ${{env.manylinux_image}}
      python_impl_name: ${{env.python_impl_name}}
      test_dirs: ${{env.test_dirs}}
      cibw_build: ${{env.CIBW_BUILD}}

  python_tests:
    if: |
      inputs.job_type == 'build-python-wheels'
    needs: [compile]
    strategy:
      fail-fast: false
      matrix:
        type: ${{fromJSON(vars.TEST_DIRS_OVERRIDE || needs.compile.outputs.test_dirs)}}
        python_deps_id: ${{fromJson(inputs.python_deps_ids)}}
        # Always run with both for cp311
        version_cache_timeout: ${{ fromJSON((inputs.version_cache_full_test || needs.compile.outputs.python_impl_name == 'cp311') && '["NoCache", "DefaultCache"]' || '["DefaultCache"]') }} 
        include:
          ${{fromJSON(inputs.matrix)}}
    name: ${{matrix.type}}${{matrix.python_deps_id}}-${{matrix.version_cache_timeout}}
    runs-on: ${{matrix.distro}}
    container: ${{matrix.os == 'linux' &&  matrix.container || null}}
    defaults:
      run: {shell: bash}
    services: ${{matrix.test_services}}
    env:
      SCCACHE_GHA_ENABLED: "true"
      python_impl_name: ${{needs.compile.outputs.python_impl_name}}
      distinguishing_name: ${{matrix.os}}-${{needs.compile.outputs.python_impl_name}}-${{matrix.type}}${{matrix.python_deps_id}}-${{matrix.version_cache_timeout}}
      ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true
      real_tests_storage_type: ${{inputs.persistent_storage || 'no'}}
    steps:
      - name: Checkout
        uses: actions/checkout@v3.3.0

      - name: Get wheel artifact
        uses: actions/download-artifact@v4
        with:
          name: wheel-${{needs.compile.outputs.cibw_build}}
          path: ${{runner.temp}}

      - name: Select Python (Linux)
        if: matrix.os == 'linux'
        run: |
          #there are cp313 and cp313t
          if [[ "${{env.python_impl_name}}" == "cp313" ]]; then
            echo /opt/python/cp313-cp313/bin >> $GITHUB_PATH;
          else
            echo /opt/python/${{env.python_impl_name}}*/bin >> $GITHUB_PATH;
          fi

      - name: Select Python (Windows and macOS)
        if: matrix.os == 'windows' || matrix.os == 'macos'
        uses: actions/setup-python@v4.7.1
        with:
          python-version: "3.${{inputs.python3}}"

      - name: Windows Pagefile
        if: matrix.os == 'windows'
        uses: al-cheb/configure-pagefile-action@v1.3
        with:
          minimum-size: 2GB
          maximum-size: 8GB
          disk-root: "D:"  # This is also the checkout directory. Total size 12GB.
        continue-on-error: true

      # GitHub runner image does not come with npm
      - name: Install npm (Linux and macOS)
        if: matrix.os == 'linux' || matrix.os == 'macos'
        uses: actions/setup-node@v3.3.0
        with:
          node-version: '16'

      - name: Install the wheel and dependencies
        run: |
          npm install -g azurite
          cmake -P cpp/CMake/CpuCount.cmake | sed 's/^-- //' | tee -a $GITHUB_ENV
          python -V
          cd "$RUNNER_TEMP" # Works for Windows-style paths as well
          python -m pip install --force-reinstall  $(ls *${{env.python_impl_name}}*.whl)[Testing] pytest-split
          if [[ -n "${{matrix.python_deps || ''}}" ]] ; then
            echo "Using deps ${{matrix.python_deps}}"
            python -m pip install --force-reinstall -r $GITHUB_WORKSPACE/build_tooling/${{matrix.python_deps}}
          fi
          python -m pip uninstall -y pytest-cpp || true # No longer works on 3.6
          python -m pip list
          echo -e "${{matrix.envs || ''}}" | tee -a $GITHUB_ENV
          if [[ -n "$MSYSTEM" ]] ; then
            echo "LOCALAPPDATA=$LOCALAPPDATA" | tee -a $GITHUB_ENV
          fi
          ${{vars.EXTRA_TEST_PREPARE_CMD || ''}}
        env:
          CMAKE_BUILD_PARALLEL_LEVEL: ${{vars.CMAKE_BUILD_PARALLEL_LEVEL}}

      - name: Set persistent storage variables
        # Should be executed for all persistent storages but not for LMDB
        if: ${{ env.real_tests_storage_type != 'no' }}
        uses: ./.github/actions/set_persistent_storage_env_vars
        with:
          aws_access_key: "${{ secrets.AWS_S3_ACCESS_KEY }}"
          aws_secret_key: "${{ secrets.AWS_S3_SECRET_KEY }}"
          gcp_access_key: "${{ secrets.GCP_S3_ACCESS_KEY }}"
          gcp_secret_key: "${{ secrets.GCP_S3_SECRET_KEY }}"
          persistent_storage: "${{ inputs.persistent_storage }}"
          strategy_branch: "${{ env.distinguishing_name }}"

      - name: Set s3 sts AWS S3 variables for Windows
        if: ${{ env.real_tests_storage_type == 'AWS_S3' && matrix.os == 'windows' }}
        uses: ./.github/actions/set_s3_sts_persistent_storage_env_vars
        with:
          aws_access_key: "${{ secrets.AWS_S3_ACCESS_KEY }}"
          aws_secret_key: "${{ secrets.AWS_S3_SECRET_KEY }}"

      - name: Run test
        run: |
          ulimit -a
          export ARCTICDB_WARN_ON_WRITING_EMPTY_DATAFRAME=0
          if [[ "$(echo "$ARCTICDB_PYTEST_ARGS" | xargs)" == pytest* ]]; then
            python -m pip  install pytest-repeat setuptools wheel
            python setup.py protoc --build-lib python
            echo "Run custom pytest command: $ARCTICDB_PYTEST_ARGS"
            eval "$ARCTICDB_PYTEST_ARGS"
          else
            build_tooling/parallel_test.sh tests/${{matrix.type}}
          fi
        env:
          TEST_OUTPUT_DIR: ${{runner.temp}}
          # Use the Mongo created in the service container above to test against
          CI_MONGO_HOST: mongodb
          HYPOTHESIS_PROFILE: ci_${{matrix.os}}
          PYTEST_XDIST_MODE: ${{inputs.pytest_xdist_mode}}
          ARCTICDB_PYTEST_ARGS: ${{inputs.pytest_args}}
          # Testing with 0(no version cache) and -1(will use default timeout)
          VERSION_MAP_RELOAD_INTERVAL: ${{matrix.version_cache_timeout == 'NoCache' && 0 || -1}}

      - name: Collect crash dumps (Windows)
        if: matrix.os == 'windows' && failure()
        uses: actions/upload-artifact@v4
        with:
          name: crashdump-${{env.distinguishing_name}}
          path: ${{env.LOCALAPPDATA}}/CrashDumps/

      # Fallback if the clean up at test fixutre tear down fails due to crash or etc
      - name: Remove AWS S3 testing account and credentials and STS roles
        if: ${{ always() && env.real_tests_storage_type == 'AWS_S3'}}
        run: |
          echo "env.real_tests_storage_type: [${{ env.real_tests_storage_type }}]"
          python -c "
          from arcticdb.storage_fixtures.s3 import real_s3_sts_clean_up
          real_s3_sts_clean_up('${ARCTICDB_REAL_S3_STS_TEST_USERNAME}', '${ARCTICDB_REAL_S3_STS_TEST_ROLE}', '${ARCTICDB_REAL_S3_STS_TEST_POLICY_NAME}')
          "
        continue-on-error: true

      - name: Disk usage
        if: always()
        run: set +e ; du -m . "${PARALLEL_TEST_ROOT:-/tmp/parallel_test}" | sort -n | tail -n 100 || true; df -h
        continue-on-error: true

      - name: Upload the logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-${{env.distinguishing_name}}
          path: |
            ${{runner.temp}}/*test*
            
