Review the current PR. Use `gh pr diff` to get the diff, and `gh pr view` to get the title and description.

You are reviewing a pull request for ArcticDB, a high-performance serverless DataFrame
database with a Python API backed by a C++ data-processing and compression engine.
It supports S3, LMDB, Azure Blob Storage, MongoDB, and in-memory backends.

Review the PR diff thoroughly. Provide two types of feedback:
1. **Inline comments** on specific lines where you find issues or concerns.
2. **A summary comment** with a checklist of overall quality gates.

You have access to the full repository checkout. Use it to look up relevant code,
understand existing patterns, and verify that changes are consistent with the codebase.
Do not assume file locations - search the repo to find the relevant implementations.

---

## REVIEW GUIDELINES

Apply these guidelines based on which files are changed. Skip sections that are
irrelevant to the PR. Focus on substantive issues, the style nitpicks should be picked up
by formatting enforced by `build_tooling/format.py`, but if you spot better patterns
highlight them in the comments.

---

### 1. PR TITLE & DESCRIPTION

Check the PR title and description for quality before reviewing the code.

- **Title clarity**: The title should be a concise, grammatically correct summary of
  the change. It should describe what the PR does, not how. Avoid vague titles like "Changes"
  or "WIP".
- **Grammar and spelling**: Check the title and description for typos, grammatical
  errors, and unclear phrasing. Suggest corrections inline.
- **Description completeness**: The description should explain *what* changed and
  *why*. Compare the description against the actual diff - flag any significant
  changes in the diff that are not mentioned in the description.
- **API changes called out**: If the diff modifies public Python API signatures,
  return types, default values, or exception behaviour, the description must
  explicitly call these out. Missing API change callouts should be flagged.
- **Breaking changes labelled**: Breaking changes or on-disk format changes must be
  highlighted prominently in the description and the PR should have appropriate labels
  (e.g., `enhancement` or `bug` for autogenerated release notes).
- **Linked issues**: If the PR fixes or relates to a GitHub issue, it should reference
  it (e.g., "Fixes #1234"). Check if the description follows the PR template
  (Reference Issues, What does this implement or fix, Checklist).
- **Scope match**: The title and description should match the scope of the actual
  changes. Flag if the PR does significantly more or less than what the description
  claims.

If the title or description has issues, include them in the summary comment (not as
inline code comments). Suggest improved wording where appropriate.

---

### 2. PUBLIC API STABILITY (Python)

ArcticDB has a layered Python API: `Arctic` -> `Library` -> `NativeVersionStore`
-> C++ `PythonVersionStore` via pybind11.

Check for:
- **Breaking changes** to public method signatures on `Arctic`, `Library`, or
  `NativeVersionStore` (parameter names, types, defaults, return types). These must be
  called out explicitly in the PR description and labelled appropriately.
- **Deprecation protocol**: Removed or renamed parameters must go through a deprecation
  cycle.
- **Library option defaults**: Changes to defaults like rows_per_segment,
  columns_per_segment, dynamic_schema affect all users silently.
- **QueryBuilder**: New operations must follow the existing fluent builder pattern and
  produce correct expression trees.
- **Return type changes**: Changes to return types from read/write operations affect
  downstream code.
- **Exception hierarchy**: The Python exception hierarchy bridges C++ categorized
  exceptions. Changes must preserve error codes and exception types that users may catch.
- **Docstrings**: Public methods must have accurate docstrings (these feed the generated
  API docs). Check that parameter documentation matches the actual signature.

---

### 3. PYTHON-C++ BOUNDARY (pybind11 Bindings)

Check for:
- **GIL management**: Long-running C++ operations must release the GIL
  (`py::call_guard<py::gil_scoped_release>()`). Callbacks into Python must reacquire it.
  Incorrect GIL handling causes deadlocks or crashes.
- **Exception translation**: C++ exceptions must be translated to appropriate Python
  exceptions with proper error propagation.
- **Object lifetime**: pybind11 `py::object`, `py::array` held across GIL release can
  cause use-after-free. Verify numpy array lifetime management.
- **Type conversion**: Python-to-C++ tensor conversion must handle None/NaN, empty
  arrays, and dtype edge cases.
- **Normalization checks**: Data validation at the Python-C++ boundary must not be
  bypassed - this risks data corruption.
- **String handling**: Python str/bytes to C++ conversion must handle Unicode correctly
  (UTF-8 encoding).

---

### 4. MEMORY MANAGEMENT (C++)

ArcticDB has a custom memory management system with detachable memory for Arrow interop,
buffer pools, chunked buffers, and an optional slab allocator.

Check for:
- **Allocator usage**: Detachable memory allocation must use the correct `AllocationType`
  (DYNAMIC, PRESIZED, DETACHABLE) matching the intended lifetime.
- **Buffer ownership**: Buffers must not be used after being moved or returned to a pool.
- **ChunkedBuffer**: Non-contiguous buffers used for column data must have correct chunk
  management and iterator validity.
- **RAII compliance**: All resources (buffers, storage handles, locks) must use RAII.
  Look for raw `new`/`delete` or manual resource management.
- **Move semantics**: The codebase provides move/copy control macros
  (`ARCTICDB_MOVE_ONLY_DEFAULT`, `ARCTICDB_NO_MOVE_OR_COPY`, etc.). Accidental copies
  of large buffers are performance bugs.
- **Slab allocator compatibility**: When the slab allocator is enabled, allocation
  patterns must be compatible.
- **Memory leaks**: New leak sanitizer suppressions need strong justification.

---

### 5. ON-DISK FORMAT & BACKWARDS COMPATIBILITY

ArcticDB guarantees that newer clients can read data written by older clients and vice
versa.

Check for:
- **Binary layout changes**: Packed structs like FixedHeader, Block, EncodedField, and
  FieldStats have fixed sizes validated by static_assert. ANY change to these structs
  breaks all existing stored data.
- **Magic numbers**: Section markers in encoded segments must not be changed - they
  break segment parsing.
- **Encoding version compatibility**: Both V1 (protobuf headers) and V2 (binary headers)
  encoding paths must remain readable.
- **Key type changes**: The `KeyType` enum values are persisted. Adding is fine,
  removing or renumbering breaks storage.
- **ValueType enum**: Must match the protobuf descriptor definition exactly. Adding new
  types requires fallback handling in older clients.
- **Version chain markers**: Special marker strings used in the version chain
  (`__write__`, `__tombstone__`, `__tombstone_all__`, etc.) must not change.
- **Protobuf schema**: Changes to `.proto` files must be backwards-compatible (no
  removed/renumbered fields, use `optional` for new fields).
- **Compat tests**: The repository has pinned-version compatibility tests. On-disk
  format changes must update these.

---

### 6. CODEC & COMPRESSION

Check for:
- **Segment structure**: The segment format (in-memory and serialized) affects all
  read/write paths. Changes here are high-risk.
- **Codec correctness**: LZ4, ZSTD, Passthrough, and PFOR codecs must produce lossless
  compression/decompression roundtrips.
- **Hash integrity**: Block-level hashing ensures data integrity. Changes to hash
  computation or verification break data validation.
- **Encoded field ordering**: The encoded field collection must maintain deterministic
  ordering.
- **Default codec changes**: Changing compression defaults affects storage efficiency
  and compatibility.
- **Buffer size calculations**: Encoding/decoding size computations must be exact.
  Off-by-one errors cause buffer overflows.

---

### 7. STORAGE BACKENDS

Each backend (S3, Azure, LMDB, MongoDB, Memory) implements an abstract Storage
interface with methods for write, read, remove, iterate, and key_exists.

Check for:
- **Interface compliance**: New backends or changes must implement ALL required virtual
  methods from the storage interface.
- **Atomic write support**: The atomic write classification (NO, YES, NEEDS_TEST) must
  be correct. S3-compatible stores may not support conditional writes. Incorrect
  classification causes data corruption under concurrent access.
- **Error handling**: Storage operations must throw appropriate storage exception
  subtypes. Generic exceptions lose error context.
- **Key encoding**: Storage key format must be consistent across backends.
- **Mock consistency**: Storage mock clients must mirror real client behaviour for
  tests to be meaningful. Mock updates should accompany implementation changes.
- **URI parsing**: Python adapters parse URI schemes (s3://, lmdb://, azure://, etc.).
  URI parsing changes can break user connection strings.
- **Credential handling**: No hardcoded credentials, tokens, or secrets. Check for
  accidental logging of sensitive connection parameters.

---

### 8. VERSION ENGINE & CONCURRENCY

The versioning engine manages a version chain (linked list of versions per symbol),
a symbol list, and snapshots with a lock-free concurrency model (last-writer-wins).

Check for:
- **Version chain integrity**: The version map manages the linked list of versions.
  Incorrect pointer manipulation corrupts the version chain.
- **Tombstone handling**: Deletion uses tombstone markers, not physical deletion.
  The load type controls how much of the chain to load. Incorrect load types cause
  stale reads or missed tombstones.
- **Symbol list consistency**: The symbol list is a lock-free concurrent data structure
  with a compaction phase using LOCK keys. Race conditions here cause `list_symbols()`
  to return incorrect results.
- **Snapshot safety**: Snapshots create named references to multiple versions and must
  be atomic - partial snapshots are corrupt.
- **De-duplication**: Segment dedup tracking must be correct. Incorrect dedup causes
  data loss (pointing to wrong segment) or storage waste.
- **Schema checks**: Schema compatibility validation on `append()` / `update()` must
  not be bypassed - this allows schema corruption.
- **Batch operations**: Batch read/write must maintain atomicity guarantees.

---

### 9. PIPELINE & QUERY PROCESSING

The pipeline handles read/write data serialization, and the processing layer implements
pushdown query execution (filter, project, aggregate, resample).

Check for:
- **Slicing correctness**: Data is split into row/column segments (default 100K rows
  x 127 columns). Off-by-one errors in slice boundaries cause data loss or duplication.
- **Frame reconstruction**: Reassembling segments into DataFrames must preserve column
  ordering, index alignment, and dtypes exactly.
- **Query clause composition**: Clauses use a polymorphic interface and must compose
  correctly (filter -> project -> aggregate -> concat). Incorrect clause ordering
  produces wrong results silently.
- **Expression evaluation**: Type promotion in expression evaluation must follow NumPy
  rules. Signed/unsigned comparison is a common source of bugs.
- **Aggregation correctness**: Groupby (sorted and unsorted paths) must correctly
  handle NaN, empty groups, and overflow for sum/mean operations.
- **Resample operations**: Time-based resampling must handle timezone-aware timestamps,
  DST transitions, and irregular intervals.
- **String pool**: Interned string management must be correct - pool corruption causes
  garbled string data.
- **Sparse data**: Sparse column handling must correctly track present/absent values
  via bitmap format.

---

### 10. ASYNC & THREADING

The async infrastructure uses Folly executors with CPU and IO thread pools.

Check for:
- **Thread pool sizing**: Unbounded task submission can exhaust memory.
- **Future composition**: Folly futures/promises must be correctly chained. Dropped
  futures silently discard errors.
- **Deadlock potential**: Blocking on a future from within the same thread pool causes
  deadlock. Verify no `.get()` calls from within task callbacks.
- **Exception propagation**: Exceptions in async tasks must be captured in the future
  and re-thrown on `.get()`. Swallowed exceptions cause silent data corruption.

---

### 11. ARROW INTEGRATION

Check for:
- **Arrow output format**: Arrow-native output must produce valid Arrow arrays/tables.
- **Memory interop**: Arrow buffers may be backed by ArcticDB's detachable allocator.
  Lifetime management must be correct across the boundary.
- **Type mapping**: Arrow types must map correctly to ArcticDB's ValueType enum.
  Missing mappings cause silent data truncation.
- **String format options**: String representation controls must handle both small and
  large string types.

---

### 12. COLUMN STORE & DATA TYPES

Check for:
- **Column type handling**: All ValueType variants must be handled in switch statements.
  Missing cases cause undefined behaviour.
- **Type promotion**: Mixed-type columns (e.g., int + float) must promote correctly
  following the descriptor merging logic.
- **Empty column handling**: Empty DataFrames, columns with all-NaN, zero-row appends
  must not crash or corrupt state.
- **Statistics**: Column statistics must be updated correctly on append/update.
- **Memory segment**: The central in-memory data structure must have correct reference
  counting, column access bounds checking, and iterator validity.

---

### 13. ENTITY & KEY TYPES

Check for:
- **Key immutability**: AtomKey and RefKey should be treated as immutable after
  construction. Mutation breaks hash-based lookups.
- **StreamDescriptor changes**: The schema descriptor for a symbol must remain
  backwards-compatible.
- **FieldCollection**: Compact typed field collections must have bounds-checked
  index-based access.
- **NativeTensor**: The numpy-to-C++ tensor bridge must correctly handle stride and
  shape matching numpy's memory layout.

---

### 14. PROTOBUF & SERIALIZATION

ArcticDB supports protobuf versions 3 through 6 via separate generated bindings.

Check for:
- **Proto backwards compatibility**: No removed or renumbered fields. New fields must be
  `optional`. Changing field types breaks deserialization.
- **Multi-version support**: Changes to .proto files must regenerate bindings for all
  supported protobuf versions.
- **msgpack compatibility**: The msgpack interop layer must preserve wire format.
- **Protobuf mapping**: C++ proto-to-native type mappings must be correct. Mapping
  errors cause silent data corruption.

---

### 15. CODE DUPLICATION

Check for:
- **Copy-pasted logic**: Look for blocks of code in the diff that duplicate existing
  logic elsewhere in the codebase. Search the repo to confirm. Suggest extracting
  shared functions or using existing utilities instead.
- **Parallel implementations**: When the same concept is implemented in both C++ and
  Python (e.g., type handling, validation, error mapping), verify the implementations
  are consistent. Divergence causes subtle bugs.
- **Storage backend boilerplate**: Storage backends share significant structure. If a
  new backend or modification duplicates patterns from other backends, suggest using
  shared base class methods or templates.
- **Test duplication**: Tests that duplicate existing test scenarios waste CI time.
  Check if similar test cases already exist and suggest parametrizing or extending
  existing tests instead of creating near-identical new ones.
- **Error handling boilerplate**: Repeated try/catch or error-wrapping patterns should
  use the project's existing error handling macros and utilities.
- **Switch statement duplication**: ValueType/KeyType switch statements are common in
  the codebase. If the PR adds a new switch that mirrors an existing one, suggest
  using dispatch tables or templated approaches that the codebase already provides.

---

### 16. TESTING ADEQUACY

Check for:
- **Test coverage for changes**: Every behavioural change should have corresponding
  tests. C++ tests use Google Test, Python tests use pytest.
- **Edge cases**: Empty DataFrames, single-row/single-column, maximum segment size,
  Unicode/special characters in symbol names, concurrent access.
- **Integration tests**: Changes to storage backends or the version engine should have
  integration tests.
- **Property-based tests**: Complex logic (especially around type promotion, slicing,
  aggregation) benefits from hypothesis/property-based tests.
- **Benchmark regression**: Performance-critical paths have C++ and Python benchmarks.
  Check for O(n^2) patterns, unnecessary copies, or allocation hotspots.
- **Backwards compat tests**: On-disk format changes need compatibility tests against
  pinned older versions.
- **Mock consistency**: If storage mocks are changed, verify they still accurately
  represent real backend behaviour.

---

### 17. ERROR HANDLING & LOGGING

Check for:
- **Error codes**: The project uses a macro-generated ErrorCode enum with categories.
  New errors must use the correct category and a unique code.
- **Preconditions**: Internal assertion macros should be used for invariants, not for
  user input validation.
- **User-facing errors**: Errors that reach Python must have clear, actionable messages
  referencing the operation that failed.
- **Log levels**: The project uses spdlog. Debug-level logging in hot paths causes
  performance degradation. No sensitive data in log output.
- **Storage exceptions**: Storage-specific exception types must be used instead of
  generic exceptions to preserve error context.

---

### 18. BUILD SYSTEM & DEPENDENCIES

Check for:
- **CMake target updates**: New source files must be added to the correct CMake target.
  Header-only changes don't need CMake updates.
- **Python dependency changes**: Changes to install_requires or build dependencies
  affect all users. Version ranges must be tested.
- **Submodule updates**: Changes to git submodule references need careful review.
  Verify the new commit is from the upstream repo and is on an official
  branch/tag (not an unreviewed fork or arbitrary commit).
- **Compiler compatibility**: C++20 features must work on all supported compilers
  (GCC, Clang, MSVC) as defined in the CMake presets.
- **Sanitizer compatibility**: Code must work under ASan, TSan, UBSan. New sanitizer
  suppressions need justification.

#### vcpkg & C++ Dependency Management

The project uses vcpkg (as a git submodule) with a manifest file and custom overlay
ports. Check the vcpkg manifest, overlay ports, and submodule reference for issues.

- **vcpkg submodule pin**: The vcpkg submodule should point to a specific commit on
  the official `microsoft/vcpkg` repository. Verify the commit exists on the upstream
  master branch. Unexplained jumps forward/backward in the vcpkg baseline are risky.
- **Manifest version pinning**: All dependencies in the vcpkg manifest must have
  explicit version pins or version overrides. Unpinned dependencies can resolve to
  different versions across builds, causing non-reproducible builds.
- **Version override consistency**: The `overrides` array in the vcpkg manifest must
  match the versions actually available at the pinned vcpkg submodule commit. A
  mismatch causes build failures.
- **Custom overlay ports**: The project has custom vcpkg overlay ports for patched
  dependencies. If a dependency is upgraded, verify the overlay port is updated to
  match (or removed if the upstream port now suffices). Stale overlay ports silently
  override newer upstream versions.
- **Builtin-baseline**: If the vcpkg manifest uses `builtin-baseline`, it must match
  the checked-out vcpkg submodule commit hash. A mismatch means the baseline
  resolution differs from what vcpkg will actually use.
- **New dependencies**: Adding a new vcpkg dependency must include a version pin.
  Check that the dependency is available at the pinned vcpkg commit and does not
  introduce license-incompatible or unnecessarily large transitive dependencies.
- **Removed dependencies**: Removing a dependency from the manifest should also
  remove any corresponding overlay port, CMake find module, and usage in CMakeLists.
- **Conda environment sync**: If a vcpkg dependency version changes, the conda
  development environment file should be updated to match so that conda-based builds
  use the same version.

---

### 19. SECURITY

Check for:
- **No hardcoded credentials**: Secrets, tokens, API keys must not appear in code or
  config files.
- **Input validation at boundaries**: URI parsing, user-supplied symbol names, metadata,
  and query expressions must be validated.
- **Buffer overflows**: C++ code handling raw bytes (codec, storage, pipeline) must
  validate sizes before accessing memory.

---

### 20. PERFORMANCE

Check for:
- **Unnecessary copies**: Large objects (segments, buffers, DataFrames) should be moved,
  not copied. Watch for missing `std::move`, pass-by-value of large types, or
  `const std::string` parameters (prefer `std::string_view`).
- **Allocation patterns**: Hot paths should minimize allocations. Prefer stack
  allocation, buffer reuse, or pool allocation over repeated heap allocation.
- **Algorithmic complexity**: Watch for O(n^2) patterns in loops, especially in version
  chain traversal, segment iteration, and query processing.
- **Lazy evaluation**: Large data operations should be lazy where possible (ArcticDB
  supports lazy DataFrame evaluation).

---

## SUMMARY COMMENT FORMAT

After reviewing, post a summary comment with this checklist. Mark items as pass/fail/NA
based on what the PR touches:

```markdown
## ArcticDB Code Review Summary

### API & Compatibility
- [ ] No breaking changes to public Python API (`Arctic`, `Library`, `NativeVersionStore`)
- [ ] Deprecation protocol followed for removed/renamed parameters
- [ ] On-disk format unchanged (or migration path documented)
- [ ] Protobuf schema backwards-compatible
- [ ] Key types and ValueType enum unchanged (or additive only)

### Memory & Safety
- [ ] RAII used for all resource management
- [ ] No use-after-move or use-after-free patterns
- [ ] Buffer size calculations correct (no overflow potential)
- [ ] GIL correctly managed at Python-C++ boundary
- [ ] No accidental copies of large objects

### Correctness
- [ ] All ValueType/KeyType switch statements exhaustive
- [ ] Edge cases handled (empty data, NaN, Unicode, concurrent access)
- [ ] Error codes unique and in correct ErrorCategory
- [ ] Storage backend interface fully implemented
- [ ] Query clause composition correct

### Code Quality
- [ ] No duplicated logic (search repo for existing utilities)
- [ ] C++ and Python implementations of shared concepts are consistent
- [ ] Tests are not duplicating existing test scenarios

### Testing
- [ ] Behavioural changes have corresponding tests
- [ ] Edge cases covered in tests
- [ ] Integration tests for storage/version engine changes
- [ ] No regression in existing test expectations

### Build & Dependencies
- [ ] New source files added to CMakeLists.txt
- [ ] Dependency changes justified and version-pinned
- [ ] Cross-platform compatibility maintained
- [ ] vcpkg submodule points to official upstream commit
- [ ] vcpkg manifest versions pinned and consistent with submodule baseline
- [ ] Custom overlay ports up-to-date with dependency versions
- [ ] Conda environment synced with vcpkg dependency versions

### Security
- [ ] No hardcoded credentials or secrets
- [ ] Input validation at system boundaries
- [ ] No buffer overflow potential in C++ code

### PR Title & Description
- [ ] Title is clear, concise, and uses imperative verb
- [ ] Title and description are free of typos and grammatical errors
- [ ] Description explains what changed and why
- [ ] All significant changes in the diff are mentioned in the description
- [ ] API/breaking changes explicitly called out in the description
- [ ] Linked issues referenced where applicable
- [ ] PR labelled appropriately (enhancement, bug, etc.)

### Documentation
- [ ] Public API docstrings accurate
- [ ] Breaking changes flagged with appropriate labels
```
