{"project": "ArcticDB", "project_url": "http://arcticdb.io/", "show_commit_url": "https://github.com/man-group/ArcticDB/commit/", "hash_length": 8, "revision_to_hash": {"1": "8aca4e9cc678650de36a12d31d2e7f3c4cc9a03b", "217": "b70029d5724883dc903fc6a9f95b17de33dde93d", "341": "03b8e00539fc031dcabd3b2ec3accfbc9e8e3587", "358": "bf01831fd860837eec04f0437ee7ee54aca60d00", "484": "c90a21a611b5c6ec2ef4b049981ac5c2ccb8ad08", "567": "4daa3de40dd67ebde39d48797ce05240dba1e05a", "575": "14209d33c87c3325c5a77879a3cfa036894fea52", "577": "95806216ea1bdfbbc0930453adc1db42d3a4e624", "579": "525e6c28ae67faa1fe4ae2bd39d287a75fd7b083", "641": "a1b39e939f509b90a6b4f58c0c846cdfd22ae8fc", "714": "7f92cbca1bab219847ea7c9b6ccfced6b3de3565", "742": "357ead11b55196b3524196164733d2b072e09449", "757": "cf1a7326d8d648f75ec0daa3044fef4dfa71c7c5", "772": "bdf81500fef77a27f59d0541a8405ff3dfe44029", "800": "d7b6e1b40613f6b4b559a6777d1ac13e63e280bc", "858": "0a87f5c08de754cfcecf8261ad2c74119069ae65", "932": "884eedc27666ab6b208a5bb7b4cc9bdb492ff58e", "1023": "3c98e789888d71133a5cccb1ad6d27a0d766f316", "1035": "2f25723f0570eacc63e03fc09de6e9106b61dc4a", "1053": "0f3dd263209c35022a0eac43583e11c7c38742fc", "1054": "1bb8fa063f725ad00282ff18eaea3754c8b085da", "1071": "09aaf7b32b5f5750e1515e7ffaea3779d5e051dc", "1107": "7a449cddb6e7a2164ea03912a934d423a3925455", "1129": "beab8372e6b276a57f4f6d6384b1e7bd8dc59704", "1141": "7baa141aa4a73dea538d6b257dda2f334c85bb46", "1165": "a54daaa294d571a2adbca6acf96edd3c3b550d96", "1183": "1e1d0d70542c2df9f5dc844c037b5136f77771dd", "1198": "0df1889b2e3cef53c00ccc10155186a31341e0ce", "1263": "5f61b27cd7898f376418e2ff7b4e0fcc61e712fc", "1337": "d4fb4834d5645785a2999381572456e2e1a0e2bc", "1346": "bccff006ed94d8a7530367daf67487a2ae2d471a", "1350": "7203c11b2dc111c6f5bc67d42ce66d48fdbafa05", "1365": "1e5b739d56edf61af3bb4d572706262432f72633", "1366": "4f5a6db83710619cb98645cca8b375778c3b7872", "1489": "91994f71840b1b2637d5dd0a3ea16d13d3640dfb", "1532": "8524cbdd76c99d5d4c580f0abc31fa52c9bcc922", "1541": "7b5853c9f38428ae84ce65c6703fd9eb03d7979d", "1715": "8c50cab75bb0a2360dfdbfe454ecda195a3464b3", "1739": "5a38563045eb98941710984f3a8a39583f23cf2f", "1933": "e5c626b209d61639d21a158b0d5cde1ae9b9a31c", "1955": "ada51233bccdab968692e952d40f1c62323cd713", "2080": "1a90cc53c775fe993b756fcdc8648ae643051780", "2129": "545807a2413a511c4f6ef64f7c679fc32f28e650", "2135": "f60ef8fb69e5c3e2939a0ee5d1f55d9017649a7f", "2180": "91b1d563714b2f3c262ceb17e4f3bd8646415a33", "2181": "05fc1a7f13e8c3cf21ff671f7a942687f88c5b42", "2229": "6b72e8652824b991eceb96aea67b2ca4ee68de3b", "2256": "b36736c7d0e6a6c06b809e755964db5ca138eeef", "2329": "7d8c24e75d6897287ba6b97a2c059af026402941", "2331": "5fab651aaee4dd50e7cf97fe9c3f85bf618c3001", "2359": "66171bedffdd5968432e3a799c506b011a34debe", "2463": "052b8e3c61e64448605171e78dccb9b6da9f1adb", "2551": "de6e3cb6f3856edaf55d5ce2ecab8dc01bb05b6c", "2560": "19d67f36b3c4636846c2825aa07d5c92b4796c99", "2581": "03fb62206e9404360616552a41b1934dfd2eeef5", "2585": "efeb3c277af32e0b3f09a481b3fb360faaed2b6a", "2613": "a8664a622b8f4c1f6aa9cacf2742145da1907d00", "2705": "724ecccd67548b5bbd4acd75bf46b317055448d6", "2710": "c14082dc9b021a0fa48185505a8a2640f98c1910", "2711": "c696f598cb7c9e0ae77227007a86e1868be274ac", "2714": "ad257eb14e59f533c4588723b7c89e2856f02c3f", "2720": "b8cf5c7f45c9c1f8aeacfb9705a9acba77ee247a", "2722": "1c5a41918fd650471750062295c9369e1679ebd4", "2726": "89fc8bb90dcc29fed6e39ca3ddd5b9824c64b2b2", "2729": "9a4fadce8190395c4a873037babce412ced6f987", "2734": "8240ab2220494344868d6a1031b58e3679c16e9b", "2741": "47967de95128ad6f69a994bfbf82c1f156319f5f", "2742": "b2c820c700eeb48514c76fd329995217c17118f9", "2751": "cd632fc84df936930989dc3e77ca541927cd8d21", "2760": "13b807c06ab950134a861efc8f8fdfa46076a6db", "2763": "65cb44d99e29a1dcedad76fbcda6af0e6ce6c7eb", "2772": "72f3730d81b3d1d86fd98832c825936f4a290e5b", "2774": "ac846a89dc11130c590798c4d8fc1ae690a37783", "2777": "5af60035e3eb9b74949ba1791e6e83c8d2cc6699", "2790": "e0aa4eaa54ab27907594bf4c6038f64bebf37509", "2792": "efd869096fb69d07ba6bd5635b3e64e799283c5c", "2794": "bf1d7885241e145b7e2c95b0ac49d107f4c873d3", "2797": "0bf5d1b74067ef4fa870f3d7665ec6d792b99d06", "2825": "356972d3c4e78efb84602d3f764dc58bf77d5bbf", "2831": "911a55c5543626d7c0da39e6db2e7d39d716cf60", "2842": "ff4309d06fe2fa506d574264dfccfcb9e7306a69", "2848": "7205a86a8ce0ef45066d78f0357f5e08031a157b", "2854": "9e3a47dd0efd27e37a4047142612b17da4cd9ddc", "2865": "03ff0dc0cde52481a3f2ab2243c35d3c26813322", "2872": "67c78fbfa02e3648ba52c0b76b83659a64696d01", "2873": "b367e7e9b4bfcb1c848a1db92352e8de2fd91bd9", "2884": "1cd7d33bf88f7148ae0aad8f525d5f0f4392147c", "2886": "483019a95f2d841b99ddf9cdb793dfa21398151f", "2900": "841bf40d284b0e5064eae47102d1800de2a446fe", "2903": "c81f8ad2b8f0831cfa855e6b3dc954b180c4c941", "2906": "9b2cbb9f1bf58b70f09370cd74de691083f8e6f6", "2918": "f46592d05942605da6336f8dbf9cf671d5a1d2d4", "2926": "8cf8279b45649f03582202e169aeb6a0f2978732", "2929": "cf391165aed6a3b89b61338c5b4c7f4e0ce92b44", "2932": "31853f18bd5b4ec291515860f9f572d01d6e67b6", "2936": "66a5d19c4a2c3b2423b52a68f984ca8d9b52afce", "2941": "dc6764fb5ef2d159f9f2e35bcf9c68493d94b140", "2944": "cb607222a64cc53012c65ebfbdbbb98090c1125b", "2947": "d404cdcbcd181dc021a162ed589373f44ee74bd1", "2950": "dfeb11207051437388e4574ab2b9636893415150", "2952": "083e0f602910ad86524d719cab1ae367eb06a5a5", "2973": "7bf8b4aa087ed4226d99111ceab18095da1d358c", "2984": "d0f7487f162fcc22239ecd84ae705773cdf9ef51", "2995": "51b8e2a327c9facebe27289946a6d80c38a67982", "2999": "eac6d09dfe67665b1b430569c91f91961987e027", "3005": "a21da2371ca7875f39ff6d14462ed16d0520b9c9", "3030": "b9aabf5942884764f6c641cd9429a5d6f47cd6a1", "3032": "471d89912c91fdae1d2c507f10d5fedd234f9edf", "3033": "8637fa2da367cf39cea88c27ab79a17f9ac308c3", "3038": "86e9f769e3fb555f0037674672c75cd3120132f7", "3041": "bbf594f8c200231bcf50a81c64e396bb42d19cda", "3052": "2881422c59fc8dd0f070daec9d3f495627f68b06", "3063": "6fc2aaa50fff579b178413a7bfd4b5c9c811e214", "3075": "0fc441cf64d46994cdb5b766e68c3d81b60e0086", "3077": "dd4617e309c5b31cebe79816ea43bf1136b59365", "3081": "683d833a675bd18b38417f104b2d0d5e6b4e3d97", "3084": "89817db81728b323ca296ba18a957c8aacfe11bb", "3105": "28feeb340c52664d5f9aec1363f47a499a0bf40e", "3111": "2858895ea4d1b2e2ec1602067c1bf16d88f9a8c7", "3112": "273b43f6b0f6cad1d490d917e2b1a150089c5683", "3120": "ec65b8e5f2d3bd556ecd1e3d73cb23705cafd7e8", "3124": "498c3311dc08f5c8872ccac0ef3dfb278776699c", "3133": "102b1c0ef09db96014a65d6fea007854744e0906", "3137": "030fe5ac1e0a7a94208844c41a3c5e6bfb26fb85", "3158": "83deb987adf6eda8fd1381b3834ddd7e702e051b", "3162": "e0456876ba5c0f749756f6de22c46d0d923f082d", "3168": "6ba2ff168b556044b54a8d6c37ee8e7ff17acf75", "3179": "4d092838343e729ef7579c947b5e8b7dc83bb563", "3191": "7df641fba906a2cba0ce406d4161dff27f6b3946", "3197": "17ef7f0c0b3970227b5fada52e7e2832615ef8d0", "3209": "8461ff17bb6f4768dbed554e52d55afc7b8d681f", "3219": "1bb57d7b02fb2a101831dc9ec1a6faf1dd3160ee", "3225": "bdf44d6a0268c55fcd5d3da619812325e0e8b44f", "3235": "2cad4622122e730032271a0e936a8c590f611cd4", "3248": "a398c24a1f875cc848d016fbfdc0347f33e81d63", "3252": "c6a0567bb3f64537c350cebb23fd97043f6e577e", "3254": "dd072b12b24ba3b3e6d033d169e2bf4e32f925e7", "3255": "56e88c19073613255cf3b606f88de6652af3987f", "3265": "07fcc09413230f136b3903deaa9262ce466bb6de", "3273": "3109deeec996ccc1c4d845c7fea9d7ab9edf2746", "3279": "52cdc81b7f175ad410b4a3626b366e3ab1aec3a1", "3281": "afaba9ad4ebdf93e8b9a9d814eba3309f34dd830", "3283": "392c9abbfee136203f31fcd4e82db59ea72516a2", "3287": "008fa54cfe123c9ac4276643ebb5e22126d7a3c0", "3295": "dd7894f2f55c13293b699826aeaf603d0793a793", "3299": "b157c924171e5bfdd4e0a2658c611917037bcaa5", "3302": "d607f94ff380497dbbf2e3811cd77d3c6113ba80", "3305": "819339e8da8325448547792a59b9656e93d17977", "3309": "5bae9ab9726635369a70a9eea95ded65215706f5", "3314": "d4e6854093138cb612a6c62b19c19a0b17d9b5b1", "3316": "70400ec7517aaf3e3b9ed598661c73c2f1a50cb1", "3321": "aa1eba344cd78605c537b37c3579664da4bc6258", "3335": "63ef514ff04a515731a0ae4b9c2eb92c1c125a8d", "3340": "9fe62421b97457f994f8d0188eebf0002038295b", "3347": "97493e6cf3b46f52204ce5ef436f1e828f6b0bb3", "3351": "8669ceee98779f78c76601272abd5663f9406f2c", "3355": "2a023239d721e69a1d645c9cd0e51232cc2815ba", "3375": "25ad2a3fded2c79b035e6bc346745863f81d3d57", "3383": "db3318090cec67f6da0812935dda29fff2605556", "3393": "07ead43a14552e2a847c253c8593a27ee508c46f", "3397": "1596c8012938bc59e0433cb56797be406cf8e445", "3413": "ef2687bf5f38e2a115f2224ab0b01a744add57e4", "3430": "3419d8f2ade78432ea180d68008e2c1a375ad08a", "3433": "2746fa0db1e8ee536bf325b328f43a8f20f94c66", "3434": "b20a6776b02e9c8120d748e85d648c26b59f0a33", "3436": "2da41171c82a027a2eadd4a0a057231d8c0eb755", "3438": "54f1979c522a7d5512926d5f8b224f5b44378317", "3442": "55e25f9205044c631c7e907e85b3d7a4047c9832", "3443": "a3f5330e47fff803fe830b416d166685ee349998", "3445": "8e4b241088f4ffd1778b0a60b935d0defe695277", "3453": "af83cfaf839507db02312e0f7402e77b0e2c87ed", "3457": "dba420f587bb8d760a4d7c89fdc60ad07a971f7f", "3459": "4719f9e87fef59b177068e6316bbc0a3eff98872", "3463": "717449fef3c05c55a02f486ee56c25f09f80e43a", "3465": "20ab093ea46c9b6e98b4a4ab9499db08f4f077a8", "3485": "b0e80e3c39ac95c3fc57ab3f040a73257cbf65fc", "3487": "1a56956ff0be53b4202dd7fbbfa4a86afd172d8e", "3489": "2032e351d53b3008165c1d6bca1157bcf339ddc8", "3491": "838804c66f6296fcb64efd068a79cc23641bed64", "3492": "56d8611b23361bebbdda20973fddf034573b6bdf", "3495": "9d073c626bd55011e1af924bd4aa5e609e277795", "3497": "85f7bf5d0ca6356de98991d9865be25f1c6b09d6", "3530": "d2ea6b112ca5c80a2ae89161c9a0067e61821169", "3534": "f4f2ddaa15a7e60c6895c2c3cb84f133f4a3adb3", "3536": "ec834d63a7067c933ce2172060238023250804b6", "3537": "f7955b31acab39f5307d9f1a7aa8261362516959", "3540": "50f1d04a514aeaaa85b0e4f5d6c35a121f6b927a", "3549": "a950d1333e34166914ba62e1a4214dbb59223675", "3551": "84678ecb47304bac0307916f75b36a5d2cc8fea3", "3555": "940f2e70ce361397a9d4abd4acc506aec2c9046a", "3563": "f24449b1aeeeb47e97f217afda8d517bac480e51", "3567": "63b8b0c553633a928e6b5cf2674913f5753a9c64", "3569": "c20c189fa835945b1cd61f48952f1dc9709638bc", "3571": "fc1d8f24c5f38bc26c01f94578ed302ed33a1164", "3573": "c9e5b04672c208f238b97825db3f6a22cff6d938", "3575": "97644bf95ff7303d0c1c10511384c303e954bf4c", "3577": "b0dad960f521565fe765eb3216c84d51010efd38", "3578": "3cdb63839988378b098d0550e9edd0e53d29d111", "3580": "5b1116cdc2a0262db512091560d2af7603230fe3", "3587": "fd0ec5d0800d9e87de32f9d07a1dc1dcbc137ed0", "3589": "501c0c17091d7efe8db9a3e68989c365a4e2e109", "3593": "9142dd4c72cc0cbec18e8c05c9b5d7fd4197b117", "3595": "f9ecab1a0ab37ca08ec2b8acea90a26dd5345b63", "3596": "8ee376319e69e82ad7e2ce1ce02758b71905c668", "3597": "600bd5a888aaec0bfacc3750553f7b03d1b22816", "3598": "0b9618581fafb726e65e56778e5011ec36380473", "3600": "c3156f020d9f7b511f46c528c65c2c6b1b28b5fc", "3602": "14ca39fb569343bd71f0647561d84173eefe1b50", "3605": "1132b9674d00defae4de7964c419cbdd6b8ed01f", "3608": "e96437d12ae18590ded9e806f5f0c8bf03b98ef8", "3609": "9f2ddbaadc7337051de966ad441c65f4fc89ef77", "3611": "ef93cdd5fd804f6285a70a10307f230740d16e45", "3613": "8a389577109930b4f7305240fcdea11d4be61042", "3618": "6abf878cea50847ea4321ee3dc9c7c52e0a18420", "3619": "19ce3611b2d5821a0c561b8570aff5d3d2f6bb7a", "3620": "86449ec46d107d52baaacb62cfd91b5b6922ca6f", "3630": "b2d4abbf50f24bd1dff1f05ef556f38c8e1013b0", "3632": "b0c174c5b888e2c310734382d806a07d54813421", "3633": "8d9bb180934ca31ee064720011a636ab755014c1", "3634": "ad59546d4564422c5f57b178a2dd913ad2f0e1b1", "3640": "cc4f51bec8bea6fc42261fab15b8cc378882f4db", "3641": "c5f9f4e9af53a2b11f090e12d433b7292854f4dc", "3650": "1442cec5094154a34af2aaa1c4ecb0b12240e871", "3661": "0ce95235fb65be01a7ff001a41c6094a3891bcb6", "3665": "f2e48c4ba5a25177ba233fff3e35428e470153b3", "3672": "c27b76ee98dbbc5c0a8f6c94df26a1cdb49220ca", "3673": "b61b6518b5341ed14b004461f693857bec73d75e", "3675": "17567b36711ce0be272dab85ea70d06601214d11", "3677": "4eb9eabc5b6e0ed28ddcaa9d3481ae2e0aea5b79", "3683": "32f4b8bd0f3d29a05a3356d21d753c8866fb68f9", "3684": "df1452dbeadd269b76ea936daf2cafd5cf7dab4f", "3685": "d19366dc31494df354a9023a5dca0fc2a0ec37ca", "3686": "c75d9d6c888861034e7f86d912eeef2bc1cf958f", "3687": "e765e8ddd4847cfdd644736b479dd1135e5eed56", "3692": "d11d99975d8b31efaf96b4315485303dd86c2596", "3693": "0414c6248f8a27b4411bce7e5de44620f8897500", "3697": "381f72f22ddba8e07ab072e8c77ee36abb5c2dfd", "3698": "e7a07227c546c4bc77c3d7e24aa9861f911c2aab", "3702": "7bdfd352038c7b29e177759f4e407c36757d4e98", "3703": "e21e5dae1b5e15738a60ba4d0f00c3843fc9ee7b", "3706": "e60258ecad49af3838539d8a01e9235eec93f83d", "3707": "f3c2867387a1203893651176d88abcaf3fadd2c0", "3714": "3cc729e591af5fa5b869f6b84d77b28a09736567", "3720": "6487f6c95b23085fde36f0b1f9bae95bfe99e26f", "3721": "0d8166338bcef0428e30787b34f0be7de36808a4", "3722": "3a6d8b026fc1b008ed40345eecc79551f605b3f2", "3723": "49e2b199f1d3c7e444f4275c0121f305272083e9", "3724": "65d8ddb5f16297541a89c3939923f4888b55702d", "3728": "b07cfbfdf910b0ce78d50ec32cf808255ce51692", "3729": "9ceee85c3cba4c548a8d104321febfacb8c61eaa", "3730": "b23dee0af5f790bf75a85b8e22b25d996e4797f2", "3731": "a38ac9da675fd65334c8378a611e50e7ed612ffd", "3737": "26a7ce88a945397d0cd71eb8516b7ac742828fe8", "3738": "5e6f279ab771ea123ee4a5978005d0d6f0beb707", "3739": "f6f0e5fe263fac68000f72ae791ae53e05e3b669", "3745": "2a6b3cdc3480126f6ce4b7c877339f412aee6444", "3746": "4c12a0ba6fb8efec6c0e8bfd540dea06f2857313", "3752": "e914bc179febea2af92e148b851e2b982a0c5f9f", "3753": "6698859b48cd6d93937950a06ba6f34af4be257e", "3754": "af98e8e9569d36e7e5fa34839fd71638dd59b316", "3756": "43a82b0df94ca333715c81d078cb31858127a6c5", "3757": "cb5051d530699806a587938f123e852e2fa766d4", "3761": "09a23a1028cf1645454d7dc6b660df3c630fb0db", "3766": "9b3eed2cf2ab2024dd4be1c351b1117046aee84b", "3769": "a02f90f7582ef69afc2336e5c04182c9230afd53", "3770": "5b1e8ba6215d8d625d5c86956e9939f7eaf767fb", "3771": "b81fdc1be84257b474e4dfaf5624b3ca5bfa7cdc", "3776": "48786216d10b73f279d52912bf2801ff73c01263", "3778": "69079c8e14870b2908f1d9ba8b83414b1d40b6f8", "3779": "860e1527e730c6b9d5266e7665f3306e6f40bfe9", "3780": "d2e568e12cc5b7ee90b622581171510fc8b90fa0", "3781": "1a012f2c69b73cba5227046f5cea009267d7fdd5", "3784": "ad0b3b4f4d370635d607d4200407822a57cb6592", "3788": "f9d955f71fbb7439ddce8724dda5ec8e23f6c171", "3795": "55b5ab8a753ae1ca33c866b3aa6baf60576082bc", "3796": "6931d3f67487dcf9f94b3cbd5bf6502818a210ff", "3799": "044ba062a50f35b6cd99eb421d3e6366f672d719", "3800": "159d9308cde3a6d5591440021ebf9ea6bc183816", "3803": "ed9f4c434c461f8a923c9925c8a59d61dc3f879a", "3804": "2f6e03c51c8123ae7ce5374ea2d5b75cf5270249", "3805": "a28bf7fe0b6ce32b6bacdd5a29546d62a53c7a99", "3807": "7cdcebcf227ad7089d5414259aaa63ba984ba448", "3810": "feb9b29f096e607875dc44b1dabf32d258116b53", "3819": "56eb4a1d9da16afde0ad2bcb3c1356ce5ee2e574", "3820": "edb779524d39e16e079ef6335cb28cf12003134e", "3826": "ffb266cd3c2e73968b99afcdfc9ef01f31fbe5a1", "3827": "32341ae7c5f283aeed2a7f92d38897287062e942", "3837": "75f311714ef69e454fae25a08eb6496b370a5b60", "3838": "5e9839d0dbb8497d2fb25fad282fdec5d79038ae", "3852": "d71a0bb585f48c29b4c9d3aa0a053fe35b4ec0af", "3853": "7ceac3bed8650eeee7b7101d59c18a9408d33d75", "3854": "64ee94de2dd11d6def2b12725e9653843f449431", "3855": "54f63a630bdd2d9f4102d865b8c300989d8e68d2", "3856": "b5a60379f844d0dbee09b4304bc6c988b42b72eb", "3861": "90d06d35f3c05e3887d2bdab5ffda3692e0bdeca", "3866": "bceddbe0233d1d2d31c2b1568c3e18094e3bacba", "3867": "e36471742ae074a5361ae4068abe95092a3185a0", "3868": "b9745d2bef0ea46a303159731fc305639ee7cd46", "3880": "24cc0b0cc8084550d1c19e57bc7c60092c815f32", "3885": "ac19c73ecf08431983455c4a79281ad6759304ff", "3888": "0c3fcfc90ac1276c2f39461df813d9d0a9fb8b0d", "3891": "f812539451bd20a9e592efefd2e2a270595dd495", "3899": "42610a8fcaa379c7afaca91966f44825a882d251", "3901": "1249086c8c90a86792279a763c060ac4336bd675", "3903": "8502cbd849a95c2d342c519e931a41c211e34c51", "3908": "17428a61ca89381950dead1eb1e29066b3cb143c", "3913": "f759485d3c17036456a7faa56a54d7f51aeffa75", "3918": "7a641d76caaa9bb842d50333b4c837334aff6534", "3921": "f4108f15cf600c1bbb103026d27c9721a16f4089", "3923": "f92a5ac78a11c9820acf0e1d423a1bc53058e7dd", "3927": "3fd6d7e9fc674ff03a44f85a746cf9a219c7f91f", "3930": "b354f05bbf7acc4887a770178cfae36b4c080265", "3932": "f42fe9f13d7cfd2f6f297ab8a4a69f0c412e8e34", "3933": "ef5e09ea5a9bcd22006177516d98f2435ad85b80", "3934": "4e5e2219abb8d8e21ffeec78bffd60289a5389ff", "3939": "18bb7bba6297d18e451019699ec9dab111bda16e", "3940": "c8f964fb288918d426fdc50223caba358e44f6ea", "3942": "453590168c91e6f8d3b66693220c912d0c0648f8", "3944": "339258cf1f3e3728366021181c58e23765871e88", "3948": "6407f7dda31007ffde537cdf2d2e616497302619", "3950": "d8b83216bbcb839e91d4519ac54a3fe2431f409b", "3951": "56a2046ae747ce5dd92ae21a9047140b8773dc2b", "3952": "4f1b23ce8204f474e8b9f2e50a32f083fa957a7c", "3953": "79a85ecd525b11223f64ac7c3b357c0189cd8567", "3954": "d504dfdf67a0d017050f26eef31060a50c75b106", "3959": "451f8fc337f60e3540e94c6efc313f011c340af0", "3960": "62bc71b6904681446b39e39cdd8ed3147003172c", "3961": "8596f67407472d78eeab12b5227e3b847073de7e", "3963": "757b4bd29e3a3de87a2265e2c021b9fb717854bc", "3968": "c60bf1f8b3551fb06a12f644029770be20045c72", "3970": "aa258b5de8e70196617cb17db7b68f0a833853ad", "3974": "a66f2ae6b28efeaeb0b6b1c41f565732dbf02b75", "3975": "d261361c07380893ecf2b5c709bee3e0d469c8f7", "3976": "f320e76b23bb856f6cb39beab80cb78a984c0cb2", "3978": "27fdea488f1c94e9ab91442a4ac4d45b563ee82e", "3980": "85ec29316356b2e27a0985f7b4d443c8d0754579", "3982": "734e50a2ce6abc7bdbc233eb19d142573a428e50", "3983": "4a2fc470e5f685536d5028b8d84dde1072e59f17", "3984": "958a6ffd4b83c725b6da4a99b3efadde102aa4cd", "3987": "0e313917e5e03e66ad373d579b6714c06d3f7d60", "3988": "27f8a15b7c32b6e7c3cb5e4b0d104552da6cb481", "3990": "1afd029c7d512edeb22625b0f10eac90cdaf2d55", "3991": "873ae6caa7e692bacb4a4a3103b3a12c5d69af02", "3995": "920391be22c4bda760ff44ac5f0a688b40745bbb", "3996": "5326160c1836fd02975cd7b8002040a4b2e29bc1", "4004": "c698c8b3a5a66cefb31271473f099b83aac88d4b", "4005": "4fb62800903c74b164f49f91f89c9790bd7aac88", "4014": "7513bf0c436044fcd6b09391b0a1d2655bdc3478", "4016": "653059eef56430b451e047ced4622f59240be3cb", "4017": "24fed2a7a38d15238b4fa893e410f34ba6a9e03b", "4019": "50e766e78add5ba7fbf5bcce80a21ac1c05dd2f8", "4020": "909ac33c058ca66bdf46d53b436b7d539d178c88", "4023": "25e4a079f3b3d005ee698d1878ad0a73704176a0", "4026": "39460a658a28b09a5a6fd619003927cf236fe5f4", "4028": "f6b8933bd858994be1995b9a91c6aeb40720dd14", "4030": "79ec8d7fabed4efc4cee4c8a67e9bca881b5fd35", "4036": "b69bf26e43595f9e86c9f616e5b1c08bda8f1a26", "4040": "f89fd125fe4fbc0e0eaca62953d7fd074441bfe6", "4041": "12b9acd769ad2debef00f2f0010ea7d53ccbb0fd", "4043": "0b58a88e4b6aa97eef3a2131aebda6e7a6411ae2", "4044": "8df06b37fd0f49a1f87c7fed48c2802b2c579a4a", "4045": "163d8bea261be3bb7e3f5c12ecc1b0dec698ac4b", "4046": "0ba710718f9e4c6c260d982e227011b2fc884fc9", "4047": "dcdd654dcdd18e844bd450cbb5926c5a8ada6faf", "4050": "155a5d0e91a6b45811d03678acae1f09e6e026b9", "4052": "1e00cfc788b4756e441275b0266096436ec49628", "4054": "8bc68b39aacaa627ac7d2d83493021ee5b13b650", "4058": "dc0d813cd33ad1e581931be9c4e42104a2b6b2e2", "4060": "b334b12d5f1638fe43b563330340435a5efb9396", "4065": "ad0b1cf1d1d329129f71681f01fb7a673df569e4", "4067": "52bc36bf2572727023b4464bfc6de3c8dfd9a346", "4069": "a49c548ae1ec8b5f1458c42f2ef96a4b03ba4df6", "4074": "8cf0cb6c2b49de5c9bad9f5bb6fe65b23871fd9d", "4081": "6c4e53dcbe8a8a803872a6af5e301292000b60eb", "4083": "391039506a1b6fd7981a31ec8acc2b21dca19e88", "4085": "ad6059bd176fb14d34702e22a617f39bcb15ebcd", "4089": "512397e133353d28825bc3ed2c33d22a71b476cf", "4090": "b44d9dddd5288a9a3f0b011b3a4e4dd3a3f18340", "4091": "b1350162ad89dd2c7ab2c8d1301bf04c889527c2", "4104": "bdc5d2442dddb2f11c045d4e35b008e1db67f0a6", "4106": "927b51ce43954c974434a48b6da8dd770828c9ca", "4107": "f5a4e1be5588baf8c7ec9439f9fdef41e6a5d522", "4111": "c6e589408c9806ed7c9f7424f3dc3446c2389a61", "4112": "e1e98656dd60382d076c4db3c81de6d675e894bd", "4113": "f3fdcd092d3d9b4bdac8c1fc451303be942c2d92", "4116": "74d771d88cf99df68f8e91cc3617945767c110e9", "4120": "fb5a9f2b697e2e3d649744db137c9bfc93047808", "4122": "509fa087a7c8c1fa1b06eaac81887020d2532e6c", "4124": "cafd654fb70f730d6fb85385fd27b48dbeae4788", "4129": "cfe2c119a589ce7f93c7f450be898de8e624f041", "4130": "080db00294341ba6ab5200da6111202a7e199b93", "4131": "4fba035b74cfc71621978691d6c355dd3b67b30e", "4132": "b914eb3f4be2acec00493c6b91e267e87095de5a", "4133": "756a0e0c20f3c3975944b018e50b32c651bc407d", "4136": "85201252b27b4cb1454c636779ea61b641264a25", "4138": "0feb684ca1519bc2f0fce5cc6b360eec50f84475", "4139": "463b9ba7fbbae3c4b9fcb3e7aa9fd81da2949d3a", "4140": "9fb0a01642d958974a523cec288e1b58e4963f5e", "4144": "d6694f7ee434ea70185f088d8cd79a9fed378247", "4147": "c6ba469bac70c1087f531fd4107db2b410c5cce4", "4156": "e55aa2f3647b3717471ca9bd11b8e07cf3e2c5bc", "4157": "6f4f0b04ad8b4b7e98db73cdfb6028cfbf0c5c92", "4158": "ce32d86efb64d0c4ec2e540275eae4d4010c9739", "4159": "12d850f0b2197bc94c991681a0d1644f07957db9", "4161": "bae315b0a847b520b470b25f42ea4f4613bd19ff", "4162": "9755f0965ef528121f48b321bee79e795b8c48e3", "4163": "b5f05fc5a94e08b2a1f9186b8c0186bfad83f686", "4166": "3521df049116a01d8c49cbeeb1dc80eca5e5040c", "4169": "39c877622426adfd1dc3df2414a1f660e45c2d5c", "4171": "2ca6f0a374928e3d16e5f9982ed6a4ec11e97204", "4178": "97a70ddcc39d838e56d4fc3791df84bc65f52387", "4179": "f690b1f3145779d0b23afea312617f412d81a5e0", "4181": "24cb9292aa7afdd77c4dcbfc2d48e29610f1baf1", "4183": "8d528c04784a6aa37df434feb5074a05e6dac21c", "4184": "609adb4ff1b65606c9a1a81d03ab6d544e0ead32", "4185": "fd4be70c649594eec870259f6a3736a72c601ca5", "4186": "7da390eea9a980e0d82c32cd750b5eb9bbd5647e", "4187": "61e304f45b8c9c8e4bec7b03a9316196e4b69181", "4190": "48da60342ba7c40b8a7ffafdd67f1bf425aadb92", "4191": "7b4bfd1381182eff629eed528f242cdef9e1756f", "4193": "6297c2ce36be134092c36df2178a426a0db9cea6", "4194": "6c150ac0f33d8923261c49f5f42a31331ab2a2a4", "4195": "b42523be0bec82fa4fdb991f3de41765820655e7", "4197": "2b7d2527d5ec6d25ac527502947b425915c7d32c", "4198": "fb136be04a76b47b2a3184b4b8e8b3d3ed37ae75", "4202": "eae20cc9128f8f7b7aaad68a1e864e00b440de6a", "4203": "310cc3a705dc1ca1957bae9f2561845be7ce0f02", "4204": "7f6f44c800b64fae03ecc28b80a6ab7fe7c05daa", "4205": "5d2fa1d32cb87de9c7310d52f8a6acd56c6334d2", "4206": "7dbba01fd953055c0af35aa4a20429046da792a4", "4207": "24b6dd7ca2ca2bcb10111c31e661ba2b93e36ecc", "4208": "1c5ffa640a3e1a435af41860b2b8f8cb8b46039e", "4209": "72992c08bff1324154891487e9ddc5c3900cbd4e", "4210": "2de4a362c0cc9b8199e4c84df0d1987944db178b", "4211": "f03d192f8523033d5e87ce716576a0d7e18fb5ab", "4212": "696dc62ce49a697fb3744f12f094d7250bc9ee79", "4213": "b3b0273556bef5b0cf8cc2cb1ac7e396f19373e7", "4216": "827fe4623aa5c40d6936ca58b670a7014512199e", "4217": "47914974bb7057d63e7771b0f233f386d28e9e26", "4218": "570d09ae1a7aa6e96c8ffee80b63f736eaf8d40d", "4222": "f3b3cc44810b8f0b6467270e5a2bf44736ce9090", "4224": "3e40f149ecb9db5cfa3e77ca21d5e838ac54a2df", "4226": "25b02da68ecc17fc7041baa139cc8183b2dcd60c", "4227": "bae4f506a561fe4cc24128fe1013b175581716d7", "4229": "fd5d2f83965127f9c4f6f483b6b0583f4b89f232", "4230": "742ccc900c0d001bf10f27ed86c0a916bb180ef4", "4231": "78d8f530f8d8520f5cf2794e29422522981be2b5", "4233": "2b12b3c718bcfe42080742cb9f803d12acd35b80", "4236": "0e6227086bfd1aad3d5b957d6d3f04448383d170", "4242": "cadd55eb3cd314fd67755d1fcfde64e80e0ba53a", "4243": "6b222463e23d77493909b173d1b39533b7212de1", "4244": "aab92195ed29a8ffcd68ddfba8a5ff59376b8263", "4246": "fc147277307bc387ca799e784a21e9f5b71bbcd1", "4247": "dedf0f0b64323d91dbf1fd7ac00bb7a112336211", "4248": "ac8791090e60c9d6a3c14bdf30bf21ecd86f0b1d", "4249": "911e9ce63e7776bd59f45c6f88a7202c7c04afe4", "4251": "8e70769aa13d8b976bb1a3436c665faaa4b716ed", "4252": "eafe3d929eee05fe0e2f882891cd4ff429703995", "4256": "7127c1876c7a92edd10343405e4e347b2fc2ef80", "4257": "97783313bff19689c4a74a0009c4127235165e91", "4259": "520919c89a7dcfb643c6b1a134c86b89e3da3c74", "4260": "c252d4778e9ca1cce591f432ff56104ead24b73b", "4261": "3eaafb60ab1971e9a75ef78b5eb561e873d2260f", "4263": "15371ea4e6114e2608623466fac09abb9e614bf7", "4268": "24dfb5473caed808b641858f744a3ce73b181f05", "4298": "04712f512dfd9315f0e027b7eabbee3059d757ca", "4299": "2cc3b04d7c135e9bbb30cd44d1176718e474b084", "4300": "fe9de294580526e921102fbdedda736f20596fc7", "4301": "9f7f6915509c57486a731709f65a8e9927519500", "4304": "9e30f6218f824eb24a636f8f4294756b791c82c5", "4308": "66a4bdcdd64cfbc81216071787adf586dd2a17c5", "4312": "082867cb30edb514a726172f66e75d37f9ceedc5", "4318": "589152fba84ee7a3e834aabfe30be4f2abb5b2b8", "4319": "b135ffa12117b90ba8fed4e54d306f68c33f5a2b", "4320": "059a1a31f420f77fb47ae2c9a86fd0fbcef733b8", "4321": "182b6e15dcfcca5a4a1344ef4ab554b902b860d4", "4322": "0dc8101137423678758be072852d3e1555c3a23e", "4324": "cb284ccb9db596a5e3a0554d093c9eea885d1f12", "4325": "074f895f8c644dfe49cc8e0ea23d31cd6a8ff4ef", "4327": "be50d94aa764d5c226786da119cf25c0110bf90b", "4328": "4f3ce004cbed8613b7600b7ffecc5152b51c8388", "4329": "d7f3d705fa4bf5c500e94076c0428901edfbef31", "4336": "60d54597194097a9bcfae4ff95ad443be08779a7", "4338": "12fe44694f4081d79bce3cdac800d4bb17b1aa84", "4339": "0e7e2865bef00a7dba442f970e00050579073f86", "4340": "f951fe2686047121e57000324a611d7bf736bea8", "4342": "62dac1556fc12b4f564d9cf6da08f28eadd111af", "4343": "e556bae894754812a008f0759e1cc070533d8cfa", "4345": "5aab6b618b8fd81f5f20562ab65912fd22f988fb", "4346": "ee5d6135a1301a939ad555c01377b251990ea31c", "4347": "36a798d41a6d6fad6003e0dae0587f4bf519ad72", "4351": "957668278e138971e1b3b7df2359ba42a511f77c", "4352": "630cd82e72786c1263b175f24d22da7cf2876078", "4358": "76c27d494eeba87254d5a40002eac8fc1140c286", "4359": "f6f966237214b37371497ad895cb7b359a88c946", "4361": "ffbb5cb3562838e67b621597381555eba439d256", "4367": "c730b5b862b1f6bd1b8895f1ad0b6411056509bd", "4368": "00edde3de4f240fa962a69e1f6e83aaafbfad1f9", "4370": "9a85ac87327d1ee665a5840fcb3d59b767d22742", "4371": "ccd538a2cb1691eec146aaa06cb8c786b55da1d9", "4372": "329bd8c648297d21649c47c08c8d6ab4a10822b4", "4373": "20fc89de7c0c38f61040fb590e0509db089dead3", "4374": "79ef81dc4db58f950ac22b0463a543dbe8625a09", "4376": "669cd17b85e01f8491757215476dd210379c864b", "4380": "0d6371c45e2a414eed5d50b22c0bb79e9216dcd4", "4381": "9f97fba617487b9558765fe8913fe3b7db313047", "4382": "7c45ec4ceecaebd1b1648029e9287f875dd4253c", "4384": "091f64aeb3fd0caf0e44a9bec6cf6e3a82b6a451", "4385": "bcd1f8b5053c465e65a316d4500fd6917a7e3c52", "4389": "e353b0bfb244aabe915c0be1e079eeeb619263b5", "4393": "0273e8ba6ce9d7e0071196ab95b3b3cc2af84d68", "4395": "2c7681071e1d62dc7e52b565435f6d54d3cbf62a", "4397": "707778fe3e3ef3716d240fb77d783471d800dd15", "4400": "d4833d0e9ecd13e0e8218808fbea6f4d29cef6b7", "4401": "68df67dea401c36fb2e8afc0db431aadcbebc245", "4402": "103ef9354fb4ce126fef3235eb4ca8d2a9693cd1", "4403": "63bf5193ce23b559ef5bac98f648e355225c39b0", "4404": "88aab789528d8b64ec0bc741e4498b88b2ae06d6", "4405": "5583265ac3fc995a7a67605adf8ce1f25e413c76", "4406": "d78ba4c6da7537172bb5fd03524c4acd4904949d", "4407": "898bf93c30687a6fe892ef930474c82f73688dd7", "4408": "64f5b11542f9dcd59894ed9dc6f85d66d11b0cc3", "4409": "075d6e4f81016937586f266c49d970b98594692b", "4410": "cc4f2710e98fd24eddd58a041d9ed762ca10fd8e", "4411": "662320f458b4e5e0564bed577fb25a82af6efd6d", "4412": "95b7a165c84375c558458f0f33ca5f29290638d1", "4413": "e9bd56e8bee23ae5f743151336dcd573b90b4bc8", "4414": "4c92503c80b5b2e69fb5a1aa8e60afac8da1103c", "4415": "4b3f74f08a0bf1649f06b035c37e06fa403e829b", "4416": "93091f57fad974d12fae10d4ef51fb4306de3e56", "4418": "6329be3c2515491121d0c3ea4b641557a675f4c6", "4419": "cfdc5500ffe533788334d863fd6f58c51383672d", "4420": "d420a4bf0359fc280f4bbe3cc9864c644c836703", "4421": "9e567c55e8ff705514cd590a4e0171c9f50bd00a", "4422": "d2d860a334a67c10408e3805cc40f183969f69cb", "4424": "89241122c77bfddc014daab103e94a4aa580ca3f", "4426": "9b3c24a1df9a8f6087b959049f6bbb3ff9028de5", "4427": "ee250526333ecb78b90267a8da28f9cdeaedd9e8", "4428": "01e41d376bb176783e43e0183a05dcff8355790e", "4430": "90a2530cd4ea3022274869c2e1247a73f439b41b", "4433": "62c79e5829d89dbfd260d3f5eb9d17f880f01553", "4436": "175909176cf3e3786079db5c702d5f899b3ce346", "4437": "2d89b49dc8d7f9edf9f895c24230b7afbd5dac40", "4438": "a9d9182fd2bedcd8843e1be34e6a5a65adfde85a", "4439": "300e121e1be47ecfbabba78f077851a9c3b0772c", "4440": "8cb2fb5882a8d9b724d59e5e8d19a89786ba9d6d", "4441": "2f1fb2bad82e0973a0eb53853ef39d6146f20d25", "4442": "a86b2ed4a67916615fdce87aab4a801f0cfea6fc", "4443": "d5fffa9699603462c3096fd72f93b68dc0a4f993", "4444": "23755a336c44f8a487bbf42c20218e0c3b811c11", "4445": "2b8863b4b6722503e4c73418829be66e17c64654", "4446": "88a22df71b975820bb3e3f9f21b32541ad01e1df", "4447": "38a8f2c04836da6e88c636e2f2d6ae37aa251a7d", "4449": "7b37536b67b8410d2d890b8ee8bf38b05181aa61", "4454": "153aa57b1a67e2306a0566ca180e30d65a9c40e4", "4455": "039c10de148b622375cbe59f4942aadfac57d9c5", "4457": "42091dbe1ea4b7b827cad4f53b2ef099eb43b4fb", "4459": "b731538199b56faa5685dddc070076150ccae368", "4460": "cbbe6541cf7ed88ee0b7a64410b717c8a7f6c47c", "4461": "b4103c02331a9351259ec0410369956bd34f1fbf", "4462": "6d8fff0e8812fbfbad6cd0be0b86139e6f3da6e0", "4463": "7e781227fdb39ea05fbec0271dc82cca75691156", "4464": "f7d5838d209eea87089a73b2e7e6c86571cfdf14", "4465": "7eea07f459883bfc59af89d79fd24803e6825d86", "4466": "109ae04f4ee50fbdfcb327aec737402e0b1407a2", "4467": "f7b0dd12899d0a4d04cc129588f03edbaf187b48", "4468": "aa585fc0a5ae60f61f1752d78614e0951047d21e", "4469": "6b3c593924808d33a39e275f921f613f77139d06", "4470": "30f4c48db0d742898f629d129b5d1caa83091662", "4471": "b89fc53dbd7cd1eee783fed1fba7b401d69b6ffd", "4472": "ba2b30bf883376cfe134d986a1593df96b843f0c", "4473": "906b13483732442acbd67e40f6bfa13f2ddea575", "4474": "06ae00203aca0acb58f661624aaab1bf910bfccc", "4475": "9d613bbe3ab49e6279cd2ec8808550cc24d6acd7", "4479": "8b60d541210731d197c0e2b8fca2f2cacbea70f6", "4480": "4eb05abef6f12ef9af9075a8f8985a8395cc1a01", "4481": "3bb66306f841abee68eb473c61838cc0a8520c28", "4482": "fea28c2aed4f35fc59495bd49b3943495f33725b", "4483": "2150d8c8322e0a1c8f0ee070c5517512fd74cb10", "4486": "0c81995631d7bd2a594a267dbd05243613693c3f", "4487": "396757028afbd460fd6325fd2403636ed8482d56", "4488": "8b8e68b7f13979b34b8ca9d8106acf1cc5e47e30", "4489": "61b00e99ce7861a0fd767572be0d58600c065b53", "4493": "17bc05531d39c4f7a4f7de6eac4ee4398154ee3d", "4494": "8d57f788eea60df17d36701dcabe8b9e92a52d5e", "4495": "aaa38f874bb3378000be1a1b6fd410ad2c95c07d", "4496": "eda4d148209b38192ce9574d3494ade1c2e64dca", "4499": "73204df44af7a81e0cc0b9de688a6a79b6880816", "4501": "674700156677b1bf6089afc311368895fb1217ee", "4502": "3178aa25ff3ff7be303b2f0dcd7a537a2a1b8b04", "4503": "41a2086963e018ffe0ac90e6fea72d3577d463f3", "4505": "f44ba2ffec070f27dc542f15d929b7848a2d21d1", "4508": "bddc9b2e87a18f97ead0a54ab2d2623553256338", "4509": "d1957eec3308584d188492dea912cb74a60ced7c", "4510": "c0b11f7e5ca848df05b364e526c33003f35c201c", "4513": "550d3e7c29a5f9d67a0e993bbabc1cbf88295ef1", "4514": "14a43de44eacfbb4b6263f5aebfa5cf60dae29ba", "4515": "08fee544b9a0ef9d37f364b73d5f075f241e3d93", "4517": "57b5865f062fc6cd56eabd91ecf5c130291f79e5", "4519": "cbbb3abef71dcfcd6f125da43edd1423e270758e", "4520": "8d9124ef9af6529ae67720bd8e5b3985d76b2155", "4521": "04a221e423fceca21b679df0f3888ce427017c25", "4522": "a966a8d98f7b9a4c5952e8ff0dc9b96ee100fb8b", "4523": "83c576fb4b72458a6c33977011cf35a1b6ef337d", "4524": "037939038e5f22858c7cbf9d28e1dfe5d2535daf", "4525": "1f422dc15d1d584ed9dc9c48f56238519683c864", "4526": "1c86b30d803cae81a978b6f34e8cb0bb0948f33f", "4527": "7ee517d79c522affd4a617a618fe57eaad90f0ac", "4528": "9422fc5d36fbaa9c4469c7448adc4999274999bb", "4542": "138d9882f004f95c0e2cd1da05ef3d83e7155e79", "4543": "8bbae6676f6c16c3a118b8dfb50c9e6e6ddeb130", "4544": "a9d0e41e47c40a34e2e146a4297b5c638375fe85", "4545": "d2177d5db4627fd97b0b772c0997a33eec31f481", "4547": "8dcd94d230edaa33614061f46a2734e3c94d907f", "4548": "649bb63a193c1712e88f77f7651b600492f9dccb", "4550": "a5c53a1fd6525f1133a5601042705d2780116a0e", "4551": "33c793d757859f27beef845537fbb729ea1fb3fc", "4552": "3e43cb7c23d31b6e6dd493200cc57a41d451f9f6", "4553": "c18df48d5be19ae8e219856945a36d76bd0d2629", "4554": "b496c151f76a9324a758aa63e17e89becb9c4e9e", "4555": "ee79c6ee97607c3002cd5bc3809b9ca206501114", "4556": "273d723200d426238c8ff255b4ef509f96e0d886", "4557": "5667516e4ad2633f4be6414e472126ad68f45d2f", "4559": "08ef258b2280ef6ae4bd0664229e7a2cae4cd9e8", "4570": "d4b40e287863960d608d52131471a88a435bf844", "4573": "9b93303adf8d5c436ae267be4d950fc5e55139de", "4575": "652d968561d473599e90508078005c4fd00a1ba4", "4576": "3a1a3a751704d3726a5f131d23672567122f0f97", "4577": "dd3223148ed3de5f8ad787f39b2c1d0fa3bb6a5f", "4578": "77648e991aff41bdb44a4216f509a877de9706ad", "4587": "6e660746325db3b3e978a1a27fd1a4f2d40e883d", "4589": "b808afac25bed84595b874f28b6b3ce2407fbd0c", "4590": "d5f6c42e5bd8b221c2654d5c1b88568d58333ca2", "4591": "d1ae7f389540cdee282e29856ef4b2f65052877d", "4592": "70a7d0fc267cadd97aa2ca69f97e6e7172d7b862", "4596": "3c2fe145cad45797356a4ec5fbd42e4dac57681a", "4597": "2612fb45f15350dc483ddde1c8d43c2d6a02731b", "4598": "bc0a3ec370f8fac681f3cd5de87e408a51079cfc", "4604": "9c8cd7635d0eca284cfcf8c79ec2d33049483d43", "4605": "9e544da9d823c3a4e76b256b741925af52a20742", "4608": "a94d44b9474cb4dad547292bdf2488898127aebd", "4609": "edd30ebe6198f8e705070af6618fef141cee1473", "4610": "7da60745a7cac50a865a82fa3a0e535863291ba5", "4612": "bb65a85ab82dd7fec5297b258956545f8b4adea7", "4617": "67d2bbe530f96a0aa5412f479e123da480ba2d99", "4618": "f9ea47c45cf6ec576ef2c08b21db95c049d452e5", "4619": "1aa32a6d504d2b41d0e70952edae4a6e5dcc6998", "4621": "a158b0c2e684c9389691744c001192ce94ddc79d", "4622": "424cd56e295afafd64444420b92fcf89a82dd1ea", "4626": "9d98a4436e376fa1623af92f23153cde5b68a68b", "4628": "3c059f4d4030dc73594f277d8754918c698a2969", "4629": "0f1f017025f9bf9b5b3cacab6262af0fb8adb3fb", "4643": "91a076cc267caf549ff38cb532dd76c5e4e168ba", "4644": "316062bc4fd262515cf0eff8a2ca97ab1dd66b9b", "4645": "d6601960cbc2ab870b4b4bd21f0f7bf4241813ca", "4647": "85d51e3b748982dc9121026a4dfcbd9f5a1dc2fb", "4658": "b8c8607285c089bcab3a642dcebf6a36a1131e5c", "4664": "64c6c7c79be08e0d80584d077e692ef78ec519d4", "4666": "e0f71a6a76409222d499fe5f18b1337793c1e239", "4671": "db2ff1dc45575f2c70dff7aa00310cd680862c78", "4675": "63ec283d2d43391b07aa2f1a461c7afd30b54014", "4676": "c199cd14509440e58b2159a9ce7ffe5922734a91", "4678": "4c06e2bffb888b6b2de77a4208efe792913caf35", "4679": "d401beb0949d177fdb29375fcf397e1ae1cfaf52", "4683": "70d99a0dbf2b4e1e04367e473129b4fc42cdc9e5", "4685": "08294edc22a9afed14a331a530630c65ca2a8c11", "4688": "e15255acee9edd687a06c5c0c93f2bd222bc4ac1", "4689": "06c4e4ffa5d3d62ab56215ad14a8636ca49ee52b", "4694": "ae985954d2c2ac1950109a379ed6efc97f5443a1", "4707": "7c326235643bbfccd227adf3ea114a5310812ff8", "4711": "b9ab89f088f9071c31a2238e1f4ea1a28e93fd8d", "4714": "7a6e92c91689c52bfb5d160d7190f26f6b3d33f6", "4731": "26541d3858515374a9963680ed76eb26a6e29666", "4738": "d8e14b8efdb1c435b2e26737850c6657f7324c3c", "4740": "4c596f222ec4276c9ec2f8abbe3f7271af394ecc", "4741": "7e76422da9d21a398794504c8faa918096663ba3", "4742": "fc9514f25712d8e86fbdbd2f7e37e64f3a10df40", "4747": "3bfcbc8ca5faf2b24403b595496e4f2a0ec78cd8", "4748": "b6fbbe2a8a81bb943e3a22b2b981b2b172675865", "4754": "6dd7f840b5f9f9fdb3a7b6f9b15313f45801e474", "4764": "ddc09602da8479fc151612c0b4df46113cc5f181", "4773": "1a77dfbad7d9016c3875518609ba2d598867d31d", "4774": "ff76aceeff0316cfab8ce8422f3b02e6217bd379", "4776": "8eaf5e7cdfd4b565f93fc7d78669374bb39c6b83", "4777": "63fe8bca517b92fbacb2482fa370a24203314bac", "4778": "88841eed69f26d4bb28ce0ba8d3295e65022a17f", "4780": "6d6b0facd0c5118217cda033397790fb92a64663", "4781": "94ecb699e1c6a955f20241927554a0804488ef64", "4782": "f6d8f28aef439d442075bf4b949c97849f69a326", "4784": "5fddf31ff6b4833ad699eb0456fb30e06754bccb", "4785": "b4705d46ee10c9ff4820c5b8c8bed10e501e1220", "4794": "fe78116f28aa60b7734dad2a32b3f11bcea50e74", "4797": "45e061f1c76eef8959b334f92a53ac75a451b1c8", "4798": "024a59446d9b806d64ca74026f0167126839f576", "4799": "bc01cf06d36c177af84e14e4554b47595d09c6ea", "4807": "229e3f55336d05923b5df1e74b30185594a13df7", "4808": "983bbaddbfcf5b87ef047ed854f6427718e2183f", "4809": "d570c07d08248a4a618d186a59dad97876d1b16e", "4810": "a6317958c386299ec1da2ea770280a447f663db5", "4814": "c4ab05155f81a40337ad595c8e05197750d54e00", "4815": "24bf7e6114f2b95a4746e10db95a3e5609586bba", "4817": "c20931526dd912557de57b59f974011a3aa305a4", "4818": "d468fec99c57b70fb1bb5b4d4c12527f05e0c150", "4820": "579ee7260a66d618ccf8ceab83f0dd2075c5e33c", "4880": "f91d3104270d68f6e88a8937600c0b240a82f974", "4881": "68f75198bc3f9c4cc661bb9c6714d5ccf00d45c7", "4886": "23e604441258b22f6868fd53c6c8264e9a945a17", "4888": "c7178f7448f7b3fa8cb8f5cc1b9f799e8c54dd31", "4889": "cc6aa4f67f1bced5ba6fded55fbb4fff996d4e0b", "4890": "1d1596bcfcefb3650259a8c087acf297cb729a0e", "4891": "6c61c3660919ec310ce153176e0b20b059b12359", "4892": "af75cdac410d87f122741a5456f83db923b31004", "4893": "68539dd9bfbe1e083ae65b56b1afba5685a03f2e", "4894": "c6dfec342411e17bc4f686c1fdeed92d6eaa254d", "4895": "be166afdb98eba9b875aef2bdf1b804c514e762f", "4896": "b59e674d0f81a1f4a581ebbdb5fc722f2d2f84a0", "4897": "20868389447bc4c9aefb986621764566909e06a7", "4904": "96102f1ad9aa1848390e49f9abca6cef9fc5b67b", "4909": "67a39ebe8d068fc91d06adc30d80aaed7943b9ac", "4911": "ba87b87f8134fc6f7eb28a8742d6fd496633a828", "4912": "cf0827400dd1a1c4ffd8d732dcbfb1740807f4fb", "4913": "e7bf9557050ba34fc49509d5680edb25379156db", "4914": "7c213574a42a65fd033fc7fcd94ee1f36bc7647f", "4916": "f3d82f06136351367d820c383f08233491de6cef", "4917": "0e6034d4166a89280aeedd284f2348533301fd97", "4918": "4028fde66a6e1e8482f7d3601223c4081e9f7183", "4919": "f151393a7632f03e6db3acba7d3b5eab2d7a95b6", "4920": "ef7e74be2461974d72a6ef5aacf2c1631ec33076", "4921": "ed66e7ab81be7d68303a01f43482a238744533f7", "4922": "6b82f1787eb877e0cae73b4af98e34e54babb130", "4924": "5c1eac1603d80aa93e9bd6d027a088d4f7210746", "4925": "690191c24912e6d3814c58c9821631cd46bbfed0", "4927": "9c600ab0bb0b791b6f446f19d798095383057f50", "4928": "7bae43ec4f3e9666cefe7cc21745dc9deef6a6bd", "4930": "ea2fa0bb7fca57ab7eb9a5e51e2d2d88c53d406c", "4931": "e5e02896400fdfc0a0a0bdaf64bbfaa9e9329963", "4932": "ddef2481c0d206c52dabe73fe8a95ad977067f3a", "4938": "ecc77c1436fe2cfc93a4feddd6914bb3d8fd11fa", "4940": "023f1a167c54e907f5cafdfce919d40423a6d115", "4941": "c651c04efcbc2bdcbc3807ce0450a60e93d6bb1e", "4942": "726a90a696b01497355373db7def03782b7a3667", "4943": "6c9f6b9660e5d97f63eb7a3b8c50e9865d2e8746", "4945": "ece6eef1343f5c83aa4b08edf4dc2ca31d98c792", "4946": "c8749fc9ba9b4f88b3f1d5cafe721daf64e4f9cb", "4947": "9f7ae176c936af7ce90391641b6be116eb376c63", "4948": "5f6ecb1963a7a4e9ca98906873dd4a8debf98061", "4951": "73260134fb96bce9e7d56d65fde7e688b99ca002", "4953": "18a3a3bb1e0d8e3c789841992e737ec94d79e218", "4954": "b512cc2eab9ee278bf5a213052a499ecf717c8d0", "4955": "dab82654bf23be9acb340812fb8db511c9aadbcf", "4977": "b7df94f765958c3edec20e471406b46b78d0bc21", "4978": "f73ddd1b286a3c9bf532c1ef95a5820ec3edb090", "4979": "c037f26bf6569ca6976a3251a28ef081368982d1", "4981": "afddbc36aaa54c4b3e65515955b8faca9f4b528a", "4982": "54a6b35d9fbda8f746c168624d98dbbd49fa0ea0", "4984": "102e5f051097e37ade34f9f04e6feb7e98f68fb0", "4990": "80975acb3c71feba3320a8be82ad7d57c4687203", "5000": "ae7e3caa313f1219c781da4d483a4fa6ed2cecbf", "5002": "187878512cdebfd722b62c5e4efcead86156e173", "5003": "9afdae6c52950315c7a384509910b48c71370bd5", "5004": "3f9b1c7410f708cb58d8c2ee489be29b769e21e7", "5005": "c92b4484a312fd7e3e726047cf8cbe2b0a4400e9", "5006": "2d41bd06a52654d22b4f18deb6d7b12a41ff5c1e", "5007": "57543e4b46451f3252305a49db4f640af42e565b", "5009": "7a6136878bd0f33feffd484e3368b9a58a92576a", "5010": "8634a5c494a6af8b1d12358d4215fdc382ad1373", "5011": "d63ff94737b6300c5a3dafc0bc7388fb6999a69c", "5012": "fa29050b09ec5eb65302f93451024c14926bc49e", "5013": "558f954fc7f424e019a37cb262c77dc1d1ed6d56", "5016": "d71bc819e3a51b21e851140f3fd561e30a22f88e", "5017": "98e88b6b578d428f0d91eb3be66a02c913e06b48", "5018": "aeacfd56f906142d7d86886109d8828487448644", "5020": "7a5f09b75adfd8ff4d34f8c6501db7d5bd25e35b", "5021": "2d65d634eaeaaf648e52c0ed0592563bae125a47", "5026": "a66322c2c9008e643b9d95c7d623c81ff88cb426", "5033": "5cb58364ff11163ba099074af43f785d14d4029e", "5034": "32661a821ae671cc56578f4c4a698c8c0bd8b1a3", "5036": "67a0f0281e1554f7f7db4ed918689cafb249bec6", "5038": "354769d55d8c76eeeccadf7310d015e23b3fc681", "5040": "177a57e5d7e20995c36a9c8f2a6e27b8ced1f30a", "5041": "38597fb9b0436663126c6ee9a40114a718a983d5", "5042": "60d7f89b7d094668a3faed5d4b0568856feb77ec", "5043": "12493a28e40e06f9accaeb39bfa62b75bc614c02", "5044": "57216a4afd59bfb9fe549501394219693ef04c90", "5045": "5ac230c30b07e326d50acc458f84460359b65c8d", "5046": "2c96000b6584d2fd15294060a6675923fb6c3f86", "5049": "faad3fef1193313f4c0563b7f5b2611e2af50bc1", "5053": "9e31113188e6451d1911c7fc3836232e5eb485bb", "5055": "f9ea93b00b5d81606dd0d763e302526a03f16004", "5056": "23ae3542f70837422a3b058011a7c295b683d907", "5057": "4a8556b8daa4fa6f0dfd19dce8b49b92b5a7f021", "5058": "44ff99a8bc9dc75b189cc52d598239e43361b490", "5060": "4973d7f8f3f1b9f583ebdc7cf07dce05c75872bf", "5062": "7c284c3cb02283cef71ae3ea4df4853955be10c7", "5063": "7ddcdb10651be5ad70b4bf3d480445744cabdf04", "5064": "e792b7b21e4b77f699fbd9929b8b6dbf082fd511", "5066": "5c926822abbfcc03fd5c93d4ad6a2519ee34a3e7", "5067": "da7257ca23a6a6f56abc6c7907be009840244c0c", "5068": "d09082c78ee575257016eef41d50ab73a9f0b1f7", "5069": "caad58bb27ccabd8fcef3f8bf3b62c1d4320e8cf", "5073": "9707d6a247d75b3d818727314fe2a85b0b6fb796", "5074": "61aaf459640d4425b76a189dab3a84898fb22ad4", "5076": "79c9a7605ee3643d292ece70b4d120660cdbe3b7", "5077": "a2ae9255a880b6c8624bb7e4d4fd22df44a852ca", "5078": "928a2487faa8f6a8d67101b86f70c79cf4d4e108", "5079": "c62d676017f4d405a58fd98db16987baa662f785", "5087": "0e9a3638143e46961c9f349433d41042c7c4bf8c", "5089": "4d03bee9a1d41778fb11a39e7ee28cdd06eefaf4", "5090": "70da00548756d362632b0924f2e0889c5aefd7a9", "5091": "668f1bb5eba55b15d7001a103373ea3bf13a7191", "5092": "7ff4b4a8c1e8b7d05201ef4da52d59c71fa6e63d", "5093": "91d1865166bd7e21558a1eafe4da3ce22ab42586", "5095": "db6c1186d3454974f6d9a488a5e0846a25cdead2", "5097": "73987330f5b7e909bf90068c1a3323f1552c86e2", "5099": "b7e113f311628867a42f4923874a8a3155edcf0a", "5101": "6e8bfe0d333ea7ec4ba349e9aa2caffa7c7bf1f3", "5102": "37c3ee53ff81fedc97d7ce6a353e19e1ece14135", "5103": "a04885b4a401f7c8a3bccf4e5f029086d537f3cf", "5105": "780638cbba5e18a7e8c03ff47a25aaad1b19a01e", "5106": "c3befca8798268f0e4fd6476fe84921cd1a309f4", "5113": "9ce935cb181251ac6d736decc9b32ebc8579e226", "5114": "a49b1e4fcfbb5d0e8b4e47237196ee45258373e2", "5115": "fe4cd108d8b4d24b55ccfb1df12a6e04b33940f9", "5116": "159bc59f3d39c0b9f5fe8cdd780885d4462451b2", "5118": "38909e54a4332d15b69ab24dc313c9e039761930", "5119": "aa9e5f0a256d2714aeabb9c77096b8300065f4aa", "5120": "946d88ed302e0fe5a27a50871bff965014dcaf82", "5121": "e53872902fece23d8e97d72d6a6feeb9b1fe9011", "5122": "77e5a1ec82e95eebdf67710c1992f6e7481bb119", "5123": "e684589b46700a477e2e2ba83b066b9732c45e7e", "5125": "e9b2136975a4cf0e88dd96028514879188de735c", "5126": "4ad6edf281fac10d0f02082b4b62c874eb079479", "5128": "9bf448e49b466b503a50175b4ed7d30844abd63f", "5129": "b23590a506758bc1531ec1d4b4ffcf77384d3373", "5132": "db6c2e396a51eeb9ba2b7a7e1b46c0c3169a055b", "5133": "17025a46aac7489fb78fc0fdc1a612bc1f44f1be", "5134": "c870641c689829b2ba4d2f309ca0927d3a921efa", "5136": "3d2ec919e9c3dbb32c3b2ae38e9551fc52e937ff", "5141": "294077743be8235723bc1b65fa03053efc52633e", "5144": "cc8cae8fd57da4835abe0f5aceab345b4d813d2a", "5151": "02efcfe88b20551d121589643d92f90be7e331c7", "5158": "e58d9c47ee6255470bb5b8f746ed684427d401b0", "5160": "a3c25646b2022b500aecb9140969547961f85d02", "5161": "dee245dcd4914833d8fd71e221907f6043b48bb8", "5162": "72d6015201b4b44dc285b266500c89205dff7852", "5163": "d73b393da5b317dbb4c0c41b5fb9c2a5054237c0", "5170": "5f542c42c4a0d3c4f811ade2c4fa5a1edb59f988", "5171": "ef74db0b6d827a41d338bb8b286857a549519d6c", "5172": "be2b57c0aa4612fc378b06a19052674437bd382d", "5174": "a7c47e920c761ccd421ac50ac24bb89587ece12a", "5175": "3e787b0eed60fb1bc3d5a8d94c2b9f0ed47f24c7", "5176": "f8484b070201a40504ff9f4c8153035d9a73e691", "5177": "f206a4b15831e3f104bbae9b31465bf3a98836d4", "5179": "d53b52a233fc8a849670262ed05596ba46124e4c", "5183": "a32ead86d6a92db7665f79318b0152c28e576a39", "5184": "827e043e8790e4bf6f58c4b62add3b8edff2ee21", "5185": "2b3fd17d032afbc6d512309b7a52dc75d6257e31", "5186": "692d387f3c4a2d73f30e7aa1177ac75392ab1313", "5224": "6e668659975bd8a786f696741dd8de40b01f0867", "5226": "fd1f3a75b8c0b3ab1793789a8160ddd029d5ae13", "5227": "ccc3ae3fd8e51fd71f39f8ea239986554397f5eb", "5230": "397ef0725d2bfda14f572658d377471d0f3c1cf4", "5232": "0a9cf6d8fa0ab11a9177e561a5be9a96aeadf758", "5234": "bf4da5d7b54c792e3f436b4f44f5cdc368ac9824", "5235": "e9493057cf96d7dce00d91ab7b114c982dcc11a2", "5236": "8849d7b19869de79ec875f48b0b3c310cea6dfbf", "5238": "e9bb350a83015f6695c0a26e5856648a9afc08ad", "5243": "448b4c78b7a8e2e0312af268a779cd9c6f11fd8b", "5244": "be189aface8db30904835083085fda9999e6a742", "5245": "8ca00bb84e9ef6d2fda426061871ae871acdedeb", "5246": "f6f75ebb1e9a022cccc28a9f5d22bff1637f5cf0", "5252": "1945a84a5fb962b4eb8e2963f14b814a8e344d7b", "5253": "791f9c8a43e025039b09a086abf2ce5f3a62b2c8", "5254": "9aa33b6d03bb3c7f8ec3a13d6d8413a3d5ce35c6", "5255": "bbbc61ad295c089802e2b12562ff1e959cb57c29", "5256": "6f020a1a7bc9563bbf99e005951c80676684a050", "5259": "d5a51d6ff2c601089fcb57f4764020782412968e", "5260": "883383666df5f17d90ef3681be5f79889b18be12", "5263": "6e6c3839f9dfe606c9513ede1b39e7603b45e4b2", "5264": "9fd815389939a8097549662f039dc44cb616cdca", "5265": "f51a2fdcafc0d243b04ab559ca3eee78455afd6a", "5268": "87656e5244bbeedfcd5b0baed192ac011582fdd9", "5269": "a52b101a7dee4fc0c4423245f88886d021e59d99", "5270": "a53dc85a0cf2b6b44ffb1815520fad238d762013", "5272": "f5582104bbbc812df081a7d61e1e4bf328de4c61", "5277": "77fb0993f7125a1b5c4dc6393ad87809c07ac0f6", "5278": "4e0b145cc96aafd19bb558089286937da4fcd0f2", "5280": "e2efffcfc0111fc8b795cf5b0a592fd5a943d78e", "5281": "42664c078d40e8bc99f174fcaa5b8f5fd13224f8", "5282": "a5a19f891626aab54d7891bb74336d161b097fef", "5283": "876b785ce4873daa7f810f5a9202eabf878af188", "5284": "2ac4631d54f504209e530ca9a354be38ebd8b9aa", "5285": "633e6fd0286e79a39b2749e1f47b3564e824dc84", "5286": "4a91485ddf38924dc139b5cc10a8cab4b3cca0cf", "5287": "f1b40e0ebcb66a07d880a33f025dbe7d65ac6acf", "5288": "999d1aa6a9b1cb1cc98149d75495ff37aa223b77", "5289": "fdbc93e3f2808ef871e4c7eedea291a8da9f0884", "5290": "effc84f080e762d84f1884b8fb564db6eb31fb3c", "5291": "f6330e766b577476a2923479a3298df2c7520e59", "5292": "ca7803489ad99ebfe090774827a7dae819a71909", "5293": "8a632c91b81d32f653e022dd3e9f8cbc5436ca37", "5296": "b3b94783d7f6ceebab58a87a488ae08c7a44a2ff", "5297": "965a44988df95d51ae5a126e54417e80bdb1f1d3", "5301": "e4f376878999f166c9d28da85caffe351cb36422", "5302": "506b33630c87e9af1a3a160b1fc86144e007f6b2", "5303": "bf839b584b03e449fe8b3f1409f7bd84e3d0123c", "5309": "1ce7d60a32aaf0c3ec5c8f2c9410e3bc0188b0cf", "5310": "a88815a2709d6d49b5d92a5a951c460aa448856a", "5313": "0054b96f3d38c6ae32d0a3e9a4a4bdb246eafe50", "5314": "2785b43084a74447fed07c19129e1f576ccba8db", "5318": "39ed0173dff869693b71e34af1b4c83b95e681be", "5319": "7f9fab1c3075ed3b8416364cd8d3e8379f08d461", "5320": "78f5d2e898f0074c2305afc3e600dca671905f0e", "5327": "52859b00ab7d28a26f9197c0b652faab1ec4b7e0", "5330": "99c0afa092fcf5e2369c1e306fe7d8746feeb326", "5331": "478b74b3e7a074d78392a3a7f8cb9cad37ed5fcf", "5332": "8d9fc3234feda89c090f7a6933641b0931657486", "5333": "67ac62a158f0fdcadaef17c510296628e086c6d5", "5334": "681b807160696a43ad7eb8e0642463092525ce02", "5335": "2d4b1c552279a6397dd58ea2f576c569dd105bc7", "5347": "1e17ec4e3ae171d5256e82185b77a2b1e2828685", "5349": "88571845d60b489c0d186eed87523caa576adc32", "5350": "ad7bc5b6d0e45dbfe709d190d0e8d02347ddb64a", "5352": "91821017262750fd21cc172e812678967c8a9b3c", "5354": "612db696e93ad22c5ed9381b2d8f05cfaccd5ea5", "5355": "434f3234e4859c8cb9c592f1452c344679c550c8", "5356": "6e1eeba5226969c58f0e7ad74584df5675c36c5f", "5361": "2074211f5924124e86170e5aff74a41b78fdcf7a", "5362": "51b7fe313b3eb7f65daa3e9837b8b8acd0c01375", "5364": "e4bd2e284d8d7ee91ba7830a77c6782749ad8672", "5371": "1352053991d5c7850c84434f467681a4841838c1", "5378": "98f6e245e3d038f967f3b64b9e4b408924964d50", "5380": "4d6bcaee5e6b4ee7bbe36e09130a44d2ae12f9cf", "5384": "d21054d5a4b95be26301cadc9cde87fbc3fa859a", "5386": "20c64fa6def9128a88369da0609564f4d83dc1c2", "5390": "4adf668ef88c8c1d0bc3506baf2b3647291d77b2", "5391": "2131f17ac5d0dae8db88bfc9bb578404100b7b6a", "5392": "d1bd12a85167ca26c8f6c1222ccb4b70c1f69855", "5394": "b4da88feb36b4bc51a36e3ca8f71c68942ec3bcb", "5395": "341e0047349c08f050f28db44010bf7a67726094", "5396": "d552af402f03eadfd9aeea698e3f1ff8fc7ff61e", "5398": "bb085440e6e690bbd8316125ea6bdcc7b6a6b855", "5403": "dc58316206e89a6743716b9f97e72b1ae9bf9456", "5406": "5d4c3be39ffed845b46457cbe287d135ec78ffe8", "5411": "487cf6bbf285cbc5a37b35bde0dadf8e14851933", "5417": "9ec67225592d8248d70ec1f0c42c05d9312a590b", "5420": "57fdde8e8f53bd2c817faf8028ff44ce13c35a64", "5427": "0cd985826f1b56c684afcb83b387c3d050098a1a", "5428": "7c06997eb897f83f83f34ca67e8bc3209887bb58", "5429": "478e5ea89de9926008089fcbec1fdccd11f33c54", "5433": "bdf74a5049b62da910d75de6950f80589ccb95fb", "5449": "fbbe5fd099deedb9fee414249600f546ee07309e", "5453": "7c82268ab0a3467a0bbd577f5b1f6f802c27efd1", "5454": "079e617ca17bb301b1e4e68c4770f1b8446b1587", "5457": "aabd28457a2506a7b47a2e50bcf7ae4ce3f9f0c0", "5459": "38b4a0a972eb5404841ce45fdbdf3bc14c684963", "5460": "a5573bec240d6e0f14c94deeae53f608df04bf9a", "5481": "5788bb37da7dc7cfab668786d005e888ce16d480", "5482": "da3dbe6b528e4910aaa850ae0bd1d4aacba27ed0", "5483": "1bcac47c2163479f5f6be8791c1558e38898df9c", "5484": "3e71b32185d37e8ce77f34371f362acd70d68d1f", "5485": "b8d9281dfcc219d5e669f5623211e67c4d3b0153", "5486": "493d8935ebe4471afd0f802d8b51c0b658fb261d", "5494": "68603dc9e7693a859e02bb88ed7e63a04527affe", "5498": "b9252f3ca733d9cf1b9bcbe264a3cd5e3568c31a", "5501": "64586b92bde9ce97ae7da309214655404d8f2321", "5502": "9381d510a04fc0d1e35e0b2142f2061bfcd2e2e2", "5503": "1b13f924d38c19ab381b99ada20d4d1e175e2e14", "5505": "f961e794231ee77f0d596db76396274bfc5cc366", "5506": "0fa105934214e5d3a9a689f65ed130a7a623b157", "5508": "ffe58bc89c63962f615385eb3a02fd0a700f2951", "5510": "889d994b9144477ba1aad2c7338bc068fee782df", "5511": "7acd347e3a035b635fd71c1bde2b54e0135ac8db", "5518": "90505d1ce3a373c4c433b4744d6d410f7cbd3565", "5519": "59551d3482475e17359115fb83bbf6bb6611fd6d", "5520": "d78b66880c7c1abbc06ff51e9d225c6ff07332ae", "5523": "f30c9212796587b1d16a38fb57ee0ff00bb0217c", "5526": "5e7c814e2d9b37a6c8f8db5cc4345f4faa51f708", "5529": "47767648479351c788ee4db211c6694afd83dffb", "5531": "0076efd4ef797f68c08d4a4a5fc1437d4e366299", "5532": "e222cbf013029e8dafe987f54e29b3bb356f644e", "5533": "c2197fbb76a05b8d104062e239b43a87bba8fcb3", "5536": "5c7b525369115c12bfac4f5bb27ed3373655cac6", "5539": "3c7a4bbbebb65affa461d244be52a5cb3b0fe245", "5541": "9169d2b701903e838b2a67ec327b599106922d82", "5543": "62e9b316bfb9dce0964f8d86988f646ce4b46ecc", "5544": "d456411c9384c3a72197ac60fdeb5c0c8edbaf06", "5547": "854136a9835099eaf413bb0e78dec6527f6ea982", "5549": "7ca3af77374c268121c43452be8ccd1ab8fab7ac", "5551": "19671d28185e7a3794460baedef978e6d2f48c77", "5555": "33dcbfecbf93f57fb19266b5bc389d8adaece8bb", "5556": "63a32f338855430c372ef04bb8f8d48765606fa6", "5561": "f93513348872581537d3d85d3b451c5aa1ca8ae1", "5563": "bfd96b308e7c15ab2a3b83f327221b5535fbbcf6", "5568": "1369842915de11b5539c131cc562a7ce38615db9", "5569": "d7e90b5b8ea53731a7a22b11eabdc65df47c9eed", "5570": "2ac0782de47b6199dd9b6cee9fb6b9459d7fc403", "5571": "810df8c4a66437753f14ca3257e37886de6fb03f", "5575": "50d094ddfa17acd14d69f7001960826fc4f7a38f", "5576": "a7e6cfe20f37136201b85d67c061141e2a778338", "5579": "f7b213a86184b98bdc8c3eb2308aeac457841c16", "5582": "e14ef988cd8ff3e36f379b4e2b6a11cdefdc3fec", "5590": "ea2c79513ffa303d3e31c14e518c27e202efb658", "5592": "17c310f758aef9bc0afbfcbc715053a5ce1456bd", "5593": "c6155ce46574ea32fed2ac261cc40e8f9f91ac3b", "5595": "f44db68d7e2f70a73720a92124b60f66d5ec0879", "5596": "d6c9ce22e80622b586063b7963daa87bbd41d898", "5598": "f7a982fab0db8e03545fc26390dc4ff862ac475c", "5600": "21da8c7bd2577a0556dc0fb638dc2abd588fdc24", "5601": "85d59a738f54c28076d7fa8616398a36eb7fc6e9", "5603": "e269b1d408b5d04237feb313e6100de44406a5d3", "5604": "a23b314c40b5cc14a558bd795beb95f46611436f", "5612": "d1c696160a5f946741f0660e7b6592c14218e886", "5620": "06bd33c28dab6148fdeb9b5c5520aa41af745ada", "5621": "b888e03acbbbc5fddbf99dcb7ff9b5c39bd35691", "5622": "98b4884bcbe82b3ac0fdb25deb2b782fc6ca297a", "5623": "5144c623fe7cd3690693b161724f8c0ed3ce8608", "5626": "4e862a3033a47b24ccf4779788b594d4a705a6d3", "5627": "14f8e2174f47f2e48f21cf551fa9a861654a602c", "5629": "f94685562c7c130ba055b818ce66ce1dd5d06975", "5644": "c5c5c1ab8e4afe355c0f5be1bf38f6512bf21747", "5647": "4663a6aac371a694997cac08c14ebbfc546c6ca9", "5650": "f2abce063098d1a88c8c664777a56414c4383c31", "5659": "941594da953a82f07c36f590066faaf822193b87", "5660": "adc6c04cbd261fe66c4369cde4aadec7f73ecb0b", "5661": "b17d618d07962c8c855fccc50a0fe067ded865c3", "5662": "d863a3544e804d0540b06f226e866cb83b0627fb", "5665": "16dd1ca3acc3f8e23ee674904e2b2afa350d535d", "5666": "aa5076c993cc1d64aa29327c33dc3d5265d5f510", "5667": "79a31c7be51dd00660db84c14fd347f1110dcc83", "5669": "f84c1c1afe0cc88c6598285966d9f8fbc42d3180", "5670": "748ac694354176fdd07457af9a08d24651211217", "5671": "36cc583a65c6ffc01fa1dc8eb8bd0ff680fbc5ef", "5675": "e12353cade98f76a297c3208adb89430dc0ec5ff", "5676": "58d9551f2937fdd767c9cae59b191a379e0f8533", "5679": "36362c9321cc233c857fb3fd1d9c1ee291442909", "5697": "c73710d25130c5b2478d0ff75c7cd741c6620183", "5702": "270df580455d2fc29692b6b7a453ba82fdc9e47a", "5703": "e68731cdf0a18c2437ced2ca1217e22c51b4c0ee", "5724": "e838dc7490b8d01bd0b9ba63e5ee1ad25ec93211", "5727": "1f0b2ad38c26c98f1dbe5462f7ef2a4c8c02d9f4"}, "revision_to_date": {"1": 1678962043000, "217": 1683553743000, "341": 1684752617000, "358": 1685003160000, "484": 1686329453000, "567": 1687514692000, "575": 1687872752000, "577": 1687875060000, "579": 1687875381000, "641": 1689023430000, "714": 1690274961000, "742": 1690383063000, "757": 1690877437000, "772": 1691153153000, "800": 1691507415000, "858": 1692029284000, "932": 1693225318000, "1023": 1693832948000, "1035": 1693959700000, "1053": 1694183836000, "1054": 1694192794000, "1071": 1694691587000, "1107": 1695302465000, "1129": 1695645568000, "1141": 1695748368000, "1165": 1695993535000, "1183": 1696342948000, "1198": 1696861881000, "1263": 1697796332000, "1337": 1698689107000, "1346": 1698773717000, "1350": 1698788569000, "1365": 1698844752000, "1366": 1698846082000, "1489": 1700501108000, "1532": 1700745145000, "1541": 1700831897000, "1715": 1702383559000, "1739": 1702566689000, "1933": 1705055585000, "1955": 1705330362000, "2080": 1706109394000, "2129": 1706519986000, "2135": 1706624458000, "2180": 1707226614000, "2181": 1707227524000, "2229": 1707409032000, "2256": 1707819609000, "2329": 1708946786000, "2331": 1708947852000, "2359": 1709033907000, "2463": 1710379468000, "2551": 1711382694000, "2560": 1711611193000, "2581": 1712137847000, "2585": 1712215927000, "2613": 1713175869000, "2705": 1714565416000, "2710": 1714829539000, "2711": 1714495986000, "2714": 1715088318000, "2720": 1715251867000, "2722": 1715273959000, "2726": 1715370783000, "2729": 1715672695000, "2734": 1715777261000, "2741": 1715784844000, "2742": 1712846843000, "2751": 1715950456000, "2760": 1716292993000, "2763": 1716453922000, "2772": 1716997758000, "2774": 1717087554000, "2777": 1717170495000, "2790": 1717414433000, "2792": 1717414901000, "2794": 1717505162000, "2797": 1717513301000, "2825": 1717592919000, "2831": 1718036933000, "2842": 1718120600000, "2848": 1718196636000, "2854": 1718291838000, "2865": 1718729150000, "2872": 1718802776000, "2873": 1718792786000, "2884": 1718980969000, "2886": 1719219102000, "2900": 1719324822000, "2903": 1719326066000, "2906": 1719403203000, "2918": 1719565491000, "2926": 1719590331000, "2929": 1719779356000, "2932": 1719857742000, "2936": 1719997980000, "2941": 1720163872000, "2944": 1720442209000, "2947": 1720508932000, "2950": 1720715859000, "2952": 1720770981000, "2973": 1721060703000, "2984": 1721117007000, "2995": 1721321907000, "2999": 1721729324000, "3005": 1721744859000, "3030": 1721812779000, "3032": 1721819059000, "3033": 1721825228000, "3038": 1722001245000, "3041": 1722271303000, "3052": 1722346568000, "3063": 1722432803000, "3075": 1722514049000, "3077": 1722514119000, "3081": 1722592500000, "3084": 1722602989000, "3105": 1722855272000, "3111": 1722935151000, "3112": 1722939960000, "3120": 1722440540000, "3124": 1723236605000, "3133": 1723308188000, "3137": 1723545798000, "3158": 1723834000000, "3162": 1724088393000, "3168": 1724164405000, "3179": 1724252182000, "3191": 1719933161000, "3197": 1724417246000, "3209": 1724832448000, "3219": 1724840138000, "3225": 1725286457000, "3235": 1725284399000, "3248": 1725474667000, "3252": 1724658986000, "3254": 1725548928000, "3255": 1725554020000, "3265": 1725887807000, "3273": 1725973036000, "3279": 1725979620000, "3281": 1726036670000, "3283": 1726248025000, "3287": 1726508701000, "3295": 1726755925000, "3299": 1727080005000, "3302": 1727161687000, "3305": 1727187284000, "3309": 1727339498000, "3314": 1727437221000, "3316": 1727688292000, "3321": 1727781021000, "3335": 1727792229000, "3340": 1728043587000, "3347": 1728297449000, "3351": 1728563742000, "3355": 1728625775000, "3375": 1728660284000, "3383": 1728639898000, "3393": 1728983385000, "3397": 1726151644000, "3413": 1729091189000, "3430": 1729100594000, "3433": 1729157707000, "3434": 1729173033000, "3436": 1729188377000, "3438": 1729245260000, "3442": 1729497839000, "3443": 1726151644000, "3445": 1729669684000, "3453": 1729759206000, "3457": 1729854933000, "3459": 1729878319000, "3463": 1730119639000, "3465": 1730119739000, "3485": 1730122298000, "3487": 1728899271000, "3489": 1730221470000, "3491": 1730272822000, "3492": 1730281261000, "3495": 1730296936000, "3497": 1730301130000, "3530": 1730377229000, "3534": 1730382436000, "3536": 1728479591000, "3537": 1730389272000, "3540": 1730799065000, "3549": 1730812670000, "3551": 1730477115000, "3555": 1730822184000, "3563": 1730819209000, "3567": 1730889307000, "3569": 1730972004000, "3571": 1730989718000, "3573": 1731075419000, "3575": 1731147566000, "3577": 1731407686000, "3578": 1731502029000, "3580": 1731509471000, "3587": 1731675638000, "3589": 1731683684000, "3593": 1731653776000, "3595": 1731932289000, "3596": 1731941827000, "3597": 1731997480000, "3598": 1732023861000, "3600": 1732087066000, "3602": 1732184398000, "3605": 1732296908000, "3608": 1732534292000, "3609": 1732550672000, "3611": 1731499132000, "3613": 1732617767000, "3618": 1733147084000, "3619": 1733147997000, "3620": 1733167033000, "3630": 1733217554000, "3632": 1733153149000, "3633": 1733234132000, "3634": 1733236319000, "3640": 1733236736000, "3641": 1733298410000, "3650": 1733302605000, "3661": 1733329626000, "3665": 1729765965000, "3672": 1733391146000, "3673": 1733391294000, "3675": 1733391543000, "3677": 1733392506000, "3683": 1733368295000, "3684": 1733398667000, "3685": 1733403494000, "3686": 1733406665000, "3687": 1733412361000, "3692": 1733398551000, "3693": 1733415321000, "3697": 1733227883000, "3698": 1733471664000, "3702": 1733481990000, "3703": 1733485402000, "3706": 1733477444000, "3707": 1733487497000, "3714": 1733489089000, "3720": 1733477444000, "3721": 1733492269000, "3722": 1733494970000, "3723": 1733497701000, "3724": 1733501237000, "3728": 1733511041000, "3729": 1733731938000, "3730": 1733732972000, "3731": 1733737041000, "3737": 1733740658000, "3738": 1733485963000, "3739": 1733753280000, "3745": 1733413595000, "3746": 1733811525000, "3752": 1733750451000, "3753": 1733846901000, "3754": 1733848588000, "3756": 1733851159000, "3757": 1727439740000, "3761": 1733983014000, "3766": 1734009601000, "3769": 1734347040000, "3770": 1733756712000, "3771": 1734360808000, "3776": 1733929695000, "3778": 1734373865000, "3779": 1734425916000, "3780": 1734432891000, "3781": 1734442108000, "3784": 1734514722000, "3788": 1734366030000, "3795": 1734606584000, "3796": 1734618168000, "3799": 1734710845000, "3800": 1734729346000, "3803": 1735302468000, "3804": 1735808218000, "3805": 1735835555000, "3807": 1736178475000, "3810": 1736256881000, "3819": 1736264055000, "3820": 1736331620000, "3826": 1736333511000, "3827": 1731584115000, "3837": 1736419549000, "3838": 1736419864000, "3852": 1736497390000, "3853": 1736509512000, "3854": 1736514359000, "3855": 1736518394000, "3856": 1736519266000, "3861": 1736526620000, "3866": 1736527759000, "3867": 1736528133000, "3868": 1736528251000, "3880": 1736778177000, "3885": 1736860274000, "3888": 1736869813000, "3891": 1736934927000, "3899": 1736523123000, "3901": 1736954553000, "3903": 1736960725000, "3908": 1737033859000, "3913": 1737043225000, "3918": 1737111923000, "3921": 1737128539000, "3923": 1734373221000, "3927": 1737467184000, "3930": 1733929695000, "3932": 1737550819000, "3933": 1734531554000, "3934": 1737636002000, "3939": 1737668575000, "3940": 1737708027000, "3942": 1726125290000, "3944": 1737983940000, "3948": 1737565214000, "3950": 1734597155000, "3951": 1738015423000, "3952": 1737736098000, "3953": 1738054560000, "3954": 1738056045000, "3959": 1738076743000, "3960": 1738083394000, "3961": 1738088736000, "3963": 1738142611000, "3968": 1738159388000, "3970": 1738172864000, "3974": 1738243159000, "3975": 1738247248000, "3976": 1738255801000, "3978": 1738315232000, "3980": 1738332344000, "3982": 1738319705000, "3983": 1738566021000, "3984": 1738337516000, "3987": 1738580377000, "3988": 1738584125000, "3990": 1738599826000, "3991": 1738676519000, "3995": 1738760860000, "3996": 1738755598000, "4004": 1738766871000, "4005": 1738584125000, "4014": 1738829031000, "4016": 1738854987000, "4017": 1738855166000, "4019": 1738860789000, "4020": 1738858167000, "4023": 1738927717000, "4026": 1738931267000, "4028": 1738942204000, "4030": 1738317412000, "4036": 1739375895000, "4040": 1739478801000, "4041": 1739552725000, "4043": 1739785595000, "4044": 1739792347000, "4045": 1739799803000, "4046": 1739803305000, "4047": 1739806727000, "4050": 1739812575000, "4052": 1739814451000, "4054": 1739873296000, "4058": 1739888579000, "4060": 1739890142000, "4065": 1739957343000, "4067": 1739959806000, "4069": 1739960552000, "4074": 1739205153000, "4081": 1739987173000, "4083": 1739995547000, "4085": 1740046484000, "4089": 1740066939000, "4090": 1740126554000, "4091": 1740132808000, "4104": 1740390135000, "4106": 1740435423000, "4107": 1740435423000, "4111": 1740475154000, "4112": 1740483871000, "4113": 1740483992000, "4116": 1740496276000, "4120": 1740502122000, "4122": 1740558239000, "4124": 1740558671000, "4129": 1740499030000, "4130": 1740647925000, "4131": 1740560429000, "4132": 1740673789000, "4133": 1740737792000, "4136": 1740745319000, "4138": 1740750316000, "4139": 1740753499000, "4140": 1740753689000, "4144": 1740763624000, "4147": 1741061110000, "4156": 1741001991000, "4157": 1741083460000, "4158": 1741099609000, "4159": 1741102041000, "4161": 1741106215000, "4162": 1741107385000, "4163": 1741112763000, "4166": 1741191547000, "4169": 1741247961000, "4171": 1741253743000, "4178": 1741172665000, "4179": 1741265283000, "4181": 1741277301000, "4183": 1741277647000, "4184": 1741333983000, "4185": 1741171230000, "4186": 1741347887000, "4187": 1741171230000, "4190": 1741357088000, "4191": 1741361198000, "4193": 1741364561000, "4194": 1741339895000, "4195": 1741371032000, "4197": 1741171230000, "4198": 1741600952000, "4202": 1741623596000, "4203": 1741636700000, "4204": 1741641876000, "4205": 1741676376000, "4206": 1741680869000, "4207": 1741683312000, "4208": 1741686107000, "4209": 1741686124000, "4210": 1741689100000, "4211": 1741693500000, "4212": 1741694342000, "4213": 1741697509000, "4216": 1741710795000, "4217": 1741720182000, "4218": 1741765267000, "4222": 1741791930000, "4224": 1741793323000, "4226": 1741844613000, "4227": 1741856758000, "4229": 1741862580000, "4230": 1741862752000, "4231": 1741863938000, "4233": 1741873350000, "4236": 1741801382000, "4242": 1741879316000, "4243": 1741879589000, "4244": 1741880458000, "4246": 1741882162000, "4247": 1741883203000, "4248": 1741889767000, "4249": 1741890870000, "4251": 1741893927000, "4252": 1741940081000, "4256": 1741952773000, "4257": 1741957602000, "4259": 1741963436000, "4260": 1741963643000, "4261": 1741967978000, "4263": 1742202033000, "4268": 1742299666000, "4298": 1742366650000, "4299": 1742373414000, "4300": 1741877825000, "4301": 1742377030000, "4304": 1742379446000, "4308": 1742456797000, "4312": 1741347887000, "4318": 1742573321000, "4319": 1742776083000, "4320": 1742803048000, "4321": 1742814008000, "4322": 1742817001000, "4324": 1742833276000, "4325": 1742403337000, "4327": 1742911638000, "4328": 1742912071000, "4329": 1742981776000, "4336": 1743091673000, "4338": 1743095812000, "4339": 1743153261000, "4340": 1743175307000, "4342": 1743179441000, "4343": 1743358599000, "4345": 1743409530000, "4346": 1743409583000, "4347": 1743436861000, "4351": 1743490697000, "4352": 1743496168000, "4358": 1743520344000, "4359": 1743520776000, "4361": 1743580923000, "4367": 1743662600000, "4368": 1743669885000, "4370": 1743670865000, "4371": 1743671077000, "4372": 1743671775000, "4373": 1743674652000, "4374": 1743751741000, "4376": 1743751920000, "4380": 1743986012000, "4381": 1743986052000, "4382": 1744005300000, "4384": 1744028386000, "4385": 1744033273000, "4389": 1744092484000, "4393": 1744117201000, "4395": 1744117577000, "4397": 1744123075000, "4400": 1744123761000, "4401": 1744125534000, "4402": 1744126269000, "4403": 1744039712000, "4404": 1744132900000, "4405": 1744134647000, "4406": 1744135599000, "4407": 1744190700000, "4408": 1744194458000, "4409": 1744194599000, "4410": 1744197334000, "4411": 1744197472000, "4412": 1744201037000, "4413": 1744204792000, "4414": 1744208568000, "4415": 1744223785000, "4416": 1744235648000, "4418": 1744222408000, "4419": 1744238015000, "4420": 1744239219000, "4421": 1744269668000, "4422": 1744277549000, "4424": 1744277549000, "4426": 1744286626000, "4427": 1744293336000, "4428": 1744297055000, "4430": 1744300479000, "4433": 1744360488000, "4436": 1744040912000, "4437": 1744368307000, "4438": 1744368417000, "4439": 1744369656000, "4440": 1744370165000, "4441": 1744373988000, "4442": 1744376120000, "4443": 1744378638000, "4444": 1744379741000, "4445": 1744590907000, "4446": 1744623884000, "4447": 1744640376000, "4449": 1744705503000, "4454": 1744728498000, "4455": 1744728838000, "4457": 1744730027000, "4459": 1744783873000, "4460": 1744785169000, "4461": 1744788027000, "4462": 1744788620000, "4463": 1744789116000, "4464": 1744789878000, "4465": 1744790305000, "4466": 1744790724000, "4467": 1744790753000, "4468": 1744791011000, "4469": 1744803177000, "4470": 1744805310000, "4471": 1744806956000, "4472": 1744807876000, "4473": 1744807895000, "4474": 1744810086000, "4475": 1744811268000, "4479": 1744868489000, "4480": 1744868804000, "4481": 1744869018000, "4482": 1744869371000, "4483": 1744869589000, "4486": 1744881360000, "4487": 1744882795000, "4488": 1744882941000, "4489": 1744895081000, "4493": 1744900760000, "4494": 1744905479000, "4495": 1745195704000, "4496": 1745195752000, "4499": 1733761738000, "4501": 1745403477000, "4502": 1745407727000, "4503": 1745407886000, "4505": 1745416032000, "4508": 1745420511000, "4509": 1745420613000, "4510": 1745502108000, "4513": 1745505921000, "4514": 1745507725000, "4515": 1745509374000, "4517": 1745566561000, "4519": 1745585714000, "4520": 1745585829000, "4521": 1745800489000, "4522": 1745800549000, "4523": 1745832092000, "4524": 1745836128000, "4525": 1745837829000, "4526": 1745837932000, "4527": 1745844282000, "4528": 1745850506000, "4542": 1745935490000, "4543": 1745942570000, "4544": 1745945048000, "4545": 1746007955000, "4547": 1746019989000, "4548": 1746023526000, "4550": 1746027392000, "4551": 1746028898000, "4552": 1746029189000, "4553": 1746030317000, "4554": 1746031441000, "4555": 1746089226000, "4556": 1746090492000, "4557": 1746091739000, "4559": 1746098438000, "4570": 1746180810000, "4573": 1746196158000, "4575": 1746234224000, "4576": 1744206494000, "4577": 1746405374000, "4578": 1746405414000, "4587": 1733761738000, "4589": 1746794777000, "4590": 1747010177000, "4591": 1747010219000, "4592": 1747039676000, "4596": 1747040235000, "4597": 1747053560000, "4598": 1747118378000, "4604": 1747131344000, "4605": 1747133153000, "4608": 1747231096000, "4609": 1747375929000, "4610": 1747377713000, "4612": 1747392078000, "4617": 1747405237000, "4618": 1747614985000, "4619": 1747615034000, "4621": 1747657731000, "4622": 1747728582000, "4626": 1747847008000, "4628": 1747903829000, "4629": 1747912740000, "4643": 1748018807000, "4644": 1748219729000, "4645": 1748219770000, "4647": 1748339648000, "4658": 1748414056000, "4664": 1748430418000, "4666": 1748506374000, "4671": 1748583013000, "4675": 1748599094000, "4676": 1748602522000, "4678": 1748824589000, "4679": 1748824633000, "4683": 1748857286000, "4685": 1748866112000, "4688": 1748870383000, "4689": 1748870449000, "4694": 1748927971000, "4707": 1748966227000, "4711": 1749019512000, "4714": 1749044673000, "4731": 1749128783000, "4738": 1749192520000, "4740": 1749193648000, "4741": 1749213171000, "4742": 1749215985000, "4747": 1749429432000, "4748": 1749429468000, "4754": 1749482348000, "4764": 1749664657000, "4773": 1750034210000, "4774": 1750034256000, "4776": 1750071612000, "4777": 1750085010000, "4778": 1750087450000, "4780": 1750087983000, "4781": 1750145159000, "4782": 1750153252000, "4784": 1750084411000, "4785": 1750170384000, "4794": 1749812789000, "4797": 1750186036000, "4798": 1750221905000, "4799": 1750228519000, "4807": 1750237406000, "4808": 1750239628000, "4809": 1750244357000, "4810": 1750244443000, "4814": 1750246311000, "4815": 1750252294000, "4817": 1750314771000, "4818": 1750318533000, "4820": 1750325154000, "4880": 1750363319000, "4881": 1750398440000, "4886": 1750413753000, "4888": 1750425356000, "4889": 1750426011000, "4890": 1750427936000, "4891": 1750429459000, "4892": 1750429571000, "4893": 1750431777000, "4894": 1750639074000, "4895": 1750639112000, "4896": 1750656345000, "4897": 1750657233000, "4904": 1750666993000, "4909": 1750675720000, "4911": 1750676070000, "4912": 1750681284000, "4913": 1750681437000, "4914": 1750683277000, "4916": 1750684699000, "4917": 1750738655000, "4918": 1750741966000, "4919": 1750749900000, "4920": 1750750130000, "4921": 1750750237000, "4922": 1750755198000, "4924": 1750764836000, "4925": 1750767459000, "4927": 1750773272000, "4928": 1750773951000, "4930": 1750833171000, "4931": 1750833681000, "4932": 1750924484000, "4938": 1750065217000, "4940": 1750944576000, "4941": 1750946831000, "4942": 1751269245000, "4943": 1751272216000, "4945": 1751280116000, "4946": 1751286298000, "4947": 1751443437000, "4948": 1751519461000, "4951": 1751536767000, "4953": 1751846870000, "4954": 1751846913000, "4955": 1751866644000, "4977": 1751878489000, "4978": 1751882141000, "4979": 1751888075000, "4981": 1751882920000, "4982": 1751892822000, "4984": 1751957453000, "4990": 1751989500000, "5000": 1752056621000, "5002": 1752050204000, "5003": 1752140432000, "5004": 1752141097000, "5005": 1752147867000, "5006": 1752148138000, "5007": 1752149708000, "5009": 1752150745000, "5010": 1752153427000, "5011": 1752160511000, "5012": 1752161933000, "5013": 1752146448000, "5016": 1752223540000, "5017": 1752242863000, "5018": 1752242681000, "5020": 1752247139000, "5021": 1752339415000, "5026": 1752451678000, "5033": 1752501287000, "5034": 1752511574000, "5036": 1752590466000, "5038": 1752659672000, "5040": 1752740814000, "5041": 1752741166000, "5042": 1752755936000, "5043": 1752756300000, "5044": 1752761499000, "5045": 1752772016000, "5046": 1752818268000, "5049": 1752842384000, "5053": 1751974504000, "5055": 1752849558000, "5056": 1752850564000, "5057": 1753056540000, "5058": 1753084275000, "5060": 1753091752000, "5062": 1753097304000, "5063": 1753097386000, "5064": 1753100400000, "5066": 1753170633000, "5067": 1753171485000, "5068": 1753186005000, "5069": 1753191655000, "5073": 1753193788000, "5074": 1753191430000, "5076": 1753197730000, "5077": 1753251220000, "5078": 1753262162000, "5079": 1753269354000, "5087": 1753190374000, "5089": 1753333924000, "5090": 1753342065000, "5091": 1753280658000, "5092": 1753346987000, "5093": 1753360267000, "5095": 1753280658000, "5097": 1753367141000, "5099": 1753372070000, "5101": 1753422044000, "5102": 1753437216000, "5103": 1753446649000, "5105": 1753448117000, "5106": 1753449768000, "5113": 1753458314000, "5114": 1753458644000, "5115": 1753661344000, "5116": 1753685816000, "5118": 1753695451000, "5119": 1753696424000, "5120": 1753702168000, "5121": 1753702225000, "5122": 1753705431000, "5123": 1753706868000, "5125": 1753760558000, "5126": 1753771726000, "5128": 1753772462000, "5129": 1753777568000, "5132": 1753780909000, "5133": 1753781007000, "5134": 1753784053000, "5136": 1753276706000, "5141": 1753799855000, "5144": 1753819255000, "5151": 1753878642000, "5158": 1753966382000, "5160": 1754036682000, "5161": 1754040139000, "5162": 1754040253000, "5163": 1754058617000, "5170": 1754295017000, "5171": 1754299179000, "5172": 1754300123000, "5174": 1754305882000, "5175": 1754306912000, "5176": 1754309941000, "5177": 1754320082000, "5179": 1754321975000, "5183": 1754387002000, "5184": 1754389383000, "5185": 1754390870000, "5186": 1749811234000, "5224": 1750931137000, "5226": 1754399281000, "5227": 1754402279000, "5230": 1754468821000, "5232": 1754470318000, "5234": 1754475777000, "5235": 1753282370000, "5236": 1754487457000, "5238": 1754496023000, "5243": 1754564933000, "5244": 1754565585000, "5245": 1754572288000, "5246": 1754573401000, "5252": 1754582246000, "5253": 1754641577000, "5254": 1754642038000, "5255": 1754656037000, "5256": 1754656247000, "5259": 1754675740000, "5260": 1749827940000, "5263": 1754870873000, "5264": 1754870919000, "5265": 1754894826000, "5268": 1754899584000, "5269": 1754902614000, "5270": 1754905920000, "5272": 1754917262000, "5277": 1754917366000, "5278": 1754982317000, "5280": 1754983016000, "5281": 1754983516000, "5282": 1754983656000, "5283": 1754983972000, "5284": 1754985415000, "5285": 1754987513000, "5286": 1754988534000, "5287": 1754989205000, "5288": 1754993333000, "5289": 1755000982000, "5290": 1755002452000, "5291": 1755002840000, "5292": 1755004488000, "5293": 1755009863000, "5296": 1755013745000, "5297": 1755069476000, "5301": 1755087487000, "5302": 1755100575000, "5303": 1755145266000, "5309": 1755172980000, "5310": 1755173040000, "5313": 1755105770000, "5314": 1755180493000, "5318": 1755262657000, "5319": 1755262673000, "5320": 1755264126000, "5327": 1755275600000, "5330": 1755501011000, "5331": 1755515073000, "5332": 1755529899000, "5333": 1755581409000, "5334": 1755589343000, "5335": 1755591571000, "5347": 1755679077000, "5349": 1755679376000, "5350": 1755680397000, "5352": 1755683362000, "5354": 1755683387000, "5355": 1755683428000, "5356": 1755683826000, "5361": 1755697468000, "5362": 1755699524000, "5364": 1755708179000, "5371": 1755782059000, "5378": 1755851931000, "5380": 1755861025000, "5384": 1755865803000, "5386": 1755871175000, "5390": 1756080433000, "5391": 1756080475000, "5392": 1756105773000, "5394": 1756115121000, "5395": 1756123476000, "5396": 1756201272000, "5398": 1756205444000, "5403": 1756279806000, "5406": 1756284612000, "5411": 1756293169000, "5417": 1756318009000, "5420": 1756366700000, "5427": 1756380223000, "5428": 1756411549000, "5429": 1756442835000, "5433": 1756447526000, "5449": 1756478636000, "5453": 1756685304000, "5454": 1756685350000, "5457": 1756740543000, "5459": 1756740665000, "5460": 1756799677000, "5481": 1757058329000, "5482": 1757290018000, "5483": 1757290061000, "5484": 1757336842000, "5485": 1757337907000, "5486": 1757337927000, "5494": 1757511165000, "5498": 1757580836000, "5501": 1757894847000, "5502": 1757894890000, "5503": 1757918691000, "5505": 1757929617000, "5506": 1757929635000, "5508": 1757931505000, "5510": 1758011656000, "5511": 1758097967000, "5518": 1758109999000, "5519": 1758126199000, "5520": 1758187970000, "5523": 1758202525000, "5526": 1758269149000, "5529": 1758274154000, "5531": 1758290197000, "5532": 1758499658000, "5533": 1758499707000, "5536": 1758608839000, "5539": 1758627815000, "5541": 1758633491000, "5543": 1758702174000, "5544": 1758705828000, "5547": 1758721187000, "5549": 1758722415000, "5551": 1758783772000, "5555": 1758904105000, "5556": 1759104476000, "5561": 1759140896000, "5563": 1759149639000, "5568": 1759216560000, "5569": 1759216568000, "5570": 1759216623000, "5571": 1759220010000, "5575": 1759253496000, "5576": 1759299393000, "5579": 1759316731000, "5582": 1759323808000, "5590": 1759391860000, "5592": 1759402492000, "5593": 1759403330000, "5595": 1759481752000, "5596": 1759502292000, "5598": 1759709265000, "5600": 1759761147000, "5601": 1759825670000, "5603": 1759854940000, "5604": 1759855064000, "5612": 1760008974000, "5620": 1760021225000, "5621": 1760098282000, "5622": 1760106408000, "5623": 1760314040000, "5626": 1760368607000, "5627": 1760369717000, "5629": 1760429430000, "5644": 1760528751000, "5647": 1760538931000, "5650": 1759396305000, "5659": 1760621162000, "5660": 1760628299000, "5661": 1760640421000, "5662": 1760660394000, "5665": 1760711385000, "5666": 1760918868000, "5667": 1760918924000, "5669": 1760960925000, "5670": 1761035414000, "5671": 1761045993000, "5675": 1761143856000, "5676": 1761231529000, "5679": 1761234349000, "5697": 1761313776000, "5702": 1761523652000, "5703": 1761523715000, "5724": 1761749717000, "5727": 1761776245000}, "params": {"machine": ["ArcticDB-Medium-Runner"], "python": ["3.11", "3.6"], "version": [1, null], "branch": ["master"]}, "graph_param_list": [{"machine": "ArcticDB-Medium-Runner", "python": "3.11", "branch": "master", "version": null}, {"machine": "ArcticDB-Medium-Runner", "python": "3.6", "branch": "master", "version": null}], "benchmarks": {"arrow.ArrowNumeric.peakmem_read": {"code": "class ArrowNumeric:\n    def peakmem_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "arrow.ArrowNumeric.peakmem_read", "param_names": ["rows", "date_range"], "params": [["100000", "100000000"], ["None", "'middle'"]], "setup_cache_key": "arrow:38", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "67ab80e297890f213706b5160255c969b0aee853adfd6ae53655c45b7e845f85"}, "arrow.ArrowNumeric.peakmem_write": {"code": "class ArrowNumeric:\n    def peakmem_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "arrow.ArrowNumeric.peakmem_write", "param_names": ["rows", "date_range"], "params": [["100000", "100000000"], ["None", "'middle'"]], "setup_cache_key": "arrow:38", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "b28a54936bacb902a56cad0b9d235bd3e13ac04ac37687cf265c5a987b7ea2d1"}, "arrow.ArrowNumeric.time_read": {"code": "class ArrowNumeric:\n    def time_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "arrow.ArrowNumeric.time_read", "number": 5, "param_names": ["rows", "date_range"], "params": [["100000", "100000000"], ["None", "'middle'"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "arrow:38", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ecb51f067ac7d40c4b9c9df11fa1a0b1f3c682a14f1ee974cf8c3eb9fcfd86d7", "warmup_time": 0}, "arrow.ArrowNumeric.time_write": {"code": "class ArrowNumeric:\n    def time_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "arrow.ArrowNumeric.time_write", "number": 5, "param_names": ["rows", "date_range"], "params": [["100000", "100000000"], ["None", "'middle'"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "arrow:38", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b0a49f584f44c8451a2fda903df76351a10368b81d639239cfa3023e9dee737b", "warmup_time": 0}, "arrow.ArrowStrings.peakmem_read": {"code": "class ArrowStrings:\n    def peakmem_read(self, rows, date_range, unique_string_count):\n        self.lib.read(self.symbol_name(rows, unique_string_count), date_range=self.date_range)\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "arrow.ArrowStrings.peakmem_read", "param_names": ["rows", "date_range", "unique_string_count"], "params": [["10000", "1000000"], ["None", "'middle'"], ["1", "100", "100000"]], "setup_cache_key": "arrow:113", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "17372c9872c2a27c6319aead89a5339c06f9741b59700e2c860032c657e3f3ed"}, "arrow.ArrowStrings.peakmem_write": {"code": "class ArrowStrings:\n    def peakmem_write(self, rows, date_range, unique_string_count):\n        # No point in running with and without date range\n        if date_range is None:\n            self.fresh_lib.write(self.symbol_name(rows, unique_string_count), self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "arrow.ArrowStrings.peakmem_write", "param_names": ["rows", "date_range", "unique_string_count"], "params": [["10000", "1000000"], ["None", "'middle'"], ["1", "100", "100000"]], "setup_cache_key": "arrow:113", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "0001f96c10c9fea4573729eb8860c6e1b4bd3a50577aa35cdc33792c4e574aa5"}, "arrow.ArrowStrings.time_read": {"code": "class ArrowStrings:\n    def time_read(self, rows, date_range, unique_string_count):\n        self.lib.read(self.symbol_name(rows, unique_string_count), date_range=self.date_range)\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "arrow.ArrowStrings.time_read", "number": 5, "param_names": ["rows", "date_range", "unique_string_count"], "params": [["10000", "1000000"], ["None", "'middle'"], ["1", "100", "100000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "arrow:113", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f3b43fb90d56ca0b3e21885c08e66821b96857707c7415453a258ca79ba3b713", "warmup_time": 0}, "arrow.ArrowStrings.time_write": {"code": "class ArrowStrings:\n    def time_write(self, rows, date_range, unique_string_count):\n        # No point in running with and without date range\n        if date_range is None:\n            self.fresh_lib.write(self.symbol_name(rows, unique_string_count), self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "arrow.ArrowStrings.time_write", "number": 5, "param_names": ["rows", "date_range", "unique_string_count"], "params": [["10000", "1000000"], ["None", "'middle'"], ["1", "100", "100000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "arrow:113", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b92e0da75511b2d40c7abbb7417570391bd80aaeab41851f5a6ad042caf933d5", "warmup_time": 0}, "basic_functions.BasicFunctions.peakmem_read": {"code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_read", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "3b847508b2a63c58c508dc7e7aec08547e2ca09a57bcf3f6777619a5cb149b7f"}, "basic_functions.BasicFunctions.peakmem_read_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_read_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "a4ca2ebef508c6560b98af94d69ba7cd459cc02976eaed2aad41ac976d932b11"}, "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "74f0d776147883a788c405dd90a8490ea4b766f2946aaf5ab43cc48076c2b929"}, "basic_functions.BasicFunctions.peakmem_read_with_columns": {"code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_read_with_columns", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "34501bd0311c8b2644012b8f713f8a596bba14dbf1976cbbded37060cf77709f"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "908e8e1658fc2235b08f2996edb57c49852791e6f6c187165b3b931e7d7e896c"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "7870e015641ad2975bbc58ad43428c0e65d3771d1cd2838d2f72f4d6c57aa926"}, "basic_functions.BasicFunctions.peakmem_write": {"code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_write", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "0db13340295d95fa06e3f786be93bbc345a0f67ffe4fdcd226189c2b82aecb5e"}, "basic_functions.BasicFunctions.peakmem_write_short_wide": {"code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_write_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "3128ed9e1a62c74b07c0c51e215080ecb9ccb8284e6221dadb0d4b229acceb2d"}, "basic_functions.BasicFunctions.peakmem_write_staged": {"code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BasicFunctions.peakmem_write_staged", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "42c10f3b6072be6b54db49e98670de0ce65dc2d7e543f6b1af53bd4fd28bba5f"}, "basic_functions.BasicFunctions.time_read": {"code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "99231807c0927256747827b6d0d3bf8d565cae9f2b6955d40e3c403ff162daac", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_short_wide": {"code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "18cb78627bc67fc910cdce20943fb32ba17bc8271a1b13de73393d76c1411f9e", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_ultra_short_wide": {"code": "class BasicFunctions:\n    def time_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_ultra_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f5a362303294a64862824aa956d62c05bac490feb7438d742720d341274beeb7", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_with_columns": {"code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_columns", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "97fab62dd3036b086e61d8e193b02c3c566b76c76cb514b44dfd3d9090a2bbe7", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_with_date_ranges": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8fe0b03e88febe68a480effc73ec81b1c3bbb24fa31f8d4868db79760d58ccdd", "warmup_time": 0}, "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "50281bd8fb1b296b9de072d72b5bb207d9b5d1700949b1d911f7475339110289", "warmup_time": 0}, "basic_functions.BasicFunctions.time_write": {"code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "a399d5220450caaa75806e22756b1bfcf8b27c050fa15dd9165be085ae2b0b63", "warmup_time": 0}, "basic_functions.BasicFunctions.time_write_short_wide": {"code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "7d9de7789dd1f63d916bd3e86d00c633177462a29f323774fd022cebf75e92ab", "warmup_time": 0}, "basic_functions.BasicFunctions.time_write_staged": {"code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_staged", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:48", "timeout": 6000, "type": "time", "unit": "seconds", "version": "00d21f9de9837bdad48c5ad99e7c3312e1a2825b119b58548d7fd703481d9501", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.peakmem_read_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1154d4d002a16465c10fd5e41721ed25ee8d4a5fa3790c7718d7f309f1b8b29c"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "eb2f74f4dab10d4472f2349a2d539c9609baaaa6debf7206c064a2b93c495bfc"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "114711c1c4ebf66c64ab33fb81f5f8cc73ce78984e2472f492d0c80f96f331d8"}, "basic_functions.BatchBasicFunctions.peakmem_update_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BatchBasicFunctions.peakmem_update_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "11bde9a31b891ee985296085ae2b9f7805556601060b2e993f257d8ed4a2144d"}, "basic_functions.BatchBasicFunctions.peakmem_write_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e29d7bab3ecd450da657f11c448f62bbb39d5a2607fb6f4cc5b92db2dee50dc9"}, "basic_functions.BatchBasicFunctions.time_read_batch": {"code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch", "number": 1, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "time", "unit": "seconds", "version": "95a87e9c330d6f1b83b13cdf87403e69a4c2d4a3992430942454acfd5021b73a", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_read_batch_pure": {"code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure", "number": 1, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9a6a0b018e486ac4f51dcd497fd51d6394c9b92418337aab1e6bbc5347b3674c", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns", "number": 1, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2c798cded56db601f7a5e81cf0d0e77407d3717695a4c4863d4e68c96618393b", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges", "number": 1, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "time", "unit": "seconds", "version": "04e761907c8cd5b7d21ef3beadec4150292a508c0de7c9fd36305f118c97ceb3", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_update_batch": {"code": "class BatchBasicFunctions:\n    def time_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_update_batch", "number": 1, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ae298ed42a2bc58a2d14509b1578a2ecd7c11cab6f15200ef8ebfbc4f9924a27", "warmup_time": 0}, "basic_functions.BatchBasicFunctions.time_write_batch": {"code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_write_batch", "number": 1, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:181", "timeout": 6000, "type": "time", "unit": "seconds", "version": "49522447c3d9ac6f75f9df9a159dbbaeb95553440cf4bbb98303fce0c490bd66", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_append_large": {"code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_large", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8d1e9a72db76b8a1a5e0215330c54476f80a05dbea2174186950370cd831245e", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_append_short_wide": {"code": "class ModificationFunctions:\n    def time_append_short_wide(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_short_wide[rows].pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b15a42ff2e9792de5c9bee4cbf108d57071a6c2b2b504997406e7a848b83b0dc", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_append_single": {"code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "52acfbd4409bdcadf8af0cf5bd559122d5d467b56650bbb381c2d332a1aece2e", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete": {"code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "672259b2998d092f121f5de4c7b4327ebe5c3e444b677994c276755dc72bcffa", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete_multiple_versions": {"code": "class ModificationFunctions:\n    def time_delete_multiple_versions(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym_delete_multiple\", list(range(99)))\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_multiple_versions", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2f548b89ece0762d3a4ca804f4cebacdc407c72b720b63927a70a48213bd7b95", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete_over_time": {"code": "class ModificationFunctions:\n    def time_delete_over_time(self, lad: LargeAppendDataModify, rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_over_time", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "77d3d9f015b1ff0e318bd419a22aeca9b56c22cd174c0efd80e2909b9b68ceb5", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_delete_short_wide": {"code": "class ModificationFunctions:\n    def time_delete_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2409d72e307e71748c55e973bf651e1c775a5ed31869e089c243223efbb83df3", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_half": {"code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_half", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "7bde302f3062e94d0fe774eeebb7ad10585c4e40e525dbd3e05b25c0a7798ce1", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_short_wide": {"code": "class ModificationFunctions:\n    def time_update_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c012d3639c8e84e4a2e654631211e110644fd75bf36de8576515355a72d9409c", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_single": {"code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "d0b4f92fc686ee91643527d18268011dbe2053d2f5463efb671d28d1fe2e388a", "warmup_time": 0}, "basic_functions.ModificationFunctions.time_update_upsert": {"code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_upsert", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "basic_functions:339", "timeout": 6000, "type": "time", "unit": "seconds", "version": "916a69cac80c70e2fe053b743904459170731ac657c2efbe0222daafcdd9ba9e", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {"code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "331c9abb4f84b01dd28765b77a88e069ec6d6b70617a12dd5aa9c3e14ca6a6ad"}, "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {"code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ae73602827d4d1b739e519e8ca6a847c5938a5744ebf371ca78511b0be1bf16f"}, "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {"code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e708975c50d2b70ebdff11efa45f9fd15ceee9861301d5552f1e8ebe2cb4d1bd"}, "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {"code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall", "param_names": ["param1"], "params": [["1", "10"]], "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "3407eb183cf3c02afbeaf04e6c31bf6b5aaf615458cd8e2ad46a21b4d2af80e2"}, "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {"code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "time", "unit": "seconds", "version": "bf5e390b01e356685500d464be897fe7cb51531dcd92fccedec980f97f361e3c", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {"code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ceeb3b3e6049c66cb2ecabbb16485e4555cefc7920697c7a34de08993be14af0", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {"code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "time", "unit": "seconds", "version": "fa74284d1e48fd396138a5f50c53d92829194b7be1f0caa8f441f8820db4157c", "warmup_time": 0}, "bi_benchmarks.BIBenchmarks.time_query_readall": {"code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "bi_benchmarks.BIBenchmarks.time_query_readall", "number": 2, "param_names": ["param1"], "params": [["1", "10"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "bi_benchmarks:68", "timeout": 6000, "type": "time", "unit": "seconds", "version": "3cd2c7d90725498da459157638eb15b5a3fcc68aa91684951717ed5ab1c8ca63", "warmup_time": 0}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe": {"code": "class ComparisonBenchmarks:\n    def peakmem_create_dataframe(self, tpl):\n        df, dict = tpl\n        df = pd.DataFrame(dict)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:44", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "3580ee58cdf2db23481a370c46cd60e321e4c5b6b763d8a56a905cd857b05b65"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic": {"code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_arctic(self, tpl):\n        self.lib.read(ComparisonBenchmarks.SYMBOL)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:44", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "f13bf3d2bdd1f6129281823f3ac4a1c6c7d7b97be21efe3ae0d1065b07f89050"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet": {"code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_parquet(self, tpl):\n        pd.read_parquet(self.path_to_read)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:44", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "7d5db25fdc879f9e0da495f2f228353425dd662cc2ad93eecf662c2dd9be1c55"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic": {"code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_arctic(self, tpl):\n        df, dict = tpl\n        self.lib.write(\"symbol\", df)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:44", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "cd90a2c0309608e8c86b47a91b2a4c26ba9ba6b7bc50ca115f0d62b4ab82d064"}, "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet": {"code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_parquet(self, tpl):\n        df, dict = tpl\n        df.to_parquet(self.path, index=True)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)", "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet", "param_names": [], "params": [], "setup_cache_key": "comparison_benchmarks:44", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "166a93976c1ae0088bbb2477ee1d3e92672e7477a811b64747ee2b97bb078f26"}, "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {"code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, param: int):\n        self.logger.info(f\"LIBRARY: {self.lib}\")\n        self.logger.info(f\"Created Symbol: {self.symbol}\")\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, param: int):\n        self.ac = Arctic(FinalizeStagedData.CONNECTION_STRING)\n        self.lib = self.ac.get_library(self.lib_name)\n        self.symbol = f\"symbol{param}\"\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache(CachedDFGenerator(350000, [5]))\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data", "param_names": ["param1"], "params": [["1000"]], "setup_cache_key": "finalize_staged_data:47", "timeout": 600, "type": "peakmemory", "unit": "bytes", "version": "b49303c7a22a40468340ba6846719d83029e19a7355bb1d0e330142132a132b3"}, "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {"code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, param: int):\n        self.logger.info(f\"LIBRARY: {self.lib}\")\n        self.logger.info(f\"Created Symbol: {self.symbol}\")\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, param: int):\n        self.ac = Arctic(FinalizeStagedData.CONNECTION_STRING)\n        self.lib = self.ac.get_library(self.lib_name)\n        self.symbol = f\"symbol{param}\"\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache(CachedDFGenerator(350000, [5]))\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 1, "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data", "number": 1, "param_names": ["param1"], "params": [["1000"]], "repeat": 5, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "finalize_staged_data:47", "timeout": 600, "type": "time", "unit": "seconds", "version": "b083f4862ad8234395f2d583c00eca4ee85bd17a2142e225946d02fbebbb849c", "warmup_time": 0}, "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data": {"code": "class FinalizeStagedDataWiderDataframeX3:\n    def peakmem_finalize_staged_data(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().peakmem_finalize_staged_data(param)\n\n    def setup(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        if not SLOW_TESTS:\n            return  # Avoid setup when skipping\n        cachedDF = CachedDFGenerator(350000, [5, 25, 50])  # 3 times wider DF with bigger string columns\n        start = time.time()\n        self._setup_cache(cachedDF)\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data", "param_names": ["param1"], "params": [["1000"]], "setup_cache_key": "finalize_staged_data:100", "timeout": 600, "type": "peakmemory", "unit": "bytes", "version": "78d99e44cce890e09c89b0a8e4c1420cd237ac7eec4bb678c96618827baef718"}, "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data": {"code": "class FinalizeStagedDataWiderDataframeX3:\n    def time_finalize_staged_data(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().time_finalize_staged_data(param)\n\n    def setup(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        if not SLOW_TESTS:\n            return  # Avoid setup when skipping\n        cachedDF = CachedDFGenerator(350000, [5, 25, 50])  # 3 times wider DF with bigger string columns\n        start = time.time()\n        self._setup_cache(cachedDF)\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 1, "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data", "number": 1, "param_names": ["param1"], "params": [["1000"]], "repeat": 5, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "finalize_staged_data:100", "timeout": 600, "type": "time", "unit": "seconds", "version": "6c8fcb770b3ba6cd93b63b830a169dd3afd65e5cf007496395938a6f8e10cf82", "warmup_time": 0}, "list_functions.ListFunctions.peakmem_list_symbols": {"code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "list_functions.ListFunctions.peakmem_list_symbols", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:29", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "36bbe8ba3ff6df423837100203b3182d32efce65a17df876ac1369d05d7523fc"}, "list_functions.ListFunctions.peakmem_list_versions": {"code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "list_functions.ListFunctions.peakmem_list_versions", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:29", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "4d771cfea089c1fb8c4b9b8331803fa8aeca814cc8c2d0558485705521bb184b"}, "list_functions.ListFunctions.time_has_symbol": {"code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "list_functions.ListFunctions.time_has_symbol", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "list_functions:29", "timeout": 6000, "type": "time", "unit": "seconds", "version": "033af6c71329e0162ae9ea485b64a83dba030888400a9e1d0c5c33d686bd0880", "warmup_time": 0}, "list_functions.ListFunctions.time_list_symbols": {"code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_symbols", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "list_functions:29", "timeout": 6000, "type": "time", "unit": "seconds", "version": "272c3535600411e60dd72ff988009bbc491bca183158c04d2aa82748e95bfc33", "warmup_time": 0}, "list_functions.ListFunctions.time_list_versions": {"code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_versions", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "list_functions:29", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2c963b861564347f6311befd6189eb42e4b882c06fb468060e38b9cda65cd12b", "warmup_time": 0}, "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list": {"code": "class SnaphotFunctions:\n    def peakmem_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list", "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "setup_cache_key": "list_snapshots:46", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "2fe5cfa52bf33e64fd73e89643e27d8f47d02159488e14d0359cacfa93e63e4a"}, "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta": {"code": "class SnaphotFunctions:\n    def peakmem_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta", "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "setup_cache_key": "list_snapshots:46", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ae3140e42a763dd7a75cc5a8df60881378dec6ce6708ac127ccc8dcf367626c0"}, "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list": {"code": "class SnaphotFunctions:\n    def time_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots()\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list", "number": 5, "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "list_snapshots:46", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8350189525f2502cc7949369b1dff78862d303da0fa2d8e27a523411f3a501bc", "warmup_time": 0}, "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta": {"code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta", "number": 5, "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "list_snapshots:46", "timeout": 6000, "type": "time", "unit": "seconds", "version": "eb27c5627ff775cc2a8f0f8841f5b28b79f0ee10c3420e77eca1f8ca4582bd0e", "warmup_time": 0}, "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta": {"code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_without_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta", "number": 5, "param_names": ["symbols_x_snaps_per_sym"], "params": [["'20x10'", "'40x20'"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "list_snapshots:46", "timeout": 6000, "type": "time", "unit": "seconds", "version": "68900e75770793f36f0f940ebab65e148dee97f5c8cebe43c4ebd3ba5350dfbf", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ea9562ac454e814117fbe50a9b5d235a37110c1745fa24bd71db420baaf072ef"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "69f411de823212396215fb4912da15f56a16ca931fd2cfcf9533204b56e696ba"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ecadcf0caca3e2382953be630263bde2c01f7ac011ac2a140080e00f3a6ebdad"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "cd35c81c46cd4eb64ccab7a6099884a9f99832a5ece8bb0bc387c0ed402f1536"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1806b540ceeb66fb0bc4476c5b07fca6d5d5edb13e441fb4a2471751a99ff7cb"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "923c4661c84571a4abb8cf5d774b2e0ec674bf671b3b11bc26ed561a0c417ef7"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "0af435e83cbba85506f4bbf0fe355dd839b3e55fd81aa7e3600fcb443dc682ee"}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2ac4df4d7d9b744d192742ba5b8c00c2f79a143cd72c4a7b7f63785dea19e219", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "e16e60780a624d94310ad7e6059e97827feb6b4b6bc2d757a1e89f67c5e7ddd5", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, num_rows):\n        pattern = f\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "104a945e32f971e4b887da7b6cc6e6e728fc78ef6a77c2f6aed0693c24113234", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_projection": {"code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f2bd34ad9342852559d34262fc059be4c6f3122909b31ee078847b1a1b93907f", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "af4473b7d93a3c30d6cfc9d8bcde05e126a1df7948b5877c1604f2882a037768", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9d0961c0292eff90c651a1989e525c7ce5ab63baa888ac4e2ccdb88b89cc9f2e", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "e32d4b66ff29029ccf51f51511afe9792412ba38d2422d44b03b7e7e8710e38b", "warmup_time": 0}, "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "local_query_builder:33", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2b556ee72fdb8b5b25072c8e0df67fb3264fc3f2fa724c453a6522ef98392f93", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "12247b929d34d209286ab85918d78e86cc91561bd9a4cc7db06beaa001d21e0d"}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "d8eeb21aa5b74f3f935a4b92c92f09f31d1f089b892b6ac712a476f33e854a4c"}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_date_range)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] > 2\n        assert read_batch_result[-1].data.shape[0] > 2\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "aebe4be8cf7590f95c29a5f5e71088dc833e85dba49c4d159ccb6aced1f7ce90"}, "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch": {"code": "class AWSBatchBasicFunctions:\n    def peakmem_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert write_batch_result[0].symbol in self.symbols\n        assert write_batch_result[-1].symbol in self.symbols\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch", "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "b58eb8a420862c585bcfbdfe92d70ba068310e4c34c7846292f47dd7363cae27"}, "real_batch_functions.AWSBatchBasicFunctions.time_read_batch": {"code": "class AWSBatchBasicFunctions:\n    def time_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "time", "unit": "seconds", "version": "501017573a8a25827024b656a55df9ed1a621d3cdaac24c830c183d9b691463a", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns": {"code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "time", "unit": "seconds", "version": "8e28fd869de381aec95ddab625b7d4e17bd262a6238f21b980a1ded0903ef3c1", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges": {"code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        self.lib.read_batch(self.read_reqs_date_range)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "time", "unit": "seconds", "version": "97ac25e33d8fb8d99080e1a9177918dc6bf503d3fc8bc429ae06dc746537f950", "warmup_time": 0}, "real_batch_functions.AWSBatchBasicFunctions.time_write_batch": {"code": "class AWSBatchBasicFunctions:\n    def time_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs", "min_run_count": 1, "name": "real_batch_functions.AWSBatchBasicFunctions.time_write_batch", "number": 3, "param_names": ["num_symbols", "num_rows"], "params": [["500", "1000"], ["25000", "50000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_batch_functions:59", "timeout": 1200, "type": "time", "unit": "seconds", "version": "79c5307f83e8be6020c49ce96f9b943180776220a624f8853cb50c7d10db11bc", "warmup_time": 0}, "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe": {"code": "class RealComparisonBenchmarks:\n    def peakmem_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe", "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "7b1c447a499ef7f7a8377e3797c6260d331ae11ca1a2f4cea0425b3ba64ba4b9"}, "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe": {"code": "class RealComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # measures base memory which need to be deducted from\n            # any measurements with actual operations\n            # see discussion above\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.parquet_to_read)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_read.read(self.s3_symbol)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe", "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "74e842504a929b308054e279b84e4d85168663dea924431ab926322c614cbc3c"}, "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe": {"code": "class RealComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_write.write(self.s3_symbol, df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe", "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "peakmemory", "unit": "bytes", "version": "376aa64074dfc1b8e319065605bc8e26898c54cf8e2d0472a317453ebf7b4915"}, "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe": {"code": "class RealComparisonBenchmarks:\n    def time_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)", "min_run_count": 1, "name": "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe", "number": 2, "param_names": ["backend_type"], "params": [["'no-operation-load'", "'create-df-pandas-from_dict'", "'pandas-parquet'", "'arcticdb-lmdb'", "'arcticdb-amazon-s3'"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_comparison_benchmarks:76", "timeout": 60000, "type": "time", "unit": "seconds", "version": "9b06683aaf9b2112fce975aaad4ccdf9ad266b297384fb73f01948a23ed9c622", "warmup_time": 0}, "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data": {"code": "class AWSFinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(0, self.df_cache.TIME_UNIT)  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache", "name": "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data", "param_names": ["num_chunks"], "params": [["500", "1000"]], "setup_cache_key": "real_finalize_staged_data:42", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "e3f95aa7923f7837767aaec1073645b8bd6bec4cfcf71d0e819232af51829284"}, "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data": {"code": "class AWSFinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(0, self.df_cache.TIME_UNIT)  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache", "min_run_count": 1, "name": "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data", "number": 1, "param_names": ["num_chunks"], "params": [["500", "1000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_finalize_staged_data:42", "timeout": 1200, "type": "time", "unit": "seconds", "version": "2cc8f1598f371894726ad98c50ef3fc5547a760f45d25e8a80bb2d563fda964e", "warmup_time": 0}, "real_list_operations.AWSListSymbols.peakmem_list_symbols": {"code": "class AWSListSymbols:\n    def peakmem_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys()  # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info()  # Always log the ArcticURIs", "name": "real_list_operations.AWSListSymbols.peakmem_list_symbols", "param_names": ["num_syms"], "params": [["500", "1000"]], "setup_cache_key": "real_list_operations:58", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "3a9437f6e472e3d916bef3fc9e15da3e5153d99f515ddd3cce9ef7ce8bb411c2"}, "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting": {"code": "class AWSListSymbols:\n    def time_has_symbol_nonexisting(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.has_symbol(\"250_sym\")\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys()  # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info()  # Always log the ArcticURIs", "min_run_count": 1, "name": "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting", "number": 1, "param_names": ["num_syms"], "params": [["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:58", "timeout": 1200, "type": "time", "unit": "seconds", "version": "d26278f69868af7bfde10e3af74abadb2aa8221e8445da75b25a788447907739", "warmup_time": 0}, "real_list_operations.AWSListSymbols.time_list_symbols": {"code": "class AWSListSymbols:\n    def time_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys()  # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info()  # Always log the ArcticURIs", "min_run_count": 1, "name": "real_list_operations.AWSListSymbols.time_list_symbols", "number": 1, "param_names": ["num_syms"], "params": [["500", "1000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:58", "timeout": 1200, "type": "time", "unit": "seconds", "version": "de9dca891016036c2c90b5a76df8ea8470d84eaad532a0c951af7eee9ee28215", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots": {"code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots", "param_names": ["num_syms"], "params": [["25", "50"]], "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "81af200a443e9dad37310396d1e94b23d81d81270d7884be7841f02e92a5b9a0"}, "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata": {"code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata", "param_names": ["num_syms"], "params": [["25", "50"]], "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "6959e1346e7402271c3233e74a4ae19545c58e7852dd8d47ceb3aba03d7aef29"}, "real_list_operations.AWSVersionSymbols.peakmem_list_versions": {"code": "class AWSVersionSymbols:\n    def peakmem_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "name": "real_list_operations.AWSVersionSymbols.peakmem_list_versions", "param_names": ["num_syms"], "params": [["25", "50"]], "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "56c7e4326a6a5a2b9a71b8fd78003501c0a1d26fa6f1873509fdc6c91cae90af"}, "real_list_operations.AWSVersionSymbols.time_list_snapshots": {"code": "class AWSVersionSymbols:\n    def time_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e7523038fd41e53befe4bc74859e59c6226cef74ec40f7d4642b337d582e8332", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata": {"code": "class AWSVersionSymbols:\n    def time_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "d606b05b0bafb3aeb519be7e19b7734b50fa29b03d35930a61771ee689ed9bae", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions": {"code": "class AWSVersionSymbols:\n    def time_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "bc1f9261fd04e65738f0494a3823d3670528f33f6d125dab804871c3e0d574a4", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only": {"code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "08f98136e4ed27c413ae42c38c3f6066c085fae581d54b4bd02716f3374c8347", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots": {"code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only_and_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True, skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "549aa0723764402ae84a33d17ba8408505357bbd2b337ba126cf68b019f5bedc", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots": {"code": "class AWSVersionSymbols:\n    def time_list_versions_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "92b243f11776f97959ea3c3902133131c571477249705cc709b1568e687ec0d4", "warmup_time": 0}, "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot": {"code": "class AWSVersionSymbols:\n    def time_list_versions_snapshot(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(snapshot=last_snapshot_names_dict[num_syms])\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict", "min_run_count": 1, "name": "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot", "number": 3, "param_names": ["num_syms"], "params": [["25", "50"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_list_operations:138", "timeout": 1200, "type": "time", "unit": "seconds", "version": "d074a8a33a65568b0815861b17e7e9fb230f2aae19a78cc56decd8df791e7527", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large": {"code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e97486b955cfba59bd57e26e7f67dfd28fa68fe5b41d4000cad23410f78e4a0f", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single": {"code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "f8d864264ef6e4f853192984c4f67100dbcc9fb5ea225dcf5a3b4e00947ff62c", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full": {"code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "6685c85cff6e17f615694061ea490e2b14d432c1628252a29d07fdcc04b97c2c", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half": {"code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "a96bc9d0a016f29c9671d713ad7a2fdfd945381695c2227c7716468b8eaebd1d", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single": {"code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "bbc2c8763db23c7c27e02f3361ee26cb68e9ead689a18a7e7bbc81edafa7a5a5", "warmup_time": 0}, "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert": {"code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert", "number": 3, "param_names": ["num_rows"], "params": [["2500", "5000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:215", "timeout": 1200, "type": "time", "unit": "seconds", "version": "b690d7836ba77e8f6ecd134bf9c3877c8c3f5b1a5d94f4d842b3b192ec8bbe07", "warmup_time": 0}, "real_modification_functions.AWSDeleteTestsFewLarge.time_delete": {"code": "class AWSDeleteTestsFewLarge:\n    def time_delete(self, cache, num_rows):\n        self.lib.delete(self.symbol)\n        self.symbol_deleted = True\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache", "min_run_count": 1, "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete", "number": 1, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:261", "timeout": 1200, "type": "time", "unit": "seconds", "version": "4588f423aa2b7d1ded777f24c8ddd1b19a282bd7c7a6f15c012fc0cf1acbdd36", "warmup_time": 0}, "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time": {"code": "class AWSDeleteTestsFewLarge:\n    def time_delete_over_time(self, cache, num_rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(25):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache", "min_run_count": 1, "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time", "number": 1, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 3, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:261", "timeout": 1200, "type": "time", "unit": "seconds", "version": "978d41f95903f476a5a0c703ce1cd6ffdf3386ad6dd9023851d9f784159d567f", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_append_large": {"code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_append_large", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ae924cc65f3fda9e7c64fcd76f6a542603cfb7242333cab7d762a62689a44aa3", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_append_single": {"code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_append_single", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "afabfcaa402bc4f4a8dd332050aaa65770b2d746b2ea6235ec9421e461dd4975", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_full": {"code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_full", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "c3a2c9b358a8c4ffd9af01ffa0b9474e2e317579a1030aeea626be4f621274a2", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_half": {"code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_half", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e865f1e39380722e6d3bfe6e3a56d2fae9389a0d95cd11c29b6f34f2007a389a", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_single": {"code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_single", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ba848d44eee0ef4595475f4def6ae17301c551096480c10b75562fd8f5c2598c", "warmup_time": 0}, "real_modification_functions.AWSLargeAppendTests.time_update_upsert": {"code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )", "min_run_count": 1, "name": "real_modification_functions.AWSLargeAppendTests.time_update_upsert", "number": 3, "param_names": ["num_rows"], "params": [["500000", "1000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_modification_functions:73", "timeout": 1200, "type": "time", "unit": "seconds", "version": "0470f242734c94e8f6d30f4241c13ffe6cc8b53cb2bad1973799878b94c3cccd", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "66587c7cad65fa03050a1e2d2cbdd37b215e36464be2dbca6667b52ada07b398"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "7a402ec7e4f09ccc8191abff900aee941fe5b983d3a54c1128332802b3aeacb8"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "41f3abfee7b61aaf01d6a7c097f50c82a3cd5aa520735e8e56d1433980423e81"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "1e238f958363598bb77ea0b61b18e872f9f2d45ace953b66286f6590b855138f"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "3f46b68395cb394e790f846e8bb368416fa13d20e8963afe38202e0afb2a8012"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "0b4095f9e4d0594d25e5334be3c0c9d41135b0460577f58f20ef358d83e58dd3"}, "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "3d94e6e1d0c466cf98b0c7673d3e695b231110e5e0aff293d7a7f75f878f49cc"}, "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric": {"code": "class AWSQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "d8f0751dfa443b7fe80c43d18b9e89bebfb248d8fd2456719e5bd712c2f54905", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin": {"code": "class AWSQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "3fbe4bd3d18d1756b709637b51e6181d1753379ec47d30de348e3d82e8069750", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_projection": {"code": "class AWSQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_projection", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "3e7e763b14722ab6c81c7835c4bbee8fb086672ad73366beefd3c6bc7781172f", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_1": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_1", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "76a7f4b0952f07784150df2b7a382356a4be815385c5258323ffc27e1d385767", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_3": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_3", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "685f6a5908e27c6198503b4bc2330769d4c764d4e16beebf4e0355b5b2c6b627", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_4": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_4", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "de79858cd635f67551cee7c9c1439573bf926264dc8e11de497d7062c5589641", "warmup_time": 0}, "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2": {"code": "class AWSQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_query_builder:76", "timeout": 1200, "type": "time", "unit": "seconds", "version": "977b46a22c5f742940aa1d126fec2fccea49721ce34ed2ad73026225387a0a0b", "warmup_time": 0}, "real_read_write.AWSReadWrite.peakmem_read": {"code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "c9472d2ab25d1a30beb1146a9c649036af89bea84f2119d9cb184408139276f3"}, "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {"code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "cc3955cfed7c5684809c86a04041374c674f26e8721128acbcd80af06012128c"}, "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "fdc284437037f6f245607caf113a0e5f0a6c6dccc8e6ad9f471611574253c50b"}, "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "a09cc022e09a57064fca48fc68fed42d6b9593fbefddc51f8be856afb7ac710b"}, "real_read_write.AWSReadWrite.peakmem_write": {"code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_write", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "7876413187267e0867f03f798f317eaa3e3960ac2375ff5df6f2095520bb1ca5"}, "real_read_write.AWSReadWrite.peakmem_write_staged": {"code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "name": "real_read_write.AWSReadWrite.peakmem_write_staged", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "d211408aa7db06df36befacdd5fd39b9422eb968773a6e2bd19f8d16745541ac"}, "real_read_write.AWSReadWrite.time_read": {"code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "time", "unit": "seconds", "version": "a2b8548a163367ba007992cefa84d7a83d4f60672b14b8a90bd4b2600b4d8131", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_read_with_column_float": {"code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read_with_column_float", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "time", "unit": "seconds", "version": "470178c2a5f27c30784904befff88ed0b75125c5ad1a4d508ee2c6d79e1f3f99", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "time", "unit": "seconds", "version": "2d8f9e98f36bf378b003b44866c8d3c39864f8160798e8b2cc7b475b074bdd38", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "time", "unit": "seconds", "version": "8aec0888e02948dc708cc304400857ce54ab1f5b91fda2bc167bce90ea4c7299", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_write": {"code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_write", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "time", "unit": "seconds", "version": "f77b10516f456c860eb05ec818f8242a43aa9adc54b34ef30eafd4098299322e", "warmup_time": 0}, "real_read_write.AWSReadWrite.time_write_staged": {"code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last", "min_run_count": 1, "name": "real_read_write.AWSReadWrite.time_write_staged", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:87", "timeout": 1200, "type": "time", "unit": "seconds", "version": "e0cd4c4a06cec3813214e5ed2e32ea0a22bf2f55e6d9cebc4f16894b16710e36", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read": {"code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "1cac6c9cf15d5fbf892a777498296d8d098711272b405b1c8e243e3e767e599b"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float": {"code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "4ff1a56334201630187cbd2ad88a520d04067e282d5a163c1d4a34230b997ab5"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "44fbc536228fe2270c8038e451223ded4f7941c9226bf95190a46b7b28f228b0"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "307ae3fb4ac3c92c44fb9c578b21907867846318087564daae4c61f32fd996fa"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_write": {"code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "3b402f9a631ceeb544fa7ee7b860e13ab870c7e61da809ba11d14c5973c97cda"}, "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged": {"code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged", "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "c1fdbb5013c30dd4c7a6461a1b9193097a831af4f6e639e8c3261c3817e6181f"}, "real_read_write.AWSReadWriteWithQueryStats.time_read": {"code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "time", "unit": "seconds", "version": "f7a86808b4972336b56bbe425b35ce39d7db682c525504edc5912736af82398e", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float": {"code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "time", "unit": "seconds", "version": "b0a5713c3639a1d5084ba6933abf2bcb14df3db143f5bfe713fbb7e448068db9", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "time", "unit": "seconds", "version": "748e8cf9fad63c651326e462f4021c28a2c128a1d7c4783fc2886a2451b19d99", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "time", "unit": "seconds", "version": "d77a903a22862f45799bed1ae703c285e21602d82da264bf013ceba202c63cf8", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_write": {"code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_write", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "time", "unit": "seconds", "version": "48483b15cbd6e2738ddf3f615179a9bafde8596faac8e1b331ad03df4b4c21d8", "warmup_time": 0}, "real_read_write.AWSReadWriteWithQueryStats.time_write_staged": {"code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSReadWriteWithQueryStats.time_write_staged", "number": 3, "param_names": ["num_rows"], "params": [["1000000", "2000000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:243", "timeout": 1200, "type": "time", "unit": "seconds", "version": "9805c7597bb79b23de9a89830bdaca81c17868b485d9ecd9ede6d0621460cf56", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.peakmem_read": {"code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "777451970fc2c9ba7c40648d51397be114005fca10b35c2beb816a0c5b11eb4e"}, "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float": {"code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "94cd6191a28611f9fdfb6f92e01fa9ccec156d60cc7bda30471fc0b6b27d8c28"}, "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "c27180c5137daf6611cffdd96923d000f03bdc4f4c12b00435502b40d5abd4da"}, "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "840af2cee86e8ba04910ceb2c04cc3c6e732c2a9eea52a2ab3cecf1a35767444"}, "real_read_write.AWSWideDataFrameTests.peakmem_write": {"code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_write", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "c55080c0aa9714cf9bbac9058877813c9fd7bab7326f66618725b67b14f21372"}, "real_read_write.AWSWideDataFrameTests.peakmem_write_staged": {"code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "name": "real_read_write.AWSWideDataFrameTests.peakmem_write_staged", "param_names": ["num_cols"], "params": [["15000", "30000"]], "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "a07309f742f72469c5f54a0962205f9272af5d741b615c6e5536f97ee16356ee"}, "real_read_write.AWSWideDataFrameTests.time_read": {"code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "time", "unit": "seconds", "version": "355ba97b40b39aa52586e966f70b2710a64b1694030d66d69a94845d11620e12", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_read_with_column_float": {"code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read_with_column_float", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "time", "unit": "seconds", "version": "5653d388b62f876452148f5aff5c29e03eda6a7548b406fbdc324ab2365bcdbe", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types": {"code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "time", "unit": "seconds", "version": "3ab9782a626f87b77d5fe64ffa7e83ae14ce2274f047d15f0c1af2c9bb8da30d", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows": {"code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "time", "unit": "seconds", "version": "4feff55363038593799ca793ef2904d32a76d0d794f5466ed75c56ba867bd1d3", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_write": {"code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_write", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ed88fa1910599e13c65662744b40b1396536a03e2397cbf12f39f37b81c8f81b", "warmup_time": 0}, "real_read_write.AWSWideDataFrameTests.time_write_staged": {"code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()", "min_run_count": 1, "name": "real_read_write.AWSWideDataFrameTests.time_write_staged", "number": 3, "param_names": ["num_cols"], "params": [["15000", "30000"]], "repeat": 1, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "real_read_write:212", "timeout": 1200, "type": "time", "unit": "seconds", "version": "a26f1ab74733278e53ed0c0913f466bfdb150dd2fa8d5a481715bfa3caf2f978", "warmup_time": 0}, "resample.Resample.peakmem_resample": {"code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "name": "resample.Resample.peakmem_resample", "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "setup_cache_key": "resample:45", "type": "peakmemory", "unit": "bytes", "version": "99c6acb45f29b2d72d54020d96990507084b8fd1c9918c594e9fa09670dec465"}, "resample.Resample.time_resample": {"code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "resample.Resample.time_resample", "number": 7, "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "resample:45", "type": "time", "unit": "seconds", "version": "1f835087b54bb1d48a11dcbdb503bbbc4c57d6c5f54b6f0f6cf22c68c2187a5c", "warmup_time": -1}, "resample.ResampleWide.peakmem_resample_wide": {"code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)", "name": "resample.ResampleWide.peakmem_resample_wide", "param_names": [], "params": [], "setup_cache_key": "resample:131", "timeout": 1200, "type": "peakmemory", "unit": "bytes", "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"}, "resample.ResampleWide.time_resample_wide": {"code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)", "min_run_count": 2, "name": "resample.ResampleWide.time_resample_wide", "number": 5, "param_names": [], "params": [], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "resample:131", "timeout": 1200, "type": "time", "unit": "seconds", "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2", "warmup_time": 0}, "version_chain.IterateVersionChain.time_list_undeleted_versions": {"code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_list_undeleted_versions", "number": 13, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "version_chain:42", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8cf2d8b7302ee0311a2bab73cb1ab31134c9676a39bc5e517411e3192a89ead7", "warmup_time": -1}, "version_chain.IterateVersionChain.time_load_all_versions": {"code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_load_all_versions", "number": 13, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "version_chain:42", "timeout": 6000, "type": "time", "unit": "seconds", "version": "32c93e66abfbbbaa80b6cdf40c50fc82e21aa1de964c5962ae200444ff26f252", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_alternating": {"code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_alternating", "number": 13, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "version_chain:42", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f1f008d2c2efb9386c21fdd3539bb3601b1078e323613d38f13ddadb066cb004", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_from_epoch": {"code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_from_epoch", "number": 13, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "version_chain:42", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ee44eb3fe3eecad30de7d9349e47e68cdeb430326b5713b7ae4bfd7abdb63707", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_v0": {"code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_v0", "number": 13, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 1, "sample_time": 0.01, "setup_cache_key": "version_chain:42", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4de46121ac1a914c7a5d77c82aa535e29da68e0e2acc7e17e483d168cac49db3", "warmup_time": -1}}, "machines": {"ArcticDB-Medium-Runner": {"machine": "ArcticDB-Medium-Runner", "python": "3.11", "version": 1}}, "tags": {"1.0.1": 1, "finalize-staged-chunk-incompletes-pre-rebase": 3885, "ostoimenov_test_tag": 4020, "v0.0.0": 3433, "v0.0.1": 3438, "v0.0.2": 3442, "v0.0.999": 3756, "v1.0.1": 1, "v1.1.0": 217, "v1.2.0": 341, "v1.2.1": 358, "v1.3.0": 484, "v1.4.0": 567, "v1.4.1": 575, "v1.4.1-pre.seatontst": 577, "v1.4.1-pre.seatontst.2": 579, "v1.5.0": 641, "v1.6.0": 714, "v1.6.1": 800, "v1.6.1-rc0": 742, "v1.6.1-rc1": 757, "v1.6.1rc2": 772, "v1.6.2": 1183, "v1.6.2rc0": 1165, "v2.0.0": 932, "v2.0.0rc0": 858, "v3.0.0": 1053, "v3.0.0rc0": 1023, "v3.0.0rc1": 1035, "v3.0.0rc2": 1054, "v4.0.0": 1141, "v4.0.0rc0": 1071, "v4.0.0rc1": 1107, "v4.0.0rc2": 1129, "v4.0.1": 1263, "v4.0.2": 1365, "v4.0.2rc0": 1346, "v4.0.3": 1532, "v4.0.4": 2463, "v4.0.4-docs": 2463, "v4.0.5": 2797, "v4.1.0": 1366, "v4.1.0-docs": 1489, "v4.1.0rc0": 1198, "v4.1.0rc1": 1337, "v4.1.0rc2": 1350, "v4.2.0": 1715, "v4.2.0-docs": 1715, "v4.2.0rc0": 1541, "v4.2.1": 1739, "v4.2.1-docs": 1955, "v4.3.0": 2180, "v4.3.0-docs": 2135, "v4.3.0rc0": 2080, "v4.3.0rc1": 2129, "v4.3.0rc2": 2181, "v4.3.1": 2229, "v4.3.1-docs": 2256, "v4.3.2rc0": 2329, "v4.3.2rc1": 2331, "v4.3.2rc2": 2359, "v4.4.0": 2585, "v4.4.0-docs": 2585, "v4.4.0rc0": 2551, "v4.4.0rc1": 2560, "v4.4.0rc2": 2581, "v4.4.1": 2613, "v4.4.1-docs": 2613, "v4.4.2": 2714, "v4.4.2-docs": 2714, "v4.4.2rc0": 2710, "v4.4.3": 2872, "v4.4.3-docs": 2872, "v4.4.3rc0": 2741, "v4.4.3rc1": 2792, "v4.4.4": 3254, "v4.4.4rc0": 2900, "v4.4.4rc1": 2929, "v4.4.4rc2": 2947, "v4.4.4rc3": 2999, "v4.4.4rc4": 3032, "v4.4.4rc5": 3081, "v4.4.4rc6": 3112, "v4.4.5": 3281, "v4.4.6": 3347, "v4.4.7": 3819, "v4.4.7-docs": 3820, "v4.5.0": 3137, "v4.5.0-docs": 3137, "v4.5.0rc0": 2886, "v4.5.0rc1": 2918, "v4.5.0rc2": 3033, "v4.5.0rc3": 3077, "v4.5.0rc4": 3111, "v4.5.1": 3436, "v4.5.1-docs": 3436, "v4.5.1rc0": 3295, "v4.5.1rc1": 3305, "v4.5.1rc2": 3355, "v4.5.1rc3": 3393, "v4.5.1rc4": 3413, "v5.0.0": 3497, "v5.0.0-docs": 3497, "v5.0.0rc0": 3465, "v5.0.0rc1": 3491, "v5.1.0": 3580, "v5.1.0-docs": 3580, "v5.1.0rc0": 3577, "v5.1.0rc1": 3578, "v5.1.1": 3661, "v5.1.1-docs": 3661, "v5.1.1rc0": 3641, "v5.1.2": 3730, "v5.1.2-docs": 3776, "v5.1.2rc0": 3707, "v5.1.3": 3921, "v5.1.3-docs": 3930, "v5.10.0": 5133, "v5.10.0+man0": 5057, "v5.10.0+man1": 5113, "v5.10.0+man2": 5128, "v5.10.0+man3": 5132, "v5.10.0+man4": 5144, "v5.10.0-docs": 5133, "v5.11.0+man0": 5115, "v5.2.0": 3939, "v5.2.0-docs": 3939, "v5.2.0rc0": 3855, "v5.2.1": 3968, "v5.2.1-docs": 3968, "v5.2.2": 3987, "v5.2.2-docs": 3987, "v5.2.3": 4026, "v5.2.3-docs": 4106, "v5.2.4": 4091, "v5.2.4-docs": 4107, "v5.2.4rc0": 4045, "v5.2.4rc1": 4067, "v5.2.4rc2": 4083, "v5.2.5": 4124, "v5.2.5-docs": 4133, "v5.2.5.dev0": 4090, "v5.2.5.dev0+29b8a26": 4089, "v5.2.5rc0": 4120, "v5.2.6": 4195, "v5.2.6-docs": 4233, "v5.3.0": 4319, "v5.3.0-docs": 4319, "v5.3.0rc0": 4260, "v5.3.1": 4346, "v5.3.1-docs": 4346, "v5.3.1rc0": 4324, "v5.3.1rc1": 4345, "v5.3.2": 4380, "v5.3.2+man0": 4361, "v5.3.2+man1": 4370, "v5.3.2+man2": 4376, "v5.3.2-docs": 4380, "v5.3.2rc0": 4352, "v5.3.3": 4445, "v5.3.3+man0": 4381, "v5.3.3+man1": 4393, "v5.3.3+man2": 4397, "v5.3.3+man3": 4400, "v5.3.3+man4": 4401, "v5.3.3+man5": 4409, "v5.3.3+man6": 4430, "v5.3.4": 4495, "v5.3.4+man0": 4446, "v5.3.4-docs": 4495, "v5.4.0": 4521, "v5.4.0+man0": 4496, "v5.4.0-docs": 4521, "v5.4.1": 4577, "v5.4.1+man0": 4522, "v5.4.1-docs": 4577, "v5.5.0": 4590, "v5.5.0+man0": 4578, "v5.5.0-docs": 4590, "v5.5.1": 4618, "v5.5.1+man0": 4591, "v5.5.1-docs": 4618, "v5.6.0": 4644, "v5.6.0+man0": 4619, "v5.6.0+man1": 4629, "v5.6.0-docs": 4644, "v5.7.0": 4678, "v5.7.0+man0": 4645, "v5.7.0-docs": 4678, "v5.8.0": 4747, "v5.8.0+man0": 4679, "v5.8.0-docs": 4747, "v5.8.1": 4773, "v5.8.1+man0": 4748, "v5.8.1+man1": 4764, "v5.8.2": 4894, "v5.8.2+man0": 4774, "v5.8.2+man1": 4780, "v5.8.2+man2": 4797, "v5.9.0": 4953, "v5.9.0+man0": 4895, "v5.9.0+man1": 4911, "v5.9.0+man2": 4945, "v5.9.0-docs": 4953, "v5.9.1": 5026, "v5.9.1+man0": 4954, "v5.9.2": 5078, "v5.9.2+man0": 5038, "v5.9.2+man1": 5073, "v5.9.2+man2": 5105, "v5.9.2-docs": 5078, "v5.9.3": 5114, "v5.9.3-docs": 5114, "v6.0.0": 5263, "v6.0.0+man0": 5141, "v6.0.0+man1": 5151, "v6.0.0+man2": 5177, "v6.1.0": 5356, "v6.1.0+man0": 5264, "v6.1.0+man1": 5277, "v6.1.0+man2": 5352, "v6.1.0-docs": 5356, "v6.1.1": 5390, "v6.1.1+man0": 5332, "v6.1.1+man1": 5354, "v6.1.1-docs": 5390, "v6.1.2": 5453, "v6.1.2+man0": 5391, "v6.2.0": 5482, "v6.2.0+man0": 5454, "v6.2.0+man1": 5459, "v6.2.0+man2": 5486, "v6.2.0-docs": 5485, "v6.2.1": 5501, "v6.2.1+man0": 5483, "v6.2.1+man1": 5506, "v6.2.1-docs": 5505, "v6.2.2": 5532, "v6.2.2+man0": 5502, "v6.2.2-docs": 5532, "v6.2.3": 5570, "v6.2.3+man0": 5533, "v6.2.3+man1": 5569, "v6.2.3-docs": 5570, "v6.2.4": 5604, "v6.2.4+man0": 5556, "v6.2.4+man1": 5568, "v6.2.4+man2": 5603, "v6.2.4-docs": 5604, "v6.2.5": 5623, "v6.2.5+man0": 5598, "v6.2.5-docs": 5623, "v6.2.6": 5666, "v6.2.6+man0": 5627, "v6.2.6-docs": 5666, "v6.3.0": 5702, "v6.3.0+man0": 5667, "v6.3.1+man0": 5703, "vtop_level_imports-docs": 1933, "vv6.1.0": 5355, "vvTEST_PUBLISH": 4020, "bisection": 2720, "aseaton/coredumps-experiments": 3693}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}