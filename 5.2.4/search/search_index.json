{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#what-is-arcticdb","title":"What is ArcticDB?","text":"<p>ArcticDB is a serverless DataFrame database engine designed for the Python Data Science ecosystem. </p> <p>ArcticDB enables you to store, retrieve and process DataFrames at scale, backed by commodity object storage (S3-compatible storages and Azure Blob Storage).</p> <p>ArcticDB requires zero additional infrastructure beyond a running Python environment and access to object storage and can be installed in seconds.</p> <p>ArcticDB is:</p> <ul> <li>Fast<ul> <li>Process up to 100 million rows per second for a single consumer</li> <li>Process a billion rows per second across all consumers</li> <li>Quick and easy to install: <code>pip install arcticdb</code></li> </ul> </li> <li>Flexible<ul> <li>Data schemas are not required</li> <li>Supports streaming data ingestion</li> <li>Bitemporal - stores all previous versions of stored data</li> <li>Easy to setup both locally and on cloud</li> <li>Scales from dev/research to production environments</li> </ul> </li> <li>Familiar<ul> <li>ArcticDB is the world's simplest shareable database</li> <li>Easy to learn for anyone with Python and Pandas experience</li> <li>Just you and your data - the cognitive overhead is very low.</li> </ul> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>This section will cover installation, setup and basic usage. More details on basics and advanced features can be found in the tutorials section.</p>"},{"location":"#installation","title":"Installation","text":"<p>ArcticDB supports Python 3.6 - 3.11. Python 3.7 is the earliest version for Windows.</p> <p>To install, simply run:</p> <pre><code>pip install arcticdb\n</code></pre>"},{"location":"#setup","title":"Setup","text":"<p>ArcticDB is a storage engine designed for object storage and also supports local-disk storage using LMDB.</p> <p>Storage</p> <p>ArcticDB supports any S3 API compatible storage, including AWS and Azure, and storage appliances like VAST Universal Storage and Pure Storage.</p> <p>ArcticDB also supports LMDB for local/file based storage - to use LMDB, pass an LMDB path as the URI: <code>adb.Arctic('lmdb://path/to/desired/database')</code>.</p> <p>To get started, we can import ArcticDB and instantiate it:</p> <pre><code>import arcticdb as adb\n# this will set up the storage using the local file system\nuri = \"lmdb://tmp/arcticdb_intro\"\nac = adb.Arctic(uri)\n</code></pre> <p>For more information on how to correctly format the <code>uri</code> string for other storages, please view the docstring (<code>help(Arctic)</code>) or read the storage access section (click the link or keep reading below this section).</p>"},{"location":"#library-setup","title":"Library Setup","text":"<p>ArcticDB is geared towards storing many (potentially millions) of tables. Individual tables (DataFrames) are called symbols and  are stored in collections called libraries. A single library can store many symbols.</p> <p>Libraries must first be initialized prior to use:</p> <p><pre><code>ac.create_library('intro')  # static schema - see note below\nac.list_libraries()\n</code></pre> output <pre><code>['intro']\n</code></pre></p> <p>The library must then be instantiated in the code ready to read/write data:</p> <pre><code>library = ac['intro']\n</code></pre> <p>Sometimes it is more convenient to combine library creation and instantiation using this form, which will automatically create the library if needed, to save you checking if it exists already:</p> <pre><code>library = ac.get_library('intro', create_if_missing=True)\n</code></pre> <p>ArcticDB Static &amp; Dynamic Schemas</p> <p>ArcticDB does not need data schemas, unlike many other databases. You can write any DataFrame and read it back later. If the shape of the data is changed and then written again, it will all just work. Nice and simple.</p> <p>The one exception where schemas are needed is in the case of functions that modify existing symbols: <code>update</code> and <code>append</code>. When modifying a symbol, the new data must have the same schema as the existing data. The schema here means the index type and the name, order, and type of each column in the DataFrame. In other words when you are appending new rows they must look like the existing rows. This is the default option and is called <code>static schema</code>.</p> <p>However, if you need to add, remove or change the type of columns via <code>update</code> or <code>append</code>, then you can do that. You simply need to create the library with the <code>dynamic_schema</code> option set. See the <code>library_options</code> parameter of the (<code>create_library</code>) method.</p> <p>So you have the best of both worlds - you can choose to either enforce a static schema on your data so it cannot be changed by modifying operations, or allow it to be flexible.</p> <p>The choice to use static or dynamic schemas must be set at library creation time.</p> <p>In this section we are using <code>static schema</code>, just to be clear.</p>"},{"location":"#reading-and-writing-data","title":"Reading And Writing Data","text":"<p>Now we have a library set up, we can get to reading and writing data. ArcticDB has a set of simple functions for DataFrame storage.</p> <p>Let's write a DataFrame to storage.</p> <p>First create the data:</p> <p><pre><code># 50 columns, 25 rows, random data, datetime indexed.\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\ncols = ['COL_%d' % i for i in range(50)]\ndf = pd.DataFrame(np.random.randint(0, 50, size=(25, 50)), columns=cols)\ndf.index = pd.date_range(datetime(2000, 1, 1, 5), periods=25, freq=\"h\")\ndf.head(5)\n</code></pre> output (the first 5 rows of the data) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     18     48     10     16     38     34     25     44  ...\n2000-01-01 06:00:00     48     10     24     45     22     36     30     19  ...\n2000-01-01 07:00:00     25     16     36     29     25      9     48      2  ...\n2000-01-01 08:00:00     38     21      2      1     37      6     31     31  ...\n2000-01-01 09:00:00     45     17     39     47     47     11     33     31  ...\n</code></pre></p> <p>Then write to the library:</p> <p><pre><code>library.write('test_frame', df)\n</code></pre> output (information about what was written) <pre><code>VersionedItem(symbol=test_frame,library=intro,data=n/a,version=0,metadata=None,host=&lt;host&gt;)\n</code></pre></p> <p>The <code>'test_frame'</code> DataFrame will be used for the remainder of this guide.</p> <p>ArcticDB index</p> <p>When writing Pandas DataFrames, ArcticDB supports the following index types:</p> <ul> <li><code>pandas.Index</code> containing <code>int64</code> (or the corresponding dedicated types <code>Int64Index</code>, <code>UInt64Index</code>)</li> <li><code>RangeIndex</code></li> <li><code>DatetimeIndex</code></li> <li><code>MultiIndex</code> composed of above supported types</li> </ul> <p>The \"row\" concept in <code>head()/tail()</code> refers to the row number ('iloc'), not the value in the <code>pandas.Index</code> ('loc').</p> <p>Read the data back from storage:</p> <p><pre><code>from_storage_df = library.read('test_frame').data\nfrom_storage_df.head(5)\n</code></pre> output (the first 5 rows but read from the database) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     18     48     10     16     38     34     25     44  ...\n2000-01-01 06:00:00     48     10     24     45     22     36     30     19  ...\n2000-01-01 07:00:00     25     16     36     29     25      9     48      2  ...\n2000-01-01 08:00:00     38     21      2      1     37      6     31     31  ...\n2000-01-01 09:00:00     45     17     39     47     47     11     33     31  ...\n</code></pre></p> <p>The data read matches the original data, of course.</p>"},{"location":"#slicing-and-filtering","title":"Slicing and Filtering","text":"<p>ArcticDB enables you to slice by row and by column. </p> <p>ArcticDB indexing</p> <p>ArcticDB will construct a full index for ordered numerical and timeseries (e.g. DatetimeIndex) Pandas indexes. This will enable optimised slicing across index entries. If the index is unsorted or not numeric your data can still be stored but row-slicing will be slower.</p>"},{"location":"#row-slicing","title":"Row-slicing","text":"<p><pre><code>library.read('test_frame', date_range=(df.index[5], df.index[8])).data\n</code></pre> output (the rows in the data range requested) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 10:00:00     23     39      0     45     15     28     10     17  ...\n2000-01-01 11:00:00     36     28     22     43     23      6     10      1  ...\n2000-01-01 12:00:00     18     42      1     15     19     36     41     36  ...\n2000-01-01 13:00:00     28     32     47     37     17     44     29     24  ...\n</code></pre></p>"},{"location":"#column-slicing","title":"Column slicing","text":"<p><pre><code>_range = (df.index[5], df.index[8])\n_columns = ['COL_30', 'COL_31']\nlibrary.read('test_frame', date_range=_range, columns=_columns).data\n</code></pre> output (the rows in the date range and columns requested) <pre><code>                     COL_30  COL_31\n2000-01-01 10:00:00      31       2\n2000-01-01 11:00:00       3      34\n2000-01-01 12:00:00      24      43\n2000-01-01 13:00:00      18       8\n</code></pre></p>"},{"location":"#filtering-and-analytics","title":"Filtering and Analytics","text":"<p>ArcticDB supports many common DataFrame analytics operations, including filtering, projections, group-bys, aggregations, and resampling. The most intuitive way to access these operations is via the <code>LazyDataFrame</code> API, which should feel familiar to experienced users of Pandas or Polars.</p> <p>The legacy <code>QueryBuilder</code> class can also be created directly and passed into <code>read</code> calls with the same effect.</p> <p>ArcticDB Analytics Philosphy</p> <p>In most cases this is more memory efficient and performant than the equivalent Pandas operation as the processing is within the C++ storage engine and parallelized over multiple threads of execution. </p> <p><pre><code>import arcticdb as adb\n_range = (df.index[5], df.index[8])\n_cols = ['COL_30', 'COL_31']\n# Using lazy evaluation\nlazy_df = library.read('test_frame', date_range=_range, columns=_cols, lazy=True)\nlazy_df = lazy_df[(lazy_df[\"COL_30\"] &gt; 10) &amp; (lazy_df[\"COL_31\"] &lt; 40)]\ndf = lazy_df.collect().data\n# Using the legacy QueryBuilder class gives the same result\nq = adb.QueryBuilder()\nq = q[(q[\"COL_30\"] &gt; 10) &amp; (q[\"COL_31\"] &lt; 40)]\nlibrary.read('test_frame', date_range=_range, columns=_cols, query_builder=q).data\n</code></pre> output (the data filtered by date range, columns and the query which filters based on the data values) <pre><code>                     COL_30  COL_31\n2000-01-01 10:00:00      31       2\n2000-01-01 13:00:00      18       8\n</code></pre></p>"},{"location":"#modifications-versioning-aka-time-travel","title":"Modifications, Versioning (aka Time Travel)","text":"<p>ArcticDB fully supports modifying stored data via two primitives: update and append.</p> <p>These operations are atomic but do not lock the symbol. Please see the section on transactions for more on this.</p>"},{"location":"#append","title":"Append","text":"<p>Let's append data to the end of the timeseries.</p> <p>To start, we will take a look at the last few records of the data (before it gets modified)</p> <p><pre><code>library.tail('test_frame', 4).data\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 02:00:00     46     12     38     47      4     31      1     42  ...\n2000-01-02 03:00:00     46     20      5     42      8     35     12      2  ...\n2000-01-02 04:00:00     17     48     36     43      6     46      5      8  ...\n2000-01-02 05:00:00     20     19     24     44     29     32      2     19  ...\n</code></pre> Then create 3 new rows to append. For append to work the new data must have its first <code>datetime</code> starting after the existing data.</p> <p><pre><code>random_data = np.random.randint(0, 50, size=(3, 50))\ndf_append = pd.DataFrame(random_data, columns=['COL_%d' % i for i in range(50)])\ndf_append.index = pd.date_range(datetime(2000, 1, 2, 7), periods=3, freq=\"h\")\ndf_append\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 07:00:00      9     15      4     48     48     35     34     49  ...\n2000-01-02 08:00:00     35      4     12     30     30     12     38     25  ...\n2000-01-02 09:00:00     25     17      3      1      1     15     33     49  ...\n</code></pre></p> <p>Now append that DataFrame to what was written previously</p> <p><pre><code>library.append('test_frame', df_append)\n</code></pre> output <pre><code>VersionedItem(symbol=test_frame,library=intro,data=n/a,version=1,metadata=None,host=&lt;host&gt;)\n</code></pre> Then look at the final 5 rows to see what happened</p> <p><pre><code>library.tail('test_frame', 5).data\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 04:00:00     17     48     36     43      6     46      5      8  ...\n2000-01-02 05:00:00     20     19     24     44     29     32      2     19  ...\n2000-01-02 07:00:00      9     15      4     48     48     35     34     49  ...\n2000-01-02 08:00:00     35      4     12     30     30     12     38     25  ...\n2000-01-02 09:00:00     25     17      3      1      1     15     33     49  ...\n</code></pre></p> <p>The final 5 rows consist of the last two rows written previously followed by the 3 new rows that we have just appended.</p> <p>Append is very useful for adding new data to the end of a large timeseries.</p>"},{"location":"#update","title":"Update","text":"<p>The update primitive enables you to overwrite a contiguous chunk of data. This results in modifying some rows and deleting others as we will see in the example below.</p> <p>Here we create a new DataFrame for the update, with only 2 rows that are 2 hours apart</p> <p><pre><code>random_data = np.random.randint(0, 50, size=(2, 50))\ndf = pd.DataFrame(random_data, columns=['COL_%d' % i for i in range(50)])\ndf.index = pd.date_range(datetime(2000, 1, 1, 5), periods=2, freq=\"2h\")\ndf\n</code></pre> output (rows 0 and 2 only as selected by the <code>iloc[]</code>) <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     47     49     15      6     22     48     45     22  ...\n2000-01-01 07:00:00     46     10      2     49     24     49      8      0  ...\n</code></pre> now update the symbol <pre><code>library.update('test_frame', df)\n</code></pre> output (information about the update) <pre><code>VersionedItem(symbol=test_frame,library=intro,data=n/a,version=2,metadata=None,host=&lt;host&gt;)\n</code></pre></p> <p>Now let's look at the first 4 rows in the symbol:</p> <p><pre><code>library.head('test_frame', 4).data  # head/tail are similar to the equivalent Pandas operations\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     47     49     15      6     22     48     45     22  ...\n2000-01-01 07:00:00     46     10      2     49     24     49      8      0  ... \n2000-01-01 08:00:00     38     21      2      1     37      6     31     31  ... \n2000-01-01 09:00:00     45     17     39     47     47     11     33     31  ...\n</code></pre></p> <p>Let's unpack how we end up with that result. The update has</p> <ul> <li>replaced the data in the symbol with the new data where the index matched (in this case the 05:00 and 07:00 rows)</li> <li>removed any rows within the date range of the new data that are not in the index of the new data (in this case the 06:00 row)</li> <li>kept the rest of the data the same (in this case 09:00 onwards)</li> </ul> <p>Logically, this corresponds to replacing the complete date range of the old data with the new data, which is what you would expect from an update.</p>"},{"location":"#versioning","title":"Versioning","text":"<p>You might have noticed that <code>read</code> calls do not return the data directly - but instead returns a <code>VersionedItem</code> structure. You may also have noticed that modification operations (<code>write</code>, <code>append</code> and <code>update</code>) increment the version number. ArcticDB versions all modifications, which means you can retrieve earlier versions of data - it is a bitemporal database:</p> <p><pre><code>library.tail('test_frame', 7, as_of=0).data\n</code></pre> output <pre><code>                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 23:00:00     16     46      3     45     43     14     10     27  ...\n2000-01-02 00:00:00     37     37     20      3     49     38     23     46  ...\n2000-01-02 01:00:00     42     47     40     27     49     41     11     26  ...\n2000-01-02 02:00:00     46     12     38     47      4     31      1     42  ...\n2000-01-02 03:00:00     46     20      5     42      8     35     12      2  ...\n2000-01-02 04:00:00     17     48     36     43      6     46      5      8  ...\n2000-01-02 05:00:00     20     19     24     44     29     32      2     19  ...\n</code></pre></p> <p>Note the timestamps - we've read the data prior to the <code>append</code> operation. Please note that you can also pass a <code>datetime</code> into any <code>as_of</code> argument, which will result in reading the last version earlier than the <code>datetime</code> passed.</p> <p>Versioning, Prune Previous &amp; Snapshots</p> <p>By default, <code>write</code>, <code>append</code>, and <code>update</code> operations will not remove the previous versions. Please be aware that this will consume more space.</p> <p>This behaviour can be can be controlled via the <code>prune_previous_versions</code> keyword argument. Space will be saved but the previous versions will then not be available.</p> <p>A compromise can be achieved by using snapshots, which allow states of the library to be saved and read back later. This allows certain versions to be protected from deletion, they will be deleted when the snapshot is deleted. See snapshot documentation for details.</p>"},{"location":"#storage-access","title":"Storage Access","text":""},{"location":"#s3-configuration","title":"S3 configuration","text":"<p>There are two methods to configure S3 access. If you happen to know the access and secret key, simply connect as follows:</p> <pre><code>import arcticdb as adb\nac = adb.Arctic('s3://ENDPOINT:BUCKET?region=blah&amp;access=ABCD&amp;secret=DCBA')\n</code></pre> <p>Otherwise, you can delegate authentication to the AWS SDK (obeys standard AWS configuration options):</p> <pre><code>ac = adb.Arctic('s3://ENDPOINT:BUCKET?aws_auth=true')\n</code></pre> <p>Same as above, but using HTTPS:</p> <pre><code>ac = adb.Arctic('s3s://ENDPOINT:BUCKET?aws_auth=true')\n</code></pre> <p>S3</p> <p>Use <code>s3s</code> if your S3 endpoint used HTTPS</p>"},{"location":"#connecting-to-a-defined-storage-endpoint","title":"Connecting to a defined storage endpoint","text":"<p>Connect to local storage (not AWS - HTTP endpoint of s3.local) with a pre-defined access and storage key:</p> <pre><code>ac = adb.Arctic('s3://s3.local:arcticdb-test-bucket?access=EFGH&amp;secret=HGFE')\n</code></pre>"},{"location":"#connecting-to-aws","title":"Connecting to AWS","text":"<p>Connecting to AWS with a pre-defined region:</p> <pre><code>ac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arcticdb-test-bucket?aws_auth=true')\n</code></pre> <p>Note that no explicit credential parameters are given. When <code>aws_auth</code> is passed, authentication is delegated to the AWS SDK which is responsible for locating the appropriate credentials in the <code>.config</code> file or in environment variables. You can manually configure which profile is being used by setting the <code>AWS_PROFILE</code> environment variable as described in the AWS Documentation.</p>"},{"location":"#using-a-specific-path-within-a-bucket","title":"Using a specific path within a bucket","text":"<p>You may want to restrict access for the ArcticDB library to a specific path within the bucket. To do this, you can use the <code>path_prefix</code> parameter:</p> <pre><code>ac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arcticdb-test-bucket?path_prefix=test&amp;aws_auth=true')\n</code></pre>"},{"location":"#azure","title":"Azure","text":"<p>ArcticDB uses the Azure connection string to define the connection: </p> <pre><code>import arcticdb as adb\nac = adb.Arctic('azure://AccountName=ABCD;AccountKey=EFGH;BlobEndpoint=ENDPOINT;Container=CONTAINER')\n</code></pre> <p>For example: </p> <pre><code>import arcticdb as adb\nac = adb.Arctic(\"azure://CA_cert_path=/etc/ssl/certs/ca-certificates.crt;BlobEndpoint=https://arctic.blob.core.windows.net;Container=acblob;SharedAccessSignature=sp=awd&amp;st=2001-01-01T00:00:00Z&amp;se=2002-01-01T00:00:00Z&amp;spr=https&amp;rf=g&amp;sig=awd%3D\")\n</code></pre> <p>For more information, see the Arctic class reference.</p>"},{"location":"#lmdb","title":"LMDB","text":"<p>LMDB supports configuring its map size. See its documentation.</p> <p>You may need to tweak it on Windows, whereas on Linux the default is much larger and should suffice. This is because Windows allocates physical space for the map file eagerly, whereas on Linux the map size is an upper bound to the physical space that will be used.</p> <p>You can set a map size in the connection string:</p> <pre><code>import arcticdb as adb\nac = adb.Arctic('lmdb://path/to/desired/database?map_size=2GB')\n</code></pre> <p>The default on Windows is 2GiB. Errors with <code>lmdb errror code -30792</code> indicate that the map is getting full and that you should increase its size. This will happen if you are doing large writes.</p> <p>In each Python process, you should ensure that you only have one Arctic instance open over a given LMDB database.</p> <p>LMDB does not work with remote filesystems.</p>"},{"location":"#in-memory-configuration","title":"In-memory configuration","text":"<p>An in-memory backend is provided mainly for testing and experimentation. It could be useful when creating files with LMDB is not desired.</p> <p>There are no configuration parameters, and the memory is owned solely by the Arctic instance.</p> <p>For example:</p> <pre><code>import arcticdb as adb\nac = adb.Arctic('mem://')\n</code></pre> <p>For concurrent access to a local backend, we recommend LMDB connected to tmpfs, see LMDB and In-Memory Tutorial.</p>"},{"location":"#transactions","title":"Transactions","text":"<ul> <li>Transactions can be be very useful but are often expensive and slow</li> <li>If we unpack ACID: Atomicity, Consistency and Durability are useful, Isolation less so</li> <li>Most analytical workflows can be constructed to run without needing transactions at all</li> <li>So why pay the cost of transactions when they are often not needed?</li> <li>ArcticDB doesn't have transactions because it is designed for high throughput analytical workloads</li> </ul>"},{"location":"aws/","title":"Getting Started with AWS S3","text":"<p>There are detailed guides on the AWS website on S3 configuration.  You can start with the Creating your first bucket page.</p> <p>Best practices with AWS S3 can depend on your situation.  If you're looking to try ArcticDB on AWS S3 then a reasonably simple approach is the following.</p>"},{"location":"aws/#1-create-an-iam-user","title":"1. Create an IAM user","text":"<p>Best practice is to manage and access S3 buckets with a Non-root IAM account in AWS.   Ideally create a new account just for trialling ArcticDB.</p> <p>You can create an IAM account at IAM &gt; Users. - Click <code>Add users</code>. - Come up with a username, e.g. <code>arcticdbtrial</code>, then click <code>Next</code>. - Select <code>Attach policies directly</code> and attach the <code>AmazonS3FullAccess</code> policy to that account, then click <code>Next</code>. - Click <code>Create user</code>.</p>"},{"location":"aws/#2-create-the-access-key","title":"2. Create the access key","text":"<ul> <li>Click on the new user in the IAM &gt; Users table.</li> <li>Go to <code>Security credentials</code> &gt; <code>Access keys</code> and click <code>Create access Key</code></li> <li>Select the <code>Local code</code> option, ArcticDB is the local code.</li> <li>Check <code>I understand...</code> and click <code>Next</code>.</li> <li>Click <code>Create access Key</code>.</li> <li>Record the Access key and Secret access key somewhere securely.</li> </ul>"},{"location":"aws/#3-create-the-bucket","title":"3. Create the bucket","text":"<p>The rest of the steps can be performed using commands on your client machine. Install the AWS CLI if you do not already have it installed.</p> <p>Use the AWS CLI to configure your client machine with the Access key and Secret access key. You will also need to select an AWS region. Selecting a region that is local to your ArcticDB client will improve performance. <pre><code>$ aws configure\nAWS Access Key ID [None]: &lt;ACCESS_KEY&gt;\nAWS Secret Access Key [None]: &lt;SECRET_KEY&gt;\nDefault region name [None]: &lt;REGION&gt;\nDefault output format [None]:\n</code></pre></p> <p>Create a new bucket.  Bucket names must be globally unique so you will need to come up with your own. <pre><code>$ aws s3 mb s3://&lt;BUCKET_NAME&gt;\n</code></pre></p>"},{"location":"aws/#4-connect-to-the-bucket","title":"4. Connect to the bucket","text":"<ul> <li>Install ArcticDB.</li> <li>Use your <code>&lt;REGION&gt;</code> and <code>&lt;BUCKET_NAME&gt;</code>.</li> <li>Setup <code>~/.aws/credentials</code> with <code>aws configure</code>, as above. <pre><code>import arcticdb as adb\narctic = adb.Arctic('s3://s3.&lt;REGION&gt;.amazonaws.com:&lt;BUCKET_NAME&gt;?aws_auth=true')\n</code></pre></li> </ul>"},{"location":"aws/#checking-connectivity-to-the-s3-bucket","title":"Checking connectivity to the S3 bucket.","text":"<p>ArcticDB currently uses five S3 methods, those are: - GetObject - PutObject - HeadObject - DeleteObject - ListObjectsV2</p> <p>Here is a short script you can use to check connectivity to a bucket from your client.  If this works, then the configuration should be correct for read and write with ArcticDB.</p> <ul> <li>Use your <code>&lt;BUCKET_NAME&gt;</code> below.  </li> <li>Install boto3.</li> <li>Setup <code>~/.aws/credentials</code> with <code>aws configure</code>, as above. <pre><code>import io\nimport boto3\ns3 = boto3.client('s3')\nbucket = '&lt;BUCKET_NAME&gt;'\ns3.put_object(Bucket=bucket, Key='_arctic_check/check.txt', Body=io.BytesIO(b'check file contents'))\ns3.list_objects_v2(Bucket=bucket, Prefix='_arctic_check/')\ns3.head_object(Bucket=bucket, Key='_arctic_check/check.txt')\ns3.get_object(Bucket=bucket, Key='_arctic_check/check.txt')\ns3.delete_object(Bucket=bucket, Key='_arctic_check/check.txt')\n</code></pre> The check object written in that script should not interfere with normal ArcticDB operation on the bucket.</li> </ul>"},{"location":"aws/#aws-security-token-service-sts-setup","title":"AWS Security Token Service (STS) setup","text":"<p>STS allows users to assume specfic roles in order to gain temporary access to AWS resources. Please refer to the offical website for more details. To use it with ArcticDB, please setup the credential in the AWS shared config file.</p> <p>File location:</p> Platform Location Linux and macOS <code>~/.aws/config</code> Windows <code>%USERPROFILE%\\.aws\\config</code> <p>Content:</p> <pre><code>[profile PROFILE]\nrole_arn = ROLE_ARN_TO_BE_ASSUMED\nsource_profile = BASE_PROFILE\n\n[profile BASE_PROFILE]\naws_access_key_id = ID\naws_secret_access_key = KEY\n</code></pre> <p>Use the configuration in ArcticDB: <pre><code>&gt;&gt;&gt; import arcticdb as adb\n&gt;&gt;&gt; arctic = adb.Arctic('s3://s3.REGION.amazonaws.com:BUCKET?aws_auth=sts&amp;aws_profile=PROFILE')\n</code></pre></p> <p>Due to limitations in the AWS C++ SDK, if ArcticDB fails to obtain the temporary token by assuming a role, an assertion error similar to the one below will occur: <pre><code>virtual void Aws::Auth::STSProfileCredentialsProvider::Reload(): Assertion `!profileIt-&gt;second.GetCredentials().IsEmpty()' failed.\n</code></pre> This is usually due to an incorrect IAM account setup or an incorrect configuration file.</p>"},{"location":"aws_permissions/","title":"Dynamic library permissions with ArticDB on AWS S3","text":""},{"location":"aws_permissions/#goal","title":"Goal","text":"<p>One of the advantages of ArcticDB is how easy it is to setup and use as a personal database.  But how can we extend this pattern to an organisation?  How can we keep it trivial to use as an individual, but allow for secure sharing of data with team-mates and groups across your organisation?  We also want this to be easy to maintain, a key challange for permissions generally.</p> <p>Here we model a small two team organisation, 'Acme', in AWS and create some flexible permissions that allow users and teams to create and use private and shared data without any per-library setup.</p> <ul> <li>Data Team<ul> <li>Jane</li> <li>Samir</li> </ul> </li> <li>Quant Team<ul> <li>Alan</li> <li>Diana</li> </ul> </li> </ul> <p>Each user should be able to, </p> <ul> <li>list all ArcticDB libraries (but not their content)</li> <li>create personal libraries that only they can read and write to</li> <li>create team libraries that only those in their team can read and write to</li> </ul> <p>Users will follow an ArcticDB library name convention.  The library name should one of <code>&lt;USERNAME&gt;/&lt;LIBRARY&gt;</code> or <code>&lt;TEAM&gt;/&lt;MYLIBRARY&gt;</code>, so if Jane wants to create a personal library for weather data, they would use <code>lib.create_library('jane@acme/weather')</code> (assuming the AWS S3 setup below).</p> <p>We can do this with path-prefix permissions in AWS S3.  Other backends, such as Minio, support path based permissioning.</p>"},{"location":"aws_permissions/#aws-s3","title":"AWS S3","text":"<p>You should have a number of users setup in AWS IAM already along with a group containing all the users.  Follow the IAM docs for help with that.</p> <p>For Acme we've setup four users and the users are tagged with the teams they are a member of:</p> aws:username aws:PrincipalTag/team jane@acme data samir@acme data alan@acme quant diana@acme quant <p>We've also created a user group, <code>acme</code> with all four users in.</p> <p>Let's create an S3 bucket for Acme, <code>acme-arcticdb</code>, using cloudshell. <pre><code>aws s3 mb s3://acme-arcticdb\n</code></pre></p>"},{"location":"aws_permissions/#setting-up-the-permissions-policy","title":"Setting up the permissions policy","text":"<p>In general for read access our users will need <code>s3:ListBucket</code> and <code>s3:GetObject</code> permissions and for write access our users will additionally need <code>s3:PutObject</code> and <code>s3:DeleteObject</code>.</p> <p>Then setup the following access policy.  We will save this snippet to <code>policy.json</code>. <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjects\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"arn:aws:s3:::acme-arcticdb\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"_arctic_cfg/*\",\n                        \"${aws:username}/*\",\n                        \"${aws:PrincipalTag/team}/*\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"PutGetDeleteObjects\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::acme-arcticdb/${aws:username}/*\",\n                \"arn:aws:s3:::acme-arcticdb/${aws:PrincipalTag/team}/*\",\n                \"arn:aws:s3:::acme-arcticdb/_arctic_cfg/cref/?sUt?${aws:username}/*\",\n                \"arn:aws:s3:::acme-arcticdb/_arctic_cfg/cref/?sUt?${aws:PrincipalTag/team}/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>Create the policy in AWS. <pre><code>aws iam create-policy --policy-name acme-arcticdb-access --policy-document file://policy.json\n</code></pre></p> <p>Take note of the <code>Arn</code> in the output to the last command as you'll need it to attach the policy to a group with all your users in, for this example the group is <code>acme</code>. <pre><code>aws iam attach-group-policy --policy-arn &lt;ARN&gt; --group-name acme\n</code></pre></p> <p>If you intend to adapt that example policy to your own situation then please note that,</p> <ul> <li><code>acme-arcticdb</code> is the name of the bucket and will need to be replaced everywhere</li> <li><code>s3:ListBucket</code> is used to permission <code>ListObjectsV2</code> and needs its own section, as it applies to the bucket as a whole.  We control access to paths by checking the <code>s3:prefix</code> argument that's part of the <code>ListObjectsV2</code> request.</li> <li><code>Put</code>, <code>Get</code> and <code>Delete</code> can be specifed for object paths in the second section.</li> <li><code>_arctic_cfg/cref/*</code> is where the ArcticDB library configuration is stored and the data for each library is stored in the root of the bucket with a path that starts with the library name.</li> <li>By using <code>${aws:username}</code> and <code>${aws:PrincipalTag/team}</code> we've restricted library access to those with a matching AWS IAM username or a matched user 'team' tag.</li> </ul>"},{"location":"aws_permissions/#security-note","title":"Security note","text":"<p>Because <code>${aws:username}</code>, <code>${aws:PrincipalTag/team}</code> and <code>_arctic_cfg</code>... are at the beginning of the path, it's important they don't contain values that can overlap. For example if you have a team called 'data' and a username for an application called 'data', they will have the same permissions, or if a username can be created that starts with <code>_arctic_cfg</code>... then that user will be able to modify all library configs.</p>"},{"location":"aws_permissions/#usage","title":"Usage","text":"<p>Jane can now list and read and write to their own libraries and team libraries, but not to others in Acme.</p> <pre><code>import numpy as np\nimport arcticdb as adb\n\n# jane@acme team=data\naccess = '&lt;REDACTED&gt;'\nsecret = '&lt;REDACTED&gt;'\nbucket='acme-arcticdb'\nregion='eu-west-2'\n\narctic = adb.Arctic(f's3://s3.{region}.amazonaws.com:{bucket}?access={access}&amp;secret={secret}')\n\n# Create library as me\narctic.create_library('jane@acme/weather')\nlib = arctic.get_library('jane@acme/weather')\nlib.write('test', np.arange(100))\n\n# Create library as data team\narctic.create_library('data/forecast')\nlib = arctic.get_library('data/forecast')\nlib.write('test', np.arange(100))\n\n# See all libraries\narctic.list_libraries()\n# ['alan@acme/bonds', 'data/forecast', 'jane@acme/weather', 'quant/stocks']\n\n# Can't use or delete Alan or Quant team data\narctic.get_library('alan@acme/bonds')\narctic.get_library('quant/stocks')\narctic.delete_libraru('alan@acme/bonds')\n# All raise:\n# PermissionException: E_PERMISSION Permission error: S3Error#15 : No response body.\n</code></pre>"},{"location":"azure/","title":"Getting started with Azure Blob Storage","text":"<p>Azure Blob Storage has an extensive set of configuration and access settings. There are detailed guides in the Azure Blob Storage documentation. Best practice can depend on your situation. This guide is intended as a quick-start, to help you get going with ArcticDB.</p> <p>You will need an azure account with permission to create storage-accounts. - Install Azure CLI or use the browser based Cloud Shell. - If you installed Azure CLI then you will also need to login.</p>"},{"location":"azure/#1-select-a-region","title":"1. Select a region.","text":"<p>A region close to your client will mean greater performance. You can list your available regions with. <pre><code>az account list-locations -o table\n</code></pre></p>"},{"location":"azure/#2-create-a-resource-group","title":"2. Create a resource-group","text":"<p>This is not required but best practice would be to create a new resource-group to try out arcticdb.  Resource groups are there to help you collect together and manage related resources in Azure. Set your chosen <code>&lt;REGION&gt;</code> here.  If you use an existing resource-group then replace that in the examples below. <pre><code>az group create --name arcticdb --location &lt;REGION&gt;\n</code></pre></p>"},{"location":"azure/#3-create-a-blob-storage-account","title":"3. Create a blob storage account","text":"<p>This is created within your resource-group.  Choose a <code>&lt;STORAGE_NAME&gt;</code>, it needs to be globally unique across all of Azure. <pre><code>az storage account create -g arcticdb --allow-blob-public-access false --sku Standard_LRS -n &lt;STORAGE_NAME&gt;\n</code></pre> <code>-g arcticdb</code> is the resource-group you created in the last step.</p>"},{"location":"azure/#4-create-a-container","title":"4. Create a container","text":"<p>Create a container within the storage account.  Depending on your account and CLI setup you may need to provide authorization for this step. <pre><code>az storage container create  --name data --account-name &lt;STORAGE_NAME&gt;\n</code></pre></p>"},{"location":"azure/#5-connect-to-the-storage-account","title":"5. Connect to the storage account","text":"<ul> <li> <p>Get the connection string. <pre><code>az storage account show-connection-string -g arcticdb --query connectionString -n &lt;STORAGE_NAME&gt; | sed 's,\",,g'\n</code></pre> The connection string includes the <code>AccountKey</code> for authentication and so you should store it securely.</p> </li> <li> <p>Install ArcticDB.</p> </li> <li>Find your CA_CERT_PATH path. See the ArcticAB API docs for more information.</li> <li>Replace <code>&lt;CONNECTION_STRING&gt;</code> and <code>&lt;CA_CERT_PATH&gt;</code> in the following example. <pre><code>import arcticdb as adb\nconnection_string = '&lt;CONNECTION_STRING&gt;'\nca_cert_path = '&lt;CA_CERT_PATH&gt;'\narctic = adb.Arctic(f\"azure://{connection_string};Container=data;CA_cert_path={ca_cert_path}\")\n</code></pre></li> </ul>"},{"location":"error_messages/","title":"Error Messages","text":"<p>This page details the exceptions and associated error messages users are most likely to encounter, what they mean, and what (if anything) can be done to resolve the issue.</p> <p>For legacy reasons, the terms <code>symbol</code>, <code>stream</code>, and <code>stream ID</code> are used interchangeably.</p>"},{"location":"error_messages/#errors-with-numeric-error-codes","title":"Errors with numeric error codes","text":"<p>Note</p> <p>We are in the process of adding error codes to all user-facing errors. As a result, this section will expand as error codes are added to existing errors.</p>"},{"location":"error_messages/#internal-errors","title":"Internal Errors","text":"Error Code Cause Resolution 1000 An invalid date range has been passed in. ArcticDB date ranges must be in increasing order. Ensure the requested range is sorted. 1001 Invalid Argument An invalid argument has been passed in. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1002 An internal ArcticDB assertion has failed. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1003 ArcticDB has encountered an internal error. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1004 Unsupported config found in storage Follow the instructions in the error message to repair configuration within your Arctic instance."},{"location":"error_messages/#normalization-errors","title":"Normalization Errors","text":"Error Code Cause Resolution 2000 Attempting to update or append an existing type with an incompatible object type NumPy arrays or Pandas DataFrames can only be mutated by a matching type. Read the latest version of the symbol and update/append with the corresponding type. 2001 Input type cannot be converted to an ArcticDB type. Please ensure all input types match supported ArcticDB types. 2003 A write of an incompatible index type has been attempted. ArcticDB only supports defined Pandas index types. Please see the documentation for more information on what types are supported. 2004 A NumPy append is attempting to change the shape of the previous version. When storing NumPy arrays, append operations must have the same shape as the previous version."},{"location":"error_messages/#missing-data-errors","title":"Missing Data Errors","text":"Error Code Cause Resolution 3000 A missing version has been requested of a symbol. Please request a valid version - see the documentation for the <code>list_versions</code> method to enumerate existing versions. 3001 A symbol from an incomplete library without any versions was requested and no incomplete segments were found. Append incomplete data to the symbol."},{"location":"error_messages/#schema-error","title":"Schema Error","text":"Error Code Cause Resolution 4000 The number, type, or name of the columns has been changed. Ensure that the type and order of the columns has not changed when appending or updating the previous version. This restriction only applies when <code>Dynamic Schema</code> is disabled - if you require the columns sets to change, please enable the <code>Dynamic Schema</code> option on your library. 4001 The specified column does not exist. Please specify a valid column - use the <code>get_description</code> method to see all of the columns associated with a given symbol. 4002 The requested operation is not supported with the type of column provided. Certain operations are not supported over all column types e.g. arithmetic in the processing pipeline over string columns - use the <code>get_description</code> method to see all of the columns associated with a given symbol, along with their types. 4003 The requested operation is not supported with the index type of the symbol provided. Certain operations are not supported over all index types e.g. column statistics generation with a string index - use the <code>get_description</code> method to see the index(es) associated with a given symbol, along with their types. 4004 The requested operation is not supported with pickled data. Certain operations are not supported with pickled data e.g. <code>date_range</code> filtering. If such operations are required, you must ensure that the data is of a normalizable type, such that it can be written using the <code>write</code> method, and does not require the <code>write_pickle</code> method."},{"location":"error_messages/#storage-errors","title":"Storage Errors","text":"Error Code Cause Resolution 5000 A missing key has been requested. ArcticDB has requested a key that does not exist in storage. Ensure that you have requested a <code>symbol</code>, <code>snapshot</code>, <code>version</code>, or column statistic that exists. 5001 ArcticDB is attempting to write to an already-existing key in storage. This error is unexpected - please ensure that no other tools are writing data the same storage location that may conflict with ArcticDB. 5002 The symbol being worked on does not exist. ArcticDB has requested a key that does not exist in storage. Ensure that the symbol exists. 5003 Don't have permissions to carry out the operation. Ensure that you have the permissions to perform the requested operation on the given key. 5010 The LMDB map is full. Close and reopen your LMDB backed Arctic instance with a larger map size. For example to open <code>/tmp/a/b/</code> with a map size of 5GB, use <code>adb.Arctic(\"lmdb:///tmp/a/b?map_size=5GB\")</code>. Also see the LMDB documentation. 5011 An unexpected LMDB error occurred. e.g. File corruption, Environment version mismatch, Page type mismatch etc. Varies depending on the type of failure. Read more on LMDB: return codes 5020 An unexpected S3 error occurred. e.g. Network error, Service not available, Throttling failure etc. Varies depending on the type of failure. 5021 An unexpected S3 error occurred which is retryable. Varies depending on the type of failure. 5030 An unexpected Azure error occurred with a given status code and error code. Varies depending on the type of failure. Read more on Azure Blob Storage error code docs. 5050 Mongo didn't acknowledge the operation. This means that the mongo apis didn't confirm whether the operation was successful. Retry running the previous operation. 5051 An unexpected Mongo error occurred with a given error code. Varies depending on the type of failure. Check MongoDB error codes."},{"location":"error_messages/#sorting-errors","title":"Sorting Errors","text":"Error Code Cause Resolution 6000 Data should be sorted for this operation. The requested operation requires data to be sorted. If this is a modification operation such as update, sort the input data. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the data to be sorted"},{"location":"error_messages/#user-input-errors","title":"User Input Errors","text":"Error Code Cause Resolution 7000 The input provided by the user is invalid in some fashion. The resolution will depend on the nature of the incorrect input, and should be explained in the associated error message. 7001 The input was expected to be a valid decimal string but it is not a valid decimal string. Pass a valid decimal string. 7002 An unsupported character was found in a symbol or library name. We support only the ASCII characters between 32-127 inclusive and exclude <code>&lt;</code>, <code>&gt;</code> and <code>*</code> specifically. Change your name so it contains only valid characters. If you want to bypass this check for symbol names, you can define an environment variable called - ARCTICDB_VersionStore_NoStrictSymbolCheck_int=1. 7003 The library or symbol name was too long. We currently only support names up to 255 characters long. Change your name so it not longer than 255 characters."},{"location":"error_messages/#compatibility-errors","title":"Compatibility Errors","text":"Error Code Cause Resolution 8000 The version of ArcticDB being used to read the column statistics does not understand the statistics format. Update ArcticDB to (at least) the same version as that being used to create the column statistics."},{"location":"error_messages/#errors-without-numeric-error-codes","title":"Errors without numeric error codes","text":""},{"location":"error_messages/#pickling-errors","title":"Pickling errors","text":"<p>These errors relate to data being pickled, which limits the operations available. Internally, pickled symbols are stored as opaque, serialised binary blobs in the data layer. No index or column information is maintained in this serialised object which is in contrast to non-pickled data, where this information is stored in the index layer.</p> <p>Furthermore, it is not possible to partially read/update/append the data using the ArcticDB API or use the processing pipeline with pickled symbols. </p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot append to pickled data  Cannot update pickled data A symbol has been created with the <code>write_pickle</code> method, and now <code>append</code>/<code>update</code> has been called on this symbol. Pickled data cannot be appended to or updated, due to the lack of indexing or column information in the index layer as explained above. If appending is required, the symbol must be created with <code>write</code>, and must therefore only contain normalizeable data types. Cannot delete date range of pickled data  Cannot use head/tail/row_range with pickled data, use plain read instead  Cannot filter pickled data   The data for this symbol is pickled and does not support date_range, row_range, or column queries A symbol has been created with the <code>write_pickle</code> method, and now <code>delete_data_in_range</code>/<code>head</code>/<code>tail</code>/<code>read</code> with a <code>QueryBuilder argument</code> has been called on this symbol. For reading operations, unpickling is inherently a Python-layer process. Therefore any operation that would cut down the amount of data returned to a user compared to a call to <code>read</code> with no optional parameters cannot be performed in the C++ layer, and would be no faster than calling <code>read</code> and then filtering the result down in Python."},{"location":"error_messages/#snapshot-errors","title":"Snapshot errors","text":"<p>Errors that can be encountered when creating  and deleting snapshots, or trying to read data from a specific snapshot.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Snapshot with name &lt;name&gt; already exists The <code>snapshot</code> method was called, but a snapshot with the specified name already exists. The old snapshot must first be deleted with <code>delete_snapshot</code>. Cannot snapshot version(s) that have been deleted... A <code>versions</code> dictionary was provided to the <code>snapshot</code> method, but one of the symbol-version pairs specified does not exist. The <code>list_versions</code> method can be used to see which versions of which symbols are in which snapshots. Only one of skip_symbols and versions can be set The <code>snapshot</code> method was called with both the <code>skip_symbols</code> and <code>versions</code> optional arguments set. Just specify <code>versions</code> on its own in this case."},{"location":"error_messages/#require-live-version-errors","title":"Require live version errors","text":"<p>A select few operations with ArcticDB require the symbol to exist and have at least one live version. These errors occur when this is not the case.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot update non-existent stream &lt;symbol&gt; The <code>update</code> method was called with the optional <code>upsert</code> defaulted or set to <code>False</code>, but this symbol has no live versions. If the symbol is expected to have a live version, then this is a genuine error. Otherwise, set <code>upsert</code> to <code>True</code>."},{"location":"error_messages/#date-range-related-errors","title":"Date-range related errors","text":"<p>All calls to <code>delete_data_in_range</code> and <code>update</code>, and calls to <code>read</code> using the <code>date_range</code> optional argument, require the existing data to have a sorted timestamp index. ArcticDB does not check this condition at write time.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot apply date range filter to symbol with non-timestamp index <code>read</code> method called with the optional <code>date_range</code> argument specified, but the symbol does not have a timestamp index. None, the <code>date_range</code> parameter does not make sense without a timestamp index. Non-contiguous rows, range search on unsorted data?... <code>read</code> method called with the optional <code>date_range</code> argument specified, and the symbol has a timestamp index, but it is not sorted. To use the <code>date_range</code> argument to <code>read</code>, the user must ensure the data is sorted on the index at write time. Delete in range will not work as expected with a non-timeseries index <code>delete_data_in_range</code> method called, but the symbol does not have a timestamp index. None, the <code>delete_data_in_range</code> method does not make sense without a timestamp index."},{"location":"error_messages/#processing-pipeline-errors","title":"Processing pipeline errors","text":"<p>Due to the client-only nature of ArcticDB, it is not possible to know if a processing operation applied to a <code>LazyDataFrame</code>, or provided to <code>read</code> with a <code>QueryBuilder</code> object, makes sense for the given symbol without interacting with the storage. In particular, we do not know:</p> <ul> <li>Whether a specified column exists</li> <li>What the type of the data held in a specified column is if it does exist</li> </ul> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Unexpected column name A column name was specified for a processing operation that does not exist for this symbol, and the library has dynamic schema disabled. Use <code>get_description</code> to ensure that column names provided in processing operations exist for the symbol. Non-numeric type provided to binary operation: &lt;typename&gt; Error messages like this imply that an operation that ArcticDB does not support was provided in a processing operation e.g. adding two string columns together. The <code>get_description</code> method can be used to inspect the types of the columns. A full list of supported operations are provided in the <code>QueryBuilder</code> API documentation. Cannot compare &lt;typename 1&gt; to &lt;typename 2&gt; (possible categorical?) If <code>get_description</code> indicates that a column is of categorical type, and this categorical is being used to store string values, then comparisons to other strings will fail with an error message like this one. Categorical support in ArcticDB is extremely limited, but may be added in the future."},{"location":"error_messages/#encoding-errors","title":"Encoding errors","text":"<p>These errors should be extremely rare, however it is possible that the encoding in the storage may change from time to time. Whilst the changes will always be backwards compatible (new clients can always read the old data), it's possible they may not be forward-compatible, and data that has been written by a new client cannot be read by an older one</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Error decoding A column was unable to be decoded by the compression algorithm. Upgrade to a later version of the client."},{"location":"error_messages/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>ArcticDB exceptions are exposed in <code>arcticdb.exceptions</code> and sit in a hierarchy:</p> <pre><code>RuntimeError\n\u2514-- ArcticException\n    |-- ArcticDbNotYetImplemented\n    |-- MissingDataException\n    |-- NoDataFoundException\n    |-- NoSuchVersionException\n    |-- NormalizationException\n    |-- SchemaException\n    |-- SortingException\n    |   \u2514-- UnsortedDataException\n    |-- StorageException\n    |   \u2514-- LmdbMapFullError\n    |   \u2514-- PermissionException\n    |   \u2514-- DuplicateKeyException\n    |-- StreamDescriptorMismatch\n    \u2514-- InternalException\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Note</p> <p>This FAQ document covers multiple topic areas - please see the contents table on the  right for more information.</p>"},{"location":"faq/#product","title":"Product","text":""},{"location":"faq/#what-is-arcticdb","title":"What is ArcticDB?","text":"<p>ArcticDB is a high performance DataFrame database built for the modern Python Data Science ecosystem. ArcticDB is an embedded database engine - which means that installing ArcticDB is as simple as installing a Python package. This also means that ArcticDB does not require any server infrastructure to function.</p> <p>ArcticDB is optimised for numerical datasets spanning millions of rows and columns, enabling you to store and retrieve massive datasets within a Pythonic, DataFrame-like API that researchers, data scientists and software engineers will find immediately familiar.</p>"},{"location":"faq/#how-does-arcticdb-differ-from-the-version-of-arctic-on-github","title":"How does ArcticDB differ from the version of Arctic on GitHub?","text":"<p>Please see the history page.</p>"},{"location":"faq/#how-does-arcticdb-differ-from-apache-parquet","title":"How does ArcticDB differ from Apache Parquet?","text":"<p>Both ArcticDB and Parquet enable the storage of columnar data without requiring additional infrastructure.</p> <p>ArcticDB however uses a custom storage format that means it offers the following functionality over Parquet:</p> <ul> <li>Versioned modifications (\"time travel\") - ArcticDB is bitemporal.</li> <li>Timeseries indexes. ArcticDB is a timeseries database and as such is optimised for slicing  and dicing timeseries data containing billions of rows.</li> <li>Data discovery - ArcticDB is built for teams. Data is structured into libraries and symbols rather  than raw filepaths.</li> <li>Support for streaming data. ArcticDB is a fully functional streaming/tick database, enabling the storage  of both batch and streaming data.</li> <li>Support for \"dynamic schemas\" - ArcticDB supports datasets with changing schemas (column sets) over time.</li> <li>Support for automatic data deduplication.</li> </ul>"},{"location":"faq/#what-sort-of-data-is-arcticdb-best-suited-to","title":"What sort of data is ArcticDB best suited to?","text":"<p>ArcticDB is an OLA(nalytical)P DBMS, rather than an OLT(ransactional)P DBMS.</p> <p>In practice, this means that ArcticDB is optimised for large numerical datasets and for queries that operate over many rows at a time.</p>"},{"location":"faq/#does-arcticdb-require-a-server","title":"Does ArcticDB require a server?","text":"<p>No. ArcticDB is a fully fledged embedded analytical database system, designed for modern cloud and on-premises object storage that does not require a server for any of the core features.</p>"},{"location":"faq/#what-languages-can-i-use-arcticdb-with","title":"What languages can I use ArcticDB with?","text":"<p>Bindings are currently only available for Python. </p>"},{"location":"faq/#what-is-the-best-practice-for-saving-data-to-arcticdb","title":"What is the best practice for saving Data to ArcticDB?","text":"<p>Users should consider how they store data based on their specific use cases. See the guide here.</p>"},{"location":"faq/#what-are-the-limitations-of-arcticdb-being-client-side","title":"What are the Limitations of ArcticDB being client-side?","text":"<p>The serverless nature of ArcticDB provides excellent performance, making it ideal for data science applications where speed and efficiency are key.  It ensures atomicity, consistency, and durability but does not isolate transactions. Changes to symbols are performed on a last-writer-wins principle, and without isolation, write-after-read transactions are not supported, which is not suitable for use cases requiring strong transactional guarantees. </p>"},{"location":"faq/#what-storage-options-does-arcticdb-support","title":"What storage options does ArcticDB support?","text":"<p>ArcticDB offers compatibility with a wide array of storage choices, both on-premises and in the cloud. It is verified to work with multiple storage systems such as AWS S3, Azure Blob Storage, LMDB, In-memory, Ceph, MinIO (Linux), Pure Flashblade S3, Scality S3, and VAST Data S3, with plans to support additional options soon. </p>"},{"location":"faq/#what-are-the-trade-offs-with-arcticdb-versioning","title":"What are the trade offs with ArcticDB Versioning?","text":"<p>ArcticDB versions data by default, allowing for point-in-time analysis and efficient data updates, including daily appends and historical corrections, making it ideal for research datasets. The database is capable of de-duplicating data that has not changed between versions, using storage space efficiently. The ArcticDB enterprise tools including data pruning and compaction to help manage storage and data-fragmentation as new versions are created. Storing large numbers of versions of data does require more storage.</p> <p>More information can be found here!</p>"},{"location":"faq/#what-granularity-of-authorization-does-arcticdb-support","title":"What granularity of authorization does ArcticDB support?","text":"<p>Authentication is at storage account level, and authorization can be done at ArcticDB Library level for most S3 backends (with directory/path permissions), otherwise also at storage account level.  There are many third-party authentication and authorization integrations available for the backends.</p>"},{"location":"faq/#how-is-arcticdb-data-catalogued-and-discoverable-by-consumers","title":"How is ArcticDB data catalogued and discoverable by consumers?","text":"<p>ArcticDB offers capabilities to list libraries and symbols, complete with metadata. You can use these functions to discover and browse data stored in ArcticDB.</p>"},{"location":"faq/#how-can-i-get-started-using-arcticdb","title":"How can I get started using ArcticDB?","text":"<p>Please see our getting started guide!</p>"},{"location":"faq/#technical","title":"Technical","text":""},{"location":"faq/#does-arcticdb-use-sql","title":"Does ArcticDB use SQL?","text":"<p>No. ArcticDB enables data access and modifications with a Python API that speaks in terms of Pandas DataFrames. See the reference documentation for more details.</p>"},{"location":"faq/#does-arcticdb-de-duplicate-data","title":"Does ArcticDB de-duplicate data?","text":"<p>Yes.</p> <p>On each <code>write</code>, ArcticDB will check the previous version of the symbol that you are writing (and only this version - other symbols will not be scanned!) and skip the write of identical segments. Please keep in mind however that this is most effective when version <code>n</code> is equal to version <code>n-1</code> plus additional data at the end - and only at the end! If there is additional data inserted into the in the middle, then all segments occuring after that modification will almost certainly differ. ArcticDB segments data at fixed intervals and data is only de-duplicated if the hashes of the data segments are identical - as a result, a one row offset will prevent effective de-duplication.</p> <p>Note that this is a library configuration option that is off by default, see <code>help(LibraryOptions)</code> for details of how to enable it.</p>"},{"location":"faq/#how-does-arcticdb-enable-advanced-analytics","title":"How does ArcticDB enable advanced analytics?","text":"<p>ArcticDB is primarily focused on filtering and transfering data from storage through to memory - at which point Pandas, NumPy, or other standard analytical packages can be utilised for analytics.</p> <p>That said, ArcticDB does offer a limited set of analytical functions that are executed inside the C++ storage engine offering significant performance benefits over Pandas. For more information, see the documentation for the <code>LazyDataFrame</code>, <code>LazyDataFrameCollection</code>, and <code>QueryBuilder</code> classes.</p>"},{"location":"faq/#what-does-pickling-mean","title":"What does Pickling mean?","text":"<p>ArcticDB has two means for storing data:</p> <ol> <li>ArcticDB can store your data using the Arctic On-Disk Storage Format.</li> <li>ArcticDB can Pickle your data, storing it as a giant binary blob.</li> </ol> <p>(1) is vastly more performant (i.e. reads and writes are faster), space efficient and unlocks data slicing as described in the getting started guide. There are no practical advantages to storing your data as a Pickled binary-blob - other than certain data types must be Pickled for ArcticDB to be able to store them at all!</p> <p>ArcticDB is only able to store the following data types natively:</p> <ol> <li>Pandas DataFrames</li> <li>NumPy arrays</li> <li>Integers (including timestamps - though timezone information in timestamps is removed)</li> <li>Floats</li> <li>Bools</li> <li>Strings (written as part of a DataFrame/NumPy array)</li> </ol> <p>Note that ArcticDB cannot efficiently store custom Python objects, even if inserted into a Pandas DataFrames/NumPy array.  Pickled data cannot be index or column-sliced, and neither <code>update</code> nor <code>append</code> primitives will function on pickled data. </p>"},{"location":"faq/#how-does-indexing-work-in-arcticdb","title":"How does indexing work in ArcticDB?","text":"<p>See the Getting Started page for details of supported index types.</p>"},{"location":"faq/#can-i-append-with-additional-columns-what-is-dynamic-schema","title":"Can I <code>append</code> with additional columns / What is Dynamic Schema?","text":"<p>You can <code>append</code> (or <code>update</code>) with differing column sets to symbols for which the containing library has <code>Dynamic Schema</code> enabled. See the documentation for the <code>create_library</code> method for more information.</p> <p>You can also change the type of numerical columns - for example, integers will be promoted to floats on read.</p>"},{"location":"faq/#how-does-arcticdb-segment-data","title":"How does ArcticDB segment data?","text":"<p>See On Disk Storage Format and the documentation for the <code>rows_per_segment</code> and <code>columns_per_segment</code> library configuration options for more details. </p>"},{"location":"faq/#how-does-arcticdb-handle-streaming-data","title":"How does ArcticDB handle streaming data?","text":"<p>ArcticDB support for streaming data is on our roadmap for the coming months</p>"},{"location":"faq/#how-does-arcticdb-handle-concurrent-writers","title":"How does ArcticDB handle concurrent writers?","text":"<p>Without a centralised server, ArcticDB does not support transactions. Instead, ArcticDB supports concurrent writers across symbols - but not to a single symbol (unless \"staging the writes\"). It is up to the writer to ensure that clients do not concurrently modify a single symbol.</p> <p>In the case of concurrent writers to a single symbol, the behaviour will be last-writer-wins. Data is not lost per se, but only the version of the last-writer will be accessible through the version chain.</p> <p>To reiterate, ArcticDB supports concurrent writers to multiple symbols, even within a single library.</p> <p>Note</p> <p>ArcticDB does support staging multiple single-symbol concurrent writes. See the documentation for <code>staged</code>. </p>"},{"location":"faq/#does-arcticdb-cache-any-data-locally","title":"Does ArcticDB cache any data locally?","text":"<p>Yes, please see the Runtime Configuration page for details.</p>"},{"location":"faq/#how-can-i-enable-detailed-logging","title":"How can I enable detailed logging?","text":"<p>Please see the Runtime Configuration page for details.</p>"},{"location":"faq/#how-can-i-tune-the-performance-of-arcticdb","title":"How can I tune the performance of ArcticDB?","text":"<p>Please see the Runtime Configuration page for details.</p>"},{"location":"faq/#does-arcticdb-support-categorical-data","title":"Does ArcticDB support categorical data?","text":"<p>ArcticDB currently offers extremely limited support for categorical data. Series and DataFrames with categorical columns can be provided to the <code>write</code> and <code>write_batch</code> methods, and will then behave as expected on <code>read</code>. However, <code>append</code> and <code>update</code> are not yet supported with categorical data, and will raise an exception if attempted. Analytics such as filtering using the <code>LazyDataFrame</code> or <code>QueryBuilder</code> classes is also not supported with categorical data, and will either raise an exception, or give incorrect results, depending on the exact operations requested.</p>"},{"location":"faq/#how-does-arcticdb-handle-nan","title":"How does ArcticDB handle <code>NaN</code>?","text":"<p>The handling of <code>NaN</code> in ArcticDB depends on the type of the column under consideration:</p> <ul> <li>For string columns, <code>NaN</code>, as well as Python <code>None</code>, are fully supported.</li> <li>For floating-point numeric columns, <code>NaN</code> is also fully supported.</li> <li>For integer numeric columns <code>NaN</code> is not supported. A column that otherwise contains only integers will be treated as a floating point column if a <code>NaN</code> is encountered by ArcticDB, at which point the usual rules around type promotion for libraries configured with or without dynamic schema all apply as usual.</li> </ul>"},{"location":"history/","title":"History of ArcticDB","text":""},{"location":"history/#arctic","title":"Arctic","text":"<p>ArcticDB is built upon the foundations of Arctic,  an open-source, high performance datastore written in Python which utilises MongoDB as the backend  storage. Arctic has been under development within Man Group since 2012 and from its very first  release has underpinned the research and trading environments at Man Group.</p> <p>Arctic was open sourced in 2015 and has since racked up over  2,800 GitHub stars and over a million package downloads.</p> <p></p>"},{"location":"history/#to-arcticdb","title":"... to ArcticDB!","text":"<p>In 2018, Man Group embarked on a ground-up rewrite in order to improve upon some of the  foundational limitations of Arctic. We called this version ArcticDB, and is differentiated from Arctic in three main ways:</p> <ol> <li>ArcticDB does not depend on Mongo. Instead, ArcticDB is designed to work with consumer grade S3 - on prem or in the cloud.</li> <li>ArcticDB is written in C++, enabling signficant performance improvements. ArcticDB is an order of magnitude faster than Arctic whilst being vastly easier to set up and get started with. </li> <li>ArcticDB unifies streaming and batch workflows behind the same easy to use, consistent API. </li> </ol> <p>All in all, ArcticDB offers tremendous, scalable and portable performance with the same intuitive  Python and Pandas-centric API as Arctic. Behind the scenes, it utilises a custom C++ storage engine, along with  modern S3-compatible object storage. Both bulk and streaming data workflows have unified APIs, offering a bi-temporal view of data history with no performance penalty. </p> <p>ArcticDB's versatility and ease-of-use have made it the database of choice for all front-office timeseries analysis at Man Group.</p> <p>For more information on how ArcticDB is licensed, please see the licensing FAQ.</p>"},{"location":"lib_config/","title":"Library Configuration","text":"<p>When creating a new library, all configuration options will default to those specified in the <code>LibraryOptions</code> documentation defaults, unless overridden by an explicitly specified <code>LibraryOptions</code> object passed in to the call to <code>create_library</code>.</p> <p>Note that currently library configuration options cannot be changed once the library has been created. The ability to modify library options for existing libraries will be added soon.</p>"},{"location":"licensing/","title":"Licensing &amp; Commercial FAQs","text":"<p>ArcticDB is licensed under the Business Source License 1.1 (BSL), reverting to an Apache 2.0 License after a two-year term. </p>"},{"location":"licensing/#is-arcticdb-free-to-use","title":"Is ArcticDB free to use?","text":"<p>All versions of ArcticDB are free to use for non-production usage - except if being used to provide an offering that allows third parties (other than your employees and contractors) to operate a database.</p> <p>Under the BSL, ArcticDB cannot be used for production usage. For more information on what constitutes production usage, please see the MariaDB BSL FAQ. </p> <p>Once a version of ArcticDB has converted to the Apache 2.0 license, it may be used for production usage without acquiring an additional license.</p>"},{"location":"licensing/#license-conversion-timeline","title":"License conversion timeline","text":"<p>This is stored in the README.md of the project.</p>"},{"location":"licensing/#production-usage-of-a-bsl-licensed-version","title":"Production usage of a BSL-licensed version","text":"<p>To use ArcticDB for production use, please contact us to purchase an ArcticDB Pro license. </p>"},{"location":"licensing/#can-i-contribute-to-arcticdb","title":"Can I contribute to ArcticDB?","text":"<p>Yes! Contributions are welcome - please see our GitHub readme for more information.</p>"},{"location":"licensing/#is-arcticdb-part-of-man-group","title":"Is ArcticDB part of Man Group?","text":"<p>Yes - ArcticDB is provided by Man Group.</p>"},{"location":"runtime_config/","title":"Runtime Configuration","text":"<p>ArcticDB features a variety of options that can be tuned at runtime. This page details the most commonly modified options, and how to configure them.</p>"},{"location":"runtime_config/#configuration-methods","title":"Configuration methods","text":"<p>All of the integer options detailed on this page can be configured using the following two methods. All of the options listed on this page are integer options except for log levels, which will be explained in their own section.</p>"},{"location":"runtime_config/#in-code","title":"In code","text":"<p>For integer options, the following code snippet demonstrates how to set values in code:</p> <pre><code>from arcticdb_ext import set_config_int\nset_config_int(setting, value)\n</code></pre> <p>where <code>setting</code> is a string containing the setting name (e.g. <code>VersionMap.ReloadInterval</code>), and <code>value</code> is an int to set the option to.</p>"},{"location":"runtime_config/#environment-variables","title":"Environment variables","text":"<p>For integer options, environment variables can be used to set options as follows:</p> <pre><code>ARCTICDB_&lt;setting&gt;_int=&lt;value&gt;\n</code></pre> <p>e.g. <code>ARCTICDB_VersionMap_ReloadInterval_int=0</code>. Note that <code>.</code> characters in setting names are replaced with underscores when setting them by environment variables.</p>"},{"location":"runtime_config/#priority","title":"Priority","text":"<p>If both the environment variable is set, and <code>set_config_int</code> is called, then the latter takes priority.</p>"},{"location":"runtime_config/#reactivity","title":"Reactivity","text":"<p>Configuration options are read once when the <code>Library</code> instance is created, and are not monitored after that point, so all options should be configured before the <code>Library</code> object is constructed.</p>"},{"location":"runtime_config/#configuration-options","title":"Configuration options","text":""},{"location":"runtime_config/#versionmapreloadinterval","title":"VersionMap.ReloadInterval","text":"<p>ArcticDB library instances maintain a short-lived cache containing what it believes is the latest version for every encountered symbol.  This cache is invalidated after 5 seconds by default.</p> <p>As a result of this caching, it is theoretically possible for two independent library instances to disagree as to what the latest version of a symbol is for a short period of time.</p> <p>This caching is designed to reduce load on storage - if this is not a concern it can be safely disabled by setting this option to <code>0</code>.</p> <p>Other than this, there is no client-side caching in ArcticDB.</p>"},{"location":"runtime_config/#symbollistmaxdelta","title":"SymbolList.MaxDelta","text":"<p>The symbol list cache is compacted when there are more than <code>SymbolList.MaxDelta</code> objects on disk in the symbol list cache.</p> <p>The default is 500.</p>"},{"location":"runtime_config/#s3storagedeletebatchsize","title":"S3Storage.DeleteBatchSize","text":"<p>The S3 API supports the <code>DeleteObjects</code> method, whereby a single HTTP request can be used to delete multiple objects. This parameter can be used to control how many objects are requested to be deleted at a time.</p> <p>The default is 1000.</p>"},{"location":"runtime_config/#s3storageverifyssl","title":"S3Storage.VerifySSL","text":"<p>Control whether the client should verify the SSL certificate of the storage. If set, this will override the library option set upon library creation.</p> <p>Values: * 0: Do not perform SSL verification. * 1: Perform SSL verification.</p>"},{"location":"runtime_config/#s3storageusewininet","title":"S3Storage.UseWinINet","text":"<p>This setting only has an effect on the Windows operating system.</p> <p>Control whether the client should use the WinINet HTTP backend rather than the default WinHTTP backend.</p> <p>WinINet can provide better error messages in AWS SDK debug logs, for example for diagnosing SSL issues. See the logging configuration section below for notes on how to set up AWS SDK debug logs.</p> <p>The INet backend does not allow SSL verification to be disabled with the current AWS SDK.</p> <p>Values: * 0: Use WinHTTP * 1: Use WinINet</p>"},{"location":"runtime_config/#versionstorenumcputhreads-and-versionstorenumiothreads","title":"VersionStore.NumCPUThreads and VersionStore.NumIOThreads","text":"<p>ArcticDB uses two threadpools in order to manage computational resources:</p> <ul> <li>CPU - used for CPU intensive tasks such as decompressing or filtering data</li> <li>IO - used to read/write data from/to the underlying storage</li> </ul> <p>By default, ArcticDB attempts to infer sensible sizes for these threadpools based on the number of cores<sup>*</sup> available on the host machine. The CPU threadpool size defaults to the number of cores available on the host machine, while the IO threadpool size defaults to x1.5 the CPU threadpool size. If these defaults are not suitable for a particular use case, these threadpool sizes can be set directly .</p> <p>If only <code>NumCPUThreads</code> is set, <code>NumIOThreads</code> will still default to x1.5 <code>NumCPUThreads</code>.</p> <p><sup>*</sup>On Linux machines, this core count takes cgroups into account. In particular, this means that CPU limits are respected in processes running in Kubernetes.</p>"},{"location":"runtime_config/#logging-configuration","title":"Logging configuration","text":"<p>ArcticDB has multiple log streams, and the verbosity of each can be configured independently.  The available streams are visible in the source code, although the most commonly useful logs are in:</p> <ul> <li><code>version</code> - contains information about versions being read, created, or destroyed, and traversal of the version layer linked list</li> <li><code>storage</code> - contains information about individual operations that interact with the storage device (read object, write object, delete object, etc)</li> </ul> <p>The available log levels in decreasing order of verbosity are are <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>CRITICAL</code>, <code>OFF</code>.  By default, all streams are set to the <code>INFO</code> level.</p> <p>There are two ways to configure log levels: </p> <ol> <li>Setting an environment variable: <code>ARCTICDB_&lt;stream&gt;_loglevel=&lt;level&gt;</code>, for example: <code>ARCTICDB_version_loglevel=DEBUG</code>. All streams can be configured together via <code>ARCTICDB_all_loglevel</code>. </li> <li> <p>In code: Calling <code>set_log_level</code> from the <code>arcticdb.config</code> module. This takes two optional arguments:</p> </li> <li> <p><code>default_level</code> - the default level for all streams. Should be a string such as <code>\"DEBUG\"</code></p> </li> <li><code>specific_log_levels</code> - a dictionary from stream names to log levels used to override the default such as <code>{\"version\": \"DEBUG\"\"}</code>.</li> </ol> <p>If both environment variables are set, and <code>set_log_level</code> is called, then the latter takes priority.</p> <p>S3 logging can also be enabled by setting the environment variable <code>ARCTICDB_AWS_LogLevel_int=6</code>, which will output all S3 logs to a file in the present working directory.  See the AWS documentation for more details.</p>"},{"location":"runtime_config/#logging-destinations","title":"Logging destinations","text":"<p>By default, all logging from ArcticDB goes to <code>stderr</code>. This can be configured using the <code>set_log_level</code> method.</p> <p>To configure logging to only a file:</p> <pre><code>from arcticdb.config import set_log_level\nset_log_level(console_output=False, file_output_path=\"/tmp/arcticdb.log\")\n</code></pre> <p>To configure logging to both <code>stderr</code> and a file:</p> <pre><code>from arcticdb.config import set_log_level\nset_log_level(console_output=True, file_output_path=\"/tmp/arcticdb.log\")\n</code></pre> <p>To configure logging to only <code>stderr</code> (this is the default configuration):</p> <pre><code>from arcticdb.config import set_log_level\nset_log_level(console_output=True, file_output_path=None)\n</code></pre>"},{"location":"api/","title":"ArcticDB Python Documentation","text":""},{"location":"api/#introduction","title":"Introduction","text":"<p>This part of the site documents the Python API of ArcticDB.</p> <p>The API is structured into the following components:</p> <ul> <li>Arctic: Arctic is the primary API used for accessing and manipulating ArcticDB libraries.</li> <li>Library: The Library API enables reading and manipulating symbols inside ArcticDB libraries.</li> <li>DataFrame Processing Operations API: Details the advanced DataFrame processing operations available within ArcticDB.</li> </ul> <p>Most of the code snippets in the API docs require importing <code>arcticdb</code> as <code>adb</code>:</p> <pre><code>import arcticdb as adb\n</code></pre>"},{"location":"api/arctic/","title":"Arctic API","text":"<p>The primary API used to access and manage ArcticDB libraries. Use this to get a handle to a <code>Library</code> instance, which can then be used for subsequent operations as documented in the Library API section.</p> Class Description Arctic Top-level library management class. LibraryOptions Configuration options that can be applied when libraries are created."},{"location":"api/arctic/#arcticdb.Arctic","title":"arcticdb.Arctic","text":"<p>Top-level library management class. Arctic instances can be configured against an S3 environment and enable the creation, deletion and retrieval of Arctic libraries.</p> METHOD DESCRIPTION <code>__init__</code> <p>Initializes a top-level Arctic library management instance.</p> <code>create_library</code> <p>Creates the library named <code>name</code>.</p> <code>delete_library</code> <p>Removes the library called <code>name</code>. This will remove the underlying data contained within the library and as</p> <code>get_library</code> <p>Returns the library named <code>name</code>.</p> <code>get_uri</code> <p>Returns the URI that was used to create the Arctic instance.</p> <code>has_library</code> <p>Query if the given library exists</p> <code>list_libraries</code> <p>Lists all libraries available.</p> <code>modify_library_option</code> <p>Modify an option for a library.</p>"},{"location":"api/arctic/#arcticdb.Arctic.__init__","title":"__init__","text":"<pre><code>__init__(\n    uri: str,\n    encoding_version: EncodingVersion = DEFAULT_ENCODING_VERSION,\n)\n</code></pre> <p>Initializes a top-level Arctic library management instance.</p> <p>For more information on how to use Arctic Library instances please see the documentation on Library.</p> PARAMETER DESCRIPTION <code>uri</code> <p>URI specifying the backing store used to access, configure, and create Arctic libraries. For more details about the parameters, please refer to the Arctic URI Documentation.</p> <p> TYPE: <code>str</code> </p> <code>encoding_version</code> <p>When creating new libraries with this Arctic instance, the default encoding version to use. Can be overridden by specifying the encoding version in the LibraryOptions argument to create_library.</p> <p> TYPE: <code>EncodingVersion</code> DEFAULT: <code>DEFAULT_ENCODING_VERSION</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ac = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')  # Leave AWS to derive credential information\n&gt;&gt;&gt; ac = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET?region=YOUR_REGION&amp;access=ABCD&amp;secret=DCBA') # Manually specify creds\n&gt;&gt;&gt; ac = adb.Arctic('azure://CA_cert_path=/etc/ssl/certs/ca-certificates.crt;BlobEndpoint=https://arctic.blob.core.windows.net;Container=acblob;SharedAccessSignature=sp=sig')\n&gt;&gt;&gt; ac.create_library('travel_data')\n&gt;&gt;&gt; ac.list_libraries()\n['travel_data']\n&gt;&gt;&gt; travel_library = ac['travel_data']\n&gt;&gt;&gt; ac.delete_library('travel_data')\n</code></pre>"},{"location":"api/arctic/#arcticdb.Arctic.create_library","title":"create_library","text":"<pre><code>create_library(\n    name: str,\n    library_options: Optional[LibraryOptions] = None,\n    enterprise_library_options: Optional[\n        EnterpriseLibraryOptions\n    ] = None,\n) -&gt; Library\n</code></pre> <p>Creates the library named <code>name</code>.</p> <p>Arctic libraries contain named symbols which are the atomic unit of data storage within Arctic. Symbols contain data that in most cases strongly resembles a DataFrame and are versioned such that all modifying operations can be tracked and reverted.</p> <p>Arctic libraries support concurrent writes and reads to multiple symbols as well as concurrent reads to a single symbol. However, concurrent writers to a single symbol are not supported other than for primitives that explicitly state support for single-symbol concurrent writes.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the library that you wish to create.</p> <p> TYPE: <code>str</code> </p> <code>library_options</code> <p>Options to use in configuring the library. Defaults if not provided are the same as documented in LibraryOptions.</p> <p> TYPE: <code>Optional[LibraryOptions]</code> DEFAULT: <code>None</code> </p> <code>enterprise_library_options</code> <p>Enterprise options to use in configuring the library. Defaults if not provided are the same as documented in EnterpriseLibraryOptions. These options are only relevant to ArcticDB enterprise users.</p> <p> TYPE: <code>Optional[EnterpriseLibraryOptions]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.create_library('test.library')\n&gt;&gt;&gt; my_library = arctic['test.library']\n</code></pre> RETURNS DESCRIPTION <code>Library that was just created</code>"},{"location":"api/arctic/#arcticdb.Arctic.delete_library","title":"delete_library","text":"<pre><code>delete_library(name: str) -&gt; None\n</code></pre> <p>Removes the library called <code>name</code>. This will remove the underlying data contained within the library and as such will take as much time as the underlying delete operations take.</p> <p>If no library with <code>name</code> exists then this is a no-op. In particular this method does not raise in this case.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the library to delete.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/arctic/#arcticdb.Arctic.get_library","title":"get_library","text":"<pre><code>get_library(\n    name: str,\n    create_if_missing: Optional[bool] = False,\n    library_options: Optional[LibraryOptions] = None,\n) -&gt; Library\n</code></pre> <p>Returns the library named <code>name</code>.</p> <p>This method can also be invoked through subscripting. <code>adb.Arctic('bucket').get_library(\"test\")</code> is equivalent to <code>adb.Arctic('bucket')[\"test\"]</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the library that you wish to retrieve.</p> <p> TYPE: <code>str</code> </p> <code>create_if_missing</code> <p>If True, and the library does not exist, then create it.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>library_options</code> <p>If create_if_missing is True, and the library does not already exist, then it will be created with these options, or the defaults if not provided. If create_if_missing is True, and the library already exists, ensures that the existing library options match these. Unused if create_if_missing is False.</p> <p> TYPE: <code>Optional[LibraryOptions]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.create_library('test.library')\n&gt;&gt;&gt; my_library = arctic.get_library('test.library')\n&gt;&gt;&gt; my_library = arctic['test.library']\n</code></pre> RETURNS DESCRIPTION <code>Library</code>"},{"location":"api/arctic/#arcticdb.Arctic.get_uri","title":"get_uri","text":"<pre><code>get_uri() -&gt; str\n</code></pre> <p>Returns the URI that was used to create the Arctic instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.get_uri()\n</code></pre> RETURNS DESCRIPTION <code>s3</code> <p> TYPE: <code>//MY_ENDPOINT:MY_BUCKET</code> </p>"},{"location":"api/arctic/#arcticdb.Arctic.has_library","title":"has_library","text":"<pre><code>has_library(name: str) -&gt; bool\n</code></pre> <p>Query if the given library exists</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the library to check the existence of.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>True if the library exists, False otherwise.</code>"},{"location":"api/arctic/#arcticdb.Arctic.list_libraries","title":"list_libraries","text":"<pre><code>list_libraries() -&gt; List[str]\n</code></pre> <p>Lists all libraries available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arctic = adb.Arctic('s3://MY_ENDPOINT:MY_BUCKET')\n&gt;&gt;&gt; arctic.list_libraries()\n['test.library']\n</code></pre> RETURNS DESCRIPTION <code>A list of all library names that exist in this Arctic instance.</code>"},{"location":"api/arctic/#arcticdb.Arctic.modify_library_option","title":"modify_library_option","text":"<pre><code>modify_library_option(\n    library: Library,\n    option: Union[\n        ModifiableLibraryOption,\n        ModifiableEnterpriseLibraryOption,\n    ],\n    option_value: Any,\n)\n</code></pre> <p>Modify an option for a library.</p> <p>See <code>LibraryOptions</code> and <code>EnterpriseLibraryOptions</code> for descriptions of the meanings of the various options.</p> <p>After the modification, this process and other processes that open the library will use the new value. Processes that already have the library open will not see the configuration change until they restart.</p> PARAMETER DESCRIPTION <code>library</code> <p>The library to modify.</p> <p> TYPE: <code>Library</code> </p> <code>option</code> <p>The library option to change.</p> <p> TYPE: <code>Union[ModifiableLibraryOption, ModifiableEnterpriseLibraryOption]</code> </p> <code>option_value</code> <p>The new setting for the library option.</p> <p> TYPE: <code>Any</code> </p>"},{"location":"api/arctic/#arcticdb.LibraryOptions","title":"arcticdb.LibraryOptions","text":"<p>Configuration options for ArcticDB libraries.</p> ATTRIBUTE DESCRIPTION <code>dynamic_schema</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>bool</code> </p> <code>dedup</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>bool</code> </p> <code>rows_per_segment</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>int</code> </p> <code>columns_per_segment</code> <p>See <code>__init__</code> for details.</p> <p> TYPE: <code>int</code> </p> METHOD DESCRIPTION <code>__init__</code> <p>Parameters</p>"},{"location":"api/arctic/#arcticdb.LibraryOptions.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    dynamic_schema: bool = False,\n    dedup: bool = False,\n    rows_per_segment: int = 100000,\n    columns_per_segment: int = 127,\n    encoding_version: Optional[EncodingVersion] = None\n)\n</code></pre> PARAMETER DESCRIPTION <code>dynamic_schema</code> <p>Controls whether the library supports dynamically changing symbol schemas.</p> <p>The schema of a symbol refers to the order of the columns and the type of the columns.</p> <p>If False, then the schema for a symbol is set on each <code>write</code> call, and cannot then be modified by successive updates or appends. Each successive update or append must contain the same column set in the same order with the same types as the initial write.</p> <p>When disabled, ArcticDB will tile stored data across both the rows and columns. This enables highly efficient retrieval of specific columns regardless of the total number of columns stored in the symbol.</p> <p>If True, then updates and appends can contain columns not originally seen in the most recent write call. The data will be dynamically backfilled on read when required for the new columns. Furthermore, Arctic will support numeric type promotions should the type of a column change - for example, should column A be of type int32 on write, and of type float on the next append, the column will be returned as a float to Pandas on read. Supported promotions include (narrow) integer to (wider) integer, and integer to float.</p> <p>When enabled, ArcticDB will only tile across the rows of the data. This will result in slower column subsetting when storing a large number of columns (&gt;1,000).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dedup</code> <p>Controls whether calls to write and write_batch will attempt to deduplicate data segments against the previous live version of the specified symbol.</p> <p>If False, new data segments will always be written for the new version of the symbol being created.</p> <p>If True, the content hash, start index, and end index of data segments associated with the previous live version of this symbol will be compared with those about to be written, and will not be duplicated in the storage device if they match.</p> <p>Keep in mind that this is most effective when version n is equal to version n-1 plus additional data at the end - and only at the end! If there is additional data inserted at the start or into the the middle, then all segments occuring after that modification will almost certainly differ. ArcticDB creates new segments at fixed intervals and data is only de-duplicated if the hashes of the data segments are identical. A one row offset will therefore prevent this de-duplication.</p> <p>Note that these conditions will also be checked with write_pickle and write_pickle_batch. However, pickled objects are always written as a single data segment, and so dedup will only occur if the written object is identical to the previous version.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rows_per_segment</code> <p>Together with columns_per_segment, controls how data being written, appended, or updated is sliced into separate data segment objects before being written to storage.</p> <p>By splitting data across multiple objects in storage, calls to read and read_batch that include the date_range and/or columns parameters can reduce the amount of data read from storage by only reading those data segments that contain data requested by the reader.</p> <p>For example, if writing a dataframe with 250,000 rows and 200 columns, by default, this will be sliced into 6 data segments: 1 - rows 1-100,000 and columns 1-127 2 - rows 100,001-200,000 and columns 1-127 3 - rows 200,001-250,000 and columns 1-127 4 - rows 1-100,000 and columns 128-200 5 - rows 100,001-200,000 and columns 128-200 6 - rows 200,001-250,000 and columns 128-200</p> <p>Data segments that cover the same range of rows are said to belong to the same row-slice (e.g. segments 2 and 5 in the example above). Data segments that cover the same range of columns are said to belong to the same column-slice (e.g. segments 2 and 3 in the example above).</p> <p>Note that this slicing is only applied to the new data being written, existing data segments from previous versions that can remain the same will not be modified. For example, if a 50,000 row dataframe with a single column is written, and then another dataframe also with 50,000 rows and one column is appended to it, there will still be two data segments each with 50,000 rows.</p> <p>Note that for libraries with dynamic_schema enabled, columns_per_segment does not apply, and there is always a single column-slice. However, rows_per_segment is used, and there will be multiple row-slices.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>columns_per_segment</code> <p>See rows_per_segment</p> <p> TYPE: <code>int</code> DEFAULT: <code>127</code> </p> <code>encoding_version</code> <p>The encoding version to use when writing data to storage. v2 is faster, but still experimental, so use with caution.</p> <p> TYPE: <code>Optional[EncodingVersion]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/arctic_uri/","title":"Arctic URI","text":"<p>The URI provided to an <code>Arctic</code> instance is used to specify the storage backend and its configuration.</p>"},{"location":"api/arctic_uri/#s3","title":"S3","text":"<p>The S3 URI connection scheme has the form <code>s3(s)://&lt;s3 end point&gt;:&lt;s3 bucket&gt;[?options]</code>.</p> <p>Use s3s as the protocol if communicating with a secure endpoint.</p> <p>Options is a query string that specifies connection specific options as <code>&lt;name&gt;=&lt;value&gt;</code> pairs joined with <code>&amp;</code>.</p> <p>Available options for S3:</p> Option Description port port to use for S3 connection region S3 region use_virtual_addressing Whether to use virtual addressing to access the S3 bucket access S3 access key secret S3 secret access key path_prefix Path within S3 bucket to use for data storage aws_auth AWS authentication method. If setting is <code>default</code> (or <code>true</code> for backward compatibility), authentication to endpoint will be computed via AWS default credential provider chain. If setting is <code>sts</code>, AWS Security Token Service (STS) will be the authentication method used. If no options are provided AWS authentication will not be used and you should specify access and secret in the URI. More info is provided below aws_profile Only when <code>aws_auth</code> is set to be <code>sts</code>. AWS profile to be used with AWS Security Token Service (STS). More info is provided below <p>Note: When connecting to AWS, <code>region</code> can be automatically deduced from the endpoint if the given endpoint specifies the region and <code>region</code> is not set.</p>"},{"location":"api/arctic_uri/#azure","title":"Azure","text":"<p>The Azure URI connection scheme has the form <code>azure://[options]</code>. It is based on the Azure Connection String, with additional options for configuring ArcticDB. Please refer to Azure for more details.</p> <p><code>options</code> is a string that specifies connection specific options as <code>&lt;name&gt;=&lt;value&gt;</code> pairs joined with <code>;</code> (the final key value pair should not include a trailing <code>;</code>).</p> <p>Additional options specific for ArcticDB:</p> Option Description Container Azure container for blobs Path_prefix Path within Azure container to use for data storage CA_cert_path (Linux platform only) Azure CA certificate path. If not set, python <code>ssl.get_default_verify_paths().cafile</code> will be used. If the certificate cannot be found in the provided path, an Azure exception with no meaningful error code will be thrown. For more details, please see here. For example, <code>Failed to iterate azure blobs 'C' 0:</code>. CA_cert_dir (Linux platform only) Azure CA certificate directory. If not set, python <code>ssl.get_default_verify_paths().capath</code> will be used. Certificates can only be used if corresponding hash files exist. If the certificate cannot be found in the provided path, an Azure exception with no meaningful error code will be thrown. For more details, please see here. For example, <code>Failed to iterate azure blobs 'C' 0:</code>. <p>For non-Linux platforms, neither <code>CA_cert_path</code> nor <code>CA_cert_dir</code> may be set. Please set CA certificate related options using operating system settings. For Windows, please see here</p> <p>Exception: Azure exceptions message always ends with <code>{AZURE_SDK_HTTP_STATUS_CODE}:{AZURE_SDK_REASON_PHRASE}</code>.</p> <p>Please refer to azure-sdk-for-cpp for more details of provided status codes.</p> <p>Note that due to a bug in Azure C++ SDK, Azure may not give meaningful status codes and reason phrases in the exception. To debug these instances, please set the environment variable <code>export AZURE_LOG_LEVEL</code> to <code>1</code> to turn on the SDK debug logging.</p>"},{"location":"api/arctic_uri/#lmdb","title":"LMDB","text":"<p>The LMDB connection scheme has the form <code>lmdb:///&lt;path to store LMDB files&gt;[?options]</code>.</p> <p>Options is a query string that specifies connection specific options as <code>&lt;name&gt;=&lt;value&gt;</code> pairs joined with <code>&amp;</code>.</p> Option Description map_size LMDB map size (see here). String. Supported formats are:\"150MB\" / \"20GB\" / \"3TB\"The only supported units are MB / GB / TB.On Windows and MacOS, LMDB will materialize a file of this size, so you need to set it to a reasonable value that your system has room for, and it has a small default (order of 1GB). On Linux, this is an upper bound on the space used by LMDB and the default is large (order of 100GB).  ArcticDB creates an LMDB database per library, so the <code>map_size</code> will be per library. <p>Example connection strings are <code>lmdb:///home/user/my_lmdb</code> or <code>lmdb:///home/user/my_lmdb?map_size=2GB</code>.</p>"},{"location":"api/arctic_uri/#in-memory","title":"In-Memory","text":"<p>The in-memory connection scheme has the form <code>mem://</code>.</p> <p>The storage is local to the <code>Arctic</code> instance.</p>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#arcticdb.DataError","title":"arcticdb.DataError","text":"<p>Return value for batch methods which fail in some way.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>Read or modified symbol.</p> <p> TYPE: <code>str</code> </p> <code>version_request_type</code> <p>For operations that support as_of, the type of version query provided. <code>None</code> otherwise.</p> <p> TYPE: <code>Optional[VersionRequestType]</code> </p> <code>version_request_data</code> <p>For operations that support as_of, the value provided in the version query:     None - Operation does not support as_of, or latest version was requested     str - The name of the snapshot provided to as_of     int - The specific version requested if version_request_type == VersionRequestType::SPECIFIC, or           nanoseconds since epoch if version_request_type == VersionRequestType::TIMESTAMP</p> <p> TYPE: <code>Optional[Union[str, int]]</code> </p> <code>error_code</code> <p>For the most common error types, the ErrorCode is included here. e.g. ErrorCode.E_NO_SUCH_VERSION if the version requested has been deleted Please see the Error Messages section of the docs for more detail.</p> <p> TYPE: <code>Optional[ErrorCode]</code> </p> <code>error_category</code> <p>If error_code is provided, the category is also provided. e.g.  ErrorCategory.MISSING_DATA if error_code is ErrorCode.E_NO_SUCH_VERSION</p> <p> TYPE: <code>Optional[ErrorCategory]</code> </p> <code>exception_string</code> <p>The string associated with the exception that was originally raised.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/exceptions/#arcticdb.version_store.library.ArcticDuplicateSymbolsInBatchException","title":"arcticdb.version_store.library.ArcticDuplicateSymbolsInBatchException","text":"<p>               Bases: <code>ArcticInvalidApiUsageException</code></p> <p>Exception indicating that duplicate symbols were passed to a batch method of this module.</p>"},{"location":"api/exceptions/#arcticdb.version_store.library.ArcticInvalidApiUsageException","title":"arcticdb.version_store.library.ArcticInvalidApiUsageException","text":"<p>               Bases: <code>ArcticException</code></p> <p>Exception indicating an invalid call made to the Arctic API.</p>"},{"location":"api/exceptions/#arcticdb.version_store.library.ArcticUnsupportedDataTypeException","title":"arcticdb.version_store.library.ArcticUnsupportedDataTypeException","text":"<p>               Bases: <code>ArcticInvalidApiUsageException</code></p> <p>Exception indicating that a method does not support the type of data provided.</p>"},{"location":"api/library/","title":"Library API","text":"<p>This page documents the <code>arcticdb.version_store.library</code> module. This module is the main interface exposing read/write functionality within a given Arctic instance.</p> <p>The key functionality is exposed through <code>arcticdb.version_store.library.Library</code> instances. See the Arctic API section for notes on how to create these. The other types exposed in this module are less important and are used as part of the signature of <code>arcticdb.version_store.library.Library</code> instance methods.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library","title":"arcticdb.version_store.library.Library","text":"<p>The main interface exposing read/write functionality within a given Arctic instance.</p> <p>Arctic libraries contain named symbols which are the atomic unit of data storage within Arctic. Symbols contain data that in most cases resembles a DataFrame and are versioned such that all modifying operations can be tracked and reverted.</p> <p>Instances of this class provide a number of primitives to write, modify and remove symbols, as well as also providing methods to manage library snapshots. For more information on snapshots please see the <code>snapshot</code> method.</p> <p>Arctic libraries support concurrent writes and reads to multiple symbols as well as concurrent reads to a single symbol. However, concurrent writers to a single symbol are not supported other than for primitives that explicitly state support for single-symbol concurrent writes.</p> METHOD DESCRIPTION <code>append</code> <p>Appends the given data to the existing, stored data. Append always appends along the index. A new version will</p> <code>append_batch</code> <p>Append data to multiple symbols in a batch fashion. This is more efficient than making multiple <code>append</code> calls in</p> <code>compact_symbol_list</code> <p>Compact the symbol list cache into a single key in the storage</p> <code>defragment_symbol_data</code> <p>Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer).</p> <code>delete</code> <p>Delete all versions of the symbol from the library, unless <code>version</code> is specified, in which case only those</p> <code>delete_data_in_range</code> <p>Delete data within the given date range, creating a new version of <code>symbol</code>.</p> <code>delete_snapshot</code> <p>Delete a named snapshot. This may take time if the given snapshot is the last reference to the underlying</p> <code>delete_staged_data</code> <p>Removes staged data.</p> <code>enterprise_options</code> <p>Enterprise library options set on this library. See also <code>options</code> for non-enterprise options.</p> <code>finalize_staged_data</code> <p>Finalizes staged data, making it available for reads. All staged segments must be ordered and non-overlapping.</p> <code>get_description</code> <p>Returns descriptive data for <code>symbol</code>.</p> <code>get_description_batch</code> <p>Returns descriptive data for a list of <code>symbols</code>.</p> <code>get_staged_symbols</code> <p>Returns all symbols with staged, unfinalized data.</p> <code>has_symbol</code> <p>Whether this library contains the given symbol.</p> <code>head</code> <p>Read the first n rows of data for the named symbol. If n is negative, return all rows except the last n rows.</p> <code>is_symbol_fragmented</code> <p>Check whether the number of segments that would be reduced by compaction is more than or equal to the</p> <code>list_snapshots</code> <p>List the snapshots in the library.</p> <code>list_symbols</code> <p>Return the symbols in this library.</p> <code>list_versions</code> <p>Get the versions in this library, filtered by the passed in parameters.</p> <code>options</code> <p>Library options set on this library. See also <code>enterprise_options</code>.</p> <code>prune_previous_versions</code> <p>Removes all (non-snapshotted) versions from the database for the given symbol, except the latest.</p> <code>read</code> <p>Read data for the named symbol.  Returns a VersionedItem object with a data and metadata element (as passed into</p> <code>read_batch</code> <p>Reads multiple symbols.</p> <code>read_metadata</code> <p>Return the metadata saved for a symbol.  This method is faster than read as it only loads the metadata, not the</p> <code>read_metadata_batch</code> <p>Reads the metadata of multiple symbols.</p> <code>reload_symbol_list</code> <p>Forces the symbol list cache to be reloaded.</p> <code>snapshot</code> <p>Creates a named snapshot of the data within a library.</p> <code>sort_and_finalize_staged_data</code> <p>Sorts and merges all staged data, making it available for reads. This differs from <code>finalize_staged_data</code> in that it</p> <code>stage</code> <p>Write a staged data chunk to storage, that will not be visible until finalize_staged_data is called on</p> <code>tail</code> <p>Read the last n rows of data for the named symbol. If n is negative, return all rows except the first n rows.</p> <code>update</code> <p>Overwrites existing symbol data with the contents of <code>data</code>. The entire range between the first and last index</p> <code>update_batch</code> <p>Perform an update operation on a list of symbols in parallel. All constrains on</p> <code>write</code> <p>Write <code>data</code> to the specified <code>symbol</code>. If <code>symbol</code> already exists then a new version will be created to</p> <code>write_batch</code> <p>Write a batch of multiple symbols.</p> <code>write_metadata</code> <p>Write metadata under the specified symbol name to this library. The data will remain unchanged.</p> <code>write_metadata_batch</code> <p>Write metadata to multiple symbols in a batch fashion. This is more efficient than making multiple <code>write_metadata</code> calls</p> <code>write_pickle</code> <p>See <code>write</code>. This method differs from <code>write</code> only in that <code>data</code> can be of any type that is serialisable via</p> <code>write_pickle_batch</code> <p>Write a batch of multiple symbols, pickling their data if necessary.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name of this library.</p> <p> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>The name of this library.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.append","title":"append","text":"<pre><code>append(\n    symbol: str,\n    data: NormalizableType,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    validate_index: bool = True,\n) -&gt; Optional[VersionedItem]\n</code></pre> <p>Appends the given data to the existing, stored data. Append always appends along the index. A new version will be created to reference the newly-appended data. Append only accepts data for which the index of the first row is equal to or greater than the index of the last row in the existing data.</p> <p>Appends containing differing column sets to the existing data are only possible if the library has been configured to support dynamic schemas.</p> <p>If <code>append</code> is called on a symbol that does not exist, it will create it. This is convenient when setting up a new symbol, but be careful - it will not work for creating a new version of an existing symbol. Use <code>write</code> in that case.</p> <p>Note that <code>append</code> is not designed for multiple concurrent writers over a single symbol.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the new symbol version. Note that the metadata is not combined in any way with the metadata stored in the previous version.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>If True, verify that the index of <code>data</code> supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store.</p> RAISES DESCRIPTION <code>UnsortedDataException</code> <p>If data is unsorted, when validate_index is set to True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...    {'column': [1,2,3]},\n...    index=pd.date_range(start='1/1/2018', end='1/03/2018')\n... )\n&gt;&gt;&gt; df\n            column\n2018-01-01       1\n2018-01-02       2\n2018-01-03       3\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; to_append_df = pd.DataFrame(\n...    {'column': [4,5,6]},\n...    index=pd.date_range(start='1/4/2018', end='1/06/2018')\n... )\n&gt;&gt;&gt; to_append_df\n            column\n2018-01-04       4\n2018-01-05       5\n2018-01-06       6\n&gt;&gt;&gt; lib.append(\"symbol\", to_append_df)\n&gt;&gt;&gt; lib.read(\"symbol\").data\n            column\n2018-01-01       1\n2018-01-02       2\n2018-01-03       3\n2018-01-04       4\n2018-01-05       5\n2018-01-06       6\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.append_batch","title":"append_batch","text":"<pre><code>append_batch(\n    append_payloads: List[WritePayload],\n    prune_previous_versions: bool = False,\n    validate_index=True,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Append data to multiple symbols in a batch fashion. This is more efficient than making multiple <code>append</code> calls in succession as some constant-time operations can be executed only once rather than once for each element of <code>append_payloads</code>. Note that this isn't an atomic operation - it's possible for one symbol to be fully written and readable before another symbol.</p> PARAMETER DESCRIPTION <code>append_payloads</code> <p>Symbols and their corresponding data. There must not be any duplicate symbols in <code>append_payloads</code>.</p> <p> TYPE: <code>`List[WritePayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>Verify that each entry in the batch has an index that supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing.</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. i-th entry corresponds to i-th element of <code>append_payloads</code>. Each result correspond to a structure containing metadata and version number of the affected symbol in the store. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> <code>ArcticUnsupportedDataTypeException</code> <p>If data that is not of NormalizableType appears in any of the payloads.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.compact_symbol_list","title":"compact_symbol_list","text":"<pre><code>compact_symbol_list() -&gt; None\n</code></pre> <p>Compact the symbol list cache into a single key in the storage</p> RETURNS DESCRIPTION <code>The number of symbol list keys prior to compaction</code> RAISES DESCRIPTION <code>PermissionException</code> <p>Library has been opened in read-only mode</p> <code>InternalException</code> <p>Storage lock required to compact the symbol list could not be acquired</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.defragment_symbol_data","title":"defragment_symbol_data","text":"<pre><code>defragment_symbol_data(\n    symbol: str,\n    segment_size: Optional[int] = None,\n    prune_previous_versions: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer). This method calls <code>is_symbol_fragmented</code> to determine whether to proceed with the defragmentation operation.</p> <p>CAUTION - Please note that a major restriction of this method at present is that any column slicing present on the data will be removed in the new version created as a result of this method. As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of the symbol (by using the <code>columns</code> parameter) may be negatively impacted in the defragmented version. If your symbol has less than 127 columns this caveat does not apply. For more information, please see <code>columns_per_segment</code> here:</p> <p>https://docs.arcticdb.io/api/arcticdb/arcticdb.LibraryOptions</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>segment_size</code> <p>Target for maximum no. of rows per segment, after compaction. If parameter is not provided, library option - \"segment_row_size\" will be used Note that no. of rows per segment, after compaction, may exceed the target. It is for achieving smallest no. of segment after compaction. Please refer to below example for further explanation.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the defragmented symbol in the store.</p> RAISES DESCRIPTION <code>1002 ErrorCategory.INTERNAL:E_ASSERTION_FAILURE</code> <p>If <code>is_symbol_fragmented</code> returns false.</p> <code>2001 ErrorCategory.NORMALIZATION:E_UNIMPLEMENTED_INPUT_TYPE</code> <p>If library option - \"bucketize_dynamic\" is ON</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"symbol\", pd.DataFrame({\"A\": [0]}, index=[pd.Timestamp(0)]))\n&gt;&gt;&gt; lib.append(\"symbol\", pd.DataFrame({\"A\": [1, 2]}, index=[pd.Timestamp(1), pd.Timestamp(2)]))\n&gt;&gt;&gt; lib.append(\"symbol\", pd.DataFrame({\"A\": [3]}, index=[pd.Timestamp(3)]))\n&gt;&gt;&gt; lib_tool = lib._dev_tools.library_tool()\n&gt;&gt;&gt; lib_tool.read_index(sym)\n                    start_index                     end_index  version_id stream_id          creation_ts          content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000001          20    b'sym'  1678974096622685727   6872717287607530038          84         2          1        2          0        1\n1970-01-01 00:00:00.000000001 1970-01-01 00:00:00.000000003          21    b'sym'  1678974096931527858  12345256156783683504          84         2          1        2          1        3\n1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          22    b'sym'  1678974096970045987   7952936283266921920          84         2          1        2          3        4\n&gt;&gt;&gt; lib.version_store.defragment_symbol_data(\"symbol\", 2)\n&gt;&gt;&gt; lib_tool.read_index(sym)  # Returns two segments rather than three as a result of the defragmentation operation\n                    start_index                     end_index  version_id stream_id          creation_ts         content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000003          23    b'sym'  1678974097067271451  5576804837479525884          84         2          1        2          0        3\n1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          23    b'sym'  1678974097067427062  7952936283266921920          84         2          1        2          3        4\n</code></pre> Notes <p>Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting in the future. This API will allow overriding the setting as well.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete","title":"delete","text":"<pre><code>delete(\n    symbol: str,\n    versions: Optional[Union[int, Iterable[int]]] = None,\n)\n</code></pre> <p>Delete all versions of the symbol from the library, unless <code>version</code> is specified, in which case only those versions are deleted.</p> <p>This may not actually delete the underlying data if a snapshot still references the version. See <code>snapshot</code> for more detail.</p> <p>Note that this may require data to be removed from the underlying storage which can be slow.</p> <p>This method does not remove any staged data, use <code>delete_staged_data</code> for that.</p> <p>If no symbol called <code>symbol</code> exists then this is a no-op. In particular this method does not raise in this case.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to delete.</p> <p> TYPE: <code>str</code> </p> <code>versions</code> <p>Version or versions of symbol to delete. If <code>None</code> then all versions will be deleted.</p> <p> TYPE: <code>Optional[Union[int, Iterable[int]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_data_in_range","title":"delete_data_in_range","text":"<pre><code>delete_data_in_range(\n    symbol: str,\n    date_range: Tuple[\n        Optional[Timestamp], Optional[Timestamp]\n    ],\n    prune_previous_versions: bool = False,\n)\n</code></pre> <p>Delete data within the given date range, creating a new version of <code>symbol</code>.</p> <p>The existing symbol version must be timeseries-indexed.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>date_range</code> <p>The date range in which to delete data. Leaving any part of the tuple as None leaves that part of the range open ended.</p> <p> TYPE: <code>Tuple[Optional[Timestamp], Optional[Timestamp]]</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"column\": [5, 6, 7, 8]}, index=pd.date_range(start=\"1/1/2018\", end=\"1/4/2018\"))\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.delete_data_in_range(\"symbol\", date_range=(datetime.datetime(2018, 1, 1), datetime.datetime(2018, 1, 2)))\n&gt;&gt;&gt; lib[\"symbol\"].version\n1\n&gt;&gt;&gt; lib[\"symbol\"].data\n                column\n2018-01-03       7\n2018-01-04       8\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_snapshot","title":"delete_snapshot","text":"<pre><code>delete_snapshot(snapshot_name: str) -&gt; None\n</code></pre> <p>Delete a named snapshot. This may take time if the given snapshot is the last reference to the underlying symbol(s) as the underlying data will be removed as well.</p> PARAMETER DESCRIPTION <code>snapshot_name</code> <p>The snapshot name to delete.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>Exception</code> <p>If the named snapshot does not exist.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.delete_staged_data","title":"delete_staged_data","text":"<pre><code>delete_staged_data(symbol: str)\n</code></pre> <p>Removes staged data.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to remove staged data for.</p> <p> TYPE: <code>`str`</code> </p> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.enterprise_options","title":"enterprise_options","text":"<pre><code>enterprise_options() -&gt; EnterpriseLibraryOptions\n</code></pre> <p>Enterprise library options set on this library. See also <code>options</code> for non-enterprise options.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.finalize_staged_data","title":"finalize_staged_data","text":"<pre><code>finalize_staged_data(\n    symbol: str,\n    mode: Optional[StagedDataFinalizeMethod] = WRITE,\n    prune_previous_versions: bool = False,\n    metadata: Any = None,\n    validate_index=True,\n    delete_staged_data_on_failure: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Finalizes staged data, making it available for reads. All staged segments must be ordered and non-overlapping. <code>finalize_staged_data</code> is less time consuming than <code>sort_and_finalize_staged_data</code>.</p> <p>If <code>mode</code> is <code>StagedDataFinalizeMethod.APPEND</code> the index of the first row of the new segment must be equal to or greater than the index of the last row in the existing data.</p> <p>If <code>Static Schema</code> is used all staged block must have matching schema (same column names, same dtype, same column ordering) and must match the existing data if mode is <code>StagedDataFinalizeMethod.APPEND</code>. For more information about schema options see documentation for <code>arcticdb.LibraryOptions.dynamic_schema</code></p> <p>If the symbol does not exist both <code>StagedDataFinalizeMethod.APPEND</code> and <code>StagedDataFinalizeMethod.WRITE</code> will create it.</p> <p>Calling <code>finalize_staged_data</code> without having staged data for the symbol will throw <code>UserInputException</code>. Use <code>get_staged_symbols</code> to check if there are staged segments for the symbol.</p> <p>Calling <code>finalize_staged_data</code> if any of the staged segments contains NaT in its index will throw <code>SortingException</code>.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to finalize data for.</p> <p> TYPE: <code>`str`</code> </p> <code>mode</code> <p>Finalize mode. Valid options are WRITE or APPEND. Write collects the staged data and writes them to a new version. Append collects the staged data and appends them to the latest version.</p> <p> TYPE: <code>`StagedDataFinalizeMethod`</code> DEFAULT: <code>StagedDataFinalizeMethod.WRITE</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>validate_index</code> <p>If True, and staged segments are timeseries, will verify that the index of the symbol after this operation supports date range searches and update operations. This requires that the indexes of the staged segments are non-overlapping with each other, and, in the case of <code>StagedDataFinalizeMethod.APPEND</code>, fall after the last index value in the previous version.</p> <p> DEFAULT: <code>True</code> </p> <code>delete_staged_data_on_failure</code> <p>Determines the handling of staged data when an exception occurs during the execution of the  <code>finalize_staged_data</code> function.</p> <ul> <li>If set to True, all staged data for the specified symbol will be deleted if an exception occurs.</li> <li>If set to False, the staged data will be retained and will be used in subsequent calls to    <code>finalize_staged_data</code>.</li> </ul> <p>To manually delete staged data, use the <code>delete_staged_data</code> function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store. The data member will be None.</p> RAISES DESCRIPTION <code>SortingException</code> <ul> <li>If any two staged segments for a given symbol have overlapping indexes</li> <li>If any staged segment for a given symbol is not sorted</li> <li>If the first index value of the new segment is not greater or equal than the last index value of     the existing data when <code>StagedDataFinalizeMethod.APPEND</code> is used.</li> <li>If any staged segment contains NaT in the index</li> </ul> <code>UserInputException</code> <ul> <li>If there are no staged segments when <code>finalize_staged_data</code> is called</li> <li> <p>If all of the following conditions are met:</p> <ol> <li>Static schema is used.</li> <li>The width of the DataFrame exceeds the value of <code>LibraryOptions.columns_per_segment</code>.</li> <li>The symbol contains data that was not written by <code>finalize_staged_data</code>.</li> <li>Finalize mode is append</li> </ol> </li> </ul> <code>SchemaException</code> <ul> <li>If static schema is used and not all staged segments have matching schema.</li> <li>If static schema is used, mode is <code>StagedDataFinalizeMethod.APPEND</code> and the schema of the new segment     is not the same as the schema of the existing data</li> <li>If dynamic schema is used and different segments have the same column names but their dtypes don't have a     common type (e.g string and any numeric type)</li> <li>If a different index name is encountered in the staged data, regardless of the schema mode</li> </ul> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"sym\", pd.DataFrame({\"col\": [3, 4]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 3), pd.Timestamp(2024, 1, 4)])), staged=True)\n&gt;&gt;&gt; lib.write(\"sym\", pd.DataFrame({\"col\": [1, 2]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 1), pd.Timestamp(2024, 1, 2)])), staged=True)\n&gt;&gt;&gt; lib.finalize_staged_data(\"sym\")\n&gt;&gt;&gt; lib.read(\"sym\").data\n            col\n2024-01-01    1\n2024-01-02    2\n2024-01-03    3\n2024-01-04    4\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.get_description","title":"get_description","text":"<pre><code>get_description(\n    symbol: str, as_of: Optional[AsOf] = None\n) -&gt; SymbolDescription\n</code></pre> <p>Returns descriptive data for <code>symbol</code>.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SymbolDescription</code> <p>Named tuple containing the descriptive data.</p> See Also <p>SymbolDescription     For documentation on each field.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.get_description_batch","title":"get_description_batch","text":"<pre><code>get_description_batch(\n    symbols: List[Union[str, ReadInfoRequest]],\n) -&gt; List[Union[SymbolDescription, DataError]]\n</code></pre> <p>Returns descriptive data for a list of <code>symbols</code>.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read.</p> <p> TYPE: <code>List[Union[str, ReadInfoRequest]]</code> </p> RETURNS DESCRIPTION <code>List[Union[SymbolDescription, DataError]]</code> <p>A list of the descriptive data, whose i-th element corresponds to the i-th element of the <code>symbols</code> parameter. If the specified version does not exist, a DataError object is returned, with symbol, version_request_type, version_request_data properties, error_code, error_category, and exception_string properties. If a key error or any other internal exception occurs, the same DataError object is also returned.</p> See Also <p>SymbolDescription     For documentation on each field.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.get_staged_symbols","title":"get_staged_symbols","text":"<pre><code>get_staged_symbols() -&gt; List[str]\n</code></pre> <p>Returns all symbols with staged, unfinalized data.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>Symbol names.</p> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.has_symbol","title":"has_symbol","text":"<pre><code>has_symbol(\n    symbol: str, as_of: Optional[AsOf] = None\n) -&gt; bool\n</code></pre> <p>Whether this library contains the given symbol.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name for the item</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>Return the data as it was as_of the point in time. See <code>read</code> for more documentation. If absent then considers symbols that are live in the library as of the current time.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the symbol is in the library, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"symbol\", pd.DataFrame())\n&gt;&gt;&gt; lib.has_symbol(\"symbol\")\nTrue\n&gt;&gt;&gt; lib.has_symbol(\"another_symbol\")\nFalse\n</code></pre> <p>The contains operator also checks whether a symbol exists in this library as of now:</p> <pre><code>&gt;&gt;&gt; \"symbol\" in lib\nTrue\n&gt;&gt;&gt; \"another_symbol\" in lib\nFalse\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.head","title":"head","text":"<pre><code>head(\n    symbol: str,\n    n: int = 5,\n    as_of: Optional[AsOf] = None,\n    columns: List[str] = None,\n    lazy: bool = False,\n) -&gt; Union[VersionedItem, LazyDataFrame]\n</code></pre> <p>Read the first n rows of data for the named symbol. If n is negative, return all rows except the last n rows.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>as_of</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> <code>columns</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[VersionedItem, LazyDataFrame]</code> <p>If lazy is False, VersionedItem object that contains a .data and .metadata element. If lazy is True, a LazyDataFrame object on which further querying can be performed prior to collect.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.is_symbol_fragmented","title":"is_symbol_fragmented","text":"<pre><code>is_symbol_fragmented(\n    symbol: str, segment_size: Optional[int] = None\n) -&gt; bool\n</code></pre> <p>Check whether the number of segments that would be reduced by compaction is more than or equal to the value specified by the configuration option \"SymbolDataCompact.SegmentCount\" (defaults to 100).</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>segment_size</code> <p>Target for maximum no. of rows per segment, after compaction. If parameter is not provided, library option for segments's maximum row size will be used</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> Notes <p>Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting in the future. This API will allow overriding the setting as well.</p> RETURNS DESCRIPTION <code>bool</code>"},{"location":"api/library/#arcticdb.version_store.library.Library.list_snapshots","title":"list_snapshots","text":"<pre><code>list_snapshots(\n    load_metadata: Optional[bool] = True,\n) -&gt; Union[List[str], Dict[str, Any]]\n</code></pre> <p>List the snapshots in the library.</p> PARAMETER DESCRIPTION <code>load_metadata</code> <p>Load the snapshot metadata. May be slow so opt for false if you don't need it.</p> <p> TYPE: <code>`Optional[bool]`</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Union[List[str], Dict[str, Any]]</code> <p>Snapshots in the library. Returns a list of snapshot names if load_metadata is False, otherwise returns a dictionary where keys are snapshot names and values are metadata associated with that snapshot.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.list_symbols","title":"list_symbols","text":"<pre><code>list_symbols(\n    snapshot_name: Optional[str] = None,\n    regex: Optional[str] = None,\n) -&gt; List[str]\n</code></pre> <p>Return the symbols in this library.</p> PARAMETER DESCRIPTION <code>regex</code> <p>If passed, returns only the symbols which match the regex.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>snapshot_name</code> <p>Return the symbols available under the snapshot. If None then considers symbols that are live in the library as of the current time.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>Symbols in the library.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.list_versions","title":"list_versions","text":"<pre><code>list_versions(\n    symbol: Optional[str] = None,\n    snapshot: Optional[str] = None,\n    latest_only: bool = False,\n    skip_snapshots: bool = False,\n) -&gt; Dict[SymbolVersion, VersionInfo]\n</code></pre> <p>Get the versions in this library, filtered by the passed in parameters.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to return versions for.  If None returns versions across all symbols in the library.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>snapshot</code> <p>Only return the versions contained in the named snapshot.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>latest_only</code> <p>Only include the latest version for each returned symbol.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>skip_snapshots</code> <p>Don't populate version list with snapshot information. Can improve performance significantly if there are many snapshots.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dict[SymbolVersion, VersionInfo]</code> <p>Dictionary describing the version for each symbol-version pair in the library. Since symbol version is a (named) tuple you can index in to the dictionary simply as shown in the examples below.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame()\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata=10)\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata=11, prune_previous_versions=False)\n&gt;&gt;&gt; lib.snapshot(\"snapshot\")\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata=12, prune_previous_versions=False)\n&gt;&gt;&gt; lib.delete(\"symbol\", versions=(1, 2))\n&gt;&gt;&gt; versions = lib.list_versions(\"symbol\")\n&gt;&gt;&gt; versions[\"symbol\", 1].deleted\nTrue\n&gt;&gt;&gt; versions[\"symbol\", 1].snapshots\n[\"my_snap\"]\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.options","title":"options","text":"<pre><code>options() -&gt; LibraryOptions\n</code></pre> <p>Library options set on this library. See also <code>enterprise_options</code>.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.prune_previous_versions","title":"prune_previous_versions","text":"<pre><code>prune_previous_versions(symbol)\n</code></pre> <p>Removes all (non-snapshotted) versions from the database for the given symbol, except the latest.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name to prune.</p> <p> TYPE: <code>`str`</code> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.read","title":"read","text":"<pre><code>read(\n    symbol: str,\n    as_of: Optional[AsOf] = None,\n    date_range: Optional[\n        Tuple[Optional[Timestamp], Optional[Timestamp]]\n    ] = None,\n    row_range: Optional[Tuple[int, int]] = None,\n    columns: Optional[List[str]] = None,\n    query_builder: Optional[QueryBuilder] = None,\n    lazy: bool = False,\n) -&gt; Union[VersionedItem, LazyDataFrame]\n</code></pre> <p>Read data for the named symbol.  Returns a VersionedItem object with a data and metadata element (as passed into write).</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>Return the data as it was as of the point in time. <code>None</code> means that the latest version should be read. The various types of this parameter mean: - <code>int</code>: specific version number. Negative indexing is supported, with -1 representing the latest version, -2 the version before that, etc. - <code>str</code>: snapshot name which contains the version - <code>datetime.datetime</code> : the version of the data that existed <code>as_of</code> the requested point in time</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> <code>date_range</code> <p>DateRange to restrict read data to.</p> <p>Applicable only for time-indexed Pandas dataframes or series. Returns only the part of the data that falls withing the given range (inclusive). None on either end leaves that part of the range open-ended. Hence specifying <code>(None, datetime(2025, 1, 1)</code> declares that you wish to read all data up to and including 20250101. The same effect can be achieved by using the date_range clause of the QueryBuilder class, which will be slower, but return data with a smaller memory footprint. See the QueryBuilder.date_range docstring for more details.</p> <p>Only one of date_range or row_range can be provided.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]]</code> DEFAULT: <code>None</code> </p> <code>row_range</code> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound lib.read(symbol, row_range=(start, end)).data should behave the same as df.iloc[start:end], including in the handling of negative start/end values.</p> <p>Only one of date_range or row_range can be provided.</p> <p> TYPE: <code>Optional[Tuple[int, int]]</code> DEFAULT: <code>None</code> </p> <code>columns</code> <p>Applicable only for Pandas data. Determines which columns to return data for. Special values: - <code>None</code>: Return a dataframe containing all columns - <code>[]</code>: Return a dataframe containing only the index columns</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>query_builder</code> <p>A QueryBuilder object to apply to the dataframe before it is returned. For more information see the documentation for the QueryBuilder class (<code>from arcticdb import QueryBuilder; help(QueryBuilder)</code>).</p> <p> TYPE: <code>Optional[QueryBuilder]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>Defer query execution until <code>collect</code> is called on the returned <code>LazyDataFrame</code> object. See documentation on <code>LazyDataFrame</code> for more details.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>output_format</code> <p>What format to return the output in. One of PANDAS or ARROW.</p> <p> </p> RETURNS DESCRIPTION <code>Union[VersionedItem, LazyDataFrame]</code> <p>If lazy is False, VersionedItem object that contains a .data and .metadata element. If lazy is True, a LazyDataFrame object on which further querying can be performed prior to collect.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'column': [5,6,7]})\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata={'my_dictionary': 'is_great'})\n&gt;&gt;&gt; lib.read(\"symbol\").data\n   column\n0       5\n1       6\n2       7\n</code></pre> <p>The default read behaviour is also available through subscripting:</p> <pre><code>&gt;&gt;&gt; lib[\"symbol\"].data\n   column\n0       5\n1       6\n2       7\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_batch","title":"read_batch","text":"<pre><code>read_batch(\n    symbols: List[Union[str, ReadRequest]],\n    query_builder: Optional[QueryBuilder] = None,\n    lazy: bool = False,\n) -&gt; Union[\n    List[Union[VersionedItem, DataError]],\n    LazyDataFrameCollection,\n]\n</code></pre> <p>Reads multiple symbols.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read.</p> <p> TYPE: <code>List[Union[str, ReadRequest]]</code> </p> <code>query_builder</code> <p>A single QueryBuilder to apply to all the dataframes before they are returned. If this argument is passed then none of the <code>symbols</code> may have their own query_builder specified in their request.</p> <p> TYPE: <code>Optional[QueryBuilder]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>Defer query execution until <code>collect</code> is called on the returned <code>LazyDataFrameCollection</code> object. See documentation on <code>LazyDataFrameCollection</code> for more details.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[List[Union[VersionedItem, DataError]], LazyDataFrameCollection]</code> <p>If lazy is False: A list of the read results, whose i-th element corresponds to the i-th element of the <code>symbols</code> parameter. If the specified version does not exist, a DataError object is returned, with symbol, version_request_type, version_request_data properties, error_code, error_category, and exception_string properties. If a key error or any other internal exception occurs, the same DataError object is also returned. If lazy is True: A LazyDataFrameCollection object on which further querying can be performed prior to collection.</p> RAISES DESCRIPTION <code>ArcticInvalidApiUsageException</code> <p>If kwarg query_builder and per-symbol query builders both used.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"s1\", pd.DataFrame())\n&gt;&gt;&gt; lib.write(\"s2\", pd.DataFrame({\"col\": [1, 2, 3]}))\n&gt;&gt;&gt; lib.write(\"s2\", pd.DataFrame(), prune_previous_versions=False)\n&gt;&gt;&gt; lib.write(\"s3\", pd.DataFrame())\n&gt;&gt;&gt; batch = lib.read_batch([\"s1\", adb.ReadRequest(\"s2\", as_of=0), \"s3\", adb.ReadRequest(\"s2\", as_of=1000)])\n&gt;&gt;&gt; batch[0].data.empty\nTrue\n&gt;&gt;&gt; batch[1].data.empty\nFalse\n&gt;&gt;&gt; batch[2].data.empty\nTrue\n&gt;&gt;&gt; batch[3].symbol\n\"s2\"\n&gt;&gt;&gt; isinstance(batch[3], adb.DataError)\nTrue\n&gt;&gt;&gt; batch[3].version_request_type\nVersionRequestType.SPECIFIC\n&gt;&gt;&gt; batch[3].version_request_data\n1000\n&gt;&gt;&gt; batch[3].error_code\nErrorCode.E_NO_SUCH_VERSION\n&gt;&gt;&gt; batch[3].error_category\nErrorCategory.MISSING_DATA\n</code></pre> See Also <p>read</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(\n    symbol: str, as_of: Optional[AsOf] = None\n) -&gt; VersionedItem\n</code></pre> <p>Return the metadata saved for a symbol.  This method is faster than read as it only loads the metadata, not the data itself.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>Return the metadata as it was as of the point in time. See documentation on <code>read</code> for documentation on the different forms this parameter can take.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the affected symbol in the store. The data attribute will be None.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.read_metadata_batch","title":"read_metadata_batch","text":"<pre><code>read_metadata_batch(\n    symbols: List[Union[str, ReadInfoRequest]],\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Reads the metadata of multiple symbols.</p> PARAMETER DESCRIPTION <code>symbols</code> <p>List of symbols to read metadata.</p> <p> TYPE: <code>List[Union[str, ReadInfoRequest]]</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>A list of the read metadata results, whose i-th element corresponds to the i-th element of the <code>symbols</code> parameter. A VersionedItem object with the metadata field set as None will be returned if the requested version of the symbol exists but there is no metadata If the specified version does not exist, a DataError object is returned, with symbol, version_request_type, version_request_data properties, error_code, error_category, and exception_string properties. If a key error or any other internal exception occurs, the same DataError object is also returned.</p> See Also <p>read_metadata</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.reload_symbol_list","title":"reload_symbol_list","text":"<pre><code>reload_symbol_list()\n</code></pre> <p>Forces the symbol list cache to be reloaded.</p> <p>This can take a long time on large libraries or certain S3 implementations, and once started, it cannot be safely interrupted. If the call is interrupted somehow (exception/process killed), please call this again ASAP.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.snapshot","title":"snapshot","text":"<pre><code>snapshot(\n    snapshot_name: str,\n    metadata: Any = None,\n    skip_symbols: Optional[List[str]] = None,\n    versions: Optional[Dict[str, int]] = None,\n) -&gt; None\n</code></pre> <p>Creates a named snapshot of the data within a library.</p> <p>By default, the latest version of every symbol that has not been deleted will be contained within the snapshot. You can change this behaviour with either <code>versions</code> (an allow-list) or with <code>skip_symbols</code> (a deny-list). Concurrent writes with prune previous versions set while the snapshot is being taken can potentially lead to corruption of the affected symbols in the snapshot.</p> <p>The symbols and versions contained within the snapshot will persist regardless of new symbols and versions being written to the library afterwards. If a version or symbol referenced in a snapshot is deleted then the underlying data will be preserved to ensure the snapshot is still accessible. Only once all referencing snapshots have been removed will the underlying data be removed as well.</p> <p>At most one of <code>skip_symbols</code> and <code>versions</code> may be truthy.</p> PARAMETER DESCRIPTION <code>snapshot_name</code> <p>Name of the snapshot.</p> <p> TYPE: <code>str</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the snapshot.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>skip_symbols</code> <p>Optional symbols to be excluded from the snapshot.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>versions</code> <p>Optional dictionary of versions of symbols to snapshot. For example <code>versions={\"a\": 2, \"b\": 3}</code> will snapshot version 2 of symbol \"a\" and version 3 of symbol \"b\".</p> <p> TYPE: <code>Optional[Dict[str, int]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>InternalException</code> <p>If a snapshot already exists with <code>snapshot_name</code>. You must explicitly delete the pre-existing snapshot.</p> <code>MissingDataException</code> <p>If a symbol or the version of symbol specified in versions does not exist or has been deleted in the library, or, the library has no symbol.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.sort_and_finalize_staged_data","title":"sort_and_finalize_staged_data","text":"<pre><code>sort_and_finalize_staged_data(\n    symbol: str,\n    mode: Optional[StagedDataFinalizeMethod] = WRITE,\n    prune_previous_versions: bool = False,\n    metadata: Any = None,\n    delete_staged_data_on_failure: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Sorts and merges all staged data, making it available for reads. This differs from <code>finalize_staged_data</code> in that it can support staged segments with interleaved time periods and staged segments which are not internally sorted. The end result will be sorted. This requires performing a full sort in memory so can be time consuming.</p> <p>If <code>mode</code> is <code>StagedDataFinalizeMethod.APPEND</code> the index of the first row of the sorted block must be equal to or greater than the index of the last row in the existing data.</p> <p>If <code>Static Schema</code> is used all staged block must have matching schema (same column names, same dtype, same column ordering) and must match the existing data if mode is <code>StagedDataFinalizeMethod.APPEND</code>. For more information about schema options see documentation for <code>arcticdb.LibraryOptions.dynamic_schema</code></p> <p>If the symbol does not exist both <code>StagedDataFinalizeMethod.APPEND</code> and <code>StagedDataFinalizeMethod.WRITE</code> will create it.</p> <p>Calling <code>sort_and_finalize_staged_data</code> without having staged data for the symbol will throw <code>UserInputException</code>. Use  <code>get_staged_symbols</code> to check if there are staged segments for the symbol.</p> <p>Calling <code>sort_and_finalize_staged_data</code> if any of the staged segments contains NaT in its index will throw <code>SortingException</code>.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol to finalize data for.</p> <p> TYPE: <code>str</code> </p> <code>mode</code> <p>Finalize mode. Valid options are WRITE or APPEND. Write collects the staged data and writes them to a new timeseries. Append collects the staged data and appends them to the latest version.</p> <p> TYPE: <code>`StagedDataFinalizeMethod`</code> DEFAULT: <code>StagedDataFinalizeMethod.WRITE</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>delete_staged_data_on_failure</code> <p>Determines the handling of staged data when an exception occurs during the execution of the  <code>sort_and_finalize_staged_data</code> function.</p> <ul> <li>If set to True, all staged data for the specified symbol will be deleted if an exception occurs.</li> <li>If set to False, the staged data will be retained and will be used in subsequent calls to    <code>sort_and_finalize_staged_data</code>.</li> </ul> <p>To manually delete staged data, use the <code>delete_staged_data</code> function.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store. The data member will be None.</p> RAISES DESCRIPTION <code>SortingException</code> <ul> <li>If the first index value of the sorted block is not greater or equal than the last index value of     the existing data when <code>StagedDataFinalizeMethod.APPEND</code> is used.</li> <li>If any staged segment contains NaT in the index</li> </ul> <code>UserInputException</code> <ul> <li>If there are no staged segments when <code>sort_and_finalize_staged_data</code> is called</li> <li> <p>If all of the following conditions are met:</p> <ol> <li>Static schema is used.</li> <li>The width of the DataFrame exceeds the value of <code>LibraryOptions.columns_per_segment</code>.</li> <li>The symbol contains data that was not written by <code>sort_and_finalize_staged_data</code>.</li> <li>Finalize mode is append</li> </ol> </li> </ul> <code>SchemaException</code> <ul> <li>If static schema is used and not all staged segments have matching schema.</li> <li>If static schema is used, mode is <code>StagedDataFinalizeMethod.APPEND</code> and the schema of the sorted and merged     staged segment is not the same as the schema of the existing data</li> <li>If dynamic schema is used and different segments have the same column names but their dtypes don't have a     common type (e.g string and any numeric type)</li> <li>If a different index name is encountered in the staged data, regardless of the schema mode</li> </ul> See Also <p>write     Documentation on the <code>staged</code> parameter explains the concept of staged data in more detail.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write(\"sym\", pd.DataFrame({\"col\": [2, 4]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 2), pd.Timestamp(2024, 1, 4)])), staged=True)\n&gt;&gt;&gt; lib.write(\"sym\", pd.DataFrame({\"col\": [3, 1]}, index=pd.DatetimeIndex([pd.Timestamp(2024, 1, 3), pd.Timestamp(2024, 1, 1)])), staged=True)\n&gt;&gt;&gt; lib.sort_and_finalize_staged_data(\"sym\")\n&gt;&gt;&gt; lib.read(\"sym\").data\n            col\n2024-01-01    1\n2024-01-02    2\n2024-01-03    3\n2024-01-04    4\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.stage","title":"stage","text":"<pre><code>stage(\n    symbol: str,\n    data: NormalizableType,\n    validate_index=True,\n    sort_on_index=False,\n    sort_columns: List[str] = None,\n)\n</code></pre> <p>Write a staged data chunk to storage, that will not be visible until finalize_staged_data is called on the symbol. Equivalent to write() with staged=True.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written. Staged data must be normalizable.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>validate_index</code> <p>Check that the index is sorted prior to writing. In the case of unsorted data, throw an UnsortedDataException</p> <p> DEFAULT: <code>True</code> </p> <code>sort_on_index</code> <p>If an appropriate index is present, sort the data on it. In combination with sort_columns the index will be used as the primary sort column, and the others as secondaries.</p> <p> DEFAULT: <code>False</code> </p> <code>sort_columns</code> <p>Sort the data by specific columns prior to writing.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/library/#arcticdb.version_store.library.Library.tail","title":"tail","text":"<pre><code>tail(\n    symbol: str,\n    n: int = 5,\n    as_of: Optional[Union[int, str]] = None,\n    columns: List[str] = None,\n    lazy: bool = False,\n) -&gt; Union[VersionedItem, LazyDataFrame]\n</code></pre> <p>Read the last n rows of data for the named symbol. If n is negative, return all rows except the first n rows.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>as_of</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>AsOf</code> DEFAULT: <code>None</code> </p> <code>columns</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>lazy</code> <p>See documentation on <code>read</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[VersionedItem, LazyDataFrame]</code> <p>If lazy is False, VersionedItem object that contains a .data and .metadata element. If lazy is True, a LazyDataFrame object on which further querying can be performed prior to collect.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.update","title":"update","text":"<pre><code>update(\n    symbol: str,\n    data: Union[DataFrame, Series],\n    metadata: Any = None,\n    upsert: bool = False,\n    date_range: Optional[\n        Tuple[Optional[Timestamp], Optional[Timestamp]]\n    ] = None,\n    prune_previous_versions: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Overwrites existing symbol data with the contents of <code>data</code>. The entire range between the first and last index entry in <code>data</code> is replaced in its entirety with the contents of <code>data</code>, adding additional index entries if required. <code>update</code> only operates over the outermost index level - this means secondary index rows will be removed if not contained in <code>data</code>.</p> <p>Both the existing symbol version and <code>data</code> must be timeseries-indexed.</p> <p>In the case where <code>data</code> has zero rows, nothing will be done and no new version will be created. This means that <code>update</code> cannot be used with <code>date_range</code> to just delete a subset of the data. We have <code>delete_data_in_range</code> for exactly this purpose and to make it very clear when deletion is intended.</p> <p>Note that <code>update</code> is not designed for multiple concurrent writers over a single symbol.</p> <p>If using static schema then all the column names of <code>data</code>, their order, and their type must match the columns already in storage.</p> <p>If dynamic schema is used then data will override everything in storage for the entire index of <code>data</code>. Update will not keep columns from storage which are not in <code>data</code>.</p> <p>The update will split the first and last segments in the storage that intersect with 'data'. Therefore, frequent calls to update might lead to data fragmentation (see the example below).</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Timeseries indexed data to use for the update.</p> <p> TYPE: <code>Union[DataFrame, Series]</code> </p> <code>metadata</code> <p>Metadata to persist along with the new symbol version.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>upsert</code> <p>If True, will write the data even if the symbol does not exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>date_range</code> <p>If a range is specified, it will delete the stored value within the range and overwrite it with the data in <code>data</code>. This allows the user to update with data that might only be a subset of the stored value. Leaving any part of the tuple as None leaves that part of the range open ended. Only data with date_range will be modified, even if <code>data</code> covers a wider date range.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]]</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...    {'column': [1,2,3,4]},\n...    index=pd.date_range(start='1/1/2018', end='1/4/2018')\n... )\n&gt;&gt;&gt; df\n            column\n2018-01-01       1\n2018-01-02       2\n2018-01-03       3\n2018-01-04       4\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; update_df = pd.DataFrame(\n...    {'column': [400, 40]},\n...    index=pd.date_range(start='1/1/2018', end='1/3/2018', freq='2D')\n... )\n&gt;&gt;&gt; update_df\n            column\n2018-01-01     400\n2018-01-03      40\n&gt;&gt;&gt; lib.update(\"symbol\", update_df)\n&gt;&gt;&gt; # Note that 2018-01-02 is gone despite not being in update_df\n&gt;&gt;&gt; lib.read(\"symbol\").data\n            column\n2018-01-01     400\n2018-01-03      40\n2018-01-04       4\n</code></pre> <p>Update will split the first and the last segment intersecting with <code>data</code></p> <pre><code>&gt;&gt;&gt; index = pd.date_range(pd.Timestamp(\"2024-01-01\"), pd.Timestamp(\"2024-02-01\"))\n&gt;&gt;&gt; df = pd.DataFrame({f\"col_{i}\": range(len(index)) for i in range(1)}, index=index)\n&gt;&gt;&gt; lib.write(\"test\", df)\n&gt;&gt;&gt; lt=lib._dev_tools.library_tool()\n&gt;&gt;&gt; print(lt.read_index(\"test\"))\nstart_index                     end_index  version_id stream_id          creation_ts         content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n2024-01-01  2024-02-01 00:00:00.000000001           0   b'test'  1738599073224386674  9652922778723941392          84         2          1        2          0       32\n&gt;&gt;&gt; update_index=pd.date_range(pd.Timestamp(\"2024-01-10\"), freq=\"ns\", periods=200000)\n&gt;&gt;&gt; update = pd.DataFrame({f\"col_{i}\": [1] for i in range(1)}, index=update_index)\n&gt;&gt;&gt; lib.update(\"test\", update)\n&gt;&gt;&gt; print(lt.read_index(\"test\"))\nstart_index                                    end_index  version_id stream_id          creation_ts          content_hash  index_type  key_type  start_col  end_col  start_row  end_row\n2024-01-01 00:00:00.000000 2024-01-09 00:00:00.000000001           1   b'test'  1738599073268200906  13838161946080117383          84         2          1        2          0        9\n2024-01-10 00:00:00.000000 2024-01-10 00:00:00.000100000           1   b'test'  1738599073256354553  15576483210589662891          84         2          1        2          9   100009\n2024-01-10 00:00:00.000100 2024-01-10 00:00:00.000200000           1   b'test'  1738599073256588040  12429442054752910013          84         2          1        2     100009   200009\n2024-01-11 00:00:00.000000 2024-02-01 00:00:00.000000001           1   b'test'  1738599073268493107   5975110026983744452          84         2          1        2     200009   200031\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.update_batch","title":"update_batch","text":"<pre><code>update_batch(\n    update_payloads: List[UpdatePayload],\n    upsert: bool = False,\n    prune_previous_versions: bool = False,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Perform an update operation on a list of symbols in parallel. All constrains on update apply to this call as well.</p> PARAMETER DESCRIPTION <code>update_payloads</code> <p>List <code>arcticdb.library.UpdatePayload</code>. Each element of the list describes an update operation for a particular symbol. Providing the symbol name, data, etc. The same symbol should not appear twice in this list.</p> <p> TYPE: <code>List[UpdatePayload]</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the library.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upsert</code> <p>If True any symbol in <code>update_payloads</code> which is not already in the library will be created.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. i-th entry corresponds to i-th element of <code>update_payloads</code>. Each result correspond to a structure containing metadata and version number of the affected symbol in the store. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> <code>ArcticUnsupportedDataTypeException</code> <p>If data that is not of NormalizableType appears in any of the payloads.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df1 = pd.DataFrame({'column_1': [1, 2, 3]}, index=pd.date_range(\"2025-01-01\", periods=3))\n&gt;&gt;&gt; df1\n            column_1\n2025-01-01         1\n2025-01-02         2\n2025-01-03         3\n&gt;&gt;&gt; df2 = pd.DataFrame({'column_2': [10, 11]}, index=pd.date_range(\"2024-01-01\", periods=2))\n&gt;&gt;&gt; df2\n            column_2\n2024-01-01        10\n2024-01-02        11\n&gt;&gt;&gt; lib.write(\"symbol_1\", df1)\n&gt;&gt;&gt; lib.write(\"symbol_2\", df1)\n&gt;&gt;&gt; lib.update_batch([arcticdb.library.UpdatePayload(\"symbol_1\", pd.DataFrame({\"column_1\": [4, 5]}, index=pd.date_range(\"2025-01-03\", periods=2))), arcticdb.library.UpdatePayload(\"symbol_2\", pd.DataFrame({\"column_2\": [-1]}, index=pd.date_range(\"2023-01-01\", periods=1)))])\n[VersionedItem(symbol='symbol_1', library='test', data=n/a, version=1, metadata=(None,), host='LMDB(path=...)', timestamp=1737542783853861819), VersionedItem(symbol='symbol_2', library='test', data=n/a, version=1, metadata=(None,), host='LMDB(path=...)', timestamp=1737542783851798754)]\n&gt;&gt;&gt; lib.read(\"symbol_1\").data\n            column_1\n2025-01-01         1\n2025-01-02         2\n2025-01-03         4\n2025-01-04         5\n&gt;&gt;&gt; lib.read(\"symbol_2\").data\n            column_2\n2023-01-01        -1\n2024-01-01        10\n2024-01-02        11\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write","title":"write","text":"<pre><code>write(\n    symbol: str,\n    data: NormalizableType,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    staged=False,\n    validate_index=True,\n) -&gt; VersionedItem\n</code></pre> <p>Write <code>data</code> to the specified <code>symbol</code>. If <code>symbol</code> already exists then a new version will be created to reference the newly written data. For more information on versions see the documentation for the <code>read</code> primitive.</p> <p><code>data</code> must be of a format that can be normalised into Arctic's internal storage structure. Pandas DataFrames, Pandas Series and Numpy NDArrays can all be normalised. Normalised data will be split along both the columns and rows into segments. By default, a segment will contain 100,000 rows and 127 columns.</p> <p>If this library has <code>write_deduplication</code> enabled then segments will be deduplicated against storage prior to write to reduce required IO operations and storage requirements. Data will be effectively deduplicated for all segments up until the first differing row when compared to storage. As a result, modifying the beginning of <code>data</code> with respect to previously written versions may significantly reduce the effectiveness of deduplication.</p> <p>Note that <code>write</code> is not designed for multiple concurrent writers over a single symbol unless the staged keyword argument is set to True. If <code>staged</code> is True, written segments will be staged and left in an \"incomplete\" stage, unable to be read until they are finalized. This enables multiple writers to a single symbol - all writing staged data at the same time - with one process able to later finalize all staged data rendering the data readable by clients. To finalize staged data, see <code>finalize_staged_data</code>.</p> <p>Note: ArcticDB will use the 0-th level index of the Pandas DataFrame for its on-disk index.</p> <p>Any non-<code>DatetimeIndex</code> will converted into an internal <code>RowCount</code> index. That is, ArcticDB will assign each row a monotonically increasing integer identifier and that will be used for the index.</p> <p>See the Metadata section of our online documentation for details about how metadata is persisted and caveats.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written. To write non-normalizable data, use <code>write_pickle</code>.</p> <p> TYPE: <code>NormalizableType</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>staged</code> <p>Whether to write to a staging area rather than immediately to the library. See documentation on <code>finalize_staged_data</code> for more information.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>If True, verify that the index of <code>data</code> supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing.</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the written symbol in the store.</p> RAISES DESCRIPTION <code>ArcticUnsupportedDataTypeException</code> <p>If <code>data</code> is not of NormalizableType.</p> <code>UnsortedDataException</code> <p>If data is unsorted and validate_index is set to True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'column': [5,6,7]})\n&gt;&gt;&gt; lib.write(\"symbol\", df, metadata={'my_dictionary': 'is_great'})\n&gt;&gt;&gt; lib.read(\"symbol\").data\n   column\n0       5\n1       6\n2       7\n</code></pre> <p>Staging data for later finalisation (enables concurrent writes):</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'column': [5,6,7]}, index=pd.date_range(start='1/1/2000', periods=3))\n&gt;&gt;&gt; lib.write(\"staged\", df, staged=True)  # Multiple staged writes can occur in parallel\n&gt;&gt;&gt; lib.finalize_staged_data(\"staged\", StagedDataFinalizeMethod.WRITE)  # Must be run after all staged writes have completed\n&gt;&gt;&gt; lib.read(\"staged\").data  # Would return error if run before finalization\n            column\n2000-01-01       5\n2000-01-02       6\n2000-01-03       7\n</code></pre> <p>WritePayload objects can be unpacked and used as parameters:</p> <pre><code>&gt;&gt;&gt; w = adb.WritePayload(\"symbol\", df, metadata={'the': 'metadata'})\n&gt;&gt;&gt; lib.write(*w, staged=True)\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_batch","title":"write_batch","text":"<pre><code>write_batch(\n    payloads: List[WritePayload],\n    prune_previous_versions: bool = False,\n    validate_index=True,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Write a batch of multiple symbols.</p> PARAMETER DESCRIPTION <code>payloads</code> <p>Symbols and their corresponding data. There must not be any duplicate symbols in <code>payload</code>.</p> <p> TYPE: <code>`List[WritePayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>validate_index</code> <p>Verify that each entry in the batch has an index that supports date range searches and update operations. This tests that the data is sorted in ascending order, using Pandas DataFrame.index.is_monotonic_increasing.</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. The data attribute will be None for each versioned item. i-th entry corresponds to i-th element of <code>payloads</code>. Each result correspond to a structure containing metadata and version number of the written symbols in the store, in the same order as <code>payload</code>. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> <code>ArcticUnsupportedDataTypeException</code> <p>If data that is not of NormalizableType appears in any of the payloads.</p> See Also <p>write: For more detailed documentation.</p> <p>Examples:</p> <p>Writing a simple batch:</p> <pre><code>&gt;&gt;&gt; df_1 = pd.DataFrame({'column': [1,2,3]})\n&gt;&gt;&gt; df_2 = pd.DataFrame({'column': [4,5,6]})\n&gt;&gt;&gt; payload_1 = adb.WritePayload(\"symbol_1\", df_1, metadata={'the': 'metadata'})\n&gt;&gt;&gt; payload_2 = adb.WritePayload(\"symbol_2\", df_2)\n&gt;&gt;&gt; items = lib.write_batch([payload_1, payload_2])\n&gt;&gt;&gt; lib.read(\"symbol_1\").data\n   column\n0       1\n1       2\n2       3\n&gt;&gt;&gt; lib.read(\"symbol_2\").data\n   column\n0       4\n1       5\n2       6\n&gt;&gt;&gt; items[0].symbol, items[1].symbol\n('symbol_1', 'symbol_2')\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(\n    symbol: str,\n    metadata: Any,\n    prune_previous_versions: bool = False,\n) -&gt; VersionedItem\n</code></pre> <p>Write metadata under the specified symbol name to this library. The data will remain unchanged. A new version will be created.</p> <p>If the symbol is missing, it causes a write with empty data (None, pickled, can't append) and the supplied metadata.</p> <p>This method should be faster than <code>write</code> as it involves no data segment read/write operations.</p> <p>See the Metadata section of our online documentation for details about how metadata is persisted and caveats.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name for the item</p> <p> TYPE: <code>str</code> </p> <code>metadata</code> <p>Metadata to persist along with the symbol</p> <p> TYPE: <code>Any</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database. Note that metadata is versioned alongside the data it is referring to, and so this operation removes old versions of data as well as metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Structure containing metadata and version number of the affected symbol in the store.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_metadata_batch","title":"write_metadata_batch","text":"<pre><code>write_metadata_batch(\n    write_metadata_payloads: List[WriteMetadataPayload],\n    prune_previous_versions: bool = False,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Write metadata to multiple symbols in a batch fashion. This is more efficient than making multiple <code>write_metadata</code> calls in succession as some constant-time operations can be executed only once rather than once for each element of <code>write_metadata_payloads</code>. Note that this isn't an atomic operation - it's possible for the metadata for one symbol to be fully written and readable before another symbol.</p> <p>See the Metadata section of our online documentation for details about how metadata is persisted and caveats.</p> PARAMETER DESCRIPTION <code>write_metadata_payloads</code> <p>Symbols and their corresponding metadata. There must not be any duplicate symbols in <code>payload</code>.</p> <p> TYPE: <code>`List[WriteMetadataPayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database. Note that metadata is versioned alongside the data it is referring to, and so this operation removes old versions of data as well as metadata.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>List of versioned items. The data attribute will be None for each versioned item. i-th entry corresponds to i-th element of <code>write_metadata_payloads</code>. Each result correspond to a structure containing metadata and version number of the affected symbol in the store. If any internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in write_metadata_payloads.</p> <p>Examples:</p> <p>Writing a simple batch:</p> <pre><code>&gt;&gt;&gt; payload_1 = adb.WriteMetadataPayload(\"symbol_1\", {'the': 'metadata_1'})\n&gt;&gt;&gt; payload_2 = adb.WriteMetadataPayload(\"symbol_2\", {'the': 'metadata_2'})\n&gt;&gt;&gt; items = lib.write_metadata_batch([payload_1, payload_2])\n&gt;&gt;&gt; lib.read_metadata(\"symbol_1\")\n{'the': 'metadata_1'}\n&gt;&gt;&gt; lib.read_metadata(\"symbol_2\")\n{'the': 'metadata_2'}\n</code></pre>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_pickle","title":"write_pickle","text":"<pre><code>write_pickle(\n    symbol: str,\n    data: Any,\n    metadata: Any = None,\n    prune_previous_versions: bool = False,\n    staged=False,\n) -&gt; VersionedItem\n</code></pre> <p>See <code>write</code>. This method differs from <code>write</code> only in that <code>data</code> can be of any type that is serialisable via the Pickle library. There are significant downsides to storing data in this way:</p> <ul> <li>Retrieval can only be done in bulk. Calls to <code>read</code> will not support <code>date_range</code>, <code>query_builder</code> or <code>columns</code>.</li> <li>The data cannot be updated or appended to via the update and append methods.</li> <li>Writes cannot be deduplicated in any way.</li> </ul> PARAMETER DESCRIPTION <code>symbol</code> <p>See documentation on <code>write</code>.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written.</p> <p> TYPE: <code>`Any`</code> </p> <code>metadata</code> <p>See documentation on <code>write</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>prune_previous_versions</code> <p>See documentation on <code>write</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>staged</code> <p>See documentation on <code>write</code>.</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>See documentation on <code>write</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lib.write_pickle(\"symbol\", [1,2,3])\n&gt;&gt;&gt; lib.read(\"symbol\").data\n[1, 2, 3]\n</code></pre> See Also <p>write: For more detailed documentation.</p>"},{"location":"api/library/#arcticdb.version_store.library.Library.write_pickle_batch","title":"write_pickle_batch","text":"<pre><code>write_pickle_batch(\n    payloads: List[WritePayload],\n    prune_previous_versions: bool = False,\n) -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Write a batch of multiple symbols, pickling their data if necessary.</p> PARAMETER DESCRIPTION <code>payloads</code> <p>Symbols and their corresponding data. There must not be any duplicate symbols in <code>payload</code>.</p> <p> TYPE: <code>`List[WritePayload]`</code> </p> <code>prune_previous_versions</code> <p>Removes previous (non-snapshotted) versions from the database.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>Structures containing metadata and version number of the written symbols in the store, in the same order as <code>payload</code>. If a key error or any other internal exception is raised, a DataError object is returned, with symbol, error_code, error_category, and exception_string properties.</p> RAISES DESCRIPTION <code>ArcticDuplicateSymbolsInBatchException</code> <p>When duplicate symbols appear in payload.</p> See Also <p>write: For more detailed documentation. write_pickle: For information on the implications of providing data that needs to be pickled.</p>"},{"location":"api/library_types/","title":"Library related objects","text":""},{"location":"api/library_types/#arcticdb.version_store.library.NormalizableType","title":"arcticdb.version_store.library.NormalizableType  <code>module-attribute</code>","text":"<pre><code>NormalizableType = Union[NORMALIZABLE_TYPES]\n</code></pre> <p>Types that can be normalised into Arctic's internal storage structure.</p> See Also <p>Library.write: for more documentation on normalisation.</p>"},{"location":"api/library_types/#arcticdb.ReadInfoRequest","title":"arcticdb.ReadInfoRequest","text":"<p>               Bases: <code>NamedTuple</code></p> <p>ReadInfoRequest is useful for batch methods like read_metadata_batch and get_description_batch, where we only need to specify the symbol and the version information. Therefore, construction of this object is only required for these batch operations.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>See <code>read_metadata</code> method.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>See <code>read_metadata</code> method.</p> <p> TYPE: <code>Optional[AsOf], default=none</code> </p> See Also <p>Library.read: For documentation on the parameters.</p>"},{"location":"api/library_types/#arcticdb.ReadRequest","title":"arcticdb.ReadRequest","text":"<p>               Bases: <code>NamedTuple</code></p> <p>ReadRequest is designed to enable batching of read operations with an API that mirrors the singular <code>read</code> API. Therefore, construction of this object is only required for batch read operations.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>str</code> </p> <code>as_of</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[AsOf], default=none</code> </p> <code>date_range</code> <p>See <code>read</code>method.</p> <p> TYPE: <code>Optional[Tuple[Optional[Timestamp], Optional[Timestamp]]], default=none</code> </p> <code>row_range</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[Tuple[int, int]], default=none</code> </p> <code>columns</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[List[str]], default=none</code> </p> <code>query_builder</code> <p>See <code>read</code> method.</p> <p> TYPE: <code>Optional[Querybuilder], default=none</code> </p> See Also <p>Library.read: For documentation on the parameters.</p>"},{"location":"api/library_types/#arcticdb.version_store.library.SymbolDescription","title":"arcticdb.version_store.library.SymbolDescription","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple. Descriptive information about the data stored under a particular symbol.</p> ATTRIBUTE DESCRIPTION <code>columns</code> <p>Columns stored under the symbol.</p> <p> TYPE: <code>Tuple[NameWithDType]</code> </p> <code>index</code> <p>Index of the symbol.</p> <p> TYPE: <code>Tuple[NameWithDType]</code> </p> <code>index_type</code> <p>Whether the index is a simple index or a multi_index. <code>NA</code> indicates that the stored data does not have an index.</p> <p> TYPE: <code>str {\"NA\", \"index\", \"multi_index\"}</code> </p> <code>row_count</code> <p>Number of rows, or None if the symbol is pickled.</p> <p> TYPE: <code>Optional[int]</code> </p> <code>last_update_time</code> <p>The time of the last update to the symbol, in UTC.</p> <p> TYPE: <code>datetime</code> </p> <code>date_range</code> <p>The values of the index column in the first and last row of this symbol. Both values will be NaT if: - the symbol is not timestamp indexed - the symbol is timestamp indexed, but the sorted field of this class is UNSORTED (see below)</p> <p> TYPE: <code>Tuple[Union[Timestamp], Union[Timestamp]]</code> </p> <code>sorted</code> <p>One of \"ASCENDING\", \"DESCENDING\", \"UNSORTED\", or \"UNKNOWN\": ASCENDING - The data has a timestamp index, and is sorted in ascending order. Guarantees that operations such as             append, update, and read with date_range work as expected. DESCENDING - The data has a timestamp index, and is sorted in descending order. Update and read with date_range              will not work. UNSORTED - The data has a timestamp index, and is not sorted. Can only be created by calling write, write_batch,            append, or append_batch with validate_index set to False. Update and read with date_range will not            work. UNKNOWN - Either the data does not have a timestamp index, or the data does have a timestamp index, but was           written by a client that predates this information being stored.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/library_types/#arcticdb.version_store.library.SymbolVersion","title":"arcticdb.version_store.library.SymbolVersion","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple. A symbol name - version pair.</p> ATTRIBUTE DESCRIPTION <code>symbol</code> <p>Symbol name.</p> <p> TYPE: <code>str</code> </p> <code>version</code> <p>Version of the symbol.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/library_types/#arcticdb.version_store.library.StagedDataFinalizeMethod","title":"arcticdb.version_store.library.StagedDataFinalizeMethod","text":"<p>               Bases: <code>Enum</code></p>"},{"location":"api/library_types/#arcticdb.VersionedItem","title":"arcticdb.VersionedItem","text":"<p>Return value for many operations that captures the result and associated information.</p> ATTRIBUTE DESCRIPTION <code>library</code> <p>Library this result relates to.</p> <p> TYPE: <code>str</code> </p> <code>symbol</code> <p>Read or modified symbol.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>For data retrieval (read) operations, contains the data read. For data modification operations, the value might not be populated.</p> <p> TYPE: <code>Any</code> </p> <code>version</code> <p>For data retrieval operations, the version the <code>as_of</code> argument resolved to. In the special case where no versions have been written yet, but data is being read exclusively from incomplete segments, this will be 2^64-1. For data modification operations, the version the data has been written under.</p> <p> TYPE: <code>int</code> </p> <code>metadata</code> <p>The metadata saved alongside <code>data</code>. Availability depends on the method used and may be different from that of <code>data</code>.</p> <p> TYPE: <code>Any</code> </p> <code>host</code> <p>Informational / for backwards compatibility.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>timestamp</code> <p>The time in nanoseconds since epoch that this version was written. In the special case where no versions have been written yet, but data is being read exclusively from incomplete segments, this will be 0.</p> <p> TYPE: <code>Optional[int]</code> </p>"},{"location":"api/library_types/#arcticdb.version_store.library.VersionInfo","title":"arcticdb.version_store.library.VersionInfo","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A named tuple. Descriptive information about a particular version of a symbol.</p> ATTRIBUTE DESCRIPTION <code>date</code> <p>Time that the version was written in UTC.</p> <p> TYPE: <code>datetime</code> </p> <code>deleted</code> <p>True if the version has been deleted and is only being kept alive via a snapshot.</p> <p> TYPE: <code>bool</code> </p> <code>snapshots</code> <p>Snapshots that refer to this version.</p> <p> TYPE: <code>List[str]</code> </p>"},{"location":"api/library_types/#arcticdb.WritePayload","title":"arcticdb.WritePayload","text":"<p>WritePayload is designed to enable batching of multiple operations with an API that mirrors the singular <code>write</code> API.</p> <p>Construction of <code>WritePayload</code> objects is only required for batch write operations.</p> <p>One instance of <code>WritePayload</code> refers to one unit that can be written through to ArcticDB.</p> METHOD DESCRIPTION <code>__init__</code> <p>Constructor.</p>"},{"location":"api/library_types/#arcticdb.WritePayload.__init__","title":"__init__","text":"<pre><code>__init__(\n    symbol: str,\n    data: Union[Any, NormalizableType],\n    metadata: Any = None,\n)\n</code></pre> <p>Constructor.</p> PARAMETER DESCRIPTION <code>symbol</code> <p>Symbol name. Limited to 255 characters. The following characters are not supported in symbols: <code>\"*\", \"&amp;\", \"&lt;\", \"&gt;\"</code></p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>Data to be written. If data is not of NormalizableType then it will be pickled.</p> <p> TYPE: <code>Any</code> </p> <code>metadata</code> <p>Optional metadata to persist along with the symbol.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> See Also <p>Library.write_pickle: For information on the implications of providing data that needs to be pickled.</p>"},{"location":"api/processing/","title":"DataFrame Processing Operations API","text":""},{"location":"api/processing/#arcticdb.LazyDataFrame","title":"arcticdb.LazyDataFrame","text":"<p>               Bases: <code>QueryBuilder</code></p> <p>Lazy dataframe implementation, allowing chains of queries to be added before the read is actually executed. Returned by <code>Library.read</code>, <code>Library.head</code>, and <code>Library.tail</code> calls when <code>lazy=True</code>.</p> See Also <p>QueryBuilder for supported querying operations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt;\n# Specify that we want version 0 of \"test\" symbol, and to only return the \"new_column\" column in the output\n&gt;&gt;&gt; lazy_df = lib.read(\"test\", as_of=0, columns=[\"new_column\"], lazy=True)\n# Perform a filtering operation\n&gt;&gt;&gt; lazy_df = lazy_df[lazy_df[\"col1\"].isin(0, 3, 6, 9)]\n# Create a new column through a projection operation\n&gt;&gt;&gt; lazy_df[\"new_col\"] = lazy_df[\"col1\"] + lazy_df[\"col2\"]\n# Actual read and processing happens here\n&gt;&gt;&gt; df = lazy_df.collect().data\n</code></pre> METHOD DESCRIPTION <code>collect</code> <p>Read the data and execute any queries applied to this object since the read call.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrame._to_read_request","title":"_to_read_request","text":"<pre><code>_to_read_request() -&gt; ReadRequest\n</code></pre> <p>Convert this object into a ReadRequest, including any queries applied to this object since the read call.</p> RETURNS DESCRIPTION <code>ReadRequest</code> <p>Object with all the parameters necessary to completely specify the data to be read.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrame.collect","title":"collect","text":"<pre><code>collect() -&gt; VersionedItem\n</code></pre> <p>Read the data and execute any queries applied to this object since the read call.</p> RETURNS DESCRIPTION <code>VersionedItem</code> <p>Object that contains a .data and .metadata element.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection","title":"arcticdb.LazyDataFrameCollection","text":"<p>               Bases: <code>QueryBuilder</code></p> <p>Lazy dataframe implementation for batch operations. Allows the application of chains of queries to be added before the actual reads are performed. Queries applied to this object will be applied to all  the symbols being read. If per-symbol queries are required, split can be used to break this class into a list of <code>LazyDataFrame</code> objects. Returned by <code>Library.read_batch</code> calls when <code>lazy=True</code>.</p> See Also <p>QueryBuilder for supported querying operations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt;\n# Specify that we want the latest version of \"test_0\" symbol, and version 0 of \"test_1\" symbol\n&gt;&gt;&gt; lazy_dfs = lib.read_batch([\"test_0\", ReadRequest(\"test_1\", as_of=0)], lazy=True)\n# Perform a filtering operation on both the \"test_0\" and \"test_1\" symbols\n&gt;&gt;&gt; lazy_dfs = lazy_dfs[lazy_dfs[\"col1\"].isin(0, 3, 6, 9)]\n# Perform a different projection operation on each symbol\n&gt;&gt;&gt; lazy_dfs = lazy_dfs.split()\n&gt;&gt;&gt; lazy_dfs[0].apply(\"new_col\", lazy_dfs[0][\"col1\"] + 1)\n&gt;&gt;&gt; lazy_dfs[1].apply(\"new_col\", lazy_dfs[1][\"col1\"] + 2)\n# Bring together again and perform the same filter on both symbols\n&gt;&gt;&gt; lazy_dfs = LazyDataFrameCollection(lazy_dfs)\n&gt;&gt;&gt; lazy_dfs = lazy_dfs[lazy_dfs[\"new_col\"] &gt; 0]\n# Actual read and processing happens here\n&gt;&gt;&gt; res = lazy_dfs.collect()\n</code></pre> METHOD DESCRIPTION <code>__init__</code> <p>Gather a list of <code>LazyDataFrame</code>s into a single object that can be collected together.</p> <code>collect</code> <p>Read the data and execute any queries applied to this object since the read_batch call.</p> <code>split</code> <p>Separate the collection into a list of LazyDataFrames, including any queries already applied to this object.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection.__init__","title":"__init__","text":"<pre><code>__init__(lazy_dataframes: List[LazyDataFrame])\n</code></pre> <p>Gather a list of <code>LazyDataFrame</code>s into a single object that can be collected together.</p> PARAMETER DESCRIPTION <code>lazy_dataframes</code> <p>Collection of <code>LazyDataFrames</code>s to gather together.</p> <p> TYPE: <code>List[LazyDataFrame]</code> </p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection.collect","title":"collect","text":"<pre><code>collect() -&gt; List[Union[VersionedItem, DataError]]\n</code></pre> <p>Read the data and execute any queries applied to this object since the read_batch call.</p> RETURNS DESCRIPTION <code>List[Union[VersionedItem, DataError]]</code> <p>See documentation on <code>Library.read_batch</code>.</p>"},{"location":"api/processing/#arcticdb.LazyDataFrameCollection.split","title":"split","text":"<pre><code>split() -&gt; List[LazyDataFrame]\n</code></pre> <p>Separate the collection into a list of LazyDataFrames, including any queries already applied to this object.</p> RETURNS DESCRIPTION <code>List[LazyDataFrame]</code>"},{"location":"api/processing/#arcticdb.QueryBuilder","title":"arcticdb.QueryBuilder","text":"<p>Build a query to process read results with. Syntax is designed to be similar to Pandas:</p> <pre><code>q = adb.QueryBuilder()\nq = q[q[\"a\"] &lt; 5] (equivalent to q = q[q.a &lt; 5] provided the column name is also a valid Python variable name)\ndataframe = lib.read(symbol, query_builder=q).data\n</code></pre> <p>For Group By and Aggregation functionality please see the documentation for the <code>groupby</code>. For projection functionality, see the documentation for the <code>apply</code> method.</p> <p>Supported arithmetic operations when projection or filtering:</p> <ul> <li>Binary arithmetic: +, -, *, /</li> <li>Unary arithmetic: -, abs</li> </ul> <p>Supported filtering operations:</p> <ul> <li> <p>isna, isnull, notna, and notnull - return all rows where a specified column is/is not NaN or None. isna is equivalent to isnull, and notna is equivalent to notnull, i.e. no distinction is made between NaN and None values in column types that support both (e.g. strings). For example:     <pre><code>q = q[q[\"col\"].isna()]\n</code></pre></p> </li> <li> <p>Binary comparisons: &lt;, &lt;=, &gt;, &gt;=, ==, !=</p> </li> <li>Unary NOT: ~</li> <li>Binary combinators: &amp;, |, ^</li> <li>List membership: isin, isnotin (also accessible with == and !=)</li> </ul> <p>isin/isnotin accept lists, sets, frozensets, 1D ndarrays, or *args unpacking. For example:</p> <pre><code>l = [1, 2, 3]\nq.isin(l)\n</code></pre> <p>is equivalent to...</p> <pre><code>q.isin(1, 2, 3)\n</code></pre> <p>Boolean columns can be filtered on directly:</p> <pre><code>q = adb.QueryBuilder()\nq = q[q[\"boolean_column\"]]\n</code></pre> <p>and combined with other operations intuitively:</p> <pre><code>q = adb.QueryBuilder()\nq = q[(q[\"boolean_column_1\"] &amp; ~q[\"boolean_column_2\"]) &amp; (q[\"numeric_column\"] &gt; 0)]\n</code></pre> <p>Arbitrary combinations of these expressions is possible, for example:</p> <pre><code>q = q[(((q[\"a\"] * q[\"b\"]) / 5) &lt; (0.7 * q[\"c\"])) &amp; (q[\"b\"] != 12)]\n</code></pre> <p>See tests/unit/arcticdb/version_store/test_filtering.py for more example uses.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder--timestamp-filtering","title":"Timestamp filtering","text":"<p>pandas.Timestamp, datetime.datetime, pandas.Timedelta, and datetime.timedelta objects are supported. Note that internally all of these types are converted to nanoseconds (since epoch in the Timestamp/datetime cases). This means that nonsensical operations such as multiplying two times together are permitted (but not encouraged).</p>"},{"location":"api/processing/#arcticdb.QueryBuilder--restrictions","title":"Restrictions","text":"<p>String equality/inequality (and isin/isnotin) is supported for printable ASCII characters only. Although not prohibited, it is not recommended to use ==, !=, isin, or isnotin with floating point values.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder--exceptions","title":"Exceptions","text":"<p>inf or -inf values are provided for comparison Column involved in query is a Categorical Symbol is pickled Column involved in query is not present in symbol Query involves comparing strings using &lt;, &lt;=, &gt;, or &gt;= operators Query involves comparing a string to one or more numeric values, or vice versa Query involves arithmetic with a column containing strings</p> METHOD DESCRIPTION <code>apply</code> <p>Apply enables new columns to be created using supported QueryBuilder numeric operations. See the documentation for the</p> <code>date_range</code> <p>DateRange to read data for.  Applicable only for Pandas data with a DateTime index. Returns only the part</p> <code>groupby</code> <p>Group symbol by column name. GroupBy operations must be followed by an aggregation operator. Currently the following five aggregation</p> <code>head</code> <p>Filter out all but the first n rows of data. If n is negative, return all rows except the last n rows.</p> <code>optimise_for_memory</code> <p>Reduce peak memory usage during the query, at the expense of some performance.</p> <code>optimise_for_speed</code> <p>Process query as fast as possible (the default behaviour)</p> <code>prepend</code> <p>Applies processing specified in other before any processing already defined for this QueryBuilder.</p> <code>resample</code> <p>Resample a symbol on the index. The symbol must be datetime indexed. Resample operations must be followed by</p> <code>row_range</code> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound.</p> <code>tail</code> <p>Filter out all but the last n rows of data. If n is negative, return all rows except the first n rows.</p> <code>then</code> <p>Applies processing specified in other after any processing already defined for this QueryBuilder.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.apply","title":"apply","text":"<pre><code>apply(name, expr)\n</code></pre> <p>Apply enables new columns to be created using supported QueryBuilder numeric operations. See the documentation for the QueryBuilder class for more information on supported expressions - any expression valid in a filter is valid when using <code>apply</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the column to be created</p> <p> </p> <code>expr</code> <p>Expression</p> <p> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"VWAP\": np.arange(0, 10, dtype=np.float64),\n        \"ASK\": np.arange(10, 20, dtype=np.uint16),\n        \"VOL_ACC\": np.arange(20, 30, dtype=np.int32),\n    },\n    index=np.arange(10),\n)\n&gt;&gt;&gt; lib.write(\"expression\", df)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.apply(\"ADJUSTED\", q[\"ASK\"] * q[\"VOL_ACC\"] + 7)\n&gt;&gt;&gt; lib.read(\"expression\", query_builder=q).data\nVOL_ACC  ASK  VWAP  ADJUSTED\n0     20   10   0.0       207\n1     21   11   1.0       238\n2     22   12   2.0       271\n3     23   13   3.0       306\n4     24   14   4.0       343\n5     25   15   5.0       382\n6     26   16   6.0       423\n7     27   17   7.0       466\n8     28   18   8.0       511\n9     29   19   9.0       558\n</code></pre> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.date_range","title":"date_range","text":"<pre><code>date_range(date_range: DateRangeInput)\n</code></pre> <p>DateRange to read data for.  Applicable only for Pandas data with a DateTime index. Returns only the part of the data that falls within the given range. If this is the only processing clause being applied, then the returned data object will use less memory than passing date_range directly as an argument to the read method, at  the cost of possibly being slightly slower.</p> PARAMETER DESCRIPTION <code>date_range</code> <p>A date range in the same format as accepted by the read method.</p> <p> TYPE: <code>DateRangeInput</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.date_range((pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2001-01-01\")))\n</code></pre> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.groupby","title":"groupby","text":"<pre><code>groupby(name: str)\n</code></pre> <p>Group symbol by column name. GroupBy operations must be followed by an aggregation operator. Currently the following five aggregation operators are supported:</p> <ul> <li>\"mean\" - compute the mean of the group</li> <li>\"sum\" - compute the sum of the group</li> <li>\"min\" - compute the min of the group</li> <li>\"max\" - compute the max of the group</li> <li>\"count\" - compute the count of group</li> </ul> <p>For usage examples, see below.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the column to group on. Note that currently GroupBy only supports single-column groupings.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <p>Average (mean) over two groups:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\", \"group_2\", \"group_2\"],\n        \"to_mean\": [1.1, 1.4, 2.5, np.nan, 2.2],\n    },\n    index=np.arange(5),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\").agg({\"to_mean\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>           to_mean\n group_1  1.666667\n group_2       2.2\n</code></pre> <p>Max over one group:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\"],\n        \"to_max\": [1, 5, 4],\n    },\n    index=np.arange(3),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\").agg({\"to_max\": \"max\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>         to_max\ngroup_1  5\n</code></pre> <p>Max and Mean:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\"],\n        \"to_mean\": [1.1, 1.4, 2.5],\n        \"to_max\": [1.1, 1.4, 2.5]\n    },\n    index=np.arange(3),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\").agg({\"to_max\": \"max\", \"to_mean\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>         to_max   to_mean\ngroup_1     2.5  1.666667\n</code></pre> <p>Min and max over one column, mean over another:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"grouping_column\": [\"group_1\", \"group_1\", \"group_1\", \"group_2\", \"group_2\"],\n        \"agg_1\": [1, 2, 3, 4, 5],\n        \"agg_2\": [1.1, 1.4, 2.5, np.nan, 2.2],\n    },\n    index=np.arange(5),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.groupby(\"grouping_column\")\n&gt;&gt;&gt; q = q.agg({\"agg_1_min\": (\"agg_1\", \"min\"), \"agg_1_max\": (\"agg_1\", \"max\"), \"agg_2\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>         agg_1_min  agg_1_max     agg_2\ngroup_1          1          3  1.666667\ngroup_2          4          5       2.2\n</code></pre> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.head","title":"head","text":"<pre><code>head(n: int = 5)\n</code></pre> <p>Filter out all but the first n rows of data. If n is negative, return all rows except the last n rows.</p> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.optimise_for_memory","title":"optimise_for_memory","text":"<pre><code>optimise_for_memory()\n</code></pre> <p>Reduce peak memory usage during the query, at the expense of some performance.</p> <p>Optimisations applied:</p> <ul> <li>Memory used by strings that are present in segments read from storage, but are not required in the final dataframe that will be presented back to the user, is reclaimed earlier in the processing pipeline.</li> </ul>"},{"location":"api/processing/#arcticdb.QueryBuilder.optimise_for_speed","title":"optimise_for_speed","text":"<pre><code>optimise_for_speed()\n</code></pre> <p>Process query as fast as possible (the default behaviour)</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.prepend","title":"prepend","text":"<pre><code>prepend(other)\n</code></pre> <p>Applies processing specified in other before any processing already defined for this QueryBuilder.</p> PARAMETER DESCRIPTION <code>other</code> <p>QueryBuilder to apply before this one in the processing pipeline.</p> <p> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.resample","title":"resample","text":"<pre><code>resample(\n    rule: Union[str, DateOffset],\n    closed: Optional[str] = None,\n    label: Optional[str] = None,\n    offset: Optional[Union[str, Timedelta]] = None,\n    origin: Union[str, Timestamp] = \"epoch\",\n)\n</code></pre> <p>Resample a symbol on the index. The symbol must be datetime indexed. Resample operations must be followed by an aggregation operator. Currently, the following 7 aggregation operators are supported:</p> <ul> <li>\"mean\" - compute the mean of the group</li> <li>\"sum\" - compute the sum of the group</li> <li>\"min\" - compute the min of the group</li> <li>\"max\" - compute the max of the group</li> <li>\"count\" - compute the count of group</li> <li>\"first\" - compute the first value in the group</li> <li>\"last\" - compute the last value in the group</li> </ul> <p>Note that not all aggregators are supported with all column types:</p> <ul> <li>Numeric columns - support all aggregators</li> <li>Bool columns - support all aggregators</li> <li>String columns - support count, first, and last aggregators</li> <li>Datetime columns - support all aggregators EXCEPT sum</li> </ul> <p>Note that time-buckets which contain no index values in the symbol will NOT be included in the returned DataFrame. This is not the same as Pandas default behaviour. Resampling is currently not supported with:</p> <ul> <li>Dynamic schema where an aggregation column is missing from one or more of the row-slices.</li> <li>Sparse data.</li> </ul> <p>The resample results match pandas resample with <code>origin=\"epoch\"</code>. We plan to add an 'origin' argument in a future release and will then change the default value to '\"start_day\"' to match the Pandas default. This will change the results in cases where the rule is not a multiple of 24 hours.</p> PARAMETER DESCRIPTION <code>rule</code> <p>The frequency at which to resample the data. Supported rule strings are ns, us, ms, s, min, h, and D, and multiples/combinations of these, such as 1h30min. pd.DataOffset objects representing frequencies from this set are also accepted.</p> <p> TYPE: <code>Union[str, DateOffset]</code> </p> <code>closed</code> <p>Which boundary of each time-bucket is closed. Must be one of 'left' or 'right'. If not provided, the default is left for all currently supported frequencies.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>label</code> <p>Which boundary of each time-bucket is used as the index value in the returned DataFrame. Must be one of 'left' or 'right'. If not provided, the default is left for all currently supported frequencies.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>offset</code> <p>Offset the start of each bucket. Supported strings are the same as in <code>pd.Timedelta</code>. If offset is larger than rule then <code>offset</code> modulo <code>rule</code> is used as an offset.</p> <p> TYPE: <code>Optional[Union[str, Timedelta]]</code> DEFAULT: <code>None</code> </p> <code>origin</code> <p>The timestamp on which to adjust the grouping. Supported string are:</p> <ul> <li>epoch: origin is 1970-01-01</li> <li>start: origin is the first value of the timeseries</li> <li>start_day: origin is the first day at midnight of the timeseries</li> <li>end: origin is the last value of the timeseries</li> <li>end_day: origin is the ceiling midnight of the last day</li> </ul> <p><code>start</code>, <code>start_day</code>, <code>end</code>, <code>end_day</code> origin values are not supported in conjunction with <code>date_range</code>.</p> <p> TYPE: <code>Union[str, Timestamp]</code> DEFAULT: <code>'epoch'</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p> RAISES DESCRIPTION <code>ArcticDbNotYetImplemented</code> <p>A frequency string or Pandas DateOffset object are provided to the rule argument outside the supported frequencies listed above.</p> <code>ArcticNativeException</code> <p>The closed or label arguments are not one of \"left\" or \"right\"</p> <code>SchemaException</code> <p>Raised on call to read if:</p> <ul> <li>If the aggregation specified is not compatible with the type of the column being aggregated as   specified above.</li> <li>The library has dynamic schema enabled, and at least one of the columns being aggregated is missing   from at least one row-slice.</li> <li>At least one of the columns being aggregated contains sparse data.</li> </ul> <code>UserInputException</code> <ul> <li><code>start</code>, <code>start_day</code>, <code>end</code>, <code>end_day</code> is used in conjunction with <code>date_range</code></li> <li><code>origin</code> is not one of <code>start</code>, <code>start_day</code>, <code>end</code>, <code>end_day</code>, <code>epoch</code> or a <code>pd.Timestamp</code></li> </ul> <p>Examples:</p> <p>Resample two hours worth of minutely data down to hourly data, summing the column 'to_sum':</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"to_sum\": np.arange(120),\n    },\n    index=pd.date_range(\"2024-01-01\", freq=\"min\", periods=120),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\").agg({\"to_sum\": \"sum\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     to_sum\n2024-01-01 00:00:00    1770\n2024-01-01 01:00:00    5370\n</code></pre> <p>As above, but specifying that the closed boundary of each time-bucket is the right hand side, and also to label the output by the right boundary:</p> <pre><code>&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\", closed=\"right\", label=\"right\").agg({\"to_sum\": \"sum\"})\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     to_sum\n2024-01-01 00:00:00       0\n2024-01-01 01:00:00    1830\n2024-01-01 02:00:00    5310\n</code></pre> <p>Nones, NaNs, and NaTs are omitted from aggregations:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"to_mean\": [1.0, np.nan, 2.0],\n    },\n    index=pd.date_range(\"2024-01-01\", freq=\"min\", periods=3),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\").agg({\"to_mean\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     to_mean\n2024-01-01 00:00:00      1.5\n</code></pre> <p>Output column names can be controlled through the format of the dict passed to agg:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n    {\n        \"agg_1\": [1, 2, 3, 4, 5],\n        \"agg_2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n    },\n    index=pd.date_range(\"2024-01-01\", freq=\"min\", periods=5),\n)\n&gt;&gt;&gt; q = adb.QueryBuilder()\n&gt;&gt;&gt; q = q.resample(\"h\")\n&gt;&gt;&gt; q = q.agg({\"agg_1_min\": (\"agg_1\", \"min\"), \"agg_1_max\": (\"agg_1\", \"max\"), \"agg_2\": \"mean\"})\n&gt;&gt;&gt; lib.write(\"symbol\", df)\n&gt;&gt;&gt; lib.read(\"symbol\", query_builder=q).data\n</code></pre> <pre><code>                     agg_1_min  agg_1_max     agg_2\n2024-01-01 00:00:00          1          5      2.75\n</code></pre>"},{"location":"api/processing/#arcticdb.QueryBuilder.row_range","title":"row_range","text":"<pre><code>row_range(row_range: Tuple[int, int])\n</code></pre> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound. Should behave the same as df.iloc[start:end], including in the handling of negative start/end values.</p> PARAMETER DESCRIPTION <code>row_range</code> <p>Row range to read data for. Inclusive of the lower bound, exclusive of the upper bound.</p> <p> TYPE: <code>Tuple[int, int]</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.tail","title":"tail","text":"<pre><code>tail(n: int = 5)\n</code></pre> <p>Filter out all but the last n rows of data. If n is negative, return all rows except the first n rows.</p> PARAMETER DESCRIPTION <code>n</code> <p>Number of rows to select if non-negative, otherwise number of rows to exclude.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"api/processing/#arcticdb.QueryBuilder.then","title":"then","text":"<pre><code>then(other)\n</code></pre> <p>Applies processing specified in other after any processing already defined for this QueryBuilder.</p> PARAMETER DESCRIPTION <code>other</code> <p>QueryBuilder to apply after this one in the processing pipeline.</p> <p> </p> RETURNS DESCRIPTION <code>QueryBuilder</code> <p>Modified QueryBuilder object.</p>"},{"location":"notebooks/","title":"Creating shareable notebooks","text":"<p>To make the notebooks in this folder portable encode any images as PNG and in base64 and include as a Data URI.</p> <p>Issues with other approaches, - Linking to images     Given we want users to open our demo notebooks in their preferred environment, a linked image may not be displayed, and if it is it requires us to maintain hosting of that image. - Embedded attachments     This doesn't seem to be well supported by third party viewers, e.g. in Github and Google Colab. - Embedded SVG files     Doesn't work in Github in a Markdown cell</p>"},{"location":"notebooks/#png-example","title":"PNG example","text":"<p>It's good to keep the images small for this, modern browsers support large \"Data URLs\" but it can be annoying as a user users, if they open the cell it will become very large and hard to scroll, etc.</p> <p>The resulting Markdown cells should contain an image tag that looks something like this small example. <pre><code>&lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==\" alt=\"Red dot\" /&gt;\n</code></pre></p>"},{"location":"notebooks/#base64-encoding","title":"Base64 encoding","text":"<p>It's important to disable line wrapping with <code>-w0</code>, Google Colab doesn't handle newlines within base64 encodings, though github does. </p> <pre><code>base64 -w0 images/ArcticDBLogo.svg\n</code></pre>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/","title":"AWS Blockchain Notebook","text":"Loading AWS Bitcoin blockchain data into ArcticDB, using AWS as storage In\u00a0[\u00a0]: Copied! <pre># s3fs is used by pandas.read_parquet('s3://...')\n%pip install arcticdb boto3 tqdm s3fs fastparquet\n</pre> # s3fs is used by pandas.read_parquet('s3://...') %pip install arcticdb boto3 tqdm s3fs fastparquet In\u00a0[2]: Copied! <pre>import os\nfrom uuid import uuid4\nfrom datetime import timedelta, datetime\nfrom tqdm import tqdm\nimport boto3\nimport numpy as np\nimport pandas as pd\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\nimport arcticdb as adb\nfrom google.colab import drive, userdata\n</pre> import os from uuid import uuid4 from datetime import timedelta, datetime from tqdm import tqdm import boto3 import numpy as np import pandas as pd from botocore import UNSIGNED from botocore.client import Config import arcticdb as adb from google.colab import drive, userdata In\u00a0[3]: Copied! <pre># mount Google Drive for the config file to live on\ndrive.mount('/content/drive')\npath = '/content/drive/MyDrive/config/awscli.ini'\nos.environ['AWS_SHARED_CREDENTIALS_FILE'] = path\n</pre> # mount Google Drive for the config file to live on drive.mount('/content/drive') path = '/content/drive/MyDrive/config/awscli.ini' os.environ['AWS_SHARED_CREDENTIALS_FILE'] = path <pre>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n</pre> In\u00a0[4]: Copied! <pre>check = boto3.session.Session()\nno_config = check.get_credentials() is None or check.region_name is None\n\nif no_config:\n    print('*'*40)\n    print('Setup your AWS S3 credentials and region before continuing.')\n    print('https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html')\n    print('*'*40)\n</pre> check = boto3.session.Session() no_config = check.get_credentials() is None or check.region_name is None  if no_config:     print('*'*40)     print('Setup your AWS S3 credentials and region before continuing.')     print('https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html')     print('*'*40) In\u00a0[5]: Copied! <pre>aws_access_key = \"my_access_key\"\naws_secret_access_key = \"my_secret_access_key\"\nregion = \"my_region\"\n\nconfig_text = f\"\"\"\n[default]\naws_access_key_id = {aws_access_key}\naws_secret_access_key = {aws_secret_access_key}\nregion = {region}\n\"\"\"\n\nwrite_aws_config_file = False\nif write_aws_config_file:\n    with open(path, 'w') as f:\n        f.write(text)\n</pre> aws_access_key = \"my_access_key\" aws_secret_access_key = \"my_secret_access_key\" region = \"my_region\"  config_text = f\"\"\" [default] aws_access_key_id = {aws_access_key} aws_secret_access_key = {aws_secret_access_key} region = {region} \"\"\"  write_aws_config_file = False if write_aws_config_file:     with open(path, 'w') as f:         f.write(text) In\u00a0[6]: Copied! <pre>s3 = boto3.resource('s3')\nregion = boto3.session.Session().region_name\n\nbucket = [b for b in s3.buckets.all() if b.name.startswith('arcticdb-data-')]\n\nif bucket:\n    bucket_name = bucket[0].name\n    print('Bucket found:', bucket_name)\nelse:\n    bucket_name = f'arcticdb-data-{uuid4()}'\n    s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint':region})\n    print('Bucket created:', bucket_name)\n</pre> s3 = boto3.resource('s3') region = boto3.session.Session().region_name  bucket = [b for b in s3.buckets.all() if b.name.startswith('arcticdb-data-')]  if bucket:     bucket_name = bucket[0].name     print('Bucket found:', bucket_name) else:     bucket_name = f'arcticdb-data-{uuid4()}'     s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint':region})     print('Bucket created:', bucket_name) <pre>Bucket found: arcticdb-data-bda6914b-2715-4acd-8b52-fa593af295bd\n</pre> In\u00a0[7]: Copied! <pre># create an arcticdb instance in the bucket\narctic = adb.Arctic(f's3://s3.{region}.amazonaws.com:{bucket_name}?aws_auth=true')\n\nif 'btc' not in arctic.list_libraries():\n    # library does not already exist\n    arctic.create_library('btc', library_options=adb.LibraryOptions(dynamic_schema=True))\nlibrary = arctic.get_library('btc')\nlibrary\n</pre> # create an arcticdb instance in the bucket arctic = adb.Arctic(f's3://s3.{region}.amazonaws.com:{bucket_name}?aws_auth=true')  if 'btc' not in arctic.list_libraries():     # library does not already exist     arctic.create_library('btc', library_options=adb.LibraryOptions(dynamic_schema=True)) library = arctic.get_library('btc') library Out[7]: <pre>Library(Arctic(config=S3(endpoint=s3.eu-north-1.amazonaws.com, bucket=arcticdb-data-bda6914b-2715-4acd-8b52-fa593af295bd)), path=btc, storage=s3_storage)</pre> In\u00a0[8]: Copied! <pre># create the list of all btc blockchain files\nbucket = s3.Bucket('aws-public-blockchain')\nobjects = bucket.objects.filter(Prefix='v1.0/btc/transactions/')\nfiles = pd.DataFrame({'path': [obj.key for obj in objects]})\n</pre> # create the list of all btc blockchain files bucket = s3.Bucket('aws-public-blockchain') objects = bucket.objects.filter(Prefix='v1.0/btc/transactions/') files = pd.DataFrame({'path': [obj.key for obj in objects]}) In\u00a0[9]: Copied! <pre># filter only the 2023-06 files to keep run time manageable\nfiles_mask = files['path'].str.contains('2023-06')\nto_load = files[files_mask]['path']\nprint(f\"Identified {len(to_load)} / {len(files)} files for processing\")\n</pre> # filter only the 2023-06 files to keep run time manageable files_mask = files['path'].str.contains('2023-06') to_load = files[files_mask]['path'] print(f\"Identified {len(to_load)} / {len(files)} files for processing\") <pre>Identified 30 / 5506 files for processing\n</pre> In\u00a0[10]: Copied! <pre>%%time\ndf_list = []\nfor path in tqdm(to_load):\n    one_day_df = pd.read_parquet('s3://aws-public-blockchain/'+path,\n                                 storage_options={\"anon\": True},\n                                 engine='fastparquet')\n    # fixup types from source data\n    one_day_df['hash'] =  one_day_df['hash'].astype(str)\n    one_day_df['block_hash'] = one_day_df['block_hash'].astype(str)\n    one_day_df['outputs'] = one_day_df['outputs'].astype(str)\n    one_day_df['date'] = pd.to_datetime(one_day_df['date'], unit='ns')\n    if 'inputs' in one_day_df.columns:\n        one_day_df['inputs'] = one_day_df['inputs'].astype(str)\n    # index on timestamp\n    one_day_df.set_index('block_timestamp', inplace=True)\n    one_day_df.sort_index(inplace=True)\n    df_list.append(one_day_df)\ndf_aws = pd.concat(df_list).sort_index()\nprint(f\"Read and assembled {len(df_aws)} transaction records from AWS\")\n# release the list to enable garbage collection\ndf_list = None\n</pre> %%time df_list = [] for path in tqdm(to_load):     one_day_df = pd.read_parquet('s3://aws-public-blockchain/'+path,                                  storage_options={\"anon\": True},                                  engine='fastparquet')     # fixup types from source data     one_day_df['hash'] =  one_day_df['hash'].astype(str)     one_day_df['block_hash'] = one_day_df['block_hash'].astype(str)     one_day_df['outputs'] = one_day_df['outputs'].astype(str)     one_day_df['date'] = pd.to_datetime(one_day_df['date'], unit='ns')     if 'inputs' in one_day_df.columns:         one_day_df['inputs'] = one_day_df['inputs'].astype(str)     # index on timestamp     one_day_df.set_index('block_timestamp', inplace=True)     one_day_df.sort_index(inplace=True)     df_list.append(one_day_df) df_aws = pd.concat(df_list).sort_index() print(f\"Read and assembled {len(df_aws)} transaction records from AWS\") # release the list to enable garbage collection df_list = None <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [02:50&lt;00:00,  5.67s/it]\n</pre> <pre>Read and assembled 12147125 transaction records from AWS\nCPU times: user 31.8 s, sys: 9.58 s, total: 41.4 s\nWall time: 2min 59s\n</pre> In\u00a0[11]: Copied! <pre>%%time\nlibrary.write('transactions', df_aws)\n</pre> %%time library.write('transactions', df_aws) <pre>CPU times: user 27.2 s, sys: 4.47 s, total: 31.6 s\nWall time: 45.3 s\n</pre> Out[11]: <pre>VersionedItem(symbol='transactions', library='btc', data=n/a, version=1487, metadata=None, host='S3(endpoint=s3.eu-north-1.amazonaws.com, bucket=arcticdb-data-bda6914b-2715-4acd-8b52-fa593af295bd)')</pre> In\u00a0[12]: Copied! <pre>%%time\nplot_start = datetime(2023, 6, 3, 0, 0)\nplot_end = plot_start + timedelta(days=25)\ndf = library.read('transactions', date_range=(plot_start, plot_end)).data\nprint(len(df))\n</pre> %%time plot_start = datetime(2023, 6, 3, 0, 0) plot_end = plot_start + timedelta(days=25) df = library.read('transactions', date_range=(plot_start, plot_end)).data print(len(df)) <pre>10097090\nCPU times: user 4.31 s, sys: 3.19 s, total: 7.51 s\nWall time: 21.1 s\n</pre> In\u00a0[13]: Copied! <pre>fees_per_day = df.groupby(pd.Grouper(freq='1D')).sum(numeric_only=True)\nt = f\"BTC Blockchain: Total fees per pay from {plot_start} to {plot_end}\"\nax = fees_per_day.plot(kind='bar', y='fee', color='red', figsize=(14, 6), title=t)\nax.figure.autofmt_xdate(rotation=60)\n</pre> fees_per_day = df.groupby(pd.Grouper(freq='1D')).sum(numeric_only=True) t = f\"BTC Blockchain: Total fees per pay from {plot_start} to {plot_end}\" ax = fees_per_day.plot(kind='bar', y='fee', color='red', figsize=(14, 6), title=t) ax.figure.autofmt_xdate(rotation=60) In\u00a0[13]: Copied! <pre>\n</pre>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#in-this-demo-we-illustrate-how-to-use-aws-with-arcticdb-we-are-going-to","title":"In this demo, we illustrate how to use AWS with ArcticDB. We are going to\u00b6","text":"<ul> <li>Set up AWS access</li> <li>Initialise ArcticDB with AWS as storage</li> <li>Read a section of the Bitcoin blockchain from an AWS public dataset</li> <li>Store the data in ArcticDB</li> <li>Read the data back</li> <li>Perform a simple analysis on the data</li> </ul> <p>Note: This is set up to run on Google colab. It will need some simple changes to remove the Google drive code to run in other enviroments</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#install-arcticdb-and-s3-libraries","title":"Install ArcticDB and S3 libraries\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#read-or-create-aws-config","title":"Read or Create AWS config\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#create-a-config-file","title":"Create a config file\u00b6","text":"<ul> <li>You should only need to run this section once</li> <li>Enter your AWS details below and change <code>write_aws_config_file</code> to True</li> <li>Future runs can pick up the config file you have save in your Drive</li> </ul>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#set-up-the-aws-bucket","title":"Set up the AWS bucket\u00b6","text":"<ul> <li>First check for existing buckets to use</li> <li>Set up a new bucket if there are no suitable existing ones</li> </ul>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#initialise-arcticdb","title":"Initialise ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#mark-the-btc-blockchain-data-from-june-2023-for-processing","title":"Mark the BTC blockchain data from June 2023 for processing\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#read-the-data-from-an-aws-public-dataset-as-parquet-files","title":"Read the data from an AWS public dataset as parquet files\u00b6","text":"<p>This takes some time to run, like 5 to 10 mins</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#write-the-data-to-arcticdb","title":"Write the data to ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#read-the-data-from-arcticdb","title":"Read the data from ArcticDB\u00b6","text":"<p>This read also applies a date_range filter to get 25 days of data from 3 June</p>"},{"location":"notebooks/ArcticDB_aws_public_blockchain/#chart-the-transaction-fees-per-day","title":"Chart the transaction fees per day\u00b6","text":""},{"location":"notebooks/ArcticDB_aws_public_blockchain/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>We have give a simple recipe for using ArcticDB with AWS for storage</li> <li>We have demonstrated that ArcticDB is significantly faster than Parquet files</li> <li>We have shown how to read a subset of dates from a symbol of timeseries data</li> <li>If we saved a much larger section of the blockchain, it could still be stored in one symbol and subset chunks read efficiently</li> <li>Feel free to play around with the notebook to read a larger set of data</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/","title":"1 Billion Row Challenge Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[1]: Copied! <pre>from arcticdb_ext import set_config_int\nset_config_int('VersionStore.NumCPUThreads', 16)\n</pre> from arcticdb_ext import set_config_int set_config_int('VersionStore.NumCPUThreads', 16) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport arcticdb as adb\n</pre> import pandas as pd import numpy as np import arcticdb as adb In\u00a0[3]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_brc\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_brc\") In\u00a0[\u00a0]: Copied! <pre>sym_1brc = 'weather_stations_1brc'\nnum_cities = 10_000\nnum_rows = 1_000_000_000\naggs = {\n    'max': ('Temperature', 'max'), \n    'min': ('Temperature', 'min'),\n    'mean': ('Temperature', 'mean')\n}\nnum_blocks = 16\nblock_size = num_rows // num_blocks\nseed = 17\ncities = np.array([f\"city_{i:04d}\" for i in range(num_cities)])\nprint(f\"block_size: {block_size:,d}, total records: {block_size*num_blocks:,d}\")\n</pre> sym_1brc = 'weather_stations_1brc' num_cities = 10_000 num_rows = 1_000_000_000 aggs = {     'max': ('Temperature', 'max'),      'min': ('Temperature', 'min'),     'mean': ('Temperature', 'mean') } num_blocks = 16 block_size = num_rows // num_blocks seed = 17 cities = np.array([f\"city_{i:04d}\" for i in range(num_cities)]) print(f\"block_size: {block_size:,d}, total records: {block_size*num_blocks:,d}\") In\u00a0[17]: Copied! <pre>lib_name = 'arcticdb_brc'\n# delete the library if it already exists\narctic.delete_library(lib_name)\n# performance tuning: a large rows_per_segment value can improve performance for dataframes with a large number of rows\nlib_options = adb.LibraryOptions(rows_per_segment=10_000_000)\nlib = arctic.get_library(lib_name, create_if_missing=True, library_options=lib_options)\n</pre> lib_name = 'arcticdb_brc' # delete the library if it already exists arctic.delete_library(lib_name) # performance tuning: a large rows_per_segment value can improve performance for dataframes with a large number of rows lib_options = adb.LibraryOptions(rows_per_segment=10_000_000) lib = arctic.get_library(lib_name, create_if_missing=True, library_options=lib_options) In\u00a0[18]: Copied! <pre>def create_block_df(rng, cities, block_size):\n    random_cities = rng.choice(cities, size=block_size)\n    random_temperatures = np.round(rng.uniform(-99.9, 99.9, size=block_size), 4)\n    return pd.DataFrame({'City': random_cities, 'Temperature': random_temperatures})\n</pre> def create_block_df(rng, cities, block_size):     random_cities = rng.choice(cities, size=block_size)     random_temperatures = np.round(rng.uniform(-99.9, 99.9, size=block_size), 4)     return pd.DataFrame({'City': random_cities, 'Temperature': random_temperatures}) In\u00a0[\u00a0]: Copied! <pre>rng = np.random.default_rng(seed)\nprint('Writing blocks: ', end='')\nfor b in range(num_blocks):\n    block_df = create_block_df(rng, cities, block_size)\n    if b==0:\n        lib.write(sym_1brc, block_df)\n    else:\n        lib.append(sym_1brc, block_df, validate_index=False)\n    print(f'{b}, ', end='')\nprint(' Finished')\n</pre> rng = np.random.default_rng(seed) print('Writing blocks: ', end='') for b in range(num_blocks):     block_df = create_block_df(rng, cities, block_size)     if b==0:         lib.write(sym_1brc, block_df)     else:         lib.append(sym_1brc, block_df, validate_index=False)     print(f'{b}, ', end='') print(' Finished') In\u00a0[20]: Copied! <pre>%%timeit\n# this runs the query several times to get an accurate timing\nlazy_df = lib.read(sym_1brc, lazy=True)\nlazy_df.groupby('City').agg(aggs)\nlazy_df.collect()\n</pre> %%timeit # this runs the query several times to get an accurate timing lazy_df = lib.read(sym_1brc, lazy=True) lazy_df.groupby('City').agg(aggs) lazy_df.collect() <pre>7.01 s \u00b1 604 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</pre> In\u00a0[22]: Copied! <pre># run the query once more to see the output\nlib.read(sym_1brc, lazy=True).groupby('City').agg(aggs).collect().data.sort_index().round(1)\n</pre> # run the query once more to see the output lib.read(sym_1brc, lazy=True).groupby('City').agg(aggs).collect().data.sort_index().round(1) Out[22]: min mean max City city_0000 -99.9 0.0 99.8 city_0001 -99.9 0.2 99.9 city_0002 -99.7 0.2 99.9 city_0003 -99.9 0.1 99.9 city_0004 -99.8 -0.2 99.9 ... ... ... ... city_9995 -99.9 -0.4 99.9 city_9996 -99.9 0.1 99.9 city_9997 -99.9 0.7 99.9 city_9998 -99.9 -1.3 99.9 city_9999 -99.8 -0.2 99.9 <p>10000 rows \u00d7 3 columns</p> In\u00a0[23]: Copied! <pre># we need to aggregate sum and count to get the aggregated mean\naggs_chunked = {\n    'max': ('Temperature', 'max'), \n    'min': ('Temperature', 'min'),\n    'sum': ('Temperature', 'sum'),\n    'count': ('Temperature', 'count')\n}\n\n# define a list of ReadRequests - the chunks are based on row_ranges\nread_requests = [adb.ReadRequest(symbol=sym_1brc, \n                                 row_range=(block_size*b, block_size*(b+1)))\n                 for b in range(num_blocks)\n                ]\n</pre> # we need to aggregate sum and count to get the aggregated mean aggs_chunked = {     'max': ('Temperature', 'max'),      'min': ('Temperature', 'min'),     'sum': ('Temperature', 'sum'),     'count': ('Temperature', 'count') }  # define a list of ReadRequests - the chunks are based on row_ranges read_requests = [adb.ReadRequest(symbol=sym_1brc,                                   row_range=(block_size*b, block_size*(b+1)))                  for b in range(num_blocks)                 ] In\u00a0[24]: Copied! <pre># these functions merge the results of the chunks into one result\ndef merge_results_pair(r0, r1):\n    join_r = r0.join(r1, lsuffix='_0', rsuffix='_1')\n    return pd.DataFrame(index=join_r.index,\n                        data={\n                            'min': join_r[['min_0', 'min_1']].min(axis=1),\n                            'max': join_r[['max_0', 'max_1']].max(axis=1),\n                            'count': join_r[['count_0', 'count_1']].sum(axis=1),\n                            'sum': join_r[['sum_0', 'sum_1']].sum(axis=1),\n                        }\n                       )\n\ndef merge_results(r):\n    res = r[0].data.sort_index()\n    for b in range(1, len(r)):\n        next_res = r[b].data.sort_index()\n        res = merge_results_pair(res, next_res)\n    res['mean'] = res['sum'] / res['count']\n    res = res.drop(columns=['sum', 'count']).loc[:, ['min', 'mean', 'max']].round(1)\n    return res\n</pre> # these functions merge the results of the chunks into one result def merge_results_pair(r0, r1):     join_r = r0.join(r1, lsuffix='_0', rsuffix='_1')     return pd.DataFrame(index=join_r.index,                         data={                             'min': join_r[['min_0', 'min_1']].min(axis=1),                             'max': join_r[['max_0', 'max_1']].max(axis=1),                             'count': join_r[['count_0', 'count_1']].sum(axis=1),                             'sum': join_r[['sum_0', 'sum_1']].sum(axis=1),                         }                        )  def merge_results(r):     res = r[0].data.sort_index()     for b in range(1, len(r)):         next_res = r[b].data.sort_index()         res = merge_results_pair(res, next_res)     res['mean'] = res['sum'] / res['count']     res = res.drop(columns=['sum', 'count']).loc[:, ['min', 'mean', 'max']].round(1)     return res In\u00a0[25]: Copied! <pre>lazy_df_collection = lib.read_batch(read_requests, lazy=True)\n# Apply the same processing to each chunk\nlazy_df_collection.groupby('City').agg(aggs_chunked)\nread_results = lazy_df_collection.collect()\nresults = merge_results(read_results)\nresults\n</pre> lazy_df_collection = lib.read_batch(read_requests, lazy=True) # Apply the same processing to each chunk lazy_df_collection.groupby('City').agg(aggs_chunked) read_results = lazy_df_collection.collect() results = merge_results(read_results) results Out[25]: min mean max City city_0000 -99.9 0.0 99.8 city_0001 -99.9 0.2 99.9 city_0002 -99.7 0.2 99.9 city_0003 -99.9 0.1 99.9 city_0004 -99.8 -0.2 99.9 ... ... ... ... city_9995 -99.9 -0.4 99.9 city_9996 -99.9 0.1 99.9 city_9997 -99.9 0.7 99.9 city_9998 -99.9 -1.3 99.9 city_9999 -99.8 -0.2 99.9 <p>10000 rows \u00d7 3 columns</p>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#arcticdb-billion-row-challenge-notebook","title":"ArcticDB Billion Row Challenge Notebook\u00b6","text":""},{"location":"notebooks/ArcticDB_billion_row_challenge/#setup","title":"Setup\u00b6","text":"<ul> <li>installs</li> <li>imports</li> <li>create ArticDB object store</li> <li>define the parameters of the problem</li> <li>create an ArticDB library to hold the data</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#write-the-data-to-arcticdb","title":"Write the Data to ArcticDB\u00b6","text":"<ul> <li>Generate the data: each row has a city chosen at random from the list and a random temperature between -99.9 and 99.9</li> <li>Data is written in blocks to control memory usage</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#read-and-aggregate-the-data","title":"Read and Aggregate the Data\u00b6","text":"<ul> <li>Uses the DataFrame processing operations in ArcticDb to group and aggregate the data</li> <li>This allows the performant multi-threaded C++ layer to do the heavy lifting</li> <li>This code uses too much memory to run on the free Google colab. The chunked version below will work</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>It was easy to solve the 1 billion row challenge using ArticDB</li> <li>The code is short and easy to read</li> <li>Almost no tuning was needed to get good performance</li> </ul>"},{"location":"notebooks/ArcticDB_billion_row_challenge/#bonus-chunked-read-and-aggregate","title":"Bonus: Chunked Read and Aggregate\u00b6","text":"<ul> <li>This version reads and aggregates in chunks</li> <li>It has the same end result as the simpler version above</li> <li>It performs almost as well and needs less memory</li> <li>In particular, it will run within the memory on the free version of Google colab</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/","title":"Equity Analytics Notebook","text":"Using ArcticDB for equity analytics: a worked example In\u00a0[2]: Copied! <pre>!pip install arcticdb yfinance\n</pre> !pip install arcticdb yfinance In\u00a0[3]: Copied! <pre>import arcticdb as adb\nimport yfinance as yf\nimport pandas as pd\nfrom typing import List, Tuple\nfrom datetime import datetime \nimport matplotlib.pyplot as plt \n</pre> import arcticdb as adb import yfinance as yf import pandas as pd from typing import List, Tuple from datetime import datetime  import matplotlib.pyplot as plt  In\u00a0[4]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_equity\")\nlib = arctic.get_library('demo', create_if_missing=True)\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_equity\") lib = arctic.get_library('demo', create_if_missing=True) In\u00a0[24]: Copied! <pre>start = datetime(2013, 1, 1) \nend = datetime(2022, 12, 31)\nfreq = '1d'\nsymbols = {\n    '^GSPC': 'S&amp;P 500',\n    'AAPL': 'Apple',\n    'MSFT': 'Microsoft',\n    'GOOG': 'Google',\n    'AMZN': 'Amazon',\n    'NVDA': 'Nvidia',\n    'META': 'Meta',\n    'TSLA': 'Tesla',\n    'TSM': 'TSMC',\n    'TCEHY': 'Tencent',\n    '005930.KS': 'Samsung',\n    'ORCL': 'Oracle',\n    'ADBE': 'Adobe',\n    'ASML': 'ASML',\n    'CSCO': 'Cisco'\n}\n</pre> start = datetime(2013, 1, 1)  end = datetime(2022, 12, 31) freq = '1d' symbols = {     '^GSPC': 'S&amp;P 500',     'AAPL': 'Apple',     'MSFT': 'Microsoft',     'GOOG': 'Google',     'AMZN': 'Amazon',     'NVDA': 'Nvidia',     'META': 'Meta',     'TSLA': 'Tesla',     'TSM': 'TSMC',     'TCEHY': 'Tencent',     '005930.KS': 'Samsung',     'ORCL': 'Oracle',     'ADBE': 'Adobe',     'ASML': 'ASML',     'CSCO': 'Cisco' } In\u00a0[25]: Copied! <pre>hist = yf.download(list(symbols.keys()), interval=freq, start=start, end=end)\n</pre> hist = yf.download(list(symbols.keys()), interval=freq, start=start, end=end) <pre>[*********************100%%**********************]  15 of 15 completed\n</pre> In\u00a0[26]: Copied! <pre># the column levels[0] are the fields for each stock\nprint(hist.columns.levels[0])\nhist['Volume'].head(5)\n</pre> # the column levels[0] are the fields for each stock print(hist.columns.levels[0]) hist['Volume'].head(5) <pre>Index(['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')\n</pre> Out[26]: 005930.KS AAPL ADBE AMZN ASML CSCO GOOG META MSFT NVDA ORCL TCEHY TSLA TSM ^GSPC Date 2013-01-02 11449650.0 560518000.0 6483800.0 65420000.0 1824000.0 40304500.0 102033017.0 69846400.0 52899300.0 47883600.0 33758400.0 362500.0 17922000.0 10226100.0 4.202600e+09 2013-01-03 14227400.0 352965200.0 3906000.0 55018000.0 1725400.0 50603500.0 93075567.0 63140600.0 48294400.0 29888800.0 21819500.0 355000.0 11130000.0 13148600.0 3.829730e+09 2013-01-04 12999800.0 594333600.0 3809300.0 37484000.0 3170800.0 36378900.0 110954331.0 72715400.0 52521100.0 52496800.0 21687300.0 101000.0 10110000.0 7464200.0 3.424290e+09 2013-01-07 12610950.0 484156400.0 3632100.0 98200000.0 2066100.0 30790700.0 66476239.0 83781800.0 37110400.0 61073200.0 14008300.0 83000.0 6630000.0 9429900.0 3.304970e+09 2013-01-08 13822250.0 458707200.0 3080900.0 60214000.0 1182400.0 33218100.0 67295297.0 45871300.0 44703100.0 46642400.0 17408900.0 49000.0 19260000.0 8112900.0 3.601600e+09 In\u00a0[27]: Copied! <pre>def field_name_to_sym(field_name: str) -&gt; str:\n    return f\"hist/price_{field_name.replace(' ', '')}\"\n</pre> def field_name_to_sym(field_name: str) -&gt; str:     return f\"hist/price_{field_name.replace(' ', '')}\" In\u00a0[28]: Copied! <pre>for l in hist.columns.levels[0]:\n    lib.write(field_name_to_sym(l), hist[l])\n</pre> for l in hist.columns.levels[0]:     lib.write(field_name_to_sym(l), hist[l]) In\u00a0[29]: Copied! <pre># read back and check that the data round-trips precisely\nfor l in hist.columns.levels[0]:\n    hist_check_db = lib.read(field_name_to_sym(l)).data\n    if not hist[l].equals(hist_check_db):\n        print(f\"Field '{l}' does not round-trip\")\n</pre> # read back and check that the data round-trips precisely for l in hist.columns.levels[0]:     hist_check_db = lib.read(field_name_to_sym(l)).data     if not hist[l].equals(hist_check_db):         print(f\"Field '{l}' does not round-trip\") In\u00a0[30]: Copied! <pre>update_start = datetime(2022, 7, 1) \nupdate_end = datetime(2023, 12, 31)\nupdate_hist = yf.download(list(symbols.keys()), interval=freq, start=update_start, end=update_end)\n</pre> update_start = datetime(2022, 7, 1)  update_end = datetime(2023, 12, 31) update_hist = yf.download(list(symbols.keys()), interval=freq, start=update_start, end=update_end) <pre>[*********************100%%**********************]  15 of 15 completed\n</pre> In\u00a0[31]: Copied! <pre>for l in update_hist.columns.levels[0]:\n    lib.update(field_name_to_sym(l), update_hist[l])\n</pre> for l in update_hist.columns.levels[0]:     lib.update(field_name_to_sym(l), update_hist[l]) In\u00a0[32]: Copied! <pre># these are the symbols we have created\nlib.list_symbols()\n</pre> # these are the symbols we have created lib.list_symbols() Out[32]: <pre>['hist/price_High',\n 'hist/price_Low',\n 'hist/price_Volume',\n 'hist/price_Close',\n 'hist/price_AdjClose',\n 'hist/price_Open']</pre> In\u00a0[33]: Copied! <pre># each symbol contains data for one price field, with the stock tickers as columns and dates as the index\nlib.head(field_name_to_sym('Close')).data\n</pre> # each symbol contains data for one price field, with the stock tickers as columns and dates as the index lib.head(field_name_to_sym('Close')).data Out[33]: 005930.KS AAPL ADBE AMZN ASML CSCO GOOG META MSFT NVDA ORCL TCEHY TSLA TSM ^GSPC Date 2013-01-02 31520.0 19.608213 38.340000 12.8655 66.779999 20.340000 18.013729 28.000000 27.620001 3.1800 34.689999 6.720 2.357333 18.100000 1462.420044 2013-01-03 30860.0 19.360714 37.750000 12.9240 65.379997 20.450001 18.024191 27.770000 27.250000 3.1825 34.310001 6.660 2.318000 18.090000 1459.369995 2013-01-04 30500.0 18.821428 38.130001 12.9575 64.709999 20.480000 18.380356 28.760000 26.740000 3.2875 34.610001 6.694 2.293333 17.959999 1466.469971 2013-01-07 30400.0 18.710714 37.939999 13.4230 63.660000 20.290001 18.300158 29.420000 26.690001 3.1925 34.430000 6.600 2.289333 17.700001 1461.890015 2013-01-08 30000.0 18.761070 38.139999 13.3190 63.139999 20.309999 18.264042 29.059999 26.549999 3.1225 34.439999 6.570 2.245333 17.540001 1457.150024 In\u00a0[36]: Copied! <pre>hist_adj_close = lib.read(field_name_to_sym('Adj Close')).data\n</pre> hist_adj_close = lib.read(field_name_to_sym('Adj Close')).data In\u00a0[37]: Copied! <pre># ffill to remove nans (missing data)\nhist_adj_close_clean = hist_adj_close.ffill()\n# the following line will return True if there are any nans\nhist_adj_close_clean.isnull().any().any()\n</pre> # ffill to remove nans (missing data) hist_adj_close_clean = hist_adj_close.ffill() # the following line will return True if there are any nans hist_adj_close_clean.isnull().any().any() Out[37]: <pre>False</pre> In\u00a0[38]: Copied! <pre>hist_daily_returns = hist_adj_close_clean.pct_change(1).iloc[1:]\nhist_daily_returns.iloc[:5, :5]\n</pre> hist_daily_returns = hist_adj_close_clean.pct_change(1).iloc[1:] hist_daily_returns.iloc[:5, :5] Out[38]: 005930.KS AAPL ADBE AMZN ASML Date 2013-01-03 -0.020939 -0.012622 -0.015389 0.004547 -0.020964 2013-01-04 -0.011666 -0.027854 0.010066 0.002592 -0.010248 2013-01-07 -0.003278 -0.005882 -0.004983 0.035925 -0.016226 2013-01-08 -0.013158 0.002691 0.005272 -0.007748 -0.008168 2013-01-09 0.000000 -0.015629 0.013634 -0.000113 0.005702 In\u00a0[40]: Copied! <pre>returns_sym = 'hist/returns_AdjClose_clean'\nlib.write(returns_sym, hist_daily_returns)\n</pre> returns_sym = 'hist/returns_AdjClose_clean' lib.write(returns_sym, hist_daily_returns) Out[40]: <pre>VersionedItem(symbol='hist/returns_AdjClose_clean', library='demo', data=n/a, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_equity)')</pre> In\u00a0[41]: Copied! <pre>snapshot_name = f\"eod:{hist_daily_returns.iloc[-1].name.date()}\"\nprint(snapshot_name)\nif snapshot_name in lib.list_snapshots():\n    lib.delete_snapshot(snapshot_name)\nlib.snapshot(snapshot_name, metadata=\"EOD audit point\")\n</pre> snapshot_name = f\"eod:{hist_daily_returns.iloc[-1].name.date()}\" print(snapshot_name) if snapshot_name in lib.list_snapshots():     lib.delete_snapshot(snapshot_name) lib.snapshot(snapshot_name, metadata=\"EOD audit point\") <pre>eod:2023-11-17\n</pre> In\u00a0[42]: Copied! <pre>plot_start = datetime(2021, 1, 1)\nplot_end = datetime(2023, 12, 31)\nreturns_plot = lib.read(returns_sym, date_range=(plot_start, plot_end)).data\ndaily_cum_returns = ((1 + returns_plot).cumprod() - 1)\ndaily_cum_returns.rename(columns=symbols).plot(figsize=(20, 10), grid=True, linewidth=0.9, title=\"Daily Cumulative Stock Returns\")\n</pre> plot_start = datetime(2021, 1, 1) plot_end = datetime(2023, 12, 31) returns_plot = lib.read(returns_sym, date_range=(plot_start, plot_end)).data daily_cum_returns = ((1 + returns_plot).cumprod() - 1) daily_cum_returns.rename(columns=symbols).plot(figsize=(20, 10), grid=True, linewidth=0.9, title=\"Daily Cumulative Stock Returns\") Out[42]: <pre>&lt;Axes: title={'center': 'Daily Cumulative Stock Returns'}, xlabel='Date'&gt;</pre> In\u00a0[43]: Copied! <pre>index_ticker = \"^GSPC\"\nroll_days = 130\nbeta_start = datetime(2018, 1, 1)\nbeta_end = datetime(2022, 12, 31)\nbeta_returns = lib.read(returns_sym, date_range=(beta_start, beta_end)).data\nindex_returns = beta_returns[index_ticker]\nstock_returns = beta_returns.drop(columns=index_ticker)\n</pre> index_ticker = \"^GSPC\" roll_days = 130 beta_start = datetime(2018, 1, 1) beta_end = datetime(2022, 12, 31) beta_returns = lib.read(returns_sym, date_range=(beta_start, beta_end)).data index_returns = beta_returns[index_ticker] stock_returns = beta_returns.drop(columns=index_ticker) In\u00a0[44]: Copied! <pre>rolling_cov = stock_returns.rolling(roll_days).cov(index_returns).iloc[roll_days-1:]\nrolling_index_var = index_returns.rolling(roll_days).var().iloc[roll_days-1:]\nrolling_beta = rolling_cov.divide(rolling_index_var, axis='index').rename(columns=symbols)\n</pre> rolling_cov = stock_returns.rolling(roll_days).cov(index_returns).iloc[roll_days-1:] rolling_index_var = index_returns.rolling(roll_days).var().iloc[roll_days-1:] rolling_beta = rolling_cov.divide(rolling_index_var, axis='index').rename(columns=symbols) In\u00a0[45]: Copied! <pre>ax = rolling_beta.plot(figsize=(20, 10), grid=True, linewidth=0.9, title=f\"Rolling {roll_days}-day beta\")\nax.legend(loc='upper left')\n</pre> ax = rolling_beta.plot(figsize=(20, 10), grid=True, linewidth=0.9, title=f\"Rolling {roll_days}-day beta\") ax.legend(loc='upper left') Out[45]: <pre>&lt;matplotlib.legend.Legend at 0x7ef5d42b1fd0&gt;</pre>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#a-simple-workflow-for-equity-price-timeseries","title":"A Simple Workflow for Equity Price Timeseries\u00b6","text":"<p>In this notebook we:</p> <ul> <li>Source historical tech equity and S&amp;P 500 prices from Yahoo! using the yfinance package</li> <li>Store 10 years of daily price raw data in a temporary local ArcticDB library set up in the notebook</li> <li>The data sourcing is broken into two steps: an initial load of history then an update for recent prices</li> <li>This suggests a typical system workflow: an initial backfill and daily update with recent data</li> <li>Take Adjusted Close prices, remove missing data and calculate returns. Then save this to library</li> <li>Read the clean returns and use them to calculate rolling beta against the S&amp;P 500 for the stocks</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#notice-this-notebook-sources-data-from-yahoo","title":"Notice: This notebook sources data from Yahoo!\u00b6","text":"<p>Please read Yahoo terms here before using it</p> <p>https://policies.yahoo.com/us/en/yahoo/terms/index.htm</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#installs-and-imports","title":"Installs and Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#arcticdb-setup","title":"ArcticDB Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#source-historical-prices-from-yahoo","title":"Source Historical Prices from Yahoo!\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#write-the-data-to-the-library","title":"Write the Data to the Library\u00b6","text":"<p>Currently ArcticDB cannot store multi-level column dataframes directly, so we use a symbol for each price field.</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#update-with-recent-data","title":"Update with Recent Data\u00b6","text":"<p>Here we consume data for the recent past and slightly overlap the time window with the original data.</p> <p>In a real workflow this technique can be used to apply restatements to the price dataset.</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#read-and-process-the-whole-price-dataset","title":"Read and Process the Whole Price Dataset\u00b6","text":"<ul> <li>Read the data, using the Adj Close field as our primary price source</li> <li>Remove missing data by forward filling. A simple but unsophisticated method</li> <li>Calculate daily returns from the prices</li> <li>Write the returns back to ArcticDB as another symbol</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#end-of-the-data-processing-make-a-snapshot","title":"End of the Data Processing - Make a Snapshot\u00b6","text":"<p>The snapshot is optional but it can be useful to record the state of all the data and the end of the daily update process.</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#visualise-the-returns","title":"Visualise the Returns\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_analytics/#analysis-compute-rolling-betas-vs-the-sp-500","title":"Analysis: Compute Rolling Betas vs the S&amp;P 500\u00b6","text":"<p>Notice the big change in betas starting in Q2 2020 which rolls out of the betas 130 days later (the size of the rolling window).</p> <p>Possibly due to the large market moves at the start of the Covid era?</p>"},{"location":"notebooks/ArcticDB_demo_equity_analytics/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>We have demonstrated a simple data pipeline for capture and analysis</li> <li>Although simple, this pattern scales well to much larger datasets</li> <li>In a real system, the data collection and storage would be separated from the analysis</li> <li>Throughout the ArcticDB usage is simple, clean and clear</li> <li>Researchers and Data Scientists like the simplicity - it allows them to focus on the data and their research</li> </ul> <p>For more information about equity beta see https://en.wikipedia.org/wiki/Beta_(finance)</p>"},{"location":"notebooks/ArcticDB_demo_equity_options/","title":"Equity Options Notebook","text":"Using ArcticDB for equity options data: a worked example A Sample Workflow for Equity Options <p>In this notebook we will:</p> <ul> <li>Download (from github) a set of market data for options on a group of tech stocks</li> <li>Store the data in ArcticDB using an incremental timeseries update workflow</li> <li>Create a range of useful option queries</li> <li>Use the queries to drive an interactive chart</li> </ul> Thank you to optiondata.org <p>We would like to express our gratitude to optiondata.org for giving us permission to use a small slice of their free data in this demo.</p> <p>This data is free and available on their website for you to download yourself.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\nimport arcticdb as adb\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nimport functools\n</pre> %matplotlib inline import arcticdb as adb import pandas as pd from datetime import datetime import matplotlib.pyplot as plt import ipywidgets as widgets import functools In\u00a0[3]: Copied! <pre>branch = 'master'\nall_dates = ['2013-06-03', '2013-06-10', '2013-06-17', '2013-06-24']\ndelta_low = 0.05\ndelta_high = 0.55\n</pre> branch = 'master' all_dates = ['2013-06-03', '2013-06-10', '2013-06-17', '2013-06-24'] delta_low = 0.05 delta_high = 0.55 In\u00a0[4]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_equity_options\")\nlib = arctic.get_library('demo_options', create_if_missing=True)\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_equity_options\") lib = arctic.get_library('demo_options', create_if_missing=True) In\u00a0[12]: Copied! <pre>def gitub_url(as_of, branch):\n    return f\"https://raw.githubusercontent.com/man-group/ArcticDB/{branch}/docs/mkdocs/docs/notebooks/data/{as_of}tech-options.csv\"\n\n# read the csv from github\ndef read_github_options_file(as_of, branch='master'):\n    try:\n        raw_df = pd.read_csv(gitub_url(as_of, branch))\n    except Exception as e:\n        raise Exception(f\"Github access error: {e}\")\n    return raw_df.set_index(pd.DatetimeIndex(raw_df['quote_date'])).drop(columns=['Unnamed: 0'])\n\ndef uly_symbol(uly):\n    return f\"options/{uly}\"\n\n# query to get option expiries for an underlying\ndef options_expiries_query(lazy_df, as_of, underlying):\n    # exclude options expiring on as_of - no time value left\n    filter = (lazy_df['expiration'] != as_of)\n    lazy_df = lazy_df[filter].groupby('expiration').agg({'volume': 'sum'})\n    return lazy_df\n\ndef read_expiries(as_of, underlying):\n    read_date = pd.Timestamp(as_of)\n    sym = uly_symbol(underlying)\n    lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)\n    lazy_df = options_expiries_query(lazy_df, as_of, underlying)\n    return lazy_df.collect().data.sort_index().index.values\n\ndef read_all_underlyings():\n    # use the symbol list to get all underlyings\n    return sorted([s.split('/')[1] for s in lib.list_symbols()])\n\n# query to get all options for an expiry\n# as_of via date_range, uly via symbol\ndef options_curve_single_expiry_query(lazy_df, expiry):\n    filter = (lazy_df['expiration'] == expiry)\n    return lazy_df[filter]\n\n# query to get all options for an expiry with delta in a specified interval\n# calls have delta &gt;= 0, puts have delta &lt;= 0\ndef options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high):\n    filter = ((lazy_df['expiration'] == expiry) &amp;\n              (abs(lazy_df['delta']) &gt;= delta_low) &amp;\n              (abs(lazy_df['delta']) &lt;= delta_high)\n             )\n    return lazy_df[filter].groupby('strike').agg({'implied_volatility': 'mean'})\n\ndef read_vol_curve_single_expiry(as_of, underlying, expiry):\n    read_date = pd.Timestamp(as_of)\n    sym = uly_symbol(underlying)\n    lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)\n    lazy_df = options_curve_single_expiry_query(lazy_df, expiry)\n    return lazy_df.collect().data\n\n@functools.cache\ndef read_vol_curve_single_expiry_exclude_itm_otm(as_of, underlying, expiry, delta_low, delta_high):\n    read_date = pd.Timestamp(as_of)\n    sym = uly_symbol(underlying)\n    lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)\n    lazy_df = options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high)\n    vol_curve_raw = lazy_df.collect().data\n    return vol_curve_raw.sort_index()*100\n</pre> def gitub_url(as_of, branch):     return f\"https://raw.githubusercontent.com/man-group/ArcticDB/{branch}/docs/mkdocs/docs/notebooks/data/{as_of}tech-options.csv\"  # read the csv from github def read_github_options_file(as_of, branch='master'):     try:         raw_df = pd.read_csv(gitub_url(as_of, branch))     except Exception as e:         raise Exception(f\"Github access error: {e}\")     return raw_df.set_index(pd.DatetimeIndex(raw_df['quote_date'])).drop(columns=['Unnamed: 0'])  def uly_symbol(uly):     return f\"options/{uly}\"  # query to get option expiries for an underlying def options_expiries_query(lazy_df, as_of, underlying):     # exclude options expiring on as_of - no time value left     filter = (lazy_df['expiration'] != as_of)     lazy_df = lazy_df[filter].groupby('expiration').agg({'volume': 'sum'})     return lazy_df  def read_expiries(as_of, underlying):     read_date = pd.Timestamp(as_of)     sym = uly_symbol(underlying)     lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)     lazy_df = options_expiries_query(lazy_df, as_of, underlying)     return lazy_df.collect().data.sort_index().index.values  def read_all_underlyings():     # use the symbol list to get all underlyings     return sorted([s.split('/')[1] for s in lib.list_symbols()])  # query to get all options for an expiry # as_of via date_range, uly via symbol def options_curve_single_expiry_query(lazy_df, expiry):     filter = (lazy_df['expiration'] == expiry)     return lazy_df[filter]  # query to get all options for an expiry with delta in a specified interval # calls have delta &gt;= 0, puts have delta &lt;= 0 def options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high):     filter = ((lazy_df['expiration'] == expiry) &amp;               (abs(lazy_df['delta']) &gt;= delta_low) &amp;               (abs(lazy_df['delta']) &lt;= delta_high)              )     return lazy_df[filter].groupby('strike').agg({'implied_volatility': 'mean'})  def read_vol_curve_single_expiry(as_of, underlying, expiry):     read_date = pd.Timestamp(as_of)     sym = uly_symbol(underlying)     lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)     lazy_df = options_curve_single_expiry_query(lazy_df, expiry)     return lazy_df.collect().data  @functools.cache def read_vol_curve_single_expiry_exclude_itm_otm(as_of, underlying, expiry, delta_low, delta_high):     read_date = pd.Timestamp(as_of)     sym = uly_symbol(underlying)     lazy_df = lib.read(sym, date_range=(read_date, read_date), lazy=True)     lazy_df = options_curve_single_expiry_delta_bracket_query(lazy_df, expiry, delta_low, delta_high)     vol_curve_raw = lazy_df.collect().data     return vol_curve_raw.sort_index()*100 In\u00a0[13]: Copied! <pre>for d in all_dates:\n    df = read_github_options_file(d, branch)\n    underlyings = df['underlying'].unique()\n    print(f\"Date: {d} - {len(df)} records, {len(underlyings)} underlyings\")\n    for u in underlyings:\n        uly_df = df[df['underlying']==u]\n        sym = uly_symbol(u)\n        # upsert option creates the symbol if it doesn't already exist\n        lib.update(sym, uly_df, upsert=True)\n</pre> for d in all_dates:     df = read_github_options_file(d, branch)     underlyings = df['underlying'].unique()     print(f\"Date: {d} - {len(df)} records, {len(underlyings)} underlyings\")     for u in underlyings:         uly_df = df[df['underlying']==u]         sym = uly_symbol(u)         # upsert option creates the symbol if it doesn't already exist         lib.update(sym, uly_df, upsert=True) <pre>Date: 2013-06-03 - 6792 records, 11 underlyings\nDate: 2013-06-10 - 6622 records, 11 underlyings\nDate: 2013-06-17 - 6442 records, 11 underlyings\nDate: 2013-06-24 - 6134 records, 11 underlyings\n</pre> In\u00a0[14]: Copied! <pre># lets take a look at the symbols we have created - one for each underlying\nlib.list_symbols()\n</pre> # lets take a look at the symbols we have created - one for each underlying lib.list_symbols() Out[14]: <pre>['options/IBM',\n 'options/NVDA',\n 'options/MSFT',\n 'options/AAPL',\n 'options/CSCO',\n 'options/ASML',\n 'options/TSM',\n 'options/ADBE',\n 'options/AMZN',\n 'options/ORCL',\n 'options/TSLA']</pre> In\u00a0[15]: Copied! <pre>all_uly = read_all_underlyings()\nall_uly\n</pre> all_uly = read_all_underlyings() all_uly Out[15]: <pre>['AAPL',\n 'ADBE',\n 'AMZN',\n 'ASML',\n 'CSCO',\n 'IBM',\n 'MSFT',\n 'NVDA',\n 'ORCL',\n 'TSLA',\n 'TSM']</pre> In\u00a0[16]: Copied! <pre>def create_vol_curve_chart_single(ax, as_of, uly, delta_low, delta_high):\n    exp = read_expiries(as_of, uly)\n    cmap = plt.get_cmap('rainbow', len(exp))\n    format_kw = {'linewidth': 2, 'alpha': 0.85}\n    for i, e in enumerate(exp):\n        vol_curve = read_vol_curve_single_expiry_exclude_itm_otm(as_of, uly, e, delta_low, delta_high)\n        vol_curve.plot(ax=ax, y='implied_volatility', label=e, grid=True, color=cmap(i), **format_kw)\n    ax.set_title(f\"Option Volatility Curves for {uly}, as of {as_of}\")\n    ax.set_ylabel(\"implied volatility\")\n    ax.legend(loc='upper right', framealpha=0.7)\n    ax.set_facecolor('whitesmoke')\n\ndef create_vol_curve_chart(as_of1, uly1, as_of2, uly2, delta_low, delta_high):\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n    create_vol_curve_chart_single(ax1, as_of1, uly1, delta_low, delta_high)\n    create_vol_curve_chart_single(ax2, as_of2, uly2, delta_low, delta_high)\n\nclass input_widgets(object):\n    def __init__(self):\n        self.container = widgets.VBox()\n        self.create_widgets()\n        self.redraw_chart()\n\n    @property\n    def as_of1(self):\n        return self.container.children[0].children[0].value\n\n    @property\n    def uly1(self):\n        return self.container.children[0].children[1].value\n\n    @property\n    def as_of2(self):\n        return self.container.children[1].children[0].value\n\n    @property\n    def uly2(self):\n        return self.container.children[1].children[1].value\n\n    @property\n    def out(self):\n        return self.container.children[2]\n\n    def create_widgets(self):\n        self.as_of1_dd = widgets.Dropdown(\n            options=all_dates,\n            value=all_dates[0],\n            description='Date1:',\n            disabled=False,\n        )\n        self.as_of1_dd.observe(self._on_change, ['value'])\n\n        self.as_of2_dd = widgets.Dropdown(\n            options=all_dates,\n            value=all_dates[0],\n            description='Date2:',\n            disabled=False,\n        )\n        self.as_of2_dd.observe(self._on_change, ['value'])\n\n        self.uly1_dd = widgets.Dropdown(\n            options=all_uly,\n            value=all_uly[0],\n            description='Underlying1:',\n            disabled=False,\n        )\n        self.uly1_dd.observe(self._on_change, ['value'])\n\n        self.uly2_dd = widgets.Dropdown(\n            options=all_uly,\n            value=all_uly[1],\n            description='Underlying2:',\n            disabled=False,\n        )\n        self.uly2_dd.observe(self._on_change, ['value'])\n\n        self.output_widget = widgets.Output(layout=widgets.Layout(height='900px'))\n\n        self.container.children = [\n            widgets.HBox([self.as_of1_dd, self.uly1_dd]),\n            widgets.HBox([self.as_of2_dd, self.uly2_dd]),\n            self.output_widget\n        ]\n\n    def _on_change(self, _):\n        self.redraw_chart()\n\n    def redraw_chart(self):\n        with self.output_widget:\n            self.output_widget.clear_output(wait=True)\n            create_vol_curve_chart(self.as_of1, self.uly1, self.as_of2, self.uly2, delta_low, delta_high)\n            plt.show()\n</pre> def create_vol_curve_chart_single(ax, as_of, uly, delta_low, delta_high):     exp = read_expiries(as_of, uly)     cmap = plt.get_cmap('rainbow', len(exp))     format_kw = {'linewidth': 2, 'alpha': 0.85}     for i, e in enumerate(exp):         vol_curve = read_vol_curve_single_expiry_exclude_itm_otm(as_of, uly, e, delta_low, delta_high)         vol_curve.plot(ax=ax, y='implied_volatility', label=e, grid=True, color=cmap(i), **format_kw)     ax.set_title(f\"Option Volatility Curves for {uly}, as of {as_of}\")     ax.set_ylabel(\"implied volatility\")     ax.legend(loc='upper right', framealpha=0.7)     ax.set_facecolor('whitesmoke')  def create_vol_curve_chart(as_of1, uly1, as_of2, uly2, delta_low, delta_high):     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))     create_vol_curve_chart_single(ax1, as_of1, uly1, delta_low, delta_high)     create_vol_curve_chart_single(ax2, as_of2, uly2, delta_low, delta_high)  class input_widgets(object):     def __init__(self):         self.container = widgets.VBox()         self.create_widgets()         self.redraw_chart()      @property     def as_of1(self):         return self.container.children[0].children[0].value      @property     def uly1(self):         return self.container.children[0].children[1].value      @property     def as_of2(self):         return self.container.children[1].children[0].value      @property     def uly2(self):         return self.container.children[1].children[1].value      @property     def out(self):         return self.container.children[2]      def create_widgets(self):         self.as_of1_dd = widgets.Dropdown(             options=all_dates,             value=all_dates[0],             description='Date1:',             disabled=False,         )         self.as_of1_dd.observe(self._on_change, ['value'])          self.as_of2_dd = widgets.Dropdown(             options=all_dates,             value=all_dates[0],             description='Date2:',             disabled=False,         )         self.as_of2_dd.observe(self._on_change, ['value'])          self.uly1_dd = widgets.Dropdown(             options=all_uly,             value=all_uly[0],             description='Underlying1:',             disabled=False,         )         self.uly1_dd.observe(self._on_change, ['value'])          self.uly2_dd = widgets.Dropdown(             options=all_uly,             value=all_uly[1],             description='Underlying2:',             disabled=False,         )         self.uly2_dd.observe(self._on_change, ['value'])          self.output_widget = widgets.Output(layout=widgets.Layout(height='900px'))          self.container.children = [             widgets.HBox([self.as_of1_dd, self.uly1_dd]),             widgets.HBox([self.as_of2_dd, self.uly2_dd]),             self.output_widget         ]      def _on_change(self, _):         self.redraw_chart()      def redraw_chart(self):         with self.output_widget:             self.output_widget.clear_output(wait=True)             create_vol_curve_chart(self.as_of1, self.uly1, self.as_of2, self.uly2, delta_low, delta_high)             plt.show() In\u00a0[17]: Copied! <pre>w = input_widgets()\nw.container\n</pre> w = input_widgets() w.container Out[17]: <pre>VBox(children=(HBox(children=(Dropdown(description='Date1:', options=('2013-06-03', '2013-06-10', '2013-06-17'\u2026</pre> <p>For the notebook preview, the chart and widgets will look like this image</p> <p></p>"},{"location":"notebooks/ArcticDB_demo_equity_options/#installs-and-imports","title":"Installs and Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#parameters-for-the-notebook","title":"Parameters for the Notebook\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#arcticdb-setup","title":"ArcticDB Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#functions-for-reading-github-and-arcticdb-queries","title":"Functions for Reading Github and ArcticDB Queries\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#read-data-from-github-and-store-in-arcticdb","title":"Read Data from Github and Store in ArcticDB\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#functions-for-creating-the-charts-and-simple-interactive-controls","title":"Functions for Creating the Charts and Simple Interactive Controls\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_equity_options/#interactive-vol-curves-for-side-by-side-viewing-of-2-datesstocks","title":"Interactive Vol Curves for Side By Side Viewing of 2 Dates/Stocks\u00b6","text":"<ul> <li>Click the dropdowns and the charts will update</li> <li>The data is read from ArcticDB as part of the chart creation</li> </ul>"},{"location":"notebooks/ArcticDB_demo_equity_options/#conclusions","title":"Conclusions\u00b6","text":"<ul> <li>We have demonstrated a simple data pipeline for capture and storage of options data</li> <li>We created a useful series of queries on the data</li> <li>These queries are used to build interactive charts</li> <li>The result is a simple visual browser for option volatility curves</li> <li>In a real system, the data collection and storage would probably be a separate process from the visualisation</li> <li>The simplicity and flexibility of ArcticDB made the data handling the easy part, compared to getting the charts and interactive widgets looking nice</li> </ul>"},{"location":"notebooks/ArcticDB_demo_lazydataframe/","title":"LazyDataFrame Notebook","text":"ArcticDB LazyDataFrame demo <p>In this demo, we will explore the DataFrame processing options available in ArcticDB using the LazyDataFrame class. We will cover various possibilities of this API, including:</p> <ul> <li>Filtering</li> <li>Projections</li> <li>Groupbys and Aggregations</li> <li>Combinations of the above features</li> </ul> <p>Why perform the processing in ArcticDB?</p> <ul> <li>Performance boost via efficient C++ implementation that uses multi-threading</li> <li>Efficient data access - only reads the data needed</li> <li>For very large data sets some queries are possible that would not fit into memory</li> </ul> <p>Note that all of the operations described here are also available using the legacy <code>QueryBuilder</code> class, but we think this API is more intuitive!</p> <p>Necessary packages installation</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb <p>Necessary libraries imports</p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport arcticdb as adb\nfrom arcticdb.util.test import random_strings_of_length\n</pre> import os import numpy as np import pandas as pd import random import arcticdb as adb from arcticdb.util.test import random_strings_of_length <p>For this demo we will configure the LMDB file based backend.  ArcticDB achieves its high performance and scale when configured with an object store backend (e.g. S3).</p> In\u00a0[\u00a0]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_demo\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_demo\") <p>You can have an unlimited number of libraries, but we will just create one to start with.</p> In\u00a0[\u00a0]: Copied! <pre>if 'sample' not in arctic.list_libraries():\n    # library does not already exist\n    arctic.create_library('sample')\nlib = arctic.get_library('sample')\n</pre> if 'sample' not in arctic.list_libraries():     # library does not already exist     arctic.create_library('sample') lib = arctic.get_library('sample') <p>Run the cell to set up preliminary variables. 100,000 unique strings is a pathological case for us, as with the default row-slicing policy there are 100,000 rows per data segment, and so each unique strings will appear around once per data segment in this column.</p> In\u00a0[\u00a0]: Copied! <pre>ten_grouping_values = random_strings_of_length(10, 10, True)\none_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True)\nrng = np.random.RandomState()\n\nsym_10M = \"demo_10M\"\nsym_100M = \"demo_100M\"\nsym_1B = \"demo_1B\"\n</pre> ten_grouping_values = random_strings_of_length(10, 10, True) one_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True) rng = np.random.RandomState()  sym_10M = \"demo_10M\" sym_100M = \"demo_100M\" sym_1B = \"demo_1B\" <p>Choose which symbol you want to work with</p> <ul> <li>sym_10M: symbol with 10 million rows</li> <li>sym_100M: symbol with 100 million rows</li> <li>sym_1B: symbol with 1 billion rows</li> </ul> <p>assign the symbol you want to work with to the sym variable</p> <ul> <li>example: sym = sym_10M</li> </ul> In\u00a0[\u00a0]: Copied! <pre>sym = sym_10M\n</pre> sym = sym_10M <p>Run this cell to set up the DataFrame according to the symbol name</p> In\u00a0[\u00a0]: Copied! <pre>if sym==sym_10M:\n    num_rows = 10_000_000\nelif sym==sym_100M:\n    num_rows = 100_000_000\nelif sym==sym_1B:\n    num_rows = 1_000_000_000\ninput_df = pd.DataFrame(\n    {\n        \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),\n        \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),\n        \"numeric_column\": rng.rand((num_rows))\n    }\n)\n</pre> if sym==sym_10M:     num_rows = 10_000_000 elif sym==sym_100M:     num_rows = 100_000_000 elif sym==sym_1B:     num_rows = 1_000_000_000 input_df = pd.DataFrame(     {         \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),         \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),         \"numeric_column\": rng.rand((num_rows))     } ) In\u00a0[\u00a0]: Copied! <pre>lib.write(sym, input_df)\n</pre> lib.write(sym, input_df) <p>Show how the data has been sliced and written to disk.</p> In\u00a0[\u00a0]: Copied! <pre>lib._nvs.read_index(sym)\n</pre> lib._nvs.read_index(sym) <p>Show the first 100 rows of data as a sample.</p> In\u00a0[\u00a0]: Copied! <pre>lib.head(sym, n=100).data\n</pre> lib.head(sym, n=100).data <p>Read the symbol without any filtering.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym)\n</pre> %%time lib.read(sym) <p>Most of the time is spent allocating Python strings in the column with 100,000 unique strings, so omitting this column is much faster.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"])\n</pre> %%time lib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"]) <p>Note that all of the values in the numeric column are between 0 and 1. This query therefore does not filter out any data. This demonstrates that doing a full table scan does not significantly impact the performance. Also note that the read call is not practically instant, as no data is read until collect is called on the LazyDataFrame.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 2.0]\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 2.0] In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df.collect()\n</pre> %%time lazy_df.collect() <p>Now we are filtering down to approximately 10% of the rows in the symbol. This is faster than reading, as there are now fewer Python strings to allocate.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1]\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1] df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>Creating a new column as a funtion of existing columns and constants is approximately the same speed as a filter that doesn't reduce the amount of data displayed.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df[\"new_column\"] = lazy_df[\"numeric_column\"] * 2.0\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df[\"new_column\"] = lazy_df[\"numeric_column\"] * 2.0 df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>Equivalently, use the apply method to achieve the same results.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_df = lib.read(sym, lazy=True)\nlazy_df.apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0)\nlazy_df.collect().data\n</pre> lazy_df = lib.read(sym, lazy=True) lazy_df.apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0) lazy_df.collect().data <p>If using apply before the <code>LazyDataFrame</code> object has been created, the <code>col</code> function can be used as placeholders for columns names.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_df = lib.read(sym, lazy=True).apply(\"new_column\", adb.col(\"numeric_column\") * 2.0)\nlazy_df.collect().data\n</pre> lazy_df = lib.read(sym, lazy=True).apply(\"new_column\", adb.col(\"numeric_column\") * 2.0) lazy_df.collect().data <p>Grouping is again faster than just reading due to the reduced number of Python string allocations, even with the extra computation performed.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"})\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"}) df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>Even grouping on a pathologically large number of unique values does not significantly reduce the performance.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"})\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"}) df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df <p>These operations can be arbitrarily combined in a seqential pipeline.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlazy_df = lib.read(sym, lazy=True)\nlazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"})\ndf = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(sym, lazy=True) lazy_df = lazy_df[lazy_df[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", lazy_df[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"}) df = lazy_df.collect().data In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df In\u00a0[\u00a0]: Copied! <pre># Setup two symbols\nbatch_sym_1 = f'{sym}_1'\nbatch_sym_2 = f'{sym}_2'\nsyms = [batch_sym_1, batch_sym_2]\nlib.write(batch_sym_1, input_df)\nlib.write(batch_sym_2, input_df)\n</pre> # Setup two symbols batch_sym_1 = f'{sym}_1' batch_sym_2 = f'{sym}_2' syms = [batch_sym_1, batch_sym_2] lib.write(batch_sym_1, input_df) lib.write(batch_sym_2, input_df) <p><code>read_batch</code> also has a <code>lazy</code> argument, which returns a <code>LazyDataFrameCollection</code>.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lib.read_batch(syms, lazy=True)\nlazy_dfs\n</pre> lazy_dfs = lib.read_batch(syms, lazy=True) lazy_dfs <p>The same processing operations can be applied to all of the symbols being read in the batch. Note in the cell output that the pipe <code>|</code> is outside the list of <code>LazyDataFrame</code>s, so the <code>WHERE</code> clause is applied to all of the symbols.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1]\nlazy_dfs\n</pre> lazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1] lazy_dfs <p>Calling <code>collect()</code> on a <code>LazyDataFrameCollection</code> uses <code>read_batch</code> under the hood, and so is generally more performant than serialised read calls.</p> In\u00a0[\u00a0]: Copied! <pre>dfs = lazy_dfs.collect()\ndfs\n</pre> dfs = lazy_dfs.collect() dfs In\u00a0[\u00a0]: Copied! <pre>dfs[0].data.head()\n</pre> dfs[0].data.head() In\u00a0[\u00a0]: Copied! <pre>dfs[1].data.head()\n</pre> dfs[1].data.head() <p>Separate processing operations can be applied to the individual symbols in the batch if desired.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lib.read_batch(syms, lazy=True)\nlazy_dfs = lazy_dfs.split()\nlazy_dfs\n</pre> lazy_dfs = lib.read_batch(syms, lazy=True) lazy_dfs = lazy_dfs.split() lazy_dfs <p>Note in the cell output that the pipes <code>|</code> are now inside the list of <code>LazyDataFrame</code>s, so the <code>PROJECT</code> clauses are applied to individual symbols.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\"))\nlazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\"))\nlazy_dfs = adb.LazyDataFrameCollection(lazy_dfs)\nlazy_dfs\n</pre> lazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\")) lazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\")) lazy_dfs = adb.LazyDataFrameCollection(lazy_dfs) lazy_dfs In\u00a0[\u00a0]: Copied! <pre>dfs = lazy_dfs.collect()\ndfs\n</pre> dfs = lazy_dfs.collect() dfs In\u00a0[\u00a0]: Copied! <pre>dfs[0].data\n</pre> dfs[0].data In\u00a0[\u00a0]: Copied! <pre>dfs[1].data\n</pre> dfs[1].data <p>If desired, these two modes of operation can be combined in an intuitive manner.</p> In\u00a0[\u00a0]: Copied! <pre>lazy_dfs = lib.read_batch(syms, lazy=True)\nlazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1]\nlazy_dfs = lazy_dfs.split()\nlazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\"))\nlazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\"))\nlazy_dfs = adb.LazyDataFrameCollection(lazy_dfs)\nlazy_dfs = lazy_dfs[lazy_dfs[\"new_column_1\"] &lt; 0.1]\nlazy_dfs\n</pre> lazy_dfs = lib.read_batch(syms, lazy=True) lazy_dfs = lazy_dfs[lazy_dfs[\"numeric_column\"] &lt; 0.1] lazy_dfs = lazy_dfs.split() lazy_dfs[0].apply(\"new_column_1\", 2 * adb.col(\"numeric_column\")) lazy_dfs[1].apply(\"new_column_1\", 4 * adb.col(\"numeric_column\")) lazy_dfs = adb.LazyDataFrameCollection(lazy_dfs) lazy_dfs = lazy_dfs[lazy_dfs[\"new_column_1\"] &lt; 0.1] lazy_dfs In\u00a0[\u00a0]: Copied! <pre>dfs = lazy_dfs.collect()\n</pre> dfs = lazy_dfs.collect() In\u00a0[\u00a0]: Copied! <pre>dfs[0].data\n</pre> dfs[0].data In\u00a0[\u00a0]: Copied! <pre>dfs[1].data\n</pre> dfs[1].data"},{"location":"notebooks/ArcticDB_demo_lazydataframe/#demo-setup","title":"Demo setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#demo-start","title":"Demo Start\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#reading","title":"Reading\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#filtering","title":"Filtering\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#projections","title":"Projections\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#groupbys-and-aggregations","title":"Groupbys and Aggregations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#combinations","title":"Combinations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lazydataframe/#batch-operations","title":"Batch Operations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/","title":"Intro Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[2]: Copied! <pre>import time\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport arcticdb as adb\n</pre> import time import numpy as np import pandas as pd from datetime import datetime import arcticdb as adb <ul> <li>Namespace \u2013 Collections of libraries. Used to separate logical environments from each other. Analogous to database server.</li> <li>Library \u2013 Contains multiple symbols which are grouped in a certain way (different users, markets etc). Analogous to database.</li> <li>Symbol \u2013 Atomic unit of data storage. Identified by a string name. Data stored under a symbol strongly resembles a Pandas DataFrame. Analogous to tables.</li> <li>Version \u2013 Every modifying action (write, append, update) performed on a symbol creates a new version of that object.</li> <li>Snapshot \u2013 Data associated with all or some symbols at a particular point-in-time can be snapshotted and later retrieved via the read method.</li> </ul> In\u00a0[3]: Copied! <pre>daily1 = pd.DataFrame(np.ones((4, 3))*1, index=pd.date_range('1/1/2023', periods=4, freq=\"D\"), columns=list('ABC'))\ndaily1\n</pre> daily1 = pd.DataFrame(np.ones((4, 3))*1, index=pd.date_range('1/1/2023', periods=4, freq=\"D\"), columns=list('ABC')) daily1 Out[3]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 In\u00a0[4]: Copied! <pre>daily2 = pd.DataFrame(np.ones((4, 3))*2, index=pd.date_range('1/5/2023', periods=4, freq=\"D\"), columns=list('ABC'))\ndaily2\n</pre> daily2 = pd.DataFrame(np.ones((4, 3))*2, index=pd.date_range('1/5/2023', periods=4, freq=\"D\"), columns=list('ABC')) daily2 Out[4]: A B C 2023-01-05 2.0 2.0 2.0 2023-01-06 2.0 2.0 2.0 2023-01-07 2.0 2.0 2.0 2023-01-08 2.0 2.0 2.0 In\u00a0[5]: Copied! <pre>daily3 = pd.DataFrame(np.ones((4, 3))*3, index=pd.date_range('1/3/2023', periods=4, freq=\"D\"), columns=list('ABC'))\ndaily3\n</pre> daily3 = pd.DataFrame(np.ones((4, 3))*3, index=pd.date_range('1/3/2023', periods=4, freq=\"D\"), columns=list('ABC')) daily3 Out[5]: A B C 2023-01-03 3.0 3.0 3.0 2023-01-04 3.0 3.0 3.0 2023-01-05 3.0 3.0 3.0 2023-01-06 3.0 3.0 3.0 In\u00a0[6]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_demo\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_demo\") <p>You can have an unlimited number of libraries, but we will just create one to start with.</p> In\u00a0[7]: Copied! <pre>lib = arctic.get_library('sample', create_if_missing=True)\n</pre> lib = arctic.get_library('sample', create_if_missing=True) <p>ArcticDB generally adheres to a philosphy of Pandas In, Pandas Out. read and write both work with Pandas DataFrames.</p> <p>Note - within a library it is common to have many thousands of symbols.</p> In\u00a0[8]: Copied! <pre>write_record = lib.write(\"DAILY\", daily1)\nwrite_record\n</pre> write_record = lib.write(\"DAILY\", daily1) write_record Out[8]: <pre>VersionedItem(symbol='DAILY', library='sample', data=n/a, version=0, metadata=None, host='LMDB(path=/content/arcticdb_demo)')</pre> In\u00a0[9]: Copied! <pre>read_record = lib.read(\"DAILY\")\nread_record\n</pre> read_record = lib.read(\"DAILY\") read_record Out[9]: <pre>VersionedItem(symbol='DAILY', library='sample', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=0, metadata=None, host='LMDB(path=/content/arcticdb_demo)')</pre> <p>NB: You can version multiple symbols/tables together with a library level Snapshot!</p> In\u00a0[10]: Copied! <pre>read_record.data\n</pre> read_record.data Out[10]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 <p>ArcticDB supports data modifications such as update and append.</p> In\u00a0[11]: Copied! <pre>lib.append(\"DAILY\", daily2)\nlib.read(\"DAILY\").data\n</pre> lib.append(\"DAILY\", daily2) lib.read(\"DAILY\").data Out[11]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 2023-01-05 2.0 2.0 2.0 2023-01-06 2.0 2.0 2.0 2023-01-07 2.0 2.0 2.0 2023-01-08 2.0 2.0 2.0 In\u00a0[12]: Copied! <pre>lib.update(\"DAILY\", daily3)\nlib.read(\"DAILY\").data\n</pre> lib.update(\"DAILY\", daily3) lib.read(\"DAILY\").data Out[12]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 3.0 3.0 3.0 2023-01-04 3.0 3.0 3.0 2023-01-05 3.0 3.0 3.0 2023-01-06 3.0 3.0 3.0 2023-01-07 2.0 2.0 2.0 2023-01-08 2.0 2.0 2.0 In\u00a0[13]: Copied! <pre># Rewind to version...\nlib.read(\"DAILY\", as_of=write_record.version).data\n</pre> # Rewind to version... lib.read(\"DAILY\", as_of=write_record.version).data Out[13]: A B C 2023-01-01 1.0 1.0 1.0 2023-01-02 1.0 1.0 1.0 2023-01-03 1.0 1.0 1.0 2023-01-04 1.0 1.0 1.0 <p>One typical use case is to store the history of &gt;100k measures in one dataframe for easy timeseries and cross-sectional analysis.</p> <p>For this demo notebook we'll just do 10,000 rows of hourly data by 10,000 columns of measures.</p> In\u00a0[14]: Copied! <pre>n = 10_000\nlarge = pd.DataFrame(np.linspace(1, n, n)*np.linspace(1, n, n)[:,np.newaxis], columns=[f'c{i}' for i in range(n)], index=pd.date_range('1/1/2020', periods=n, freq=\"H\"))\nlarge.tail()\n</pre> n = 10_000 large = pd.DataFrame(np.linspace(1, n, n)*np.linspace(1, n, n)[:,np.newaxis], columns=[f'c{i}' for i in range(n)], index=pd.date_range('1/1/2020', periods=n, freq=\"H\")) large.tail() Out[14]: c0 c1 c2 c3 c4 c5 c6 c7 c8 c9 ... c9990 c9991 c9992 c9993 c9994 c9995 c9996 c9997 c9998 c9999 2021-02-20 11:00:00 9996.0 19992.0 29988.0 39984.0 49980.0 59976.0 69972.0 79968.0 89964.0 99960.0 ... 99870036.0 99880032.0 99890028.0 99900024.0 99910020.0 99920016.0 99930012.0 99940008.0 99950004.0 99960000.0 2021-02-20 12:00:00 9997.0 19994.0 29991.0 39988.0 49985.0 59982.0 69979.0 79976.0 89973.0 99970.0 ... 99880027.0 99890024.0 99900021.0 99910018.0 99920015.0 99930012.0 99940009.0 99950006.0 99960003.0 99970000.0 2021-02-20 13:00:00 9998.0 19996.0 29994.0 39992.0 49990.0 59988.0 69986.0 79984.0 89982.0 99980.0 ... 99890018.0 99900016.0 99910014.0 99920012.0 99930010.0 99940008.0 99950006.0 99960004.0 99970002.0 99980000.0 2021-02-20 14:00:00 9999.0 19998.0 29997.0 39996.0 49995.0 59994.0 69993.0 79992.0 89991.0 99990.0 ... 99900009.0 99910008.0 99920007.0 99930006.0 99940005.0 99950004.0 99960003.0 99970002.0 99980001.0 99990000.0 2021-02-20 15:00:00 10000.0 20000.0 30000.0 40000.0 50000.0 60000.0 70000.0 80000.0 90000.0 100000.0 ... 99910000.0 99920000.0 99930000.0 99940000.0 99950000.0 99960000.0 99970000.0 99980000.0 99990000.0 100000000.0 <p>5 rows \u00d7 10000 columns</p> In\u00a0[15]: Copied! <pre>t1 = time.time()\nlib.write('large', large)\nt2 = time.time()\nprint(f'Wrote {n*n/(t2-t1)/1e6:.2f} million floats per second.')\n</pre> t1 = time.time() lib.write('large', large) t2 = time.time() print(f'Wrote {n*n/(t2-t1)/1e6:.2f} million floats per second.') <pre>Wrote 13.30 million floats per second.\n</pre> <p>You can select out rows and columns efficiently, necessary when the data doesn't fit into ram.</p> In\u00a0[16]: Copied! <pre>subframe = lib.read(\n    \"large\",\n    columns=[\"c0\", \"c1\", \"c5000\", \"c5001\", \"c9998\", \"c9999\"],\n    date_range=(datetime(2020, 6, 13, 8), datetime(2020, 6, 13, 13))\n).data\nsubframe\n</pre> subframe = lib.read(     \"large\",     columns=[\"c0\", \"c1\", \"c5000\", \"c5001\", \"c9998\", \"c9999\"],     date_range=(datetime(2020, 6, 13, 8), datetime(2020, 6, 13, 13)) ).data subframe Out[16]: c0 c1 c5000 c5001 c9998 c9999 2020-06-13 08:00:00 3945.0 7890.0 19728945.0 19732890.0 39446055.0 39450000.0 2020-06-13 09:00:00 3946.0 7892.0 19733946.0 19737892.0 39456054.0 39460000.0 2020-06-13 10:00:00 3947.0 7894.0 19738947.0 19742894.0 39466053.0 39470000.0 2020-06-13 11:00:00 3948.0 7896.0 19743948.0 19747896.0 39476052.0 39480000.0 2020-06-13 12:00:00 3949.0 7898.0 19748949.0 19752898.0 39486051.0 39490000.0 2020-06-13 13:00:00 3950.0 7900.0 19753950.0 19757900.0 39496050.0 39500000.0 In\u00a0[17]: Copied! <pre>n = 100_000_000\nlong = pd.DataFrame(np.linspace(1, n, n), columns=['Price'], index=pd.date_range('1/1/2020', periods=n, freq=\"S\"))\nlong.tail()\n</pre> n = 100_000_000 long = pd.DataFrame(np.linspace(1, n, n), columns=['Price'], index=pd.date_range('1/1/2020', periods=n, freq=\"S\")) long.tail() Out[17]: Price 2023-03-03 09:46:35 99999996.0 2023-03-03 09:46:36 99999997.0 2023-03-03 09:46:37 99999998.0 2023-03-03 09:46:38 99999999.0 2023-03-03 09:46:39 100000000.0 In\u00a0[18]: Copied! <pre>t1 = time.time()\nlib.write('long', long)\nt2 = time.time()\nprint(f'Wrote {n/(t2-t1)/1e6:.2f} million floats per second.')\n</pre> t1 = time.time() lib.write('long', long) t2 = time.time() print(f'Wrote {n/(t2-t1)/1e6:.2f} million floats per second.') <pre>Wrote 12.20 million floats per second.\n</pre> In\u00a0[19]: Copied! <pre>%%time\nlazy_df = lib.read(\"long\", lazy=True)\nlazy_df = lazy_df[(lazy_df[\"Price\"] &gt; 49e6) &amp; (lazy_df[\"Price\"] &lt; 51e6)]\nfiltered = lazy_df.collect().data\n</pre> %%time lazy_df = lib.read(\"long\", lazy=True) lazy_df = lazy_df[(lazy_df[\"Price\"] &gt; 49e6) &amp; (lazy_df[\"Price\"] &lt; 51e6)] filtered = lazy_df.collect().data <pre>CPU times: user 3.07 s, sys: 525 ms, total: 3.6 s\nWall time: 2.3 s\n</pre> In\u00a0[20]: Copied! <pre>len(filtered)\n</pre> len(filtered) Out[20]: <pre>1999999</pre> In\u00a0[21]: Copied! <pre>filtered.tail()\n</pre> filtered.tail() Out[21]: Price 2021-08-13 06:39:54 50999995.0 2021-08-13 06:39:55 50999996.0 2021-08-13 06:39:56 50999997.0 2021-08-13 06:39:57 50999998.0 2021-08-13 06:39:58 50999999.0"},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-concepts-and-terminology","title":"ArcticDB Concepts and Terminology\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-is-designed-for-time-series-data","title":"ArcticDB is designed for Time Series data\u00b6","text":"<p>Let's create a few small dataframes of daily data so we can see what's happening.</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#library-management","title":"Library Management\u00b6","text":"<p>For this demo we will configure the LMDB file based backend.  ArcticDB achieves its high performance and scale when configured with an object store backend (e.g. S3).</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#reading-writing-data","title":"Reading &amp; writing data\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#read-a-pandas-dataframe-from-source-and-write-it-to-the-target","title":"Read a pandas dataframe from source, and write it to the target\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#modifying-data","title":"Modifying Data\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-is-bitemporal","title":"ArcticDB is bitemporal\u00b6","text":"<p>All ArcticDB operations are versioned - rewind through time to understand historical revisions and enable point-in-time analysis of data!</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-supports-extremely-large-dataframes","title":"ArcticDB supports extremely large DataFrames\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_lmdb/#arcticdb-supports-extremely-long-dataframes","title":"ArcticDB supports extremely long DataFrames\u00b6","text":"<p>Another typical use case is high frequency data with billions of rows.</p> <p>For this demo notebook we will just try a modest 100 million rows of second frequency data.</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#you-can-query-the-data-with-with-the-familiarity-of-pandas-and-the-efficiency-of-c","title":"You can query the data with with the familiarity of Pandas and the efficiency of C++\u00b6","text":"<p>For more information please check out our LazyDataFrame and QueryBuilder docs.</p>"},{"location":"notebooks/ArcticDB_demo_lmdb/#where-to-go-from-here","title":"Where to go from here?\u00b6","text":"<ol> <li>Read the docs</li> <li>Signup for Slack via our website</li> <li>Checkout the code on Github</li> </ol>"},{"location":"notebooks/ArcticDB_demo_querybuilder/","title":"Querybuilder Notebook","text":"ArcticDB Query Builder demo <p>Note that all of the functionality demonstrated here is available in the more intuitive LazyDataFrame API, which we recommend is used in preference to the QueryBuilder.</p> <p>In this demo, we will explore the different functionalities of the QueryBuilder for ArcticDB. We will cover various possibilities of this API, including:</p> <ul> <li>Filtering</li> <li>Projections</li> <li>Groupbys and Aggregations</li> <li>Combinations of the above features</li> </ul> <p>Why use QueryBuilder?</p> <ul> <li>Performance boost via efficient C++ implementation that uses multi-threading</li> <li>Efficient data access - only reads the data needed</li> <li>For very large data sets some queries are possible that would not fit into memory</li> </ul> <p>Necessary packages installation</p> In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb <p>Necessary libraries imports</p> In\u00a0[3]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport arcticdb as adb\nfrom arcticdb.util.test import random_strings_of_length\n</pre> import os import numpy as np import pandas as pd import random import arcticdb as adb from arcticdb.util.test import random_strings_of_length <p>For this demo we will configure the LMDB file based backend.  ArcticDB achieves its high performance and scale when configured with an object store backend (e.g. S3).</p> In\u00a0[4]: Copied! <pre>arctic = adb.Arctic(\"lmdb://arcticdb_demo\")\n</pre> arctic = adb.Arctic(\"lmdb://arcticdb_demo\") <p>You can have an unlimited number of libraries, but we will just create one to start with.</p> In\u00a0[5]: Copied! <pre>if 'sample' not in arctic.list_libraries():\n    # library does not already exist\n    arctic.create_library('sample')\nlib = arctic.get_library('sample')\n</pre> if 'sample' not in arctic.list_libraries():     # library does not already exist     arctic.create_library('sample') lib = arctic.get_library('sample') <p>Run the cell to set up preliminary variables. 100,000 unique strings is a pathological case for us, as with the default row-slicing policy there are 100,000 rows per data segment, and so each unique strings will appear around once per data segment in this column.</p> In\u00a0[6]: Copied! <pre>ten_grouping_values = random_strings_of_length(10, 10, True)\none_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True)\nrng = np.random.RandomState()\n\nsym_10M = \"demo_10M\"\nsym_100M = \"demo_100M\"\nsym_1B = \"demo_1B\"\n</pre> ten_grouping_values = random_strings_of_length(10, 10, True) one_hundred_thousand_grouping_values = random_strings_of_length(100_000, 10, True) rng = np.random.RandomState()  sym_10M = \"demo_10M\" sym_100M = \"demo_100M\" sym_1B = \"demo_1B\" <p>Choose which symbol you want to work with</p> <ul> <li>sym_10M: symbol with 10 million rows</li> <li>sym_100M: symbol with 100 million rows</li> <li>sym_1B: symbol with 1 billion rows</li> </ul> <p>assign the symbol you want to work with to the sym variable</p> <ul> <li>example: sym = sym_10M</li> </ul> In\u00a0[7]: Copied! <pre>sym = sym_10M\n</pre> sym = sym_10M <p>Run this cell to set up the DataFrame according to the symbol name</p> In\u00a0[8]: Copied! <pre>if sym==sym_10M:\n    num_rows = 10_000_000\nelif sym==sym_100M:\n    num_rows = 100_000_000\nelif sym==sym_1B:\n    num_rows = 1_000_000_000\ndf = pd.DataFrame(\n    {\n        \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),\n        \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),\n        \"numeric_column\": rng.rand((num_rows))\n    }\n)\n</pre> if sym==sym_10M:     num_rows = 10_000_000 elif sym==sym_100M:     num_rows = 100_000_000 elif sym==sym_1B:     num_rows = 1_000_000_000 df = pd.DataFrame(     {         \"grouping_column_10\": list(random.choices(ten_grouping_values, k=num_rows)),         \"grouping_column_100_000\": list(random.choices(one_hundred_thousand_grouping_values, k=num_rows)),         \"numeric_column\": rng.rand((num_rows))     } ) In\u00a0[\u00a0]: Copied! <pre>lib.write(sym, df)\n</pre> lib.write(sym, df) <p>Show how the data has been sliced and written to disk.</p> In\u00a0[\u00a0]: Copied! <pre>lib._nvs.read_index(sym)\n</pre> lib._nvs.read_index(sym) <p>Show the first 100 rows of data as a sample.</p> In\u00a0[\u00a0]: Copied! <pre>lib.head(sym, n=100).data\n</pre> lib.head(sym, n=100).data <p>Read the symbol without any filtering.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym)\n</pre> %%time lib.read(sym) <p>Most of the time is spent allocating Python strings in the column with 100,000 unique strings, so omitting this column is much faster.</p> In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"])\n</pre> %%time lib.read(sym, columns=[\"grouping_column_10\", \"numeric_column\"]) <p>Note that all of the values in the numeric column are between 0 and 1. Thisquery therefore does not filter out any data. This demonstrates that doing a full table scan does not significantly impact the performance.</p> In\u00a0[16]: Copied! <pre>q = adb.QueryBuilder()\nq = q[q[\"numeric_column\"] &lt; 2.0]\n</pre> q = adb.QueryBuilder() q = q[q[\"numeric_column\"] &lt; 2.0] In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) <p>Now we are filtering down to approximately 10% of the rows in the symbol. This is faster than reading, as there are now fewer Python strings to allocate.</p> In\u00a0[18]: Copied! <pre>q = adb.QueryBuilder()\nq = q[q[\"numeric_column\"] &lt; 0.1]\n</pre> q = adb.QueryBuilder() q = q[q[\"numeric_column\"] &lt; 0.1] In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q).data\n</pre> %%time lib.read(sym, query_builder=q).data In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>Creating a new column as a funtion of existing columns and constants is approximately the same speed as a filter that doesn't reduce the amount of data displayed.</p> In\u00a0[21]: Copied! <pre>q = adb.QueryBuilder()\nq = q.apply(\"new_column\", q[\"numeric_column\"] * 2.0)\n</pre> q = adb.QueryBuilder() q = q.apply(\"new_column\", q[\"numeric_column\"] * 2.0) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>Grouping is again faster than just reading due to the reduced number of Python string allocations, even with the extra computation performed.</p> In\u00a0[24]: Copied! <pre>q = adb.QueryBuilder()\nq = q.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"})\n</pre> q = adb.QueryBuilder() q = q.groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\"}) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>Even grouping on a pathologically large number of unique values does not significantly reduce the performance.</p> In\u00a0[27]: Copied! <pre>q = adb.QueryBuilder()\nq = q.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"})\n</pre> q = adb.QueryBuilder() q = q.groupby(\"grouping_column_100_000\").agg({\"numeric_column\": \"mean\"}) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data <p>These operations can be arbitrarily combined in a seqential pipeline.</p> In\u00a0[30]: Copied! <pre>q = adb.QueryBuilder()\nq = q[q[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", q[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"})\n</pre> q = adb.QueryBuilder() q = q[q[\"numeric_column\"] &lt; 0.1].apply(\"new_column\", q[\"numeric_column\"] * 2.0).groupby(\"grouping_column_10\").agg({\"numeric_column\": \"mean\", \"new_column\": \"max\"}) In\u00a0[\u00a0]: Copied! <pre>%%time\nlib.read(sym, query_builder=q)\n</pre> %%time lib.read(sym, query_builder=q) In\u00a0[\u00a0]: Copied! <pre>lib.read(sym, query_builder=q).data\n</pre> lib.read(sym, query_builder=q).data"},{"location":"notebooks/ArcticDB_demo_querybuilder/#demo-setup","title":"Demo setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#demo-start","title":"Demo Start\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#reading","title":"Reading\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#filtering","title":"Filtering\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#projections","title":"Projections\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#groupbys-and-aggregations","title":"Groupbys and Aggregations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_querybuilder/#combinations","title":"Combinations\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_resample/","title":"Resample Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport arcticdb as adb\n</pre> import numpy as np import pandas as pd import arcticdb as adb In\u00a0[3]: Copied! <pre># object store\narctic = adb.Arctic(\"lmdb://arcticdb_resample\")\n</pre> # object store arctic = adb.Arctic(\"lmdb://arcticdb_resample\") In\u00a0[4]: Copied! <pre># library\nlib = arctic.get_library('resample', create_if_missing=True)\n</pre> # library lib = arctic.get_library('resample', create_if_missing=True) In\u00a0[5]: Copied! <pre># data for resampling\nindex = pd.date_range(\"1990-01-01\", periods=12_000_000, freq=\"s\")\nint_data = np.arange(len(index), dtype=np.uint64)\nfloat_data = np.round(np.random.uniform(95., 105., len(index)), 3)\nletters = ['a','b','c','d','e','f','g']\nmkt_data = pd.DataFrame(\n    index=index,\n    data={\n        \"id\": int_data,\n        \"price\": float_data,\n        \"category\": (letters*(len(index)//len(letters) + 1))[:len(index)]\n    }\n)\n</pre> # data for resampling index = pd.date_range(\"1990-01-01\", periods=12_000_000, freq=\"s\") int_data = np.arange(len(index), dtype=np.uint64) float_data = np.round(np.random.uniform(95., 105., len(index)), 3) letters = ['a','b','c','d','e','f','g'] mkt_data = pd.DataFrame(     index=index,     data={         \"id\": int_data,         \"price\": float_data,         \"category\": (letters*(len(index)//len(letters) + 1))[:len(index)]     } ) In\u00a0[6]: Copied! <pre># view the first 10 rows of the data\nmkt_data.head(10)\n</pre> # view the first 10 rows of the data mkt_data.head(10) Out[6]: id price category 1990-01-01 00:00:00 0 95.176 a 1990-01-01 00:00:01 1 97.872 b 1990-01-01 00:00:02 2 104.930 c 1990-01-01 00:00:03 3 103.573 d 1990-01-01 00:00:04 4 97.052 e 1990-01-01 00:00:05 5 103.435 f 1990-01-01 00:00:06 6 99.339 g 1990-01-01 00:00:07 7 103.358 a 1990-01-01 00:00:08 8 104.301 b 1990-01-01 00:00:09 9 104.651 c In\u00a0[7]: Copied! <pre># write the data into ArcticDB\nsym = 'market_data'\nlib.write(sym, mkt_data)\n</pre> # write the data into ArcticDB sym = 'market_data' lib.write(sym, mkt_data) Out[7]: <pre>VersionedItem(symbol='market_data', library='resample', data=n/a, version=0, metadata=None, host='LMDB(path=~/arcticdb_resample)', timestamp=1718958796318913629)</pre> In\u00a0[8]: Copied! <pre># frequency and aggregator params\nfreq1 = '1min'\naggs1 = {'id': 'max', 'price': 'last', 'category': 'count'}\n</pre> # frequency and aggregator params freq1 = '1min' aggs1 = {'id': 'max', 'price': 'last', 'category': 'count'} In\u00a0[9]: Copied! <pre>%%time\n# create the resample query and apply it on the read\nmarket_data_1min_df = lib.read(sym, lazy=True).resample(freq1).agg(aggs1).collect().data\nprint(len(market_data_1min_df))\nmarket_data_1min_df.tail()\n</pre> %%time # create the resample query and apply it on the read market_data_1min_df = lib.read(sym, lazy=True).resample(freq1).agg(aggs1).collect().data print(len(market_data_1min_df)) market_data_1min_df.tail() <pre>200000\nCPU times: user 684 ms, sys: 251 ms, total: 935 ms\nWall time: 171 ms\n</pre> Out[9]: id price category 1990-05-19 21:15:00 11999759 104.106 60 1990-05-19 21:16:00 11999819 104.456 60 1990-05-19 21:17:00 11999879 95.570 60 1990-05-19 21:18:00 11999939 103.967 60 1990-05-19 21:19:00 11999999 97.899 60 In\u00a0[10]: Copied! <pre>%%time\n# read the full data set and resample in Pandas\nfull_df = lib.read(sym).data\nmarket_data_1min_pd_df = full_df.resample(freq1).agg(aggs1)\nprint(len(market_data_1min_pd_df))\nmarket_data_1min_pd_df.tail()\n</pre> %%time # read the full data set and resample in Pandas full_df = lib.read(sym).data market_data_1min_pd_df = full_df.resample(freq1).agg(aggs1) print(len(market_data_1min_pd_df)) market_data_1min_pd_df.tail() <pre>200000\nCPU times: user 1.6 s, sys: 401 ms, total: 2 s\nWall time: 1.15 s\n</pre> Out[10]: id price category 1990-05-19 21:15:00 11999759 104.106 60 1990-05-19 21:16:00 11999819 104.456 60 1990-05-19 21:17:00 11999879 95.570 60 1990-05-19 21:18:00 11999939 103.967 60 1990-05-19 21:19:00 11999999 97.899 60 In\u00a0[11]: Copied! <pre>freq2 = '5min'\naggs2 = {'id': 'max', 'price_last': ('price' ,'last'), 'price_count': ('price' ,'count'), 'category': 'first'}\n</pre> freq2 = '5min' aggs2 = {'id': 'max', 'price_last': ('price' ,'last'), 'price_count': ('price' ,'count'), 'category': 'first'} In\u00a0[12]: Copied! <pre>%%time\nlib.read(sym, lazy=True).resample(freq2).agg(aggs2).collect().data\n</pre> %%time lib.read(sym, lazy=True).resample(freq2).agg(aggs2).collect().data <pre>CPU times: user 1.07 s, sys: 415 ms, total: 1.49 s\nWall time: 151 ms\n</pre> Out[12]: id category price_count price_last 1990-01-01 00:00:00 299 a 300 102.172 1990-01-01 00:05:00 599 g 300 101.450 1990-01-01 00:10:00 899 f 300 96.718 1990-01-01 00:15:00 1199 e 300 96.345 1990-01-01 00:20:00 1499 d 300 98.955 ... ... ... ... ... 1990-05-19 20:55:00 11998799 d 300 100.277 1990-05-19 21:00:00 11999099 c 300 103.596 1990-05-19 21:05:00 11999399 b 300 96.182 1990-05-19 21:10:00 11999699 a 300 99.911 1990-05-19 21:15:00 11999999 g 300 97.899 <p>40000 rows \u00d7 4 columns</p> In\u00a0[14]: Copied! <pre>%%time\nlib.read(sym, lazy=True).resample('2min30s').agg({'id': 'min', 'category': 'first'}).groupby('category').agg({'id': 'mean'}.collect().data\n</pre> %%time lib.read(sym, lazy=True).resample('2min30s').agg({'id': 'min', 'category': 'first'}).groupby('category').agg({'id': 'mean'}.collect().data <pre>CPU times: user 1.12 s, sys: 309 ms, total: 1.43 s\nWall time: 183 ms\n</pre> Out[14]: id category a 5999700.0 b 5999925.0 d 5999850.0 e 6000075.0 f 5999775.0 g 6000000.0 c 6000150.0 In\u00a0[15]: Copied! <pre>freq_ohlc = '5min'\nagg_ohlc = {\n    'open': ('price', 'first'),\n    'high': ('price', 'max'),\n    'low': ('price', 'min'),\n    'close': ('price', 'last')\n}\n</pre> freq_ohlc = '5min' agg_ohlc = {     'open': ('price', 'first'),     'high': ('price', 'max'),     'low': ('price', 'min'),     'close': ('price', 'last') } In\u00a0[16]: Copied! <pre>%%time\nohlc_5min_bars = lib.read(sym, lazy=True).resample(freq_ohlc).agg(agg_ohlc).collect().data\n</pre> %%time ohlc_5min_bars = lib.read(sym, lazy=True).resample(freq_ohlc).agg(agg_ohlc).collect().data <pre>CPU times: user 1.26 s, sys: 492 ms, total: 1.75 s\nWall time: 118 ms\n</pre> In\u00a0[17]: Copied! <pre>ohlc_5min_bars.head()\n</pre> ohlc_5min_bars.head() Out[17]: close low high open 1990-01-01 00:00:00 102.172 95.076 104.992 95.176 1990-01-01 00:05:00 101.450 95.008 104.999 98.520 1990-01-01 00:10:00 96.718 95.053 104.990 103.959 1990-01-01 00:15:00 96.345 95.070 104.969 95.878 1990-01-01 00:20:00 98.955 95.011 104.983 103.538"},{"location":"notebooks/ArcticDB_demo_resample/#arcticdb-resample-demo","title":"ArcticDB Resample Demo\u00b6","text":"<p>This demo notebook showcases the high-performance resample capability of ArcticDB.</p> <p>This is what you need to know about it:</p> <ul> <li>It runs on-the-fly as part of the read</li> <li>This makes it much more efficient than Pandas on large datasets</li> <li>The usage is similar to the Pandas resample function</li> <li>You can apply multiple aggregators to each column</li> <li>It can be used for downsampling high frequency data and generating \"bar\" data (see example 4)</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_resample/#create-some-data","title":"Create Some Data\u00b6","text":"<ul> <li>timeseries with 12,000,000 rows and a 1-second index</li> <li>int, float, string columns</li> <li>write the data into ArcticDB</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#1-simple-resample","title":"1. Simple Resample\u00b6","text":"<ul> <li>Resample to 1-minute</li> <li>Use different aggregators</li> <li>Resample can be thought of as a time-based groupby</li> <li>The groups are all the rows within a time interval</li> <li>Run also in Pandas to compare performance and results</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#2-multiple-aggregators-per-column","title":"2. Multiple Aggregators per Column\u00b6","text":"<ul> <li>Similar to NamedAgg in Pandas</li> <li>Downsample to 5-minute frequency</li> <li>Apply both\u00a0max and last aggregators to the price column</li> <li>For multiple aggregators, the syntax is\u00a0<code>output_column_name: (input_column_name: aggregator)</code></li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#3-processing-pipeline-chaining-operations","title":"3. Processing Pipeline: Chaining Operations\u00b6","text":"<ul> <li>Downsample to 2.5-minutes frequency</li> <li>Group the resampled data by the category column</li> <li>Aggregate the category groups using mean</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#4-example-ohlc-open-high-low-close-bars","title":"4. Example: OHLC (Open High Low Close) Bars\u00b6","text":"<ul> <li>Downsample to 5-minute frequency</li> <li>Use multiple aggregators on the price column</li> <li>This is a simple example of how to convert tick data to OHLC bar data</li> </ul>"},{"location":"notebooks/ArcticDB_demo_resample/#conclusion","title":"Conclusion\u00b6","text":"<p>We have demonstrated the following about the ArcticDB resample feature:</p> <ul> <li>Easy to use, especially if you already resample in Pandas</li> <li>Very high performance - in particular much faster than reading all the data then resampling in Pandas</li> <li>Can be combined with other query functions to build processing pipelines</li> <li>Can be used to generate timeseries bars</li> </ul>"},{"location":"notebooks/ArcticDB_demo_snapshots/","title":"Snapshots Notebook","text":"Snapshots: how to use them and why they are useful In\u00a0[1]: Copied! <pre>!pip install arcticdb\n</pre> !pip install arcticdb In\u00a0[2]: Copied! <pre>import pandas as pd\nimport logging\nimport arcticdb as adb\n</pre> import pandas as pd import logging import arcticdb as adb In\u00a0[3]: Copied! <pre>lib_name = 'demo'\narctic = adb.Arctic(\"lmdb://arcticdb_snapshot_demo\")\nif lib_name in arctic.list_libraries():\n    arctic.delete_library(lib_name)\nlib = arctic.get_library('demo', create_if_missing=True)\n</pre> lib_name = 'demo' arctic = adb.Arctic(\"lmdb://arcticdb_snapshot_demo\") if lib_name in arctic.list_libraries():     arctic.delete_library(lib_name) lib = arctic.get_library('demo', create_if_missing=True) In\u00a0[4]: Copied! <pre>num_symbols = 4\nsymbols = [f\"sym_{idx}\" for idx in range(num_symbols)]\nhalf_symbols = symbols[:num_symbols // 2]\nprint(symbols)\nprint(half_symbols)\n</pre> num_symbols = 4 symbols = [f\"sym_{idx}\" for idx in range(num_symbols)] half_symbols = symbols[:num_symbols // 2] print(symbols) print(half_symbols) <pre>['sym_0', 'sym_1', 'sym_2', 'sym_3']\n['sym_0', 'sym_1']\n</pre> In\u00a0[5]: Copied! <pre># write data for each symbol\nfor idx, symbol in enumerate(symbols):\n    lib.write(symbol, pd.DataFrame({\"col\": [idx]}))\n</pre> # write data for each symbol for idx, symbol in enumerate(symbols):     lib.write(symbol, pd.DataFrame({\"col\": [idx]})) In\u00a0[6]: Copied! <pre># write data only for the first half of the symbols\nfor idx, symbol in enumerate(half_symbols):\n    lib.write(symbol, pd.DataFrame({\"col\": [idx+10]}))\n</pre> # write data only for the first half of the symbols for idx, symbol in enumerate(half_symbols):     lib.write(symbol, pd.DataFrame({\"col\": [idx+10]})) In\u00a0[7]: Copied! <pre>lib.snapshot(\"snapshot_0\", metadata=\"this is the core of the demo\")\n</pre> lib.snapshot(\"snapshot_0\", metadata=\"this is the core of the demo\") In\u00a0[8]: Copied! <pre># list all snapshots\nlib.list_snapshots()\n</pre> # list all snapshots lib.list_snapshots() Out[8]: <pre>{'snapshot_0': 'this is the core of the demo'}</pre> In\u00a0[9]: Copied! <pre># list the symbols in a snapshot\nlib.list_symbols(snapshot_name=\"snapshot_0\")\n</pre> # list the symbols in a snapshot lib.list_symbols(snapshot_name=\"snapshot_0\") Out[9]: <pre>['sym_2', 'sym_1', 'sym_0', 'sym_3']</pre> In\u00a0[10]: Copied! <pre># list the versions in a snapshot\nlib.list_versions(snapshot=\"snapshot_0\")\n</pre> # list the versions in a snapshot lib.list_versions(snapshot=\"snapshot_0\") Out[10]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_0']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_0']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_0']),\n sym_0_v1: (date=2023-11-20 10:24:45.413203317+00:00, snapshots=['snapshot_0'])}</pre> In\u00a0[11]: Copied! <pre># list all versions in the library, with associated snapshots\nlib.list_versions()\n</pre> # list all versions in the library, with associated snapshots lib.list_versions() Out[11]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_0']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_0']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_0']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00),\n sym_0_v1: (date=2023-11-20 10:24:45.413203317+00:00, snapshots=['snapshot_0']),\n sym_0_v0: (date=2023-11-20 10:24:45.041944641+00:00)}</pre> In\u00a0[12]: Copied! <pre>vit = lib.read(\"sym_0\", as_of=\"snapshot_0\")\nprint(vit)\nprint(vit.data)\n</pre> vit = lib.read(\"sym_0\", as_of=\"snapshot_0\") print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_0', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0   10\n</pre> In\u00a0[13]: Copied! <pre>vit = lib.read(\"sym_3\", as_of=\"snapshot_0\")\nprint(vit)\nprint(vit.data)\n</pre> vit = lib.read(\"sym_3\", as_of=\"snapshot_0\") print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_3', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=0, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0    3\n</pre> In\u00a0[14]: Copied! <pre># delete the symbol sym_0\nlib.delete(\"sym_0\")\n</pre> # delete the symbol sym_0 lib.delete(\"sym_0\") In\u00a0[15]: Copied! <pre># show that sym_0 has been deleted\nlib.list_symbols()\n</pre> # show that sym_0 has been deleted lib.list_symbols() Out[15]: <pre>['sym_2', 'sym_1', 'sym_3']</pre> In\u00a0[16]: Copied! <pre># sym_0 does not appear in the current library versions\nlib.list_versions()\n</pre> # sym_0 does not appear in the current library versions lib.list_versions() Out[16]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_0']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_0']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_0']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00)}</pre> In\u00a0[17]: Copied! <pre># however we can still read the version of sym_0 that was recorded in the snapshot\nvit = lib.read(\"sym_0\", as_of=\"snapshot_0\")\nprint(vit)\nprint(vit.data)\n</pre> # however we can still read the version of sym_0 that was recorded in the snapshot vit = lib.read(\"sym_0\", as_of=\"snapshot_0\") print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_0', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0   10\n</pre> In\u00a0[18]: Copied! <pre>vit = lib.read(\"sym_0\", as_of=1)\nprint(vit)\nprint(vit.data)\n</pre> vit = lib.read(\"sym_0\", as_of=1) print(vit) print(vit.data) <pre>VersionedItem(symbol='sym_0', library='demo', data=&lt;class 'pandas.core.frame.DataFrame'&gt;, version=1, metadata=None, host='LMDB(path=/users/isys/nclarke/jupyter/arctic/demos/arcticdb_snapshot_demo)')\n   col\n0   10\n</pre> In\u00a0[19]: Copied! <pre># version 0 was not in the snapshot, so it has been removed\ntry:\n    vit = lib.read(\"sym_0\", as_of=0)\n    print(vit)\n    print(vit.data)\nexcept adb.exceptions.NoSuchVersionException:\n    logging.error(\"Version not found\")\n</pre> # version 0 was not in the snapshot, so it has been removed try:     vit = lib.read(\"sym_0\", as_of=0)     print(vit)     print(vit.data) except adb.exceptions.NoSuchVersionException:     logging.error(\"Version not found\")  <pre>ERROR:root:Version not found\n</pre> In\u00a0[20]: Copied! <pre>lib.delete_snapshot(\"snapshot_0\")\n</pre> lib.delete_snapshot(\"snapshot_0\") In\u00a0[21]: Copied! <pre>lib.list_snapshots()\n</pre> lib.list_snapshots() Out[21]: <pre>{}</pre> In\u00a0[22]: Copied! <pre># version 1, which was kept as part of the snapshot, has now been deleted\ntry:\n    vit = lib.read(\"sym_0\", as_of=1)\n    print(vit)\n    print(vit.data)\nexcept adb.exceptions.NoSuchVersionException:\n    logging.error(\"Version not found\")\n</pre> # version 1, which was kept as part of the snapshot, has now been deleted try:     vit = lib.read(\"sym_0\", as_of=1)     print(vit)     print(vit.data) except adb.exceptions.NoSuchVersionException:     logging.error(\"Version not found\") <pre>ERROR:root:Version not found\n</pre> In\u00a0[23]: Copied! <pre>lib.list_versions()\n</pre> lib.list_versions() Out[23]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00)}</pre> In\u00a0[24]: Copied! <pre>lib.snapshot(\"snapshot_1\", metadata=\"demo snapshot names need to be unique\")\n</pre> lib.snapshot(\"snapshot_1\", metadata=\"demo snapshot names need to be unique\") In\u00a0[25]: Copied! <pre>try:\n    lib.snapshot(\"snapshot_1\")\nexcept Exception as e:\n    logging.error(e)\n</pre> try:     lib.snapshot(\"snapshot_1\") except Exception as e:     logging.error(e) <pre>ERROR:root:E_ASSERTION_FAILURE Snapshot with name snapshot_1 already exists\n</pre> In\u00a0[26]: Copied! <pre>lib.list_snapshots()\n</pre> lib.list_snapshots() Out[26]: <pre>{'snapshot_1': 'demo snapshot names need to be unique'}</pre> In\u00a0[27]: Copied! <pre># exclude sym_1 from snapshot\nlib.snapshot(\"snapshot_2\", skip_symbols=[\"sym_1\"], metadata=\"demo skip_symbols\")\n</pre> # exclude sym_1 from snapshot lib.snapshot(\"snapshot_2\", skip_symbols=[\"sym_1\"], metadata=\"demo skip_symbols\") In\u00a0[28]: Copied! <pre>lib.list_versions()\n</pre> lib.list_versions() Out[28]: <pre>{sym_3_v0: (date=2023-11-20 10:24:45.103129257+00:00, snapshots=['snapshot_1', 'snapshot_2']),\n sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_1', 'snapshot_2']),\n sym_1_v1: (date=2023-11-20 10:24:45.431966093+00:00, snapshots=['snapshot_1']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00)}</pre> In\u00a0[29]: Copied! <pre># include specific versions of sym_1 and sym_2 from snapshot\nlib.snapshot(\"snapshot_3\", versions={\"sym_1\": 0, \"sym_2\": 0}, metadata=\"demo versions\")\n</pre> # include specific versions of sym_1 and sym_2 from snapshot lib.snapshot(\"snapshot_3\", versions={\"sym_1\": 0, \"sym_2\": 0}, metadata=\"demo versions\") In\u00a0[30]: Copied! <pre>lib.list_versions(snapshot=\"snapshot_3\")\n</pre> lib.list_versions(snapshot=\"snapshot_3\") Out[30]: <pre>{sym_2_v0: (date=2023-11-20 10:24:45.086132551+00:00, snapshots=['snapshot_1', 'snapshot_2', 'snapshot_3']),\n sym_1_v0: (date=2023-11-20 10:24:45.066268214+00:00, snapshots=['snapshot_3'])}</pre> In\u00a0[31]: Copied! <pre>lib.list_snapshots()\n</pre> lib.list_snapshots() Out[31]: <pre>{'snapshot_1': 'demo snapshot names need to be unique',\n 'snapshot_2': 'demo skip_symbols',\n 'snapshot_3': 'demo versions'}</pre>"},{"location":"notebooks/ArcticDB_demo_snapshots/#an-introduction-to-snapshots","title":"An Introduction to Snapshots\u00b6","text":"<p>In order to understand snapshots we first need to be clear about versions.</p> <p>In ArcticDB, every time a change is made to a symbol a new version is created. So each symbol has a sequence of versions through time.</p> <p>In a library there will typically be many symbols with each having many versions.</p> <p>Suppose we reach a point where we wish to record the current state of the data in the library. This is exactly the purpose of a snapshot.</p> <p>A snapshot records the current versions of all the symbols in the library (or a custom set of versions, see below)</p> <p>The data recorded in the snapshot can then be read back using the <code>as_of</code> parameter in the read.</p> <p>Versions that are part of a snapshot are protected from deletion, even if their symbol is deleted.</p> <p>Below is a simple example that demonstrates snapshots in action.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#installs-and-imports","title":"Installs and Imports\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#set-up-articdb","title":"Set up ArticDB\u00b6","text":"<p>Note: In this example we delete the library if it exists. That is not normal but we want to make sure we have a clean library in this case.</p> <p>Don't copy those lines unless you are sure that is what you need.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#create-some-symbols","title":"Create some symbols\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#create-the-snapshot","title":"Create the snapshot\u00b6","text":"<p>The metadata is optional</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#functions-to-discover-and-inspect-snapshots","title":"Functions to discover and inspect snapshots\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#reading-a-snapshot-version-of-a-symbol","title":"Reading a snapshot version of a symbol\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#demonstration-that-snapshot-versions-are-protected-from-deletion","title":"Demonstration that snapshot versions are protected from deletion\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#although-it-works-we-advise-not-to-read-snapshot-versions-directly-using-the-version-number","title":"Although it works, we advise not to read snapshot versions directly using the version number\u00b6","text":"<p>These versions only exist because they are in a snapshot, so it is much more obvious to code to access them via the snapshot.</p> <p>Accessing snapshot protected versions via the version number leads to code that will fail (if the snapshot is deleted) in a way that is difficult to understand.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#deleting-a-snapshot","title":"Deleting a snapshot\u00b6","text":"<p>When we delete a snapshot, any versions that are only referenced by that snapshot will be deleted.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#snapshot-names-must-be-unique","title":"Snapshot names must be unique\u00b6","text":"<p>Creating a snapshot with a name that already has a snapshot causes an error.</p>"},{"location":"notebooks/ArcticDB_demo_snapshots/#modifiers-for-snapshot-creation-exclude-or-include-symbols","title":"Modifiers for snapshot creation: exclude or include symbols\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#snapshots-why-and-why-not-to-use-them","title":"Snapshots: why and why not to use them\u00b6","text":""},{"location":"notebooks/ArcticDB_demo_snapshots/#why","title":"Why\u00b6","text":"<ul> <li>Snapshots record the current state of the library</li> <li>They can be thought of as recoverable checkpoints in the evolution of the data</li> <li>Snapshots can create an audit trail</li> <li>Snapshots protect their data from deletion by other activity in the library</li> </ul>"},{"location":"notebooks/ArcticDB_demo_snapshots/#why-not","title":"Why Not\u00b6","text":"<ul> <li>Generally we encourage the use of snapshots</li> <li>However if many snapshots are created they can impose a slight performance penalty on some operations due to the deletion protection</li> <li>Snapshots can also increase the storage used by ArcticDB, through protecting older versions that would otherwise be deleted</li> <li>Use snapshots in a considered fashion and delete them when they are no longer needed</li> </ul>"},{"location":"notebooks/ArcticDB_demo_snapshots/#further-info-extras","title":"Further Info / Extras\u00b6","text":"<p>For full descriptions of the functions used above, please see the ArcticDb documentation:</p> <ul> <li><code>snapshot()</code> https://docs.arcticdb.io/latest/api/library/#arcticdb.version_store.library.Library.snapshot</li> <li><code>list_snapshots()</code> https://docs.arcticdb.io/latest/api/library/#arcticdb.version_store.library.Library.list_snapshots</li> <li><code>list_versions()</code> https://docs.arcticdb.io/latest/api/library/#arcticdb.version_store.library.Library.list_versions</li> </ul>"},{"location":"technical/architecture/","title":"Architecture","text":""},{"location":"technical/architecture/#overview","title":"Overview","text":"<p>ArcticDB is deployed as a shared library, using PyBind for operability between CPython and the core database engine which is written in C++.</p> <p>Users interact with the C++ storage engine, and therefore the storage itself, via the Python/C++ bindings. The engine transforms Python objects which are typically DataFrames, Series or numpy arrays to and from its internal columnar structure. The data is then tiled, indexed, compressed, and written to storage. The storage format is tailor-designed for the storage and retrieval of dense and sparse timeseries data. </p> <p>Please note that there is no required server component.</p> <p></p>"},{"location":"technical/architecture/#arcticdb-dataflow","title":"ArcticDB DataFlow","text":"<p>The below diagram visualises the flow of data through ArcticDB from source to storage and back again:</p> <p></p> <p>For more information on the storage format, please see On-Disk Storage.</p>"},{"location":"technical/contributing/","title":"Contribution Licensing","text":"<p>Since this project is distributed under the terms of the BSL license, contributions that you make are licensed under the same terms. For us to be able to accept your contributions, we will need explicit confirmation from you that you are able and willing to provide them under these terms, and the mechanism we use to do this is the ArcticDB Individual Contributor License Agreement.</p> <p>Individuals - To participate under these terms, please include the following line as the last line of the commit message for each commit in your contribution. You must use your real name (no pseudonyms, and no anonymous contributions).</p> <pre><code>Signed-Off By: Random J. Developer &lt;random@developer.example.org&gt;. By including this sign-off line I agree to the terms of the Contributor License Agreement.\n</code></pre> <p>Corporations - For corporations who wish to make contributions to ArcticDB, please contact info@arcticdb.io and we will arrange for the CLA to be sent to the signing authority within your corporation.</p>"},{"location":"technical/contributing/#docker-quickstart","title":"Docker Quickstart","text":"<p>This quickstart builds a release using build dependencies from vcpkg. ArcticDB releases on PyPi use vcpkg dependencies in the manner as described below.</p> <p>Note the below instructions will build a Linux X86_64 release.</p>"},{"location":"technical/contributing/#1-start-the-arcticdb-build-docker-image","title":"1) Start the ArcticDB build docker image","text":"<p>Run in a Linux terminal:</p> <pre><code>docker pull ghcr.io/man-group/cibuildwheel_manylinux:2.12.1-3a897\ndocker run -it ghcr.io/man-group/cibuildwheel_manylinux:2.12.1-3a897\n</code></pre> <p>:warning: The below instructions do not have to be run in the provided docker image. They can be run against any Python installation on any Linux distribution as long as the basic build dependencies are available.</p> <p>If running outside the provided docker image, please change <code>/opt/python/cp39-cp39/bin/python3</code> in the examples below to an appropriate path for Python.</p>"},{"location":"technical/contributing/#2-check-out-arcticdb-including-submodules","title":"2) Check out ArcticDB including submodules","text":"<pre><code>cd\ngit clone https://github.com/man-group/ArcticDB.git\ncd ArcticDB\ngit submodule init &amp;&amp; git submodule update\n</code></pre>"},{"location":"technical/contributing/#3-kick-off-the-build","title":"3) Kick off the build","text":"<pre><code>MY_PYTHON=/opt/python/cp39-cp39/bin/python3  # change if outside docker container/building against a different python\n$MY_PYTHON -m pip install -U pip setuptools wheel grpcio-tools\nARCTIC_CMAKE_PRESET=skip $MY_PYTHON setup.py develop\n# Change the below Python_EXECUTABLE value to build against a different Python version\ncmake -DPython_EXECUTABLE=$MY_PYTHON -DTEST=off --preset linux-debug cpp\npushd cpp\ncmake --build --preset linux-debug\npopd\n</code></pre>"},{"location":"technical/contributing/#4-run-arcticdb","title":"4) Run ArcticDB","text":"<p>Ensure the below is run in the Git project root:</p> <pre><code># PYTHONPATH first part = Python module, second part compiled C++ binary\nPYTHONPATH=`pwd`/python:`pwd`/cpp/out/linux-debug-build/arcticdb/ $MY_PYTHON\n</code></pre> <p>Now, inside the Python shell:</p> <pre><code>from arcticdb import Arctic\n</code></pre> <p>Rather than setting the <code>PYTHONPATH</code> environment variable, you could install the appropriate paths into your Python environment by running (note that this will invoke the build tooling so will compile any changed files since the last compilation):</p> <pre><code>$MY_PYTHON -m pip install -ve .\n</code></pre> <p>Note that as this will copy the binary to your Python installation this will have to be run after each and every change of a C++ file.</p>"},{"location":"technical/contributing/#building-using-mamba-and-conda-forge","title":"Building using mamba and conda-forge","text":"<p>This section uses build dependencies from conda-forge. It is a pre-requisite for releasing ArcticDB on conda-forge.</p> <p>\u26a0\ufe0f At the time of writing, installing ArcticDB with this setup under Windows is not possible due to a linkage problems with libprotobuf. See: https://github.com/man-group/ArcticDB/pull/449</p> <ul> <li>Install <code>mamba</code></li> <li>Create the <code>arcticdb</code> environment from its specification (<code>environment_unix.yml</code>):</li> </ul> <pre><code>mamba env create -f environment_unix.yml\n</code></pre> <ul> <li>Activate the <code>arcticdb</code> environment (you will need to do this for every new shell session):</li> </ul> <pre><code>mamba activate arcticdb\n</code></pre>"},{"location":"technical/contributing/#building-cmake-targets","title":"Building CMake targets","text":"<p>Several CMake presets are defined for build types, OS's, and build system.</p> <p>For instance:</p> <ul> <li>for debug build on Linux with mamba and conda-forge, use:</li> </ul> <pre><code>export ARCTICDB_USING_CONDA=1\ncmake -DTEST=off --preset linux-conda-debug cpp\ncd cpp\n\n# You might need to use fewer threads than what's possible on your machine\n# to not have it swap and freeze (e.g. we use 1 here).\ncmake --build --preset linux-conda-debug -j 1\n</code></pre> <ul> <li>for release build on MacOS with mamba and conda-forge, use:</li> </ul> <p><pre><code>export ARCTICDB_USING_CONDA=1\ncmake -DTEST=off --preset darwin-conda-release cpp\ncd cpp\n\n# You might need to use fewer threads than what's possible your machine\n# not to have it swap and freeze (e.g. we use 1 here).\ncmake --build --preset linux-conda-debug -j 1\n</code></pre> Note: If you don't use presets, you may want to set some useful environment variables:</p> <pre><code># To define the number of threads to use\nexport CMAKE_BUILD_PARALLEL_LEVEL=1\n# To enable C++ tests\nexport ARCTICDB_BUILD_CPP_TESTS=1\n</code></pre>"},{"location":"technical/contributing/#building-and-installing-the-python-package","title":"Building and installing the Python Package","text":"<ul> <li>Build and install ArcticDB in the <code>arcticdb</code> environment using dependencies installed in this environment.    We recommend using the <code>editable</code> installation for development:</li> </ul> <pre><code>export ARCTICDB_USING_CONDA=1\n# Adapt the CMake preset to your setup.\nexport ARCTIC_CMAKE_PRESET=linux-conda-debug\npython -m pip install --no-build-isolation --no-deps --verbose --editable .\n</code></pre> <ul> <li>Use ArcticDB from Python:</li> </ul> <pre><code>from arcticdb import Arctic\n</code></pre>"},{"location":"technical/contributing/#faq","title":"FAQ","text":""},{"location":"technical/contributing/#how-do-i-build-against-different-python-versions","title":"How do I build against different Python versions?","text":"<p>Run <code>cmake</code> (configure, not build) with either:</p> <ol> <li>A different version of Python as the first version of Python on your PATH or...</li> <li>Point the <code>Python_EXECUTABLE</code> CMake variable to a different Python binary</li> </ol> <p>Note that to build the ArcticDB C++ tests you must have the Python static library available in your installation!</p>"},{"location":"technical/contributing/#how-do-i-run-the-python-tests","title":"How do I run the Python tests?","text":"<p>See running tests below.</p>"},{"location":"technical/contributing/#how-do-i-run-the-c-tests","title":"How do I run the C++ tests?","text":"<p>See running tests below.</p>"},{"location":"technical/contributing/#how-do-i-specify-how-many-cores-to-build-using","title":"How do I specify how many cores to build using?","text":"<p>This is determined auto-magically by CMake at build time, but can be manually set by passing in <code>--parallel &lt;num cores&gt;</code> into the build command.</p>"},{"location":"technical/contributing/#detailed-build-information","title":"Detailed Build Information","text":""},{"location":"technical/contributing/#docker-image-construction","title":"Docker Image Construction","text":"<p>The above docker image is built from ManyLinux. Build script is located here.</p> <p>GitHub output here.</p> <p>We recommend you use this image for compilation and testing!</p>"},{"location":"technical/contributing/#setting-up-linux","title":"Setting up Linux","text":"<p>The codebase and build system can work with any reasonably recent Linux distribution with at least GCC 8 (10+ recommended) and CMake 3.12 (these instructions assume 3.21+).</p> <p>A development install of Python 3.6+ (with <code>libpython.a</code> or <code>.so</code> and full headers) is also necessary. See pybind11 configuration.</p> <p>We require a Mongo executable for a couple of Python tests on Linux. You can check whether you have it with <code>mongod --version</code>.</p> <p>Search the internet for \"mongo installation Linux\" for instructions for your distro if you do not already have <code>mongod</code> available.</p>"},{"location":"technical/contributing/#dependencies-by-distro","title":"Dependencies by distro","text":"Distro Versions reported to work Packages Ubuntu 20.04, 22.04 build-essential g++-10 libpcre3-dev libsasl2-dev libsodium-dev libkrb5-dev libcurl4-openssl-dev python3-dev Centos 7 devtoolset-10-gcc-c++ openssl-devel cyrus-sasl-devel devtoolset-10-libatomic-devel libcurl-devel python3-devel"},{"location":"technical/contributing/#setting-up-windows","title":"Setting up Windows","text":"<p>We recommend using Visual Studio 2022 (or later) to install the compiler (MSVC v142 or newer) and tools (Windows SDK, CMake, Python).</p> <p>The Python that comes with Visual Studio is sufficient for creating release builds, but for debug builds, you will have to separately download from Python.org.</p> <p>After building <code>arcticdb_ext</code>, you need to symlink to the <code>.pyd</code> for the Python tests to run against it:</p> <pre><code># From the root of the ArcticDB git root checkout, in an administrator powershell session\n# Change Python version as appropriate - below is for Python 3.11.\nNew-Item -Path .\\python\\arcticdb_ext.cp311-win_amd64.pyd -ItemType SymbolicLink -Value .\\cpp\\out\\windows-cl-debug-build\\arcticdb\\arcticdb_ext.cp311-win_amd64.pyd\n</code></pre>"},{"location":"technical/contributing/#running-python-tests","title":"Running Python tests","text":"<p>With <code>python</code> pointing to a Python interpeter with <code>ArcticDB</code> installed/on the <code>PYTHON_PATH</code>:</p> <pre><code>python -m pip install arcticdb[Testing]\npython -m pytest python/tests\n</code></pre>"},{"location":"technical/contributing/#running-c-tests","title":"Running C++ tests","text":"<p>Configure ArcticDB with TEST=ON (default is OFF):</p> <pre><code>cmake -DPython_EXECUTABLE=&lt;path to python&gt; -DTEST=ON --preset linux-debug cpp\n</code></pre> <p>Or you can set the following environment variable:</p> <pre><code>export ARCTICDB_BUILD_CPP_TESTS=1\n</code></pre> <p>Note that <code>&lt;path to python&gt;</code> must point to a Python that is compatible with Development.Embed. This will probably be the result of installing <code>python3-devel</code> from your dependency manager.</p> <p>Inside the provided docker image, <code>python3-devel</code> resolves to Python 3.6 installed at <code>/usr/bin/python3</code>, so the resulting command will be:</p> <pre><code>cmake -DPython_EXECUTABLE=/usr/bin/python3 -DTEST=ON --preset linux-debug cpp\n</code></pre> <p>Then build and run the tests:</p> <pre><code>cd cpp/out/linux-debug-build\nmake -j 1 arcticdb_rapidcheck_tests\nmake -j 1 test_unit_arcticdb\nmake test\n</code></pre>"},{"location":"technical/contributing/#cibuildwheel","title":"CIBuildWheel","text":"<p>Our source repo works with CIBuildWheel which runs the compilation and tests against all supported Python versions in isolated environments. Please follow their documentation.</p>"},{"location":"technical/contributing/#configurations","title":"Configurations","text":""},{"location":"technical/contributing/#cmake-presets","title":"CMake presets","text":"<p>To make it easier to set and share all the environment variables, config and commands, we recommend using the CMake presets feature.</p> <p>Recent versions of some popular C++ IDEs support reading/importing these presets: * Visual Studio &amp; Code * CLion</p> <p>And it's equally easy to use on the command line.</p> <p>We already ship a <code>CMakePresets.json</code> in the cpp directory, which is used by our builds. You can add a <code>CMakeUserPresets.json</code> in the same directory for local overrides. Inheritance is supported.</p> <p>If you're working on Linux but not using our Docker image, you may want to create a preset with these <code>cacheVariables</code>: * <code>CMAKE_MAKE_PROGRAM</code> - <code>make</code> or <code>ninja</code> should work * <code>CMAKE_C_COMPILER</code> and <code>CMAKE_CXX_COMPILER</code> - If your preferred compiler is not <code>cc</code> and <code>cxx</code></p> <p>More examples:</p> Windows Preset to specify a Python version <pre><code>{\n  \"version\": 3,\n  \"configurePresets\": [\n    {\n      \"name\": \"alt-vcpkg-debug:py3.10\",\n      \"inherits\": \"windows-cl-debug\",\n      \"cacheVariables\": {\n        \"Python_ROOT_DIR\": \"C:\\\\Program Files\\\\Python310\"\n      },\n      \"environment\": {\n        \"PATH\": \"C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\Scripts;C:\\\\Program Files\\\\Python310;$penv{PATH}\",\n        \"PYTHONPATH\": \"C:\\\\Program Files\\\\Python310\\\\Lib;C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\"\n      }\n    }\n  ],\n  \"buildPresets\": [\n    {\n      \"name\": \"alt-vcpkg-debug:py3.10\",\n      \"configurePreset\": \"alt-vcpkg-debug:py3.10\",\n      \"inheritConfigureEnvironment\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"technical/contributing/#vcpkg-caching","title":"vcpkg caching","text":"<p>We use vcpkg to manage the C++ dependencies.</p> <p>Compiling the dependencies uses a lot of disk space. Once CMake configuration is done, you can remove the <code>cpp\\vcpkg\\buildtrees</code> folder.</p> <p>You may also want to configure some caches: * Binary caching * Asset caching</p>"},{"location":"technical/contributing/#pybind11-configuration","title":"pybind11 configuration","text":"<p>We augmented pybind11's Python discovery with our own PythonUtils to improve diagnostics. Please pay attention to warning messages from <code>PythonUtils.cmake</code> in the CMake output which highlights any configuration issues with Python.</p> <p>We compile against the first <code>python</code> on the <code>PATH</code> by default.</p> <p>To override that, use one of the following CMake variables*:</p> <ul> <li> <p><code>Python_ROOT_DIR</code> - The common path \"prefix\" for a particular Python install.   Usually, the <code>python</code> executable is in the same directory or the <code>bin</code> subdirectory.   This directory should also contain the <code>include</code> and <code>lib(rary)</code> subdirectories.   E.g. <code>/usr</code> for a system-wide install on *nix;   <code>/opt/pythonXX</code> for a locally-managed Python install;   <code>/home/user/my_virtualenv</code> for a virtual env;   <code>C:\\Program Files\\PythonXX</code> for a Windows Python install</p> </li> <li> <p><code>Python_EXECUTABLE</code> - The path to the Python executable. (CMake 3.15+)   CMake will try to extract the <code>include</code> and <code>library</code> paths by running this program.   This differs from the default behaviour of FindPython.</p> </li> </ul> <p>(* Note CMake variables are set with <code>-D</code> on the CMake command line or with the <code>cacheVariables</code> key in <code>CMake*Presets.json</code>.    The names are case-sensitive.</p> <p>(Only) <code>Python_ROOT_DIR</code> can also be set as an environment variable.    Setting the others in the environment might have no effect.)</p>"},{"location":"technical/contributing/#development-guidelines","title":"Development Guidelines","text":"<p>ArcticDB has a lot of write-time options to configure how the data is stored on disk. When adding new features, it is important to test not only the simplest case, but also how the new feature interacts with these various options. This section serves as a guide as to what should be considered when adding a new feature.</p>"},{"location":"technical/contributing/#backwards-compatibility","title":"Backwards compatibility","text":""},{"location":"technical/contributing/#data-stored-on-disk","title":"Data stored on disk","text":"<p>Due to ArcticDB being a purely client-side database, it is possible for data to be written by clients that are of later versions than clients that need to read the same data. If the change is breaking, such that older clients will not be able to read the data, then this should be clearly documented both in the PR and in the online documentation. If possible, a version number should also be added such that future changes in the same area will display a more clear error message.</p>"},{"location":"technical/contributing/#api","title":"API","text":"<p>Any changes to the API (including when exceptions are thrown and the type of the exception) must be weighed up against the change breaking behaviour for existing users. Please make this as clear to reviewers as possible by ensuring API changes are clearly described in the PR description.  If the change is breaking, please also ensure that that is appropriately highlighted in the PR description as well. This is particularly true of the <code>NativeVersionStore</code> API, as this has many users inside Man Group.</p>"},{"location":"technical/contributing/#snapshots","title":"Snapshots","text":"<p>Symbols are almost completely decoupled from one another in ArcticDB. The major feature for which this is not the case is snapshots. Whenever a change involves deleting any keys from the storage, care must be taken that those keys are not in fact still needed by a snapshot. Similarly, any operation that modifies a snapshot, including deleting it, must ensure that any keys for which this snapshot was the last reference are also deleted. See the tutorial and API documentation for more details.</p>"},{"location":"technical/contributing/#batch-methods","title":"Batch methods","text":"<p>Almost all of the major reading and writing methods have batch equivalents. If a feature is added for the non-batch version, it should almost always work with the batch version as well. We should also strive for consistency of behaviour across the batch methods in circumstances that could be encountered by any of them. e.g. <code>read_batch</code> and <code>read_metadata_batch</code> should have the same behaviour if the specified version does not exist.</p>"},{"location":"technical/contributing/#dynamic-schema","title":"Dynamic schema","text":"<p>Static schema is much simpler than dynamic schema, and so features added and only tested with static schema may not \"just work\" with dynamic schema. In particular, the following cases should be considered:</p> <ul> <li>A column that is not present in an initial call to <code>write</code> is present in a subsequent call to <code>append</code>.</li> <li>A column that is present in an initial call to <code>write</code> is not present in a subsequent call to <code>append</code>.</li> <li>A numeric column with a \"small\" type (e.g. <code>uint8</code>) is appended to with a numeric column of a larger type (e.g. int16), and vice versa.</li> </ul>"},{"location":"technical/contributing/#segmentation","title":"Segmentation","text":"<p>Most tests are written with small amounts of data to keep the runtime down as low as possible. However, many bugs are only exposed when data being written is column-sliced and/or row-sliced. Using test fixtures configured to slice into small data segments (e.g. 2x2) can help to catch these issues early.</p>"},{"location":"technical/contributing/#data-sub-selection","title":"Data sub-selection","text":"<p>Many use cases of ArcticDB involve writing huge symbols with hundreds of thousands of columns or billions of rows. In these cases, methods for reading data will almost never be called without some parameters to cut down the amount of data returned to the user. The most commonly used are:</p> <ul> <li>The stand-alone methods <code>head</code> and <code>tail</code>*</li> <li><code>columns</code> - select only a subset of the columns</li> <li><code>date_range</code> and <code>row_range</code> - select a contiguous range of rows</li> <li>More advanced filtering - see section on processing pipeline</li> </ul>"},{"location":"technical/contributing/#processing-pipeline","title":"Processing pipeline","text":"<p>Reading data using only <code>columns</code>, <code>date_range</code>, and <code>row_range</code> arguments is heavily optimised to avoid copying data in memory unnecessarily.  Any read-like method provided with the <code>query_builder</code> optional argument, or equivalently, lazy reads with further processing applied, takes quite a different code path, as we do not know the shape of the output data a priori.  While it is not necessary to exhaustively test all new features against every possible analytics operations, it is generally worth having a test using a simple filter that excludes some of the data that would otherwise be returned to the user.</p>"},{"location":"technical/contributing/#sparse-columns","title":"Sparse columns","text":"<p>The majority of data written into ArcticDB uses dense columns, meaning that within a given row-slice, every row has an associated value in every column (even if that value is <code>NaN</code>). The concept of sparse columns is implemented as well, where columns may have values missing for none, some, or all rows within a row-slice. Any read-like method should be able to handle both types of stored column.</p>"},{"location":"technical/contributing/#pickling","title":"Pickling","text":"<p>When data that cannot be normalized to a supported data type in ArcticDB, it can still be stored using <code>write_pickle</code> and similar. There are many operations that cannot be performed on pickled data, such as <code>append</code>, <code>update</code>, <code>date_range</code> search, and many more. It is important that if a user attempts an operation that is not supported with pickled data, that they receive a helpful error message.</p>"},{"location":"technical/migration/","title":"Migrating from Arctic","text":""},{"location":"technical/migration/#arcticdb-vs-arctic","title":"ArcticDB vs Arctic","text":"<p>ArcticDB is API-compatible with Arctic. Please note however that it is not data compatible. You cannot use ArcticDB to read Arctic data, nor use Arctic to write data that ArcticDB can read.</p>"},{"location":"technical/migration/#how-can-i-migrate-from-arctic-to-arcticdb","title":"How can I migrate from Arctic to ArcticDB","text":"<p>There are two ways that you can migrate data from Arctic to ArcticDB.</p> <ul> <li>Manual migration: The easiest way to migrate relatively small datasets is to read the data out of Arctic, and then write it using ArcticDB. As the ArcticDB is API compatible, this should be relatively simple.</li> <li>Data Conversion: Large data estates can be converted from Arctic to ArcticDB by using automated tooling that we can provide. For more information, please contact our commercial team via our website or email us at info@arcticdb.io. </li> </ul>"},{"location":"technical/on_disk_storage/","title":"ArcticDB On-Disk Storage Format","text":"<p>ArcticDB uses a custom storage format that differs from Parquet, HDF5 or other similar well-known columnar storage formats. This page provides a high-level description of this format.</p> <p>The ArcticDB storage engine is designed to work with any key-value storage backend and currently has full optimised support for the following backends:</p> <ol> <li>S3</li> <li>LMDB</li> </ol> <p>The structure of the stored data is optimised for that storage - for example, when using S3 all related paths (or keys) share a common prefix to optimise for <code>ListObjects</code> calls.</p> <p>Despite this storage specific optimisation, the hierarchy and segment format remain identical regardless of the backend storage. These components are described below.</p>"},{"location":"technical/on_disk_storage/#structural-overview","title":"Structural overview","text":"<p>The above diagram provides an overview of this format, illustrating how ArcticDB manages the symbol metadata, version history and index layout of the data it stores. Not shown is the binary format used to store ArcticDB segments.</p> <p>Note</p> <p>Please note that the key formatting illustrated on the left hand side of the above diagram is specific to S3. The formatting may differ depending on the underlying storage and as such the key paths might not match  when using another storage engine such as LMDB.</p> <p>The ArcticDB storage format is comprised of 4 layers; the Reference Layer, the Version Layer, the Index Layer and finally, the Data Layer. For a full definition of a key, please see the above diagram. </p>"},{"location":"technical/on_disk_storage/#reference-layer","title":"Reference Layer","text":"<p>The reference layer maintains an active pointer to the head of the version layer linked list, which enables fast retrieval of the latest version of a symbol. This pointer is stored in the Data Segment as illustrated in the above diagram. As a result, the reference layer is the only mutable part of ArcticDB's storage format with the value of each reference-layer-key able to be overwritten (hence using a Reference Key, rather than an Atom Key)</p> <p>The reference layer primarily stores keys of type Version Reference, as documented below.</p>"},{"location":"technical/on_disk_storage/#version-layer","title":"Version Layer","text":"<p>The version layer contains a linked list of immutable atom keys/values. Each element of the linked list contains two pointers in the data segment; one points to the next entry in the linked list, and the other points to an index key, providing a route through to the index layer of the storage structure. As a result, traversing the version layer linked list for a symbol allows you to travel backwards through time to retrieve data as it were at a previous version/point in time. The version layer also contains information about which versions have been deleted, and which are still \"live\".</p> <p>This means that for symbols with a lot of versions, this linked list can get quite long, and so reading old versions (using the <code>as_of</code> argument) can become slower as lots of tiny object reads are required in order to find the relevant index key. A method will soon be added to the <code>Library</code> API allowing users to \"compact\" this linked list into a few larger objects.</p> <p>The version layer primarily stores keys of type Version, as documented below.</p>"},{"location":"technical/on_disk_storage/#index-layer","title":"Index Layer","text":"<p>The index layer is an immutable layer that provides a B-Tree index over the data layer. Much like the reference and version layer, this utilises data segments containing data pointers. Each pointer is simply a key that that contains a data segment. </p> <p>For more information on the data stored in this layer, see the Structural Overview diagram.</p> <p>The index layer primarily stores keys of type Table Index, as documented below.</p>"},{"location":"technical/on_disk_storage/#data-layer","title":"Data Layer","text":"<p>The data layer is an immutable layer that contains compressed data segments. Dataframes provided by the user are sliced by both columns and rows, in order to facilitate rapid date-range and column searching during read operations. See the documentation for the <code>rows_per_segment</code> and <code>columns_per_segment</code> library configuration options for more details.</p> <p>The data layer primarily stores keys of type Table Data, as documented below.</p>"},{"location":"technical/on_disk_storage/#arcticdb-key-types","title":"ArcticDB Key Types","text":""},{"location":"technical/on_disk_storage/#what-is-an-arcticdb-key","title":"What is an ArcticDB key?","text":"<p>ArcticDB stores data in either LMDB or object storage (S3). Regardless of the backend though, ArcticDB stores data in well-defined key types where a key type defines the purpose of the associated value and the structure of the name/path of the key. Examples of how the paths differ for S3 are shown below.</p> <p>Note that where this documentation refers to a Key Type key (for example a version reference key), it refers to both the S3/LMDB key and the associated S3/LMDB value.</p>"},{"location":"technical/on_disk_storage/#key-types","title":"Key Types","text":"Key Type Atom/Reference S3 Prefix Purpose Version Reference Reference vref Maintains the top level pointer to the latest version of a symbol Version Atom ver Maintains pointers to index keys for one or more versions of a symbol Table Index Atom tindex Maintains an index structure over the data Table Data Atom tdata Maintains the data for a table Symbol List Atom sl Caches symbol addition/removals <p>Note that Atom keys are immutable. Reference keys are not immutable and therefore the associated value can be updated. </p> <p>Version Ref</p> <p>In this documentation, <code>Version Reference key</code> is sometimes shortened to <code>Version Ref key</code>. </p>"},{"location":"technical/on_disk_storage/#example-paths","title":"Example paths","text":"<p>The paths below are examples of S3-based paths. LMDB ArcticDB libraries may have slightly different paths. </p> Key Type Example Path Version Ref {bucket}/{library prefix}/{library storage id}/vref/sUtMYSYMBOL Symbol List {bucket}/{library prefix}/{library storage id}/*sSt*__add__*6*1632469542244313925*8986477046003934300*ORJDSG8GU4_6*ORJDSG8GU4_6"},{"location":"technical/on_disk_storage/#additional-information","title":"Additional Information","text":""},{"location":"technical/on_disk_storage/#data-layer-fragmentation","title":"Data Layer Fragmentation","text":"<p>As the example given in the Structural Overview diagram demonstrates, Atom Keys in the data layer from older versions are re-used (i.e. pointed to) by the new version's index layer if the data in them is still needed. This is always trivially true for <code>append</code> operations, and this is conceptually simpler, so the example we will work with here is based on <code>append</code>. It is worth noting that all of the same arguments also apply to <code>update</code> as well though.</p> <p>By re-using data layer keys from the previous version, calls to <code>append</code> are as efficient as possible, as they do not need to read any of the data layer keys from the previous version. However, this can result in the data layer becoming fragmented, in the sense that data in the data layer is spread over a lot of small objects in storage, and this can have a negative performance impact on <code>read</code> operations.</p> <p>Consider a use case where a symbol with 10 columns, all of 8 byte numeric types, is appended to once per minute. This means that each day, 1,440 data layer segments are written, each with just 80 bytes of information in. Calling <code>read</code> for a days worth of data therefore requires 1,440 reads of these tiny data layer objects, which will be much less efficient than reading a single object of 115,200 bytes.</p> <p>We will soon be adding an API to perform exactly this operation, re-slicing data such that subsequent reads can be as efficient as possible, without harming the efficiency of existing <code>append</code> or <code>update</code> operations.</p>"},{"location":"technical/on_disk_storage/#symbol-list-caching","title":"Symbol List Caching","text":"<p>Speeding up listing symbols</p> <p>The below documentation details the architecture for the symbol list cache.</p> <p>It explains that to speed up <code>list_symbols</code>, simply run <code>list_symbols</code> through to completion frequently.  The cache is built on first run and compacted afterwards.  This will speed up <code>list_symbols</code> for all accessors of the library - not just the user that runs <code>list_symbols</code>. </p> <p><code>list_symbols</code> is a common operation to perform on an ArcticDB library. As this returns a list of \"live\" symbols (those for which at least one version has not been deleted), using the data structures described above, this involves:</p> <ul> <li>Loading a list of version reference keys in the library.</li> <li>For each version reference key:<ul> <li>Traverse the version key linked-list until either a live version is found, or it is established that all versions have been deleted.</li> </ul> </li> </ul> <p>For many libraries, these operations will be quick enough. However, if there are millions of symbols, or if the first live version to be found for symbols is very far back in the version key linked-list, then this method can be prohibitively expensive.</p> <p>Therefore, ArcticDB maintains a cache of live symbols, such that the <code>list_symbols</code> call should always return quickly. This works by writing special atom keys into the storage that track when symbols are created or deleted, and the time that the operation happened. These Symbol List atom keys are of the form:</p> <ul> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*&lt;timestamp 1&gt;*&lt;content hash&gt;*&lt;symbol name&gt;*&lt;symbol name&gt;</code> - signifies that <code>&lt;symbol name&gt;</code> was created at <code>&lt;timestamp 1&gt;</code></li> <li><code>&lt;library prefix&gt;/sl/*sSt*__delete__*0*&lt;timestamp 2&gt;*&lt;content hash&gt;*&lt;symbol name&gt;*&lt;symbol name&gt;</code> - signifies that <code>&lt;symbol name&gt;</code> was deleted at <code>&lt;timestamp 2&gt;</code></li> </ul> <p>The operation to list symbols then involves:</p> <ul> <li>Read all of the symbol list atom keys for the library (this resolves to a <code>ListObjects</code> call with S3 storage).</li> <li>Separate the keys by the symbol they refer to being created/deleted.</li> <li>For each symbol:<ul> <li>Check if the most recent operation was a creation or a deletion.</li> </ul> </li> </ul> <p>Symbols for which the latest operation is a creation are then returned to the caller of <code>list_symbols</code>.</p> <p>Without any housekeeping, this process could lead to unbounded growth in the number of symbol list atom keys in the library. Even worse, many of these keys would contain redundant information, as we only care about the latest operation for each symbol. Therefore, whenever <code>list_symbols</code> is called by a client with write permissions on the library, if there are too many (500 by default, see the Runtime Configuration page for details on how to configure) symbol list atom keys, all of the information from these keys is compacted into a single symbol list atom key, and the old keys are deleted. For example, if there were 4 symbol list atom keys:</p> <ul> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t0*&lt;content hash&gt;*symbol1*symbol1</code> Create \"symbol1\" at t0</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__delete__*0*t1*&lt;content hash&gt;*symbol1*symbol1</code> Delete \"symbol1\" at t1</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t2*&lt;content hash&gt;*symbol12*symbol2</code> Create \"symbol2\" at t2</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t3*&lt;content hash&gt;*symbol1*symbol1</code> Create \"symbol1\" at t3</li> </ul> <p>They would be compacted into a single object stating that \"symbol1\" and \"symbol2\" were both alive at time t3. The key for this object is of the form <code>&lt;library prefix&gt;/sl/*sSt*__symbols__*0*t3*&lt;content hash&gt;*0*0</code>.</p> <p>Astute observers will correctly take issue with basing logical decisions on timestamps in a purely client-side database with no synchronisation between clocks of different clients enforced. As such, this cache can diverge from reality, if two different clients create and delete the same symbol at about the same time, and this is the most likely cause of odd behaviour such as <code>lib.read(symbol)</code> working, but <code>symbol in lib.list_symbols()</code> returning <code>False</code>. If this happens, <code>lib.reload_symbol_list()</code> should resolve the issue.</p>"},{"location":"technical/release_checks/","title":"Release checks","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom pandas.testing import assert_frame_equal\nimport numpy as np\n</pre> import pandas as pd from pandas.testing import assert_frame_equal import numpy as np In\u00a0[\u00a0]: Copied! <pre>from arcticdb import Arctic\nimport arcticdb\n</pre> from arcticdb import Arctic import arcticdb In\u00a0[\u00a0]: Copied! <pre>ver = arcticdb.__version__\n</pre> ver = arcticdb.__version__ In\u00a0[\u00a0]: Copied! <pre>ac = Arctic(f\"lmdb:///tmp/{ver}\")\nassert ac.list_libraries() == []\n</pre> ac = Arctic(f\"lmdb:///tmp/{ver}\") assert ac.list_libraries() == [] In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(np.random.randint(0,100,size=(1000000, 4)), columns=list('ABCD'))\nlib = ac.create_library(\"test\")\nassert ac.list_libraries() == [\"test\"]\n</pre> df = pd.DataFrame(np.random.randint(0,100,size=(1000000, 4)), columns=list('ABCD')) lib = ac.create_library(\"test\") assert ac.list_libraries() == [\"test\"] In\u00a0[\u00a0]: Copied! <pre>lib.write(\"s\", df)\nassert_frame_equal(lib.read(\"s\").data, df)\n</pre> lib.write(\"s\", df) assert_frame_equal(lib.read(\"s\").data, df) In\u00a0[\u00a0]: Copied! <pre>ac.delete_library(\"test\")\nassert ac.list_libraries() == []\n</pre> ac.delete_library(\"test\") assert ac.list_libraries() == []"},{"location":"technical/upgrade_storage/","title":"Upgrade storage config","text":"<p>You only need to follow this guide when pointed to it by an error message when accessing your library or when asked to during an upgrade.</p> <p>This indicates that the stored config for your library is unsupported by this version of ArcticDB.</p> <p>The rest of this guide explains how to update the stored config across all libraries in an Arctic instance.</p> <p>Since this requires write access on the storage, this should be performed by a suitably permissioned user.</p>"},{"location":"technical/upgrade_storage/#upgrade-script","title":"Upgrade script","text":""},{"location":"technical/upgrade_storage/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>Ensure that all users are on at least version 3.0.0 of ArcticDB</li> <li>Install latest ArcticDB <code>pip install -U arcticdb</code> or <code>conda install -c conda-forge arcticdb</code></li> <li>You must have write credentials on the storage you are using as your ArcticDB backend (eg S3 bucket / Azure blob storage)</li> <li>Create a <code>uri</code> suitable to use with an <code>Arctic</code> instance for your backend, with write credentials. See docs</li> <li>Run <code>arcticdb_update_storage --uri \"&lt;uri&gt;\"</code> where <code>&lt;uri&gt;</code> is that created in the step above. This will not modify anything, but will log the affected libraries with \"Config NOT OK for \""},{"location":"technical/upgrade_storage/#run-script","title":"Run Script","text":"<p>If no libraries were shown as affected after following the steps above, you can stop now. You do not need to do any more.</p> <p>:warning: Running this script will break access for clients on less than version 3.0.0 of ArcticDB for affected libraries. The affected libraries were shown in the step above. Ensure users have upgraded to at least version <code>arcticdb==3.0.0</code> first.</p> <ul> <li>Run <code>arcticdb_update_storage --uri \"&lt;uri&gt;\" --run</code> where <code>&lt;uri&gt;</code> is the same as the one above.</li> </ul>"},{"location":"technical/upgrade_storage/#release-history","title":"Release History","text":"ArcticDB Version Upgrade Github Issue 3.0.0 Removes credentials from stored config. #802"},{"location":"tutorials/data_organisation/","title":"Guidelines for Organising Your Data","text":""},{"location":"tutorials/data_organisation/#introduction","title":"Introduction","text":"<p>We are often asked about how best to organise data in ArcticDB. There is no single answer to this question - it will depend on many factors, but especially on the data itself and how it will be used.</p> <p>ArcticDB offers a high degree of flexibility in how data is arranged. Although this is a strength of the system, sometimes the choices are so broad that it is hard to know where to start.</p> <p>In this guide we aim to outline some design principles and rules of thumb to help you decide the best scheme for your data, system and users.</p> <p>We begin by taking a design view of the way ArcticDB structures data and the system design concerns, then from those consider how to assess and balance the trade-offs of some typical organisation strategies.</p>"},{"location":"tutorials/data_organisation/#arcticdbs-data-hierarchy","title":"ArcticDB's Data Hierarchy","text":"<p>Let's revisit the structures that ArcticDB provides for organising data, illustrated below</p> <p></p>"},{"location":"tutorials/data_organisation/#object-store","title":"Object Store","text":"<p>The object stores available will typically be decided by external factors such as</p> <ul> <li>Environment: prod/uat/research</li> <li>Permissions: enviroments that grant/deny read/write permission to different groups of users/systems</li> <li>Accounting: different cloud buckets may be charged to different internal accounts</li> <li>Storage Quotas: different amounts of storage may be allocated for different purposes</li> <li>Storage Performance: faster/slower storage for different applications according to requirements and cost</li> </ul>"},{"location":"tutorials/data_organisation/#library","title":"Library","text":"<p>ArcticDB can handle a large number of libraries per object store - up to 50,000 is perfectly reasonable.</p> <p>A library groups together a set of related data. Typically its primary purpose is to help catalog the data - well named/documented libraries help people find the data they need.</p>"},{"location":"tutorials/data_organisation/#symbol","title":"Symbol","text":"<p>The symbol is the base unit of storage in ArcticDB. Each symbol is a DataFrame and has optimised data access on the columns and index of the DataFrame.</p> <p>So the choice of how to use the (symbol, columns, index) is important to how the system will perform. It is also the full key to access a specific single data item from a DataFrame (including the version, when needed).</p>"},{"location":"tutorials/data_organisation/#versions","title":"Versions","text":"<p>Each symbol gets a new version every time it is modified. A consideration of how versions will be treated is a key design choice - in particular it will have a big impact how much storage is used. The most common choices are:</p> <ul> <li>Keep only the latest versions. Delete older versions either with <code>prune_previous_version</code> or in a background process (see Enterprise Features).</li> <li>Keep key versions only. Often implemented using snapshots.</li> <li>Keep all versions. This can use large amounts of storage if the data modifications are frequent but is very powerful for fully archiving the data.</li> </ul>"},{"location":"tutorials/data_organisation/#system-design-concerns","title":"System Design Concerns","text":"<p>The following topics are necessary concerns when running a system. To that extent the data organisation plan must bear them in mind.</p>"},{"location":"tutorials/data_organisation/#initial-data-population","title":"Initial Data Population","text":"<p>This is usually a one-off process to get a system up and running. Ideally the same scripts can be used for initial data population and regular updates, with different parameters.</p>"},{"location":"tutorials/data_organisation/#data-update-process","title":"Data Update Process","text":"<p>Regular updates to the data are the norm for most systems. We must consider in our design</p> <ul> <li>Delivery format (the format that the data source uses) vs storage format of the data. The trade-offs are<ul> <li>Storing data in a format close to delivery format makes it easier to reconcile with the source</li> <li>The delivery format is often not the best format for downstream processes</li> </ul> </li> <li>Frequency of data updates</li> <li>Storage growth characteristics over time</li> <li>System performance over time</li> </ul>"},{"location":"tutorials/data_organisation/#efficient-downstream-access","title":"Efficient Downstream Access","text":"<p>Often the data is accessed by downstream users and systems much more frequently than it is updated. If that is the case, it is worth organising data to optimise downstream access performance.</p>"},{"location":"tutorials/data_organisation/#data-problems-investigation-and-remediation","title":"Data Problems: Investigation and Remediation","text":"<p>System problems are often caused by suspected bad data. It is worth thinking about how to investigate and remediate such problems. For example</p> <ul> <li>Tooling to find and examine suspected bad data</li> <li>Consider how to reconcile suspected bad data vs the original data source</li> <li>Manual overrides to fix bad data to allow the system to run effectively while data is re-sourced</li> <li>Versioning and snaphsots makes it easy to return to a last known good state</li> </ul>"},{"location":"tutorials/data_organisation/#maintenance","title":"Maintenance","text":"<p>Some routine maintenance is required to keep systems running efficiently and deal with problems, some of which may have causes from outside the system.</p> <p>For ArcticDB specifically, the following issues are worth consideration:</p> <ul> <li>Version management (see versions)</li> <li>Defragmentation</li> <li>Replication / Backup of data</li> </ul> <p>There are Enterprise Features to help with these issues and more.</p>"},{"location":"tutorials/data_organisation/#general-performance-guidelines-for-arcticdb","title":"General Performance Guidelines for ArcticDB","text":"<p>In this section we want to highlight some general rules of thumb that will help you get the best performance from ArcticDB.</p>"},{"location":"tutorials/data_organisation/#properties-of-arcticdb","title":"Properties of ArcticDB","text":"<p>It is worth understanding and bearing in mind these properties of the system</p>"},{"location":"tutorials/data_organisation/#indices","title":"Indices","text":"<ul> <li>The primary and performant index is the first DataFrame row index.</li> <li>The columns names are also a performant index.</li> <li>There are no secondary row or column indices.</li> <li>The symbol-list is a good index when compacted.</li> <li>The version-list is a good index when compacted.</li> </ul>"},{"location":"tutorials/data_organisation/#data","title":"Data","text":"<ul> <li>DataFrame data is always stored columnar and compressed. ArcticDB is optimised to read and write these.</li> <li>DataFrame data is tiled and stored in objects (default is 100,000 rows x 127 cols)</li> <li>Data in different DataFrames is stored in different objects</li> </ul>"},{"location":"tutorials/data_organisation/#arcticdb-is-optimised-for-large-dataframes","title":"ArcticDB is Optimised for Large DataFrames","text":"<ul> <li>Prefer a smaller number of symbols that have a large amount of data each</li> <li>Accessing date ranges and subsets of columns is very efficient</li> <li>Collecting together data from a large number of symbols can be slower</li> </ul>"},{"location":"tutorials/data_organisation/#plan-your-version-management","title":"Plan your Version Management","text":"<ul> <li>In particular plan which versions to keep and how to delete the others</li> <li>Snapshots are useful for version management</li> <li>There are interactions between snapshots and versions: snapshots keep versions from being deleted</li> </ul> <p>Please see the snapshots documentation and snapshots notebook for more details.</p>"},{"location":"tutorials/data_organisation/#lots-of-small-appends-or-updates-can-fragment-your-data","title":"Lots of Small Appends or Updates Can Fragment Your Data","text":"<p>Append and Update are efficient because they always add new chunks of data rather than reorganising the existing data (actually update will reorganise but only where necessary which is typically only a small amount).</p> <p>This means that lots of small appends or updates can result in lots of small data chunks, which makes reads slower.</p> <p>You can defragment a symbol manually using the defragment_symbol_data library function. Altnernatively the Enterprise Features offer background processes that will take care of defragmentation for you.</p>"},{"location":"tutorials/data_organisation/#examples-based-on-market-data","title":"Examples Based on Market Data","text":""},{"location":"tutorials/data_organisation/#seperate-symbol-for-each-security","title":"Seperate Symbol for Each Security","text":"<p>The data for a single security would be a timeseries of market data. In this set of sample data this would be all the price data for AAPL.</p> <p></p> <p>Every security has its own symbol with the same data shape.</p>"},{"location":"tutorials/data_organisation/#pros","title":"Pros","text":"<ul> <li>Ideal for single security analysis</li> <li>The delivery format for the data is often per security</li> <li>The update process is simple</li> </ul>"},{"location":"tutorials/data_organisation/#cons","title":"Cons","text":"<ul> <li>Analysis involving many securities requires reading many symbols</li> <li>Time index for raw data may not match between securities.</li> </ul> <p>The data is easier to use with time index alignment.</p>"},{"location":"tutorials/data_organisation/#single-symbol-for-all-securities","title":"Single Symbol for all Securities","text":"<p>The data for all securities is merged together into a single symbol.</p> <p></p> <p>The security identifier is included as an extra index column. Note that DataFrames with a Pandas MultiIndex will round trip correctly, which is useful. However the high performance ArcticDB indexing is only on the primary index level.</p>"},{"location":"tutorials/data_organisation/#pros_1","title":"Pros","text":"<ul> <li>Good for analysis involving many securities at once eg. portfolio analysis</li> <li>A single large DataFrame tends to give very good read performance</li> </ul>"},{"location":"tutorials/data_organisation/#cons_1","title":"Cons","text":"<ul> <li>The update process is more complicated. Probably needs a read, modify, write sequence of operations (although see Future Features)</li> <li>Time index for raw data may not match between securities.</li> </ul> <p>The data is easier to use with time index alignment.</p>"},{"location":"tutorials/data_organisation/#single-symbol-for-each-data-item","title":"Single Symbol for each Data Item","text":"<p>A symbol holds the timeseries for all securities for a single field. In the example below are the close prices over time for all securities.</p> <p></p>"},{"location":"tutorials/data_organisation/#pros_2","title":"Pros","text":"<ul> <li>Good for analysis involving many securities at once eg. portfolio analysis</li> <li>A single large DataFrame tends to give very good read performance</li> <li>Update process is reasonably simple.</li> </ul>"},{"location":"tutorials/data_organisation/#cons_2","title":"Cons","text":"<ul> <li>Time index for raw data may not match between securities. </li> </ul> <p>The data is easier to use with time index alignment.</p>"},{"location":"tutorials/data_organisation/#time-index-alignment","title":"Time Index Alignment","text":"<p>Raw data may not be time-aligned</p> <ul> <li>The underlying tick data arrives at arbitrary times</li> <li>Different products have different trading hours</li> <li>Some securities may trade less frequently</li> </ul> <p>For many purposes time-aligned data is either necessary or makes the task much simpler.</p> <p>Resampling is one way to align data. There is a Pandas resample function and a faster function planned in ArcticDB (see Future Features).</p>"},{"location":"tutorials/data_organisation/#enterprise-features","title":"Enterprise Features","text":"<p>Many of the housekeeping and maintenance procedures recommended in this guide are taken care of by processes available in the ArcticDB Enterprise package. Please contact us if you would like to explore further. Click the Get ArcticDB button on our website to contact us.</p>"},{"location":"tutorials/data_organisation/#planned-future-features-and-improvements","title":"Planned Future Features and Improvements","text":"<p>We are always improving and adding new features to ArcticDB. Below are a few relevant new features that are in our plans</p> <ul> <li>Multi-Index: support for more flexible data updates for multi-index symbols.</li> <li>Arrow: support for Arrow-backed Pandas and Polars DataFrames.</li> </ul> <p>If you have a suggestion for a new feature, please raise an issue on our github. Please include as much detail as possible.</p>"},{"location":"tutorials/fundamentals/","title":"ArcticDB Fundamentals","text":"<p>This tutorial will walk through the fundamentals of ArcticDB:</p> <ol> <li>Accessing libraries</li> <li>Writing data</li> <li>Reading data</li> <li>Modifying data</li> </ol> <p>To start, let's import <code>arcticdb</code>:</p> <pre><code>import arcticdb as adb\n</code></pre>"},{"location":"tutorials/fundamentals/#accessing-libraries","title":"Accessing libraries","text":"<p>Connect to your storage:</p> <pre><code># Connect using defined keys\nac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arctic-test-aws?access=&lt;access key&gt;&amp;secret=&lt;secret key&gt;')\n# Leave AWS SDK to work out auth details \nac = adb.Arctic('s3s://s3.eu-west-2.amazonaws.com:arctic-test-aws?aws_auth=true)\n</code></pre> <p>For more information on how the AWS SDK configures authentication without utilising defined keys, please see the AWS documentation.</p> <p>Access a library using either the <code>[library_name]</code> notation or the <code>get_library</code> method:</p> <pre><code>lib = ac['library']\n# ...equivalent to...\nlib = ac.get_library['library']\n</code></pre> <p>Let's see what data is already present:</p> <pre><code>&gt;&gt;&gt; lib.list_symbols()\n['sym_2', 'sym_1', 'symbol']\n</code></pre>"},{"location":"tutorials/fundamentals/#arcticdb-api","title":"ArcticDB API","text":"<p>ArcticDB's API is built around four main primitives that each operate over a single symbol.</p> <ol> <li>write: Creates a new version consisting solely of the item passed in.</li> <li>append: Creates a new version consisting of the item appended to the previously-written data.</li> <li>update: Creates a new version consisting the previous data patched with the provided item.</li> <li>read: Retrieves the given version (if no version is provided the latest version is used).</li> </ol> <p>These primitives aren't exhaustive, but cover most use cases. We'll show the usage of these primitives.</p>"},{"location":"tutorials/fundamentals/#writing-data","title":"Writing data","text":"<p>Let's start by generating some data. The below snippet generates some random data with a datetime index:</p> <pre><code>import numpy as np\nimport pandas as pd\nNUM_COLUMNS=10\nNUM_ROWS=100_000\ndf = pd.DataFrame(np.random.randint(0,100,size=(NUM_ROWS, NUM_COLUMNS)), columns=[f\"COL_{i}\" for i in range(NUM_COLUMNS)], index=pd.date_range('2000', periods=NUM_ROWS, freq='h'))\n</code></pre> <p>Let's take this data and write it to ArcticDB:</p> <pre><code>&gt;&gt;&gt; lib.write(\"my_data\", df)\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'NoneType'&gt;,version=0,metadata=None,host=local)\n</code></pre>"},{"location":"tutorials/fundamentals/#reading-data","title":"Reading data","text":"<p>To read the data, simply use the <code>read</code> primitive:</p> <pre><code>&gt;&gt;&gt; data = lib.read(\"my_data\")\n&gt;&gt;&gt; data\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'pandas.core.frame.DataFrame'&gt;,version=0,metadata=None,host=local)\n</code></pre> <p>Note that you get back a <code>VersionedItem</code> - it allows us to retrieve the version of the written data:</p> <pre><code>&gt;&gt;&gt; data.version\n0\n</code></pre> <p>As this is the first write to this symbol, the version is <code>0</code>. To retrieve the data:</p> <pre><code>&gt;&gt;&gt; data.data\n</code></pre>"},{"location":"tutorials/fundamentals/#slicing-and-filtering","title":"Slicing and filtering","text":"<p>See the getting started guide for more information.</p>"},{"location":"tutorials/fundamentals/#modifying-data","title":"Modifying data","text":"<p>Let's append some data. First, note that the data we've written ends in 2011:</p> <pre><code>&gt;&gt;&gt; data.data.tail()\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  COL_8  COL_9\n2011-05-29 11:00:00     44     94     70     32     91      4     35     19     74     53\n2011-05-29 12:00:00     79     51     67     48      8     83     46     54     86     38\n2011-05-29 13:00:00     60     98     74      4     81     86     64     78     13     32\n2011-05-29 14:00:00     27     24     16      6     84     99     11     94     29      4\n2011-05-29 15:00:00     81     76     52     93     31     91     64      2     26     78\n</code></pre> <p>That's simply because the data we generated started on January 1st, 2000 at 00:00 at consists of 100,000 rows, incrementing one hour at a time. When <code>append</code>-ing data, the data you are appending must begin  after the existing data ends. As a result, let's generate some data that begins in 2012:</p> <pre><code>df_to_append = pd.DataFrame(np.random.randint(0,100,size=(NUM_ROWS, NUM_COLUMNS)), columns=[f\"COL_{i}\" for i in range(NUM_COLUMNS)], index=pd.date_range('2012', periods=NUM_ROWS, freq='h'))\n</code></pre> <p>Now let's append!</p> <pre><code>&gt;&gt;&gt; lib.append(\"my_data\", df_to_append)\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'NoneType'&gt;,version=1,metadata=None,host=local)\n</code></pre> <p><code>append</code> has created a new version of the data. When reading version 0, the data will end in 2011. When reading version 1, the data will end in 2023.</p>"},{"location":"tutorials/fundamentals/#update","title":"Update","text":"<p>If append can only append date that begins after the existing data ends, then it begs the question - how do we mutate data?</p> <p>The answer is that we use the <code>update</code> primitive. <code>update</code> overwrites (creating a new version - nothing is lost!) existing symbol data with the data that is passed in.  Note that the entire range between the first and last index entry in the existing data is replaced in its entirety with the data that is passed in, adding additional index entries if required. This means <code>update</code> is a contiguous operation - see the documentation of <code>update</code> for more information.</p>"},{"location":"tutorials/fundamentals/#time-travel","title":"Time travel!","text":"<p>ArcticDB is bitemporal - all new versions are timestamped! Let's pull in the first version of the data, prior to the <code>append</code>:</p> <pre><code>&gt;&gt;&gt; lib.read(\"my_date\", as_of=0).data.tail()\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  COL_8  COL_9\n2011-05-29 11:00:00     44     94     70     32     91      4     35     19     74     53\n2011-05-29 12:00:00     79     51     67     48      8     83     46     54     86     38\n2011-05-29 13:00:00     60     98     74      4     81     86     64     78     13     32\n2011-05-29 14:00:00     27     24     16      6     84     99     11     94     29      4\n2011-05-29 15:00:00     81     76     52     93     31     91     64      2     26     78\n</code></pre> <p>Note that it ends in 2011 - it's like the <code>append</code> never happened. <code>as_of</code> can take a timestamp (<code>datatime.datetime</code> or <code>pd.Timestamp</code>) as well.</p>"},{"location":"tutorials/lmdb_and_in_memory/","title":"In-Memory Storage Backends","text":"<p>ArcticDB can use a file-based LMDB backend, or a RAM-based in-memory storage, as alternatives to object storage such as S3 and Azure.</p> <p>For temporary in-memory solutions, LMDB can be set up to write to tmpfs. As this guide will explore, with this solution multiple writers can access the database concurrently, and additionally benefit from increased performance when compared to LMDB writing to a permanent on-disk filesystem.</p> <p>On Linux, the steps to set up a tmpfs filesystem are:</p> <pre><code>$ mkdir ./tmpfs_mount_point\n$ sudo mount -t tmpfs -o size=1g tmpfs tmpfs_mount_point\n</code></pre> <p>And we can inspect that it is there with:</p> <pre><code>$ df -h\nFilesystem               Size  Used Avail Use% Mounted on\ntmpfs                    1.0G     0  1.0G   0% /somedir/tmpfs_mount_point\n</code></pre> <p>From ArcticDB you can connect to this filesystem with LMDB as you would usually:</p> <pre><code>import arcticdb as adb\nimport pandas as pd\n\nac_lmdb = adb.Arctic('lmdb:///somedir/tmpfs_mount_point')\nlib = ac_lmdb.get_library('lib', create_if_missing=True)\nlib.write('symbol', pd.DataFrame({'a': [1, 2, 3]})\nprint(lib.read('symbol').data)\n</code></pre> <p>which gives:</p> <pre><code>   a\n0  1\n1  2\n2  3\n</code></pre> <p>The equivalent instantiation for an in-memory store uses the URI <code>'mem://'</code> as in:</p> <pre><code>ac_mem = adb.Arctic('mem://')\n</code></pre> <p>The <code>ac_mem</code> instance owns the storage, which lives only for the lifetime of the <code>ac_mem</code> Python object. Behind the scenes, a Folly Concurrent Hash Map is used as the key/value store. For test cases and experimentation, the in-memory backend is a good option.</p> <p>If multiple processes want concurrent access to the same data, then we recommend using LMDB over a tmpfs.</p>"},{"location":"tutorials/lmdb_and_in_memory/#how-to-handle-concurrent-writers","title":"How to handle concurrent writers?","text":"<p>Again, it should be noted that ArcticDB achieves its highest performance and scale when configured with an object storage backed (e.g. S3). Nevertheless, applications may want to concurrently write to LMDB stores. In-memory stores are not appropriate for this use case.</p> <p>The following Python code uses <code>multiprocessing</code> to spawn 50 processes that concurrently write to different symbols. (Non-staged parallel writes to the same symbol are not supported).</p> <pre><code># Code tested on Linux\nimport arcticdb as adb\nimport pandas as pd\nfrom multiprocessing import Process, Queue\nimport numpy as np\n\nlmdb_dir = 'data.lmdb'\nnum_processes = 50\ndata_KB = 1000\nncols = 10\n\nnrows = int(data_KB * 1e3 / ncols / np.dtype(float).itemsize)\n\nac = adb.Arctic(f'lmdb://{lmdb_dir}')\nac.create_library('lib')\nlib = ac['lib']\n\ntimings = Queue()\ndef connect_and_write_symbol(symbol, lmdb_dir, timings_):\n    ac = adb.Arctic(f'lmdb://{lmdb_dir}')\n    lib = ac['lib']\n    start = pd.to_datetime('now')\n    lib.write(\n        symbol,\n        pd.DataFrame(\n            np.random.randn(nrows, ncols),\n            columns=[f'c{i}' for i in range(ncols)]\n        )\n    )\n    timings_.put((start, pd.to_datetime('now')))\n\nsymbol_names = {f'symbol_{i}' for i in range(num_processes)}\nconcurrent_writers = {\n    Process(target=connect_and_write_symbol, args=(symbol, lmdb_dir, timings))\n    for symbol in symbol_names\n}\n\n# Start all processes\nfor proc in concurrent_writers:\n    proc.start()\n# Wait for them to complete\nfor proc in concurrent_writers:\n    proc.join()\n\nassert set(lib.list_symbols()) == symbol_names\n\ntimings_list = []\nwhile not timings.empty():\n    timings_list.append(timings.get())\n\npd.DataFrame(timings_list, columns=['start', 'end']).to_csv('timings.csv')\n</code></pre> <p>Plotting the lifetimes of each process with matplotlib we get:</p> <p></p> <p>Explanation of graph: Each line segment represents the execution of a process writing to the shared LMDB backend. File locks are repeatedly obtained and released by LMDB throughout the calls to <code>lib.write(..)</code>.</p> <p>The LMDB documentation homepage states that multi-threaded concurrency is also possible. However as explained on this page we should not call <code>mdb_env_open()</code> multiple times from a single process. Hence, since this is called in the <code>Arctic</code> instantiation, the above code could not be transferred to a multi-threaded application.</p>"},{"location":"tutorials/lmdb_and_in_memory/#is-lmdb-on-tmpfs-any-faster-than-lmdb-on-disk","title":"Is LMDB on tmpfs any faster than LMDB on disk?","text":"<p>See the timing results below and the Appendix for the code to generate this data. The differences are not huge with tmpfs out-performing disk only for symbol writing operations. Nevertheless, tmpfs is clearly a better option for an ephemeral LMDB backend. We can also see that the in-memory store is significantly faster across the board as would be expected.</p> <p></p> <p>Note: the ranges are 95% confidence intervals based on five repeats for each data size. The hardware limits are for writing to disk (not tmpfs) using the <code>np.save</code> and <code>np.load</code> functions. No data was sought for the RAM's hardware limit.</p>"},{"location":"tutorials/lmdb_and_in_memory/#appendix-profiling-script","title":"Appendix: Profiling script","text":"<p>The profiling script used to generate the above graphs, and discussion of determining theoretical maximum read and write speeds using <code>fio</code> have been moved to this Wiki page.</p>"},{"location":"tutorials/metadata/","title":"Metadata","text":"<p>ArcticDB enables you to store arbitrary binary-blobs alongside symbols and versions.</p> <p>The below example shows a basic example of writing and reading metadata (in this case a Python dictionary):</p> <pre><code>import arcticdb as adb\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\nlibrary = \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nmetadata = {\n    \"This\": \"is\",\n    \"a\": \"Python\",\n    \"Dictionary\": \"!\"\n}\n\nlib.write(\"meta\", data=pd.DataFrame(), metadata=metadata)  # or write_metadata can be used - will still create a new version, but doesn't require `data` to be passed in\n\nassert lib.read(\"meta\").metadata == metadata\nassert lib.read_metadata(\"meta\").metadata == metadata  # Same as read, but doesn't return data from storage\n</code></pre> <p>New versions of symbols do not \"inherit\" the metadata of a previous version. Metadata needs to be specified explicitly each time that you create a new version of the symbol:</p> <pre><code>lib.write(\"new_sym\", data=pd.DataFrame(), metadata=metadata)\nlib.write(\"new_sym\", data=pd.DataFrame())\n\nassert lib.read(\"new_sym\").metadata is None\nassert lib.read(\"new_sym\", as_of=0).metadata == metadata\n</code></pre>"},{"location":"tutorials/metadata/#serialization-format","title":"Serialization Format","text":"<p>We use <code>msgpack</code> serialization for metadata when possible. We support the built-in <code>msgpack</code> types and also:</p> <ul> <li>Pandas timestamps <code>pd.Timestamp</code></li> <li>Python datetime <code>datetime.datetime</code></li> <li>Python timedelta <code>datetime.timedelta</code></li> </ul> <p>Documentation of supported <code>msgpack</code> structures is available here. Arrays and maps correspond to Python lists and dicts.</p> <p>When this <code>msgpack</code> serialization of the metadata fails due to unsupported types we fall back to pickling the metadata. Pickling can have serious downsides as it may not be possible to unpickle data written with one set of library versions from a client with a different set of library versions.</p> <p>Because of this, we log a warning when metadata gets pickled. You can disable the warning by setting an environment variable <code>ARCTICDB_PickledMetadata_loglevel_str</code> to <code>DEBUG</code>. The log message looks like:</p> <pre><code>Pickling metadata - may not be readable by other clients\n</code></pre> <p>The metadata may be up to 4GB in size.</p>"},{"location":"tutorials/metadata/#practical-example-using-metadata-to-track-vendor-timelines","title":"Practical example - using metadata to track vendor timelines","text":"<p>One common example for metadata is to store the vendor-provided date alongside the version. For example, let's say we are processing three files - <code>data-2004-01-01.csv</code>, <code>data-2004-01-02.csv</code> and <code>data-2004-01-03.csv</code>. Each file name contains a date which we'd like to be able to store along side the version information in ArcticDB.</p> <p>We can do this using the following code:</p> <pre><code>import glob\nimport datetime\nimport arcticdb as adb\n\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\nlibrary = \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nfile_names = glob.glob('*.csv')  # returns ['data-2004-01-01.csv', 'data-2004-01-02.csv', 'data-2004-01-03.csv']\n\nfor i, name in enumerate(file_names):\n    data = pd.read_csv('name')\n    date = datetime.datetime.strptime(name[5:][:-4], '%Y-%m-%d')\n\n    if i == 0:\n        lib.write(\"symbol\", data, metadata={\"vendor_date\": date})\n    else:\n        lib.append(\"symbol\", data, metadata={\"vendor_date\": date})\n</code></pre> <p>We'll now use this to read the data along the vendor-provided timeline - that is, we'll retrieve the data as if we had written each file on the day it was generated. We'll read all metadata entries for all versions of the symbol and select the date that is most recent with respect to a given date (in this case 2004-01-02):</p> <pre><code># Continuing from the previous code snippet\nmetadata = [\n    (v[\"version\"], lib.read_metadata(\"symbol\", as_of=v[\"version\"]).metadata.get(\"vendor_date\"))\n    for v in lib.list_versions(\"symbol\")\n]\nsorted_metadata = sorted(metadata, key=lambda x: x[1])\n\nversion_to_read_index = bisect_right([x[1] for x in sorted_metadata], datetime.datetime(2004, 1, 2))\nlib.read(\"symbol\", as_of=sorted_metadata[version_to_read_index - 1][0])\n</code></pre> <p>Note that if the data is written across multiple symbols, then ArcticDB Snapshots can be used to achieve the same result. </p>"},{"location":"tutorials/parallel_writes/","title":"Parallel Writes","text":"<p>As mentioned, ArcticDB fundamentally does not support concurrent writers to a single symbol - unless the data is concurrently written as staged data!</p> <p>Staged data is not available to read, and requires a process to finalize all staged data prior to it being available for reading. Each unit of staged data must not overlap with any other unit of staged data -  as a result staged data must be timeseries indexed. The below code uses Spark to concurrently write to a single symbol in parallel, before finalizing the data:</p> <pre><code>import pyspark\nimport arcticdb as adb\n\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\n\ndef _load(work):\n    # This method is run in parallel via Spark.\n    host, bucket, access, secret, symbol, library, file_path = work\n    ac = adb.Arctic(f\"s3://{host}:{bucket}?access={access}&amp;secret={secret}\")\n\n    library = ac[library]\n\n    df = pd.read_csv(file_path)\n    df = df.set_index(df.columns[0])\n    df.index = df.index.to_datetime()\n\n    # When staged, the written data is not available to read until finalized.\n    library.write(symbol, df, staged=True)\n\nsymbol = \"my_data\"\nlibrary = \"my_library\"\n\nconf = SparkConf().setAppName('appName').setMaster('local')\nsc = SparkContext(conf=conf)\n\n# Assumes there are a set of CSV files in the current directory to load from\ndata = [(host, bucket, access, secret, symbol, library, f) for f in glob.glob(\"*.csv\")]\ndist_data = sc.parallelize(data)\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nret = dist_data.map(_load)\nret.collect()\n\nlibrary.finalize_staged_data(symbol)\n\ndata = library.read(symbol)\n</code></pre>"},{"location":"tutorials/snapshots/","title":"Snapshots","text":"<p>ArcticDB enables multi-symbol snapshotting. Snapshots enable multiple symbols to be versioned together via a human readable string name. </p> <p>In practise this is useful to tie derived data with the source data.</p> <pre><code>import arcticdb as adb\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = adb.Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\n\nlibrary= \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\n# Assumes there are CSV files containing pricing data and factor data. Each time we've written ALL new factor files\n# to their symbol, we'll take a snapshot across all symbols.\nfor i, f in enumerate(sorted(glob.glob('*.csv'), key=lambda f: f.split('_')[1].split('.')[0])):\n    df = pd.read_csv(f)\n    if 'FACTORS' in f:\n        library.write(f, df)\n        # SNAP_{i} will forever point to all symbols that exist at this time at their current latest version\n        library.snapshot(f\"SNAP_{i}\")\n    else:\n        df = df.set_index(df.columns[0])\n        df.index = df.index.to_datetime()\n\n        library.append(pricing_symbol, df, write_if_missing=True)\n\nsnapshots = library.list_snapshots()\nsymbols = library.list_symbols(snapshot_name=list(snapshots.keys())[0])\n</code></pre> <p>To generate this data, the following code can be used:</p> <pre><code>import argparse\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef run(num_files, num_symbols):\n    starting_date = datetime.today() - timedelta(weeks=num_files)\n    starting_date = datetime(starting_date.year, starting_date.month, starting_date.day)\n\n    for file_num in range(num_files):\n        index_size = 7 * 24\n        this_file_starting_date = starting_date + timedelta(weeks=file_num)\n\n        df = pd.DataFrame(np.random.randint(0,index_size,size=(index_size, num_symbols)), columns=['SYM_%d' % i for i in range(num_symbols)])\n\n        df.index = pd.date_range(this_file_starting_date, periods=index_size, freq=\"H\")\n\n        df.to_csv(f\"PRICING_{this_file_starting_date}.csv\")\n        if file_num % 3 == 0:\n            df = pd.DataFrame(np.random.randint(0, 5,size=(5, num_symbols)), columns=['SYM_%d' % i for i in range(num_symbols)])\n            df['FACTORS'] = ['FACTOR_1', 'FACTOR_2', 'FACTOR_3', 'FACTOR_4', 'FACTOR_5']\n            df.to_csv(f\"FACTORS_{this_file_starting_date}.csv\")\n\n        print(f\"Written {file_num + 1} / {num_files}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--num-files', type=int, default=15)\n    parser.add_argument('--symbols-per-file', type=int, default=500)\n\n    args = parser.parse_args()\n\n    run(args.num_files, args.symbols_per_file)\n</code></pre>"}]}