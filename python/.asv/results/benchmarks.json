{
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5350d84365ea3a972c4aec4c3b1a7eb48520c116eba1aca60f68671c7ceb7db8"
    },
    "basic_functions.BasicFunctions.peakmem_read_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "050f863cc095eb3c53b80d320695ae92e69050877742108775b5fad2709eee10"
    },
    "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c7b3ab2fb3717f309d2627beb6032db5459cbf5648aa1a0e091290c88af77250"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "047f4275a35a623a40ef0a98f437cfdabdf9f7ff5f6c29e6f4246e8bcc11a36a"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "eb643d25455a8fe0f500a3e587b0ce0002907f43ae6e39d7775aaac1e7a2fe93"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0ae28943a0c68ba7c67616ce2199fe35bf51d2cdbe2d2b2c962c96664bcefb19"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "07a76278d58a355f560d7d5bfd1ca0457cbde13ae5327909587acb001b06c45f"
    },
    "basic_functions.BasicFunctions.peakmem_write_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_write_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6173c4098439ead864ff0ff9389f68ca2fce88c1454e7d6a162185f1aa6f8aab"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "bea653ed2c0175761c392b80884986c1fbe5ab70bdde3d2114e5f92a84f7da47"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a70c622c0ad6fd9d008c2b36342666f99b8c14f75d14c859d619cf2cc37e34ad",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8527c0bb585ee2f0d4d115dd110b8911b243e6ddd28139771dcdd158fc4ad124",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_ultra_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_ultra_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "591d23399ad85009e024d4ee18eb2b5a4a844d6ea0b937c49adee40d8f24bc8e",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "19bbbb9d179141472b122afd3ad90408e2a3546eaeb640514c2c6b1fca9fa686",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f59257c55a9106758070701cec081f4bf021bf1cbcb6d698dcf694419cd9bddc",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2c5d17c2d1bd6212c16a5127aea1ca379224ef610b06111a93c5b80e04922f8d",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "54f11aa38218f73c7a34bec5c71fc51af8b57ff2ac64f2cf3db688dc462efcdf",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write_short_wide": {
        "code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9c7467ee5cd1c182f05a78de9dff71956edf647d369b0cdbcc0cc431f96e2c24",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"ultra_short_wide_sym\",\n            generate_random_floats_dataframe(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:44",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "71e4fffc77870273e8186b7b0a8a275fd2062d2eec72a11834004b88c2bca07c",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ad4186536870277e6d6b172d16c78e25ca0a24077b06d47c0ec85f2029105468"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c3bfc7e448a19b3714478a4bf68abc2727b2377cb6e4ea40ef477da286053af0"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5c6e176c14f36368bc30cfba9d19586bf9d32ebffa8f5e4d1f0644afe8879c4d"
    },
    "basic_functions.BatchBasicFunctions.peakmem_update_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_update_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "35b7cab5345fc9d919975af82e8566fe696e0e5ebb823621d977ebfa37814db2"
    },
    "basic_functions.BatchBasicFunctions.peakmem_write_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "20c4987d92ab25712c9af326e22a5253ec27eec3b8a162a4e56830b8e2889fd4"
    },
    "basic_functions.BatchBasicFunctions.time_read_batch": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "91fcac5344d14a496a34cbd4cafc3b187e9579d1912b6156ecae8087d80df7b2",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_pure": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "bd42aec09d03cbf1c6f55d86f114ea0f5627950c8a255ce64ccdcc8cedaa029a",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e11e8495b1ac21d015735c6d9bd38a219ad0c386063557efa4673c881155d331",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b90235fb10cb19836b85cf7de7316d780ebd9b4d015e906fbbb3840a556f55b9",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_update_batch": {
        "code": "class BatchBasicFunctions:\n    def time_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_update_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "15c4acc46acc92479e795e0cbf8664b0a75afd63f626f799457d10fde36d928b",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_write_batch": {
        "code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_write_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:167",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "71706b100e9d76919fe5a09b6dd6d60999519ea4acd92879edbbf7a95ee919a0",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_large": {
        "code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_large",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e0f0f9a0466ec28333d3f62c9573349dad1a2196b0a689aa36dc84a278dd85f9",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_short_wide": {
        "code": "class ModificationFunctions:\n    def time_append_short_wide(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_short_wide[rows].pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "24aaf8279ce51c426c441a72627f7e92355854d79b2696ce28f11bdded0bb8ff",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_single": {
        "code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "64309481ad0e09f83ffdf23907dfefef6ae5693a19632f0e65d559d06fc67c7a",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete": {
        "code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "83a387a99e2752c62d1bfcaee3bc60d519a3476cb6eb84b550622bc4aed573b2",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_multiple_versions": {
        "code": "class ModificationFunctions:\n    def time_delete_multiple_versions(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym_delete_multiple\", list(range(99)))\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_multiple_versions",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e83fb6e5a7df255f8958f7a788591c2ce139b27f02f1b7452ef480b61d656a0f",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_over_time": {
        "code": "class ModificationFunctions:\n    def time_delete_over_time(self, lad: LargeAppendDataModify, rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_over_time",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "0b6c95b8c3be0ec971f33cb33adff68cdd21a6a45d05eddfacf93070e4466c34",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_short_wide": {
        "code": "class ModificationFunctions:\n    def time_delete_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4ab9a6b853b21803a718b9482c8da6bb608c9ed0f17397344dc916f194c9ce39",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_half": {
        "code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_half",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5e6dc60fc2e1e05d60ee28e519c20a4287636ebcd42521b96b555a50b4b5e4a6",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_short_wide": {
        "code": "class ModificationFunctions:\n    def time_update_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "0d7782f15f49e0949eff17e5b713b5bf944160c3a58972f65e72c981e5647a12",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_single": {
        "code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "623f9f87c5901e77a9bd521ed89d304ede6789595fbc3211cfe06706056c3241",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_upsert": {
        "code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_upsert",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:316",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "22856662392bae1926e4c8c152895fb9fe9ff465ad9ee0a9c3948ad02a92f4e7",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4aaa19de6a81f69d2a57d3d00f2d06c2eaf3a68c4180c2e6bac0c01650a48645"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fb9dfb58c26e19e743c423f9a049ef0680bf6b7772019bc36e2b8f91b50ba77f"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1c03264219ad059962b4deaa334a0c8574833075c87dd548898ce4172cf7f5cf"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {
        "code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d55ec77300186fc84bbc3d9402694f991fdaf329eda0750551d57425e41e5087"
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f3e937a52faf63c837949a6eb48debf02755ddd4b699c5c3800831a5cda9ad67",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5dc7443c6e30c3fe69bfc4f04cb28fb5aa503162cab5e204606a9f6e2233e581",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "46c41f587be844def6cad1cefd995ea32c9719285172e8a0ecc3ad193b991247",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_readall": {
        "code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        _df.sort_index(inplace=True, axis=1)\n        arctic_df.sort_index(inplace=True, axis=1)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_readall",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:73",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "16923258b23421434453a4143833cd446747e065f3e982bfd61ed19a3ee0130f",
        "warmup_time": 0
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_create_dataframe(self, tpl):\n        df, dict = tpl\n        df = pd.DataFrame(dict)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:38",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3fba994ac7997b310a49e3a6eaff3aac7aaea9072e7dce6025959b53c8e891c9"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_arctic(self, tpl):\n        self.lib.read(ComparisonBenchmarks.SYMBOL)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:38",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3f0b5ba30cf1ba9ff3ac3a9e89aeea8f4b68a1f94cef660ead8f66845d61978b"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_parquet(self, tpl):\n        pd.read_parquet(self.path_to_read)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:38",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ac09370986d4696bde103c09c29fca234c5f6de88f367e1f9d2cab6c939740b3"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_arctic(self, tpl):\n        df, dict = tpl\n        self.lib.write(\"symbol\", df)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:38",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8f859b71fa3b8c3c4acefdc333d07f57aff7d9a897e0bd93096dbdc44f99a2e4"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_parquet(self, tpl):\n        df, dict = tpl\n        df.to_parquet(self.path, index=True)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        print(f\"DF generated {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:38",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "219e33422ea9ced5dc92ced39e0eea27dee1870cf991a82db139498f17446fe0"
    },
    "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        print(\">>> Library:\", self.lib)\n        print(\">>> Symbol:\", self.symbol)\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        cachedDF = cache\n    \n        # Unfortunately there is no way to tell asv to run single time\n        # each of finalize_stage_data() tests if we do the large setup in the\n        # setup_cache() method. We can only force it to work with single execution\n        # if the symbol setup with stage data is in the setup() method\n    \n        self.ac = Arctic(f\"lmdb://{self.lib_name}{param}?map_size=40GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, cachedDF.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = cachedDF.generate_dataframe_timestamp_indexed(200, 0, cachedDF.TIME_UNIT)\n        list_of_chunks = [10000] * param\n        self.symbol\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, cachedDF, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data types\n        cachedDF = CachedDFGenerator(350000, [5])\n        return cachedDF",
        "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:41",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9dcfdaf896125a0fe0d16b5538b5a8b556997064e107c8b58b93dc6e6f32d8b1"
    },
    "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        print(\">>> Library:\", self.lib)\n        print(\">>> Symbol:\", self.symbol)\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        cachedDF = cache\n    \n        # Unfortunately there is no way to tell asv to run single time\n        # each of finalize_stage_data() tests if we do the large setup in the\n        # setup_cache() method. We can only force it to work with single execution\n        # if the symbol setup with stage data is in the setup() method\n    \n        self.ac = Arctic(f\"lmdb://{self.lib_name}{param}?map_size=40GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, cachedDF.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = cachedDF.generate_dataframe_timestamp_indexed(200, 0, cachedDF.TIME_UNIT)\n        list_of_chunks = [10000] * param\n        self.symbol\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, cachedDF, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data types\n        cachedDF = CachedDFGenerator(350000, [5])\n        return cachedDF",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:41",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "c3c02d1e2369dd420b2e241fc69c4c8872d31da89d0c19c1111d503a84fb9521",
        "warmup_time": 0
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().peakmem_finalize_staged_data(cache, param)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(cache, param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        return cachedDF",
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:92",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "90cde854b0e3346d50d63ab29182811b92cd7fae6c4ce0be4011a62c534e5e0f"
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().time_finalize_staged_data(cache, param)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(cache, param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        return cachedDF",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:92",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "a7673a8f559a07772f7a7a8e105774090534c7eb1b644b2d6247e7b792645809",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.peakmem_list_symbols": {
        "code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_symbols",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:24",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "efa8557e59868203fde3f8d2921698b505ae7a1ce7ff442b3e4c9bebc9ce2771"
    },
    "list_functions.ListFunctions.peakmem_list_versions": {
        "code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_versions",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:24",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "30457537b6ea77365ec0021b6f78a618dd0e990631d64cf0ae6b85baddca7081"
    },
    "list_functions.ListFunctions.time_has_symbol": {
        "code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_has_symbol",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:24",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "00a6aba7cd18f9fbbfa18c85961d58a03a291bfe32bf033e8d7b88c7b960da90",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.time_list_symbols": {
        "code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_symbols",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:24",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7457ceb57b7adfda687387a4599ff60b20ecb6ef556b80329ad2e8ec433fbb17",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.time_list_versions": {
        "code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_versions",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:24",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "cc2c68ce66d0087882fffcb8be554f525c3f314c8693a37897d37cc18373f1ff",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:41",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "55c5726e8be0eafb5adfab9f83954dc1fd727249fc74a1df3d27bed647ad1040"
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:41",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b7f4df120eb3ef915bcfd99cc75efe38896ffeafd5b6a8c3e5f3a8db67ccaad3"
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots()\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:41",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "706e811eef8155e3494d1a30ae8920e15ae32e0b69826eaa92388d620a5bb4ff",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:41",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "d4090a8af79a8cd45f87d87a807c4f344d86fa4e51e528809922b6a602ce06f9",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_without_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:41",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "71446e73206e086160b0733fdec4689d0c813c036e061f99d6f4ef372dd49b3b",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1f8093c32e1c5195eb0efb1004c228524cb54aa35d8c79359b17fc91597391a6"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fa0a87f2f2956453b825adcdb9e95be6a7e8887b2a66839923aa8a433e296e4e"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ce91e45ba6ec5f5dcd9499b423014b431774a7d81f07daa90d6c29cb8bc84d02"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4cad6e9389f20fc4a168893003dff16e0577770525b847e71e3b97f0f9f5ecdd"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "91dfe91e2fd6e9d562d89d8aee902dbb5c2380f3cd0a11eb85229cb375a7ea0b"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "94edfd985cb9746d21b85be1c91e97423797af2faa7a3343ad1c3aa7f9fa4536"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e047abda334f31dda20959739f2a3816f4dc96c130db00ebb75f5adcb9c14999"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "1fd26d5df8e3bd47278b0f1acca9528cc0dadba82788af6e3cfd1812058abef9",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a0f79b58b7744e63b2b7df3562f57094fa4ff3a111c172fbe0b03aec197afec8",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, num_rows):\n        pattern = f\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5206453b05dddbb325596b08eae2200fd2193c2e2382ae66982838b1cc97859b",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c7f842a915ebd3e278a9a5cea838835a804b463451ebec69829afe871adccfcc",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8972136efca70caee7530d031766c4653737a79d09b7c7badaaee274c1caa7da",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "17ef74af58c623de0ce47d10ad9d52ffc8a1b3c3bb2f57d1391dde34f4af4f29",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "509ffd471564124f5ea73eab19903e52e70eba728ea59b97ad6bd5b8544c2e60",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:26",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9a923014466d420b857d297f2a8a41983d03d0c3242559a8488a2a9a642440e1",
        "warmup_time": 0
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:63",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9d97dcd98574b9edb2038a9d43166c03fb90874813e5fac9c3a44b51194f3dd9",
        "warmup_time": 0
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:63",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b1364bf72e616201e384c0b7a9f18b03b078e22452929466a06b35fc64a91bd6",
        "warmup_time": 0
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:63",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8f27fb785c7b8b40220191dae6dbb120a49f55e011ae0f7cea6516a47e38c18a",
        "warmup_time": 0
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:63",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ed1d1ccb6458095a627788bfa2b53afa310ca8c8118a6405c91204724c865d6c",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5f940b32e17b1e08e0e79df3ddb81fd60d6217ec6274ee308a3cccf5a90cc72f"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e5f752bdf60df192471e9f0a0bb7ee74f3582679fd461247cda321614ecfc952"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_date_range)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] > 2\n        assert read_batch_result[-1].data.shape[0] > 2\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0bc60840654ec805851574b1f8ef76987cbd0ac99806d08abf47e7a5c415fd4c"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert write_batch_result[0].symbol in self.symbols\n        assert write_batch_result[-1].symbol in self.symbols\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e3de99f1307e75a7b5fddd8d7f3e4fba1975fda0f0f196603caa638b4cb3569f"
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "919b109aa3f63f22826be5a5f1255dcd06e284fac7035d1d2b8446ef182d4f3f",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "c4f2b10ea3bae71c069942dc0b9ed61b161076d0c9ed9e9a3eabdd56aa720675",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        self.lib.read_batch(self.read_reqs_date_range)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "21629b37558919e369c6b23aab6179b57f09fc95247b5819b07cac4a46ce608c",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_write_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "7dc9bf11079cd6affdcf109a9d3f2ea057c2f52f63593fd9260004930ac7a6e6",
        "warmup_time": 0
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:75",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6d29e295671e2ea22e0d30278ef69d9d7740a8e9c7dd806efa5d6144cc3647da"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # measures base memory which need to be deducted from\n            # any measurements with actual operations\n            # see discussion above\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.parquet_to_read )\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_read.read(self.s3_symbol)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:75",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "659f77bdc21378d3f9210e0f791c2cea56ca30f7a06644ec82d3d8cad9964efe"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_write.write(self.s3_symbol, df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:75",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6e7f2fc17ff2e3550a927ac3659eae946384ea1f88bfb66049eae9f7969b6f02"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def time_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "min_run_count": 1,
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe",
        "number": 2,
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_comparison_benchmarks:75",
        "timeout": 60000,
        "type": "time",
        "unit": "seconds",
        "version": "f775e01172180c26b5d2a21e04b658cb3389adb8256920c58a20a2798f39131c",
        "warmup_time": 0
    },
    "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class AWSFinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, self.df_cache.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache",
        "name": "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "num_chunks"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_finalize_staged_data:43",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "807052d5c2c0c054cc0b91e1255cb54af1e074c1e216974289db9fa905eff800"
    },
    "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data": {
        "code": "class AWSFinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, self.df_cache.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache",
        "min_run_count": 1,
        "name": "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "num_chunks"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_finalize_staged_data:43",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e637474610e2fa47bd54a3077f46e8b35f94f808c4aadccbd01e8ff63d51dc06",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.peakmem_list_symbols": {
        "code": "class AWSListSymbols:\n    def peakmem_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs",
        "name": "real_list_operations.AWSListSymbols.peakmem_list_symbols",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_list_operations:51",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fd9a84c3b5ca3f9971e5460376018e832367ebcb746eb52aff37606f622eff17"
    },
    "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting": {
        "code": "class AWSListSymbols:\n    def time_has_symbol_nonexisting(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.has_symbol(\"250_sym\")\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting",
        "number": 1,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:51",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e61b7963339a5c66c99a44b8a5ed4246ce52e18c56de05ad969ccfe36e1007df",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.time_list_symbols": {
        "code": "class AWSListSymbols:\n    def time_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_list_symbols",
        "number": 1,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:51",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "740a4dee3a32d8cb9246934e1860041a0a4d07cf870f17a851c804c95797d293",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fbbf510c321854b58ec444401f123c69f5740b256525334c302218a2ae6d0066"
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "34c30a2205a1449369b92853def2c677da806e1fd0c092f8ff90a06fec22e7eb"
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_versions": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_versions",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "68626a85492c785d254c3253ac781ae868deafb7d32980f62984e011ac0c5f07"
    },
    "real_list_operations.AWSVersionSymbols.time_list_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0a97c0c02ba988159c2ba09b8e7f02ca34474bf249980aaea55c37d440d7721d",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata": {
        "code": "class AWSVersionSymbols:\n    def time_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "6221253e0b62c144c7223b4969b58a430a1717462bd939900b602a5481d24bc2",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2f3663667131b7a0be1abbf0400dafceb3af124a460735034097a907a3f9bcb9",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "193b11a4617959d826ba0c91617e552bdffd44feb3b78ced90c5300c0c0450ec",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only_and_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True, skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d16cac6fad1d99375dbec55fcb0890d7444f076796e6952dab891f2d79b4b70c",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3493206fe0748c6626346d9cb1573b137114e32a4a30dc7aa082ba90ccb3c57a",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_snapshot(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(snapshot=last_snapshot_names_dict[num_syms])\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2aff065897965f79c4a26f9107a7acda0fa16d88dbfb3ffc50cf98ca90570155",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e97486b955cfba59bd57e26e7f67dfd28fa68fe5b41d4000cad23410f78e4a0f",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f8d864264ef6e4f853192984c4f67100dbcc9fb5ea225dcf5a3b4e00947ff62c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "6685c85cff6e17f615694061ea490e2b14d432c1628252a29d07fdcc04b97c2c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a96bc9d0a016f29c9671d713ad7a2fdfd945381695c2227c7716468b8eaebd1d",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "bbc2c8763db23c7c27e02f3361ee26cb68e9ead689a18a7e7bbc81edafa7a5a5",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b690d7836ba77e8f6ecd134bf9c3877c8c3f5b1a5d94f4d842b3b192ec8bbe07",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete(self, cache, num_rows):\n        self.lib.delete(self.symbol)\n        self.symbol_deleted = True\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:261",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4588f423aa2b7d1ded777f24c8ddd1b19a282bd7c7a6f15c012fc0cf1acbdd36",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete_over_time(self, cache, num_rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(25):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:261",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "978d41f95903f476a5a0c703ce1cd6ffdf3386ad6dd9023851d9f784159d567f",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ae924cc65f3fda9e7c64fcd76f6a542603cfb7242333cab7d762a62689a44aa3",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "afabfcaa402bc4f4a8dd332050aaa65770b2d746b2ea6235ec9421e461dd4975",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "c3a2c9b358a8c4ffd9af01ffa0b9474e2e317579a1030aeea626be4f621274a2",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e865f1e39380722e6d3bfe6e3a56d2fae9389a0d95cd11c29b6f34f2007a389a",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ba848d44eee0ef4595475f4def6ae17301c551096480c10b75562fd8f5c2598c",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0470f242734c94e8f6d30f4241c13ffe6cc8b53cb2bad1973799878b94c3cccd",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9e0bfdbf626113f82fdfd1ffcd6e02ac70422c95277dc9b2b71b9eff44dd5844"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ded8cd45317d8b9ef7d267bbe014125a0cb892b87d4f249a7b113c7fa8d09df0"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1e538c9cadf80b7110caa89fd8da6c930281e5f0d8f0f77b874ff590cd359fcb"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4a5c834b77a9e0290a0acc55ea4aad637b747af99e048bd0f7a953f8b5fc763f"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9fcfddabe4a1ac18529dc03b6cc3aaad336249b652774a16aaba62e2132702b5"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6f7f2edde2507709dc18b41fa471455f6689bc62c76f009646bcfa9337ca8485"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "56097ba0e08cd9b953d9731b60fc7c04ecba690f615098d26be69fc8cf6f105f"
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "5051b70ae6f5cbc9493af6ec64ea7f644db64a60d0a58f5de1930870d44c02a4",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ef199c9d78fc43c44984acaf08e9867a6efa89ab5b0012e51ecd4e5fd1fedce4",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_projection",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "df07f61fb088929ad901e2fea6f172430f060daaadd6457d9eea51bad1129a8d",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_1",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "9720f704e40e9de6ae39cdab8db71d34c19b46fc05f4151e49ecebd50f01fd0d",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_3",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b0ebeed0c3c098fafd4f9f1f1b4031dcddfeec4eb44ec0789a60f9b9ff3df04b",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_4",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "da26f9fa31e449bf2696324ee811ee3a949836935c265a28d8728bce3ca03579",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:65",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "7542c6a85b7d4791481f3106886009ee800f6fdfb132008b9f6fa60c6ed93848",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0689b459c6c7bb49e10edc6df8a37976cdefa6017472521a35896aeddf267ae0"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "61215cb103adee49fa6f3f56e1f611dcd697f9b44e8f2be23d35b3c6b997883b"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "285c5f2f9ba3da391ed62f3dfcf5e754258744e6ee152ccf26bf8a2ca571c972"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "67423f3bf528cd9f5242633d93b484b558436e5c843c939d5071217f435ef906"
    },
    "real_read_write.AWSReadWrite.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0625aa02db843284e2bd88936c72d6ed89e4f8399179c6230fffb4e15096e141"
    },
    "real_read_write.AWSReadWrite.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7ba6fea1dcf0c0e751683e095293ce820525ee930e234bbd516af15b7636d80f"
    },
    "real_read_write.AWSReadWrite.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "612120fc41c174f1a09dadd9b53367c87432990752569e4fbfccc84a8d0094b6",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f2264b620c766af4a90a0eb7e2e06b5fbb47f300449e8ed6a8e3a265e695b9ff",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8c7fb09e61e2d7bd0f229b2ca10e61cde1f54eadb187d6f9f8136eddf32d7b1b",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "84f2f904f9ca0aa633bec423ac9385f780529edd8cf618e13690b07781a90b23",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2e85d7bf669c8aab495b34003fc9d5f536e37ee77b0154c98debb39612a1898e",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:78",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "aa2c3915c2cc65992b8d64669e2bc74ccfbd4d83469b0eb253739b11211b46be",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "bcdfc7aa551bc9ae43c6bf8dbb5ea2b9b92e03e7e6da8487ac110b302443beb4"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d8263d005135750e39a3e13bd7de6702642ec7991a7dfde787b4bdba7a47125c"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "febc2581d6ee0495f21fe39fd5b9e139ee3784074ebef2305ea52a167d60a449"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ac2ade0b551d6e6eca41a7b098855fc013f95c1ad95a97d15fcb716a99784bca"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ebfae974146eb06e3015c78f663185f041fecba4f2d21265a95a0f8e0b3ed114"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8e0b84e936c8b20c9ab1100d19b36f06abe0690a3d42f1a121d487556d98fa66"
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "02de728e3f80213fda1fc979b4aaf61786cd350dc31f266000478ce15d5c04e0",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e730587e51a99d028e7438d4874d1e0b1f098b49ec2773dbabf9b06d7c562315",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "69b8202ed5e2e62aefd161b99db3b55667643789c66fc99599abf2649c144ab7",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "35c60f81beba373aaf062739dc364a9835f7f7bcccd9ac4ae7187f2f97a6a0c5",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "dc443b21285eae3a4009ce860f6d4be6cb0164245da60aad1b63ec6fecd5d4f8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:227",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ed3fedef28f86763035af2a64963966bf6724343de519001c9ac1a4a72d84928",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "777451970fc2c9ba7c40648d51397be114005fca10b35c2beb816a0c5b11eb4e"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "94cd6191a28611f9fdfb6f92e01fa9ccec156d60cc7bda30471fc0b6b27d8c28"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6b035607aea2f64b0266d23fd98a15543efda0abc94c68902099d2525db7050d"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "840af2cee86e8ba04910ceb2c04cc3c6e732c2a9eea52a2ab3cecf1a35767444"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c55080c0aa9714cf9bbac9058877813c9fd7bab7326f66618725b67b14f21372"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write_staged",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a07309f742f72469c5f54a0962205f9272af5d741b615c6e5536f97ee16356ee"
    },
    "real_read_write.AWSWideDataFrameTests.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "355ba97b40b39aa52586e966f70b2710a64b1694030d66d69a94845d11620e12",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "5653d388b62f876452148f5aff5c29e03eda6a7548b406fbdc324ab2365bcdbe",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "7f4881ccccbe07a5dc1f77fcf1e75aaa331f0f76a590eda75c876742eb30c613",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4feff55363038593799ca793ef2904d32a76d0d794f5466ed75c56ba867bd1d3",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ed88fa1910599e13c65662744b40b1396536a03e2397cbf12f39f37b81c8f81b",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write_staged",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:199",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a26f1ab74733278e53ed0c0913f466bfdb150dd2fa8d5a481715bfa3caf2f978",
        "warmup_time": 0
    },
    "resample.Resample.peakmem_resample": {
        "code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)",
        "name": "resample.Resample.peakmem_resample",
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "setup_cache_key": "resample:38",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "760c9d62e17a5467f1e93abb258d89057e8fdf9ee67d98ceb376e731157a4d2e"
    },
    "resample.Resample.time_resample": {
        "code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)",
        "min_run_count": 2,
        "name": "resample.Resample.time_resample",
        "number": 5,
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:38",
        "type": "time",
        "unit": "seconds",
        "version": "1381d2db90e66cb5cd04febf62398827a3ac9928795eaced908daec35d5c0c31",
        "warmup_time": 0
    },
    "resample.ResampleWide.peakmem_resample_wide": {
        "code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "name": "resample.ResampleWide.peakmem_resample_wide",
        "param_names": [],
        "params": [],
        "setup_cache_key": "resample:106",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"
    },
    "resample.ResampleWide.time_resample_wide": {
        "code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "min_run_count": 2,
        "name": "resample.ResampleWide.time_resample_wide",
        "number": 5,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:106",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2",
        "warmup_time": 0
    },
    "version": 2,
    "version_chain.IterateVersionChain.time_list_undeleted_versions": {
        "code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_list_undeleted_versions",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:37",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "6bdd43d7f191d2bbbd30ef740909969e25cbe1cec77f1755c5c3ba58a77f2b88",
        "warmup_time": 0
    },
    "version_chain.IterateVersionChain.time_load_all_versions": {
        "code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_load_all_versions",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:37",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c40fe3123db9e5d6fdf5f35caecaf42d266328deb78c237e293096ae3a4bcf98",
        "warmup_time": 0
    },
    "version_chain.IterateVersionChain.time_read_alternating": {
        "code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_alternating",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:37",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ec1a61c37c4cc7317cfafe554f3eeb7fe2a426068ec412c1d7c6b78f510f6c45",
        "warmup_time": 0
    },
    "version_chain.IterateVersionChain.time_read_from_epoch": {
        "code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_from_epoch",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:37",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5c6aace0b39c7a75f064a61c182cbbb42a35f0e0ee46546579bc641e68dc954a",
        "warmup_time": 0
    },
    "version_chain.IterateVersionChain.time_read_v0": {
        "code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_v0",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:37",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4bf693e490128c1cff7500c93799432e7bf150925d3714757219604aa7fa5e9c",
        "warmup_time": 0
    }
}