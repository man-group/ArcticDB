{
    "arrow.ArrowNumeric.peakmem_read": {
        "code": "class ArrowNumeric:\n    def peakmem_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowNumeric.peakmem_read",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "100000",
                "100000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "67ab80e297890f213706b5160255c969b0aee853adfd6ae53655c45b7e845f85"
    },
    "arrow.ArrowNumeric.peakmem_write": {
        "code": "class ArrowNumeric:\n    def peakmem_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowNumeric.peakmem_write",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "100000",
                "100000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b28a54936bacb902a56cad0b9d235bd3e13ac04ac37687cf265c5a987b7ea2d1"
    },
    "arrow.ArrowNumeric.time_read": {
        "code": "class ArrowNumeric:\n    def time_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowNumeric.time_read",
        "number": 5,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "100000",
                "100000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ecb51f067ac7d40c4b9c9df11fa1a0b1f3c682a14f1ee974cf8c3eb9fcfd86d7",
        "warmup_time": 0
    },
    "arrow.ArrowNumeric.time_write": {
        "code": "class ArrowNumeric:\n    def time_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowNumeric.time_write",
        "number": 5,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "100000",
                "100000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b0a49f584f44c8451a2fda903df76351a10368b81d639239cfa3023e9dee737b",
        "warmup_time": 0
    },
    "arrow.ArrowReadStrings.peakmem_read": {
        "code": "class ArrowReadStrings:\n    def peakmem_read(self, rows, date_range, unique_string_count):\n        self.lib.read(self.symbol_name(rows, unique_string_count), date_range=self.date_range)\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name)\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowReadStrings.peakmem_read",
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count"
        ],
        "params": [
            [
                "10000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "1",
                "100",
                "100000"
            ]
        ],
        "setup_cache_key": "arrow:111",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e4507030a33067ee8dc5e23a40f2ef36232ac8e8cf42fc70b77bdd0ca16c72ea"
    },
    "arrow.ArrowReadStrings.time_read": {
        "code": "class ArrowReadStrings:\n    def time_read(self, rows, date_range, unique_string_count):\n        self.lib.read(self.symbol_name(rows, unique_string_count), date_range=self.date_range)\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name)\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowReadStrings.time_read",
        "number": 5,
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count"
        ],
        "params": [
            [
                "10000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "1",
                "100",
                "100000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:111",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4ff5c086861a792b7bc9591bbe8d74f6c29d420929db8f89fa6781dbb508b349",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3b847508b2a63c58c508dc7e7aec08547e2ca09a57bcf3f6777619a5cb149b7f"
    },
    "basic_functions.BasicFunctions.peakmem_read_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a4ca2ebef508c6560b98af94d69ba7cd459cc02976eaed2aad41ac976d932b11"
    },
    "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "74f0d776147883a788c405dd90a8490ea4b766f2946aaf5ab43cc48076c2b929"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "34501bd0311c8b2644012b8f713f8a596bba14dbf1976cbbded37060cf77709f"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "908e8e1658fc2235b08f2996edb57c49852791e6f6c187165b3b931e7d7e896c"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7870e015641ad2975bbc58ad43428c0e65d3771d1cd2838d2f72f4d6c57aa926"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0db13340295d95fa06e3f786be93bbc345a0f67ffe4fdcd226189c2b82aecb5e"
    },
    "basic_functions.BasicFunctions.peakmem_write_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3128ed9e1a62c74b07c0c51e215080ecb9ccb8284e6221dadb0d4b229acceb2d"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "42c10f3b6072be6b54db49e98670de0ce65dc2d7e543f6b1af53bd4fd28bba5f"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "99231807c0927256747827b6d0d3bf8d565cae9f2b6955d40e3c403ff162daac",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "18cb78627bc67fc910cdce20943fb32ba17bc8271a1b13de73393d76c1411f9e",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_ultra_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_ultra_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f5a362303294a64862824aa956d62c05bac490feb7438d742720d341274beeb7",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "97fab62dd3036b086e61d8e193b02c3c566b76c76cb514b44dfd3d9090a2bbe7",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8fe0b03e88febe68a480effc73ec81b1c3bbb24fa31f8d4868db79760d58ccdd",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "50281bd8fb1b296b9de072d72b5bb207d9b5d1700949b1d911f7475339110289",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a399d5220450caaa75806e22756b1bfcf8b27c050fa15dd9165be085ae2b0b63",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write_short_wide": {
        "code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7d9de7789dd1f63d916bd3e86d00c633177462a29f323774fd022cebf75e92ab",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "00d21f9de9837bdad48c5ad99e7c3312e1a2825b119b58548d7fd703481d9501",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1154d4d002a16465c10fd5e41721ed25ee8d4a5fa3790c7718d7f309f1b8b29c"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "eb2f74f4dab10d4472f2349a2d539c9609baaaa6debf7206c064a2b93c495bfc"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "114711c1c4ebf66c64ab33fb81f5f8cc73ce78984e2472f492d0c80f96f331d8"
    },
    "basic_functions.BatchBasicFunctions.peakmem_update_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_update_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "11bde9a31b891ee985296085ae2b9f7805556601060b2e993f257d8ed4a2144d"
    },
    "basic_functions.BatchBasicFunctions.peakmem_write_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e29d7bab3ecd450da657f11c448f62bbb39d5a2607fb6f4cc5b92db2dee50dc9"
    },
    "basic_functions.BatchBasicFunctions.time_read_batch": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "95a87e9c330d6f1b83b13cdf87403e69a4c2d4a3992430942454acfd5021b73a",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_pure": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9a6a0b018e486ac4f51dcd497fd51d6394c9b92418337aab1e6bbc5347b3674c",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2c798cded56db601f7a5e81cf0d0e77407d3717695a4c4863d4e68c96618393b",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "04e761907c8cd5b7d21ef3beadec4150292a508c0de7c9fd36305f118c97ceb3",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_update_batch": {
        "code": "class BatchBasicFunctions:\n    def time_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_update_batch",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ae298ed42a2bc58a2d14509b1578a2ecd7c11cab6f15200ef8ebfbc4f9924a27",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_write_batch": {
        "code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_write_batch",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "49522447c3d9ac6f75f9df9a159dbbaeb95553440cf4bbb98303fce0c490bd66",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_large": {
        "code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_large",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8d1e9a72db76b8a1a5e0215330c54476f80a05dbea2174186950370cd831245e",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_short_wide": {
        "code": "class ModificationFunctions:\n    def time_append_short_wide(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_short_wide[rows].pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b15a42ff2e9792de5c9bee4cbf108d57071a6c2b2b504997406e7a848b83b0dc",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_single": {
        "code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "52acfbd4409bdcadf8af0cf5bd559122d5d467b56650bbb381c2d332a1aece2e",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete": {
        "code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "672259b2998d092f121f5de4c7b4327ebe5c3e444b677994c276755dc72bcffa",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_multiple_versions": {
        "code": "class ModificationFunctions:\n    def time_delete_multiple_versions(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym_delete_multiple\", list(range(99)))\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_multiple_versions",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2f548b89ece0762d3a4ca804f4cebacdc407c72b720b63927a70a48213bd7b95",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_over_time": {
        "code": "class ModificationFunctions:\n    def time_delete_over_time(self, lad: LargeAppendDataModify, rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_over_time",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "77d3d9f015b1ff0e318bd419a22aeca9b56c22cd174c0efd80e2909b9b68ceb5",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_short_wide": {
        "code": "class ModificationFunctions:\n    def time_delete_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2409d72e307e71748c55e973bf651e1c775a5ed31869e089c243223efbb83df3",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_half": {
        "code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_half",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7bde302f3062e94d0fe774eeebb7ad10585c4e40e525dbd3e05b25c0a7798ce1",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_short_wide": {
        "code": "class ModificationFunctions:\n    def time_update_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c012d3639c8e84e4a2e654631211e110644fd75bf36de8576515355a72d9409c",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_single": {
        "code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "d0b4f92fc686ee91643527d18268011dbe2053d2f5463efb671d28d1fe2e388a",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_upsert": {
        "code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_upsert",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "916a69cac80c70e2fe053b743904459170731ac657c2efbe0222daafcdd9ba9e",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "331c9abb4f84b01dd28765b77a88e069ec6d6b70617a12dd5aa9c3e14ca6a6ad"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae73602827d4d1b739e519e8ca6a847c5938a5744ebf371ca78511b0be1bf16f"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e708975c50d2b70ebdff11efa45f9fd15ceee9861301d5552f1e8ebe2cb4d1bd"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {
        "code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3407eb183cf3c02afbeaf04e6c31bf6b5aaf615458cd8e2ad46a21b4d2af80e2"
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "bf5e390b01e356685500d464be897fe7cb51531dcd92fccedec980f97f361e3c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ceeb3b3e6049c66cb2ecabbb16485e4555cefc7920697c7a34de08993be14af0",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "fa74284d1e48fd396138a5f50c53d92829194b7be1f0caa8f441f8820db4157c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_readall": {
        "code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_readall",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "3cd2c7d90725498da459157638eb15b5a3fcc68aa91684951717ed5ab1c8ca63",
        "warmup_time": 0
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_create_dataframe(self, tpl):\n        df, dict = tpl\n        df = pd.DataFrame(dict)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3580ee58cdf2db23481a370c46cd60e321e4c5b6b763d8a56a905cd857b05b65"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_arctic(self, tpl):\n        self.lib.read(ComparisonBenchmarks.SYMBOL)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f13bf3d2bdd1f6129281823f3ac4a1c6c7d7b97be21efe3ae0d1065b07f89050"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_parquet(self, tpl):\n        pd.read_parquet(self.path_to_read)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7d5db25fdc879f9e0da495f2f228353425dd662cc2ad93eecf662c2dd9be1c55"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_arctic(self, tpl):\n        df, dict = tpl\n        self.lib.write(\"symbol\", df)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cd90a2c0309608e8c86b47a91b2a4c26ba9ba6b7bc50ca115f0d62b4ab82d064"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_parquet(self, tpl):\n        df, dict = tpl\n        df.to_parquet(self.path, index=True)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "166a93976c1ae0088bbb2477ee1d3e92672e7477a811b64747ee2b97bb078f26"
    },
    "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, param: int):\n        self.logger.info(f\"LIBRARY: {self.lib}\")\n        self.logger.info(f\"Created Symbol: {self.symbol}\")\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, param: int):\n        self.ac = Arctic(FinalizeStagedData.CONNECTION_STRING)\n        self.lib = self.ac.get_library(self.lib_name)\n        self.symbol = f\"symbol{param}\"\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache(CachedDFGenerator(350000, [5]))\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:47",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b49303c7a22a40468340ba6846719d83029e19a7355bb1d0e330142132a132b3"
    },
    "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, param: int):\n        self.logger.info(f\"LIBRARY: {self.lib}\")\n        self.logger.info(f\"Created Symbol: {self.symbol}\")\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, param: int):\n        self.ac = Arctic(FinalizeStagedData.CONNECTION_STRING)\n        self.lib = self.ac.get_library(self.lib_name)\n        self.symbol = f\"symbol{param}\"\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache(CachedDFGenerator(350000, [5]))\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "repeat": 5,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:47",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "b083f4862ad8234395f2d583c00eca4ee85bd17a2142e225946d02fbebbb849c",
        "warmup_time": 0
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def peakmem_finalize_staged_data(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().peakmem_finalize_staged_data(param)\n\n    def setup(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        if not SLOW_TESTS:\n            return  # Avoid setup when skipping\n        cachedDF = CachedDFGenerator(350000, [5, 25, 50])  # 3 times wider DF with bigger string columns\n        start = time.time()\n        self._setup_cache(cachedDF)\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:100",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "78d99e44cce890e09c89b0a8e4c1420cd237ac7eec4bb678c96618827baef718"
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def time_finalize_staged_data(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().time_finalize_staged_data(param)\n\n    def setup(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        if not SLOW_TESTS:\n            return  # Avoid setup when skipping\n        cachedDF = CachedDFGenerator(350000, [5, 25, 50])  # 3 times wider DF with bigger string columns\n        start = time.time()\n        self._setup_cache(cachedDF)\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "repeat": 5,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:100",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "6c8fcb770b3ba6cd93b63b830a169dd3afd65e5cf007496395938a6f8e10cf82",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.peakmem_list_symbols": {
        "code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_functions.ListFunctions.peakmem_list_symbols",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "36bbe8ba3ff6df423837100203b3182d32efce65a17df876ac1369d05d7523fc"
    },
    "list_functions.ListFunctions.peakmem_list_versions": {
        "code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_functions.ListFunctions.peakmem_list_versions",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4d771cfea089c1fb8c4b9b8331803fa8aeca814cc8c2d0558485705521bb184b"
    },
    "list_functions.ListFunctions.time_has_symbol": {
        "code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_has_symbol",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "033af6c71329e0162ae9ea485b64a83dba030888400a9e1d0c5c33d686bd0880",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.time_list_symbols": {
        "code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_symbols",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "272c3535600411e60dd72ff988009bbc491bca183158c04d2aa82748e95bfc33",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.time_list_versions": {
        "code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_versions",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2c963b861564347f6311befd6189eb42e4b882c06fb468060e38b9cda65cd12b",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "2fe5cfa52bf33e64fd73e89643e27d8f47d02159488e14d0359cacfa93e63e4a"
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae3140e42a763dd7a75cc5a8df60881378dec6ce6708ac127ccc8dcf367626c0"
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots()\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8350189525f2502cc7949369b1dff78862d303da0fa2d8e27a523411f3a501bc",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "eb27c5627ff775cc2a8f0f8841f5b28b79f0ee10c3420e77eca1f8ca4582bd0e",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_without_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "68900e75770793f36f0f940ebab65e148dee97f5c8cebe43c4ebd3ba5350dfbf",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ea9562ac454e814117fbe50a9b5d235a37110c1745fa24bd71db420baaf072ef"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "69f411de823212396215fb4912da15f56a16ca931fd2cfcf9533204b56e696ba"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ecadcf0caca3e2382953be630263bde2c01f7ac011ac2a140080e00f3a6ebdad"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cd35c81c46cd4eb64ccab7a6099884a9f99832a5ece8bb0bc387c0ed402f1536"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1806b540ceeb66fb0bc4476c5b07fca6d5d5edb13e441fb4a2471751a99ff7cb"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "923c4661c84571a4abb8cf5d774b2e0ec674bf671b3b11bc26ed561a0c417ef7"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0af435e83cbba85506f4bbf0fe355dd839b3e55fd81aa7e3600fcb443dc682ee"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2ac4df4d7d9b744d192742ba5b8c00c2f79a143cd72c4a7b7f63785dea19e219",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e16e60780a624d94310ad7e6059e97827feb6b4b6bc2d757a1e89f67c5e7ddd5",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, num_rows):\n        pattern = f\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "104a945e32f971e4b887da7b6cc6e6e728fc78ef6a77c2f6aed0693c24113234",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f2bd34ad9342852559d34262fc059be4c6f3122909b31ee078847b1a1b93907f",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "af4473b7d93a3c30d6cfc9d8bcde05e126a1df7948b5877c1604f2882a037768",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9d0961c0292eff90c651a1989e525c7ce5ab63baa888ac4e2ccdb88b89cc9f2e",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e32d4b66ff29029ccf51f51511afe9792412ba38d2422d44b03b7e7e8710e38b",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2b556ee72fdb8b5b25072c8e0df67fb3264fc3f2fa724c453a6522ef98392f93",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "12247b929d34d209286ab85918d78e86cc91561bd9a4cc7db06beaa001d21e0d"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d8eeb21aa5b74f3f935a4b92c92f09f31d1f089b892b6ac712a476f33e854a4c"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_date_range)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] > 2\n        assert read_batch_result[-1].data.shape[0] > 2\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "aebe4be8cf7590f95c29a5f5e71088dc833e85dba49c4d159ccb6aced1f7ce90"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert write_batch_result[0].symbol in self.symbols\n        assert write_batch_result[-1].symbol in self.symbols\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b58eb8a420862c585bcfbdfe92d70ba068310e4c34c7846292f47dd7363cae27"
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "501017573a8a25827024b656a55df9ed1a621d3cdaac24c830c183d9b691463a",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8e28fd869de381aec95ddab625b7d4e17bd262a6238f21b980a1ded0903ef3c1",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        self.lib.read_batch(self.read_reqs_date_range)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "97ac25e33d8fb8d99080e1a9177918dc6bf503d3fc8bc429ae06dc746537f950",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_write_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "79c5307f83e8be6020c49ce96f9b943180776220a624f8853cb50c7d10db11bc",
        "warmup_time": 0
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7b1c447a499ef7f7a8377e3797c6260d331ae11ca1a2f4cea0425b3ba64ba4b9"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # measures base memory which need to be deducted from\n            # any measurements with actual operations\n            # see discussion above\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.parquet_to_read)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_read.read(self.s3_symbol)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "74e842504a929b308054e279b84e4d85168663dea924431ab926322c614cbc3c"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_write.write(self.s3_symbol, df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "376aa64074dfc1b8e319065605bc8e26898c54cf8e2d0472a317453ebf7b4915"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def time_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df: pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read, index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "min_run_count": 1,
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe",
        "number": 2,
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "time",
        "unit": "seconds",
        "version": "9b06683aaf9b2112fce975aaad4ccdf9ad266b297384fb73f01948a23ed9c622",
        "warmup_time": 0
    },
    "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class AWSFinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(0, self.df_cache.TIME_UNIT)  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache",
        "name": "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "num_chunks"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_finalize_staged_data:42",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e3f95aa7923f7837767aaec1073645b8bd6bec4cfcf71d0e819232af51829284"
    },
    "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data": {
        "code": "class AWSFinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(0, self.df_cache.TIME_UNIT)  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache",
        "min_run_count": 1,
        "name": "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "num_chunks"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_finalize_staged_data:42",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2cc8f1598f371894726ad98c50ef3fc5547a760f45d25e8a80bb2d563fda964e",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.peakmem_list_symbols": {
        "code": "class AWSListSymbols:\n    def peakmem_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys()  # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info()  # Always log the ArcticURIs",
        "name": "real_list_operations.AWSListSymbols.peakmem_list_symbols",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_list_operations:58",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3a9437f6e472e3d916bef3fc9e15da3e5153d99f515ddd3cce9ef7ce8bb411c2"
    },
    "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting": {
        "code": "class AWSListSymbols:\n    def time_has_symbol_nonexisting(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.has_symbol(\"250_sym\")\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys()  # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting",
        "number": 1,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:58",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d26278f69868af7bfde10e3af74abadb2aa8221e8445da75b25a788447907739",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.time_list_symbols": {
        "code": "class AWSListSymbols:\n    def time_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys()  # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_list_symbols",
        "number": 1,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:58",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "de9dca891016036c2c90b5a76df8ea8470d84eaad532a0c951af7eee9ee28215",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "81af200a443e9dad37310396d1e94b23d81d81270d7884be7841f02e92a5b9a0"
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6959e1346e7402271c3233e74a4ae19545c58e7852dd8d47ceb3aba03d7aef29"
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_versions": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_versions",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "56c7e4326a6a5a2b9a71b8fd78003501c0a1d26fa6f1873509fdc6c91cae90af"
    },
    "real_list_operations.AWSVersionSymbols.time_list_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e7523038fd41e53befe4bc74859e59c6226cef74ec40f7d4642b337d582e8332",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata": {
        "code": "class AWSVersionSymbols:\n    def time_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d606b05b0bafb3aeb519be7e19b7734b50fa29b03d35930a61771ee689ed9bae",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "bc1f9261fd04e65738f0494a3823d3670528f33f6d125dab804871c3e0d574a4",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "08f98136e4ed27c413ae42c38c3f6066c085fae581d54b4bd02716f3374c8347",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only_and_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True, skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "549aa0723764402ae84a33d17ba8408505357bbd2b337ba126cf68b019f5bedc",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "92b243f11776f97959ea3c3902133131c571477249705cc709b1568e687ec0d4",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_snapshot(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(snapshot=last_snapshot_names_dict[num_syms])\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info()  # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:138",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d074a8a33a65568b0815861b17e7e9fb230f2aae19a78cc56decd8df791e7527",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e97486b955cfba59bd57e26e7f67dfd28fa68fe5b41d4000cad23410f78e4a0f",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f8d864264ef6e4f853192984c4f67100dbcc9fb5ea225dcf5a3b4e00947ff62c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "6685c85cff6e17f615694061ea490e2b14d432c1628252a29d07fdcc04b97c2c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a96bc9d0a016f29c9671d713ad7a2fdfd945381695c2227c7716468b8eaebd1d",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "bbc2c8763db23c7c27e02f3361ee26cb68e9ead689a18a7e7bbc81edafa7a5a5",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b690d7836ba77e8f6ecd134bf9c3877c8c3f5b1a5d94f4d842b3b192ec8bbe07",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete(self, cache, num_rows):\n        self.lib.delete(self.symbol)\n        self.symbol_deleted = True\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:261",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4588f423aa2b7d1ded777f24c8ddd1b19a282bd7c7a6f15c012fc0cf1acbdd36",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete_over_time(self, cache, num_rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(25):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:261",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "978d41f95903f476a5a0c703ce1cd6ffdf3386ad6dd9023851d9f784159d567f",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ae924cc65f3fda9e7c64fcd76f6a542603cfb7242333cab7d762a62689a44aa3",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "afabfcaa402bc4f4a8dd332050aaa65770b2d746b2ea6235ec9421e461dd4975",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "c3a2c9b358a8c4ffd9af01ffa0b9474e2e317579a1030aeea626be4f621274a2",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e865f1e39380722e6d3bfe6e3a56d2fae9389a0d95cd11c29b6f34f2007a389a",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ba848d44eee0ef4595475f4def6ae17301c551096480c10b75562fd8f5c2598c",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0470f242734c94e8f6d30f4241c13ffe6cc8b53cb2bad1973799878b94c3cccd",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "66587c7cad65fa03050a1e2d2cbdd37b215e36464be2dbca6667b52ada07b398"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7a402ec7e4f09ccc8191abff900aee941fe5b983d3a54c1128332802b3aeacb8"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "41f3abfee7b61aaf01d6a7c097f50c82a3cd5aa520735e8e56d1433980423e81"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1e238f958363598bb77ea0b61b18e872f9f2d45ace953b66286f6590b855138f"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3f46b68395cb394e790f846e8bb368416fa13d20e8963afe38202e0afb2a8012"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0b4095f9e4d0594d25e5334be3c0c9d41135b0460577f58f20ef358d83e58dd3"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3d94e6e1d0c466cf98b0c7673d3e695b231110e5e0aff293d7a7f75f878f49cc"
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d8f0751dfa443b7fe80c43d18b9e89bebfb248d8fd2456719e5bd712c2f54905",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3fbe4bd3d18d1756b709637b51e6181d1753379ec47d30de348e3d82e8069750",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_projection",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3e7e763b14722ab6c81c7835c4bbee8fb086672ad73366beefd3c6bc7781172f",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_1",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "76a7f4b0952f07784150df2b7a382356a4be815385c5258323ffc27e1d385767",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_3",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "685f6a5908e27c6198503b4bc2330769d4c764d4e16beebf4e0355b5b2c6b627",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_4",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "de79858cd635f67551cee7c9c1439573bf926264dc8e11de497d7062c5589641",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:76",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "977b46a22c5f742940aa1d126fec2fccea49721ce34ed2ad73026225387a0a0b",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c9472d2ab25d1a30beb1146a9c649036af89bea84f2119d9cb184408139276f3"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cc3955cfed7c5684809c86a04041374c674f26e8721128acbcd80af06012128c"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fdc284437037f6f245607caf113a0e5f0a6c6dccc8e6ad9f471611574253c50b"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a09cc022e09a57064fca48fc68fed42d6b9593fbefddc51f8be856afb7ac710b"
    },
    "real_read_write.AWSReadWrite.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7876413187267e0867f03f798f317eaa3e3960ac2375ff5df6f2095520bb1ca5"
    },
    "real_read_write.AWSReadWrite.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d211408aa7db06df36befacdd5fd39b9422eb968773a6e2bd19f8d16745541ac"
    },
    "real_read_write.AWSReadWrite.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a2b8548a163367ba007992cefa84d7a83d4f60672b14b8a90bd4b2600b4d8131",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "470178c2a5f27c30784904befff88ed0b75125c5ad1a4d508ee2c6d79e1f3f99",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2d8f9e98f36bf378b003b44866c8d3c39864f8160798e8b2cc7b475b074bdd38",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8aec0888e02948dc708cc304400857ce54ab1f5b91fda2bc167bce90ea4c7299",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f77b10516f456c860eb05ec818f8242a43aa9adc54b34ef30eafd4098299322e",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:87",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e0cd4c4a06cec3813214e5ed2e32ea0a22bf2f55e6d9cebc4f16894b16710e36",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1cac6c9cf15d5fbf892a777498296d8d098711272b405b1c8e243e3e767e599b"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4ff1a56334201630187cbd2ad88a520d04067e282d5a163c1d4a34230b997ab5"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "44fbc536228fe2270c8038e451223ded4f7941c9226bf95190a46b7b28f228b0"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "307ae3fb4ac3c92c44fb9c578b21907867846318087564daae4c61f32fd996fa"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3b402f9a631ceeb544fa7ee7b860e13ab870c7e61da809ba11d14c5973c97cda"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c1fdbb5013c30dd4c7a6461a1b9193097a831af4f6e639e8c3261c3817e6181f"
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f7a86808b4972336b56bbe425b35ce39d7db682c525504edc5912736af82398e",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b0a5713c3639a1d5084ba6933abf2bcb14df3db143f5bfe713fbb7e448068db9",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "748e8cf9fad63c651326e462f4021c28a2c128a1d7c4783fc2886a2451b19d99",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d77a903a22862f45799bed1ae703c285e21602d82da264bf013ceba202c63cf8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "48483b15cbd6e2738ddf3f615179a9bafde8596faac8e1b331ad03df4b4c21d8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:243",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "9805c7597bb79b23de9a89830bdaca81c17868b485d9ecd9ede6d0621460cf56",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "777451970fc2c9ba7c40648d51397be114005fca10b35c2beb816a0c5b11eb4e"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "94cd6191a28611f9fdfb6f92e01fa9ccec156d60cc7bda30471fc0b6b27d8c28"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c27180c5137daf6611cffdd96923d000f03bdc4f4c12b00435502b40d5abd4da"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "840af2cee86e8ba04910ceb2c04cc3c6e732c2a9eea52a2ab3cecf1a35767444"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c55080c0aa9714cf9bbac9058877813c9fd7bab7326f66618725b67b14f21372"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write_staged",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a07309f742f72469c5f54a0962205f9272af5d741b615c6e5536f97ee16356ee"
    },
    "real_read_write.AWSWideDataFrameTests.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "355ba97b40b39aa52586e966f70b2710a64b1694030d66d69a94845d11620e12",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "5653d388b62f876452148f5aff5c29e03eda6a7548b406fbdc324ab2365bcdbe",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3ab9782a626f87b77d5fe64ffa7e83ae14ce2274f047d15f0c1af2c9bb8da30d",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4feff55363038593799ca793ef2904d32a76d0d794f5466ed75c56ba867bd1d3",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ed88fa1910599e13c65662744b40b1396536a03e2397cbf12f39f37b81c8f81b",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write_staged",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:212",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a26f1ab74733278e53ed0c0913f466bfdb150dd2fa8d5a481715bfa3caf2f978",
        "warmup_time": 0
    },
    "resample.Resample.peakmem_resample": {
        "code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "resample.Resample.peakmem_resample",
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "setup_cache_key": "resample:45",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "99c6acb45f29b2d72d54020d96990507084b8fd1c9918c594e9fa09670dec465"
    },
    "resample.Resample.time_resample": {
        "code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "resample.Resample.time_resample",
        "number": 7,
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "resample:45",
        "type": "time",
        "unit": "seconds",
        "version": "1f835087b54bb1d48a11dcbdb503bbbc4c57d6c5f54b6f0f6cf22c68c2187a5c",
        "warmup_time": -1
    },
    "resample.ResampleWide.peakmem_resample_wide": {
        "code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "name": "resample.ResampleWide.peakmem_resample_wide",
        "param_names": [],
        "params": [],
        "setup_cache_key": "resample:131",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"
    },
    "resample.ResampleWide.time_resample_wide": {
        "code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "min_run_count": 2,
        "name": "resample.ResampleWide.time_resample_wide",
        "number": 5,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "resample:131",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2",
        "warmup_time": 0
    },
    "version": 2,
    "version_chain.IterateVersionChain.time_list_undeleted_versions": {
        "code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_list_undeleted_versions",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8cf2d8b7302ee0311a2bab73cb1ab31134c9676a39bc5e517411e3192a89ead7",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_load_all_versions": {
        "code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_load_all_versions",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "32c93e66abfbbbaa80b6cdf40c50fc82e21aa1de964c5962ae200444ff26f252",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_alternating": {
        "code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_alternating",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f1f008d2c2efb9386c21fdd3539bb3601b1078e323613d38f13ddadb066cb004",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_from_epoch": {
        "code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_from_epoch",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ee44eb3fe3eecad30de7d9349e47e68cdeb430326b5713b7ae4bfd7abdb63707",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_v0": {
        "code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_v0",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4de46121ac1a914c7a5d77c82aa535e29da68e0e2acc7e17e483d168cac49db3",
        "warmup_time": -1
    }
}