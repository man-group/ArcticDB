{
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "2aac10521743bd6cc38b0651c5889d474b5468c1166994261a08aa12c090e354"
    },
    "basic_functions.BasicFunctions.peakmem_read_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8dc0558241a4d313bcc9c607e79ceae1848864130859c3d5512889303c5b1ab2"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "514794820c0bbf4a1c82aaaf47090ca4ed7a6834bc59a8d30757623798f409f7"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a88b0d9a342fd1be6fd7564bfcac0597bd9046ebe0d4ddd01ef4c10ac95326f6"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8b2b9b8fa4fff3c9b39e8282371bbb03daf0cfda234931598dc9103ef557e87c"
    },
    "basic_functions.BasicFunctions.peakmem_write_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_write_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "22e1d7860c2f63edbe004b56bb87f88495e8b012c7b802a4ff08a12b141c434a"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7292f3cb4cf929adc3a00d2fa29bc2c8e1472bd52f1001564cc652d0a9433b04"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c9e34f0d193d0b815bb954c03915b389b6e05cdfc18545517b411e335e14e3f8",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8845b4655ab7a2fc0b836da115b599fe8bb7ba8001660b6af543acb5fe765170",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "edfa58702d2a4ee0ee84197a7ba81194e5e3aecc0f8520f2a8d4b06d45896021",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "111bbd81442ec1a42e9c28ce665a629c3875d7c4dc1e6771f4cdce9fed080e2b",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5be801860fd4852357fa9010f680596a23a606e1f85e672341de2a1b472ce1e1",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_short_wide": {
        "code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "25b8f595dd9b3b81d5544e539fb0f4c24455015c2ccb2bceb82c28fbf0698680",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:38",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4e330332407ef3b614f0cafbed8ff8fd944b2ecf9854ab225cef0cda6bb8ce8c",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8fe50f627f8d9efb117cadb0f26ee0cbe39662ba1b2905cf3984f33fb03b6f6e"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b260f7db89580432675d89ba8d08e74ff830f74b81cf7803d58ba44068c9a49c"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE)\n            for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f440048c530dbb90f782985f7c45e774e3cda60f0d134665e4ef6adf02bb18b7"
    },
    "basic_functions.BatchBasicFunctions.peakmem_write_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "add75de5523f5002ae08e9f3c01ab33698a49635fe16d01031164b203e4fdfe1"
    },
    "basic_functions.BatchBasicFunctions.time_read_batch": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "32b2eed380d4e7863cb87b2011f6add3881e424eeae2cd9b11dc75507fbf8640",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_pure": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2d1bcf173d39e007c6e55c195e669dc59c57c6852e3ec435d30e421f6baa606e",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ee90e0674ceeebbac5b24babbd514a31eecf352b2fd8d232fce2fe886c559da3",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE)\n            for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9ce9e0532a98436b27f547980b8b4ec1914e34acab556ec30e8f3727daf7290d",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_write_batch": {
        "code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_write_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:139",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "6989b158867091ab3cb5c68d4d6418425164561776e8b86e37c24e635dd3b722",
        "warmup_time": -1
    },
    "basic_functions.ModificationFunctions.time_append_large": {
        "code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(f\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_large",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2b592208898cce70f9bc92ae629f428d8a4c7e9dc1a3445686c2420888afeeb5",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_short_wide": {
        "code": "class ModificationFunctions:\n    def time_append_short_wide(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_short_wide[rows].pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "29559b9bb03141c96f325ce1019c95e339620bba07d872e91e78cf07c12e92ee",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_single": {
        "code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(f\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8f398155deb342c70fe4c65e8da636b1f18c9296632b4649aab8dae306aa8453",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete": {
        "code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(f\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "6d8afae2414e0f842495a7962f5950472814bde20e99eebc474db6953d8e1ae3",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_short_wide": {
        "code": "class ModificationFunctions:\n    def time_delete_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f867fc9cac4d0706b01166662af37434100460706d4f6118de0bc2e0e3087bae",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_half": {
        "code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(f\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_half",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "6a011f58b79c483849a70576915c2d56deed1227d38489a21140341ca860ce33",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_short_wide": {
        "code": "class ModificationFunctions:\n    def time_update_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "111496c5bd4a4c498df28819d3cbcd9d699c4d3363ad3969f102a1d2076b3086",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_single": {
        "code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(f\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c45c168d5713f3028a9a5b97959d52116c8d228870ad580be06d86336d2476c6",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_upsert": {
        "code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(f\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            df = self.init_dfs[rows]\n            lib.write(\"sym\", df)\n            print(f\"INITIAL DATAFRAME {rows} rows has Index {df.iloc[0].name} - {df.iloc[df.shape[0] - 1].name}\")\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)\n    \n        number_iteration = ModificationFunctions.repeat * ModificationFunctions.number * ModificationFunctions.rounds\n    \n        lad = ModificationFunctions.LargeAppendDataModify(ModificationFunctions.params, number_iteration)\n    \n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_upsert",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:280",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7f139bf03457104abe937914aa3572503ed52330b3a271d82112696060331d8f",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a6be28bf68c237bc424c84b3af930d32da53053e5ddb11b97910376560ab0918"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9a608927df33b903bb6dd7ec33fea2c8172dd638e0169d8fffddb5069e188e47"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0ae4a1c3ebcac6600a0636c80e757fb34ef285156f9f01a10285fb6c803e2bf7"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {
        "code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "2957ec25dedc5ee645e69a28ed4a38ebd27415c53828b3bf6fb4b57f146bfa13"
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a7307ac55b614273b8a71fff12b16beeb9a256d49c760415422b54ec023a6126",
        "warmup_time": -1
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5e4efb31734abc4f731146ae302ddef56e66ada452c319e8a0dea8ac9e3e82a4",
        "warmup_time": -1
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "89cad46eb5100e61cfa5aecda0a0a34d755bb7e6e60434bb8979176681926006",
        "warmup_time": -1
    },
    "bi_benchmarks.BIBenchmarks.time_query_readall": {
        "code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n    \n        start_time = time.time()\n    \n        file = os.path.join(Path(__file__).resolve().parent.parent, BIBenchmarks.CITY_BI_FILE2)\n        if (not os.path.exists(file)) :\n            dfo = download_and_process_city_to_parquet(file)\n            dff = pd.read_parquet(file)\n            pd.testing.assert_frame_equal(dfo,dff)\n        else:\n            print(\"Parquet file exists!\")\n    \n        # read data from bz.2 file\n        # abs_path = os.path.join(Path(__file__).resolve().parent.parent,BIBenchmarks.CITY_BI_FILE)\n        # self.df : pd.DataFrame = process_city(abs_path)\n    \n        self.df : pd.DataFrame = pd.read_parquet(file)\n    \n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        print(\"The procedure is creating N times larger dataframes\")\n        print(\"by concatenating original DF N times\")\n        print(\"Size of original Dataframe: \", self.df.shape[0])\n        for num in BIBenchmarks.params:\n            _df = pd.concat([self.df] * num)\n            print(\"DF for iterration xSize original ready: \", num)\n            self.lib.write(f\"{self.symbol}{num}\", _df)\n    \n        print(\"If pandas query produces different dataframe than arctic one stop tests!\")\n        print(\"This will mean query problem is there most likely\")\n    \n        print(\"Pre-check correctness for query_groupby_city_count_all\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_all(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_all(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_isin_filter\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_isin_filter(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_isin_filter(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"Pre-check correctness for query_groupby_city_count_filter_two_aggregations\")\n        _df = self.df.copy(deep=True)\n        arctic_df = self.time_query_groupby_city_count_filter_two_aggregations(BIBenchmarks.params[0])\n        _df = get_query_groupby_city_count_filter_two_aggregations(_df)\n        assert_frame_equal(_df, arctic_df)\n    \n        print(\"All pre-checks completed SUCCESSFULLY. Time: \", time.time() - start_time)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_readall",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:72",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c746faf05e4dbb872efa770cbe5ae057dafe3ecc1fb8969d1026db2dee7bfd99",
        "warmup_time": -1
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_write_dataframe": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_create_write_dataframe(self, tpl, btype):\n        \"\"\"\n        This scenario includes creation of dataframe and then its serialization to storage\n        \"\"\"\n        df, dict = tpl\n        if btype == NO_OPERATION:\n            # What is the tool mem load?\n            return\n        df = pd.DataFrame(dict) # always create dataframe in this scenario\n        if btype == CREATE_DATAFRAME:\n            pass\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.path, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        logger.info(f\"Setup started\")\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n        del df, dict, tpl\n        gc.collect()\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {ComparisonBenchmarks.NUMBER_ROWS} rows generated for {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'"
            ]
        ],
        "setup_cache_key": "comparison_benchmarks:57",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "180ad4c706c8c62ee76e3b71f9fe99a51bc5d5504bfd0d971e6cdf68d3d32e6e"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == NO_OPERATION:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.path_to_read)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        logger.info(f\"Setup started\")\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n        del df, dict, tpl\n        gc.collect()\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {ComparisonBenchmarks.NUMBER_ROWS} rows generated for {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'"
            ]
        ],
        "setup_cache_key": "comparison_benchmarks:57",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8c154762a28156dc60b763b51d5e71ec8c3b79894928384e4bfd1ba12dbdffaa"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == NO_OPERATION:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.path, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        logger.info(f\"Setup started\")\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n        del df, dict, tpl\n        gc.collect()\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        st = time.time()\n        dict = self.create_dict(ComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {ComparisonBenchmarks.NUMBER_ROWS} rows generated for {time.time() - st}\")\n        ac = Arctic(ComparisonBenchmarks.URL)\n        ac.delete_library(ComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(ComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=ComparisonBenchmarks.SYMBOL, data=df)\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'"
            ]
        ],
        "setup_cache_key": "comparison_benchmarks:57",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "bf6621ce70605c68cc765d2e1e1be1377b72cf7071bc01dd3c1a2c85a8b86ef9"
    },
    "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        print(\">>> Library:\", self.lib)\n        print(\">>> Symbol:\", self.symbol)\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        cachedDF = cache\n    \n        # Unfortunately there is no way to tell asv to run single time\n        # each of finalize_stage_data() tests if we do the large setup in the\n        # setup_cache() method. We can only force it to work with single execution\n        # if the symbol setup with stage data is in the setup() method\n    \n        self.ac = Arctic(f\"lmdb://{self.lib_name}{param}?map_size=40GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, cachedDF.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = cachedDF.generate_dataframe_timestamp_indexed(200, 0, cachedDF.TIME_UNIT)\n        list_of_chunks = [10000] * param\n        self.symbol\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, cachedDF, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data types\n        cachedDF = CachedDFGenerator(350000, [5])\n        return cachedDF",
        "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:40",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9dcfdaf896125a0fe0d16b5538b5a8b556997064e107c8b58b93dc6e6f32d8b1"
    },
    "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        print(\">>> Library:\", self.lib)\n        print(\">>> Symbol:\", self.symbol)\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        cachedDF = cache\n    \n        # Unfortunately there is no way to tell asv to run single time\n        # each of finalize_stage_data() tests if we do the large setup in the\n        # setup_cache() method. We can only force it to work with single execution\n        # if the symbol setup with stage data is in the setup() method\n    \n        self.ac = Arctic(f\"lmdb://{self.lib_name}{param}?map_size=40GB\")\n        self.ac.delete_library(self.lib_name)\n        self.lib = self.ac.create_library(self.lib_name)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, cachedDF.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = cachedDF.generate_dataframe_timestamp_indexed(200, 0, cachedDF.TIME_UNIT)\n        list_of_chunks = [10000] * param\n        self.symbol\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, cachedDF, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data types\n        cachedDF = CachedDFGenerator(350000, [5])\n        return cachedDF",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:40",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "c3c02d1e2369dd420b2e241fc69c4c8872d31da89d0c19c1111d503a84fb9521",
        "warmup_time": -1
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().peakmem_finalize_staged_data(cache, param)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(cache, param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        return cachedDF",
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:90",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "90cde854b0e3346d50d63ab29182811b92cd7fae6c4ce0be4011a62c534e5e0f"
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().time_finalize_staged_data(cache, param)\n\n    def setup(self, cache: CachedDFGenerator, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(cache, param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        return cachedDF",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000",
                "2000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:90",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "a7673a8f559a07772f7a7a8e105774090534c7eb1b644b2d6247e7b792645809",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.peakmem_list_symbols": {
        "code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_symbols",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "efa8557e59868203fde3f8d2921698b505ae7a1ce7ff442b3e4c9bebc9ce2771"
    },
    "list_functions.ListFunctions.peakmem_list_versions": {
        "code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_versions",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "30457537b6ea77365ec0021b6f78a618dd0e990631d64cf0ae6b85baddca7081"
    },
    "list_functions.ListFunctions.time_has_symbol": {
        "code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_has_symbol",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "00a6aba7cd18f9fbbfa18c85961d58a03a291bfe32bf033e8d7b88c7b960da90",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.time_list_symbols": {
        "code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_symbols",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7457ceb57b7adfda687387a4599ff60b20ecb6ef556b80329ad2e8ec433fbb17",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.time_list_versions": {
        "code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_versions",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "cc2c68ce66d0087882fffcb8be554f525c3f314c8693a37897d37cc18373f1ff",
        "warmup_time": -1
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:40",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "55c5726e8be0eafb5adfab9f83954dc1fd727249fc74a1df3d27bed647ad1040"
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:40",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b7f4df120eb3ef915bcfd99cc75efe38896ffeafd5b6a8c3e5f3a8db67ccaad3"
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots()\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:40",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "706e811eef8155e3494d1a30ae8920e15ae32e0b69826eaa92388d620a5bb4ff",
        "warmup_time": -1
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:40",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "d4090a8af79a8cd45f87d87a807c4f344d86fa4e51e528809922b6a602ce06f9",
        "warmup_time": -1
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_without_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n    \n        self.create_test_library(True)\n        self.create_test_library(False)\n    \n        print(f\"Libraries generation took [{time.time() - start}]\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:40",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "71446e73206e086160b0733fdec4689d0c813c036e061f99d6f4ef372dd49b3b",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1f8093c32e1c5195eb0efb1004c228524cb54aa35d8c79359b17fc91597391a6"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fa0a87f2f2956453b825adcdb9e95be6a7e8887b2a66839923aa8a433e296e4e"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ce91e45ba6ec5f5dcd9499b423014b431774a7d81f07daa90d6c29cb8bc84d02"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4cad6e9389f20fc4a168893003dff16e0577770525b847e71e3b97f0f9f5ecdd"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "91dfe91e2fd6e9d562d89d8aee902dbb5c2380f3cd0a11eb85229cb375a7ea0b"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "94edfd985cb9746d21b85be1c91e97423797af2faa7a3343ad1c3aa7f9fa4536"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e047abda334f31dda20959739f2a3816f4dc96c130db00ebb75f5adcb9c14999"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "1fd26d5df8e3bd47278b0f1acca9528cc0dadba82788af6e3cfd1812058abef9",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a0f79b58b7744e63b2b7df3562f57094fa4ff3a111c172fbe0b03aec197afec8",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c7f842a915ebd3e278a9a5cea838835a804b463451ebec69829afe871adccfcc",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8972136efca70caee7530d031766c4653737a79d09b7c7badaaee274c1caa7da",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "17ef74af58c623de0ce47d10ad9d52ffc8a1b3c3bb2f57d1391dde34f4af4f29",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "509ffd471564124f5ea73eab19903e52e70eba728ea59b97ad6bd5b8544c2e60",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9a923014466d420b857d297f2a8a41983d03d0c3242559a8488a2a9a642440e1",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:62",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9d97dcd98574b9edb2038a9d43166c03fb90874813e5fac9c3a44b51194f3dd9",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:62",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b1364bf72e616201e384c0b7a9f18b03b078e22452929466a06b35fc64a91bd6",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:62",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8f27fb785c7b8b40220191dae6dbb120a49f55e011ae0f7cea6516a47e38c18a",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:62",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ed1d1ccb6458095a627788bfa2b53afa310ca8c8118a6405c91204724c865d6c",
        "warmup_time": -1
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_create_write_dataframe(self, tpl, btype):\n        \"\"\"\n        This scenario includes creation of dataframe and then its serialization to storage\n        \"\"\"\n        df, dict = tpl\n        if btype == NO_OPERATION:\n            # What is the tool mem load?\n            return\n        df = pd.DataFrame(dict) # always create dataframe in this scenario\n        if btype == CREATE_DATAFRAME:\n            pass\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib.write(self.s3_symbol, df)\n            pass\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df, dict = tpl\n        logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        # therefore we initialize the symbol here also not in setup_cache\n        self.pid = os.getpid()\n        self.s3_lib = self.storage().get_modifiable_library(self.pid)\n        self.s3_symbol = f\"symbol_{self.pid}\"\n        self.s3_lib.write(self.s3_symbol, df)\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        st = time.time()\n        dict = self.create_dict(RealComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {RealComparisonBenchmarks.NUMBER_ROWS} rows generated for {time.time() - st}\")\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=RealComparisonBenchmarks.SYMBOL, data=df)\n        self.storage().remove_all_modifiable_libraries(confirm=True)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:63",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5df6a39256850660b31f65f14d7ea80e1e11136ebaec4096560b34aeff011129"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == NO_OPERATION:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.parquet_to_read )\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib.read(self.s3_symbol)\n            pass\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df, dict = tpl\n        logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        # therefore we initialize the symbol here also not in setup_cache\n        self.pid = os.getpid()\n        self.s3_lib = self.storage().get_modifiable_library(self.pid)\n        self.s3_symbol = f\"symbol_{self.pid}\"\n        self.s3_lib.write(self.s3_symbol, df)\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        st = time.time()\n        dict = self.create_dict(RealComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {RealComparisonBenchmarks.NUMBER_ROWS} rows generated for {time.time() - st}\")\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=RealComparisonBenchmarks.SYMBOL, data=df)\n        self.storage().remove_all_modifiable_libraries(confirm=True)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:63",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4b8f413459623a5371039ecdc9eb94c1f4c7d158d76d5ed335fa32371be7a8e0"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == NO_OPERATION:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib.write(self.s3_symbol, df)\n            pass\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df, dict = tpl\n        logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        # therefore we initialize the symbol here also not in setup_cache\n        self.pid = os.getpid()\n        self.s3_lib = self.storage().get_modifiable_library(self.pid)\n        self.s3_symbol = f\"symbol_{self.pid}\"\n        self.s3_lib.write(self.s3_symbol, df)\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        st = time.time()\n        dict = self.create_dict(RealComparisonBenchmarks.NUMBER_ROWS)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {RealComparisonBenchmarks.NUMBER_ROWS} rows generated for {time.time() - st}\")\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=RealComparisonBenchmarks.SYMBOL, data=df)\n        self.storage().remove_all_modifiable_libraries(confirm=True)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:63",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "66d0401dd3c260a95f3f6f51e26874710d951da4d5d0f428bfdf3031d3b5a82c"
    },
    "real_list_operations.AWSListSymbols.peakmem_list_symbols": {
        "code": "class AWSListSymbols:\n    def peakmem_list_symbols(self, storage_info, num_syms):\n        self.lib.list_symbols()\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws_setup = AWSListSymbols.SETUP_CLASS\n        if aws_setup.check_ok():\n            aws_setup.clear_symbols_cache()\n        else:\n            aws_setup.setup_all()\n        info = aws_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_list_operations.AWSListSymbols.peakmem_list_symbols",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_list_operations:31",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a90567eacca263ebb46de7fdf55118d53eb6dc876e9255905d3a0b635b30fce9"
    },
    "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting": {
        "code": "class AWSListSymbols:\n    def time_has_symbol_nonexisting(self, storage_info, num_syms):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws_setup = AWSListSymbols.SETUP_CLASS\n        if aws_setup.check_ok():\n            aws_setup.clear_symbols_cache()\n        else:\n            aws_setup.setup_all()\n        info = aws_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:31",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4b41ddcf6145b06ad7f80701da1523cf2f2947883fc4b07d92cef1c91ae93894",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.time_list_symbols": {
        "code": "class AWSListSymbols:\n    def time_list_symbols(self, storage_info, num_syms):\n        self.lib.list_symbols()\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws_setup = AWSListSymbols.SETUP_CLASS\n        if aws_setup.check_ok():\n            aws_setup.clear_symbols_cache()\n        else:\n            aws_setup.setup_all()\n        info = aws_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_list_symbols",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:31",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "da598987df4d306401cb705b74e7ee9d9cf727ad60975249ac2b0f9b60b85200",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.time_list_symbols_first_snapshot": {
        "code": "class AWSListSymbols:\n    def time_list_symbols_first_snapshot(self, storage_info, num_syms):\n        self.lib.list_symbols(self.aws.first_snapshot)\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws_setup = AWSListSymbols.SETUP_CLASS\n        if aws_setup.check_ok():\n            aws_setup.clear_symbols_cache()\n        else:\n            aws_setup.setup_all()\n        info = aws_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_list_symbols_first_snapshot",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:31",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e1a8191d64d59b0eb5d62ed65b922e13c3d0c20fd36bd050b8e55e6651b54d02",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.time_list_symbols_last_snapshot": {
        "code": "class AWSListSymbols:\n    def time_list_symbols_last_snapshot(self, storage_info, num_syms):\n        self.lib.list_symbols(self.aws.last_snapshot)\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws_setup = AWSListSymbols.SETUP_CLASS\n        if aws_setup.check_ok():\n            aws_setup.clear_symbols_cache()\n        else:\n            aws_setup.setup_all()\n        info = aws_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_list_symbols_last_snapshot",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:31",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "de7a1a4e6d575955a3edf0b414363fe0a72fa8710c9be747278abfa7b7f9abab",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_versions": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_versions(self, storage_info, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws = AWSVersionSymbols.SETUP_CLASS.setup_environment()\n        info = aws.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_versions",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:81",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e7c9ebfe5bcb25a0b5ba9d46c334cb6aa4f24fcde1b3edb9a8702b05a6e9dbab"
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions(self, storage_info, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws = AWSVersionSymbols.SETUP_CLASS.setup_environment()\n        info = aws.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:81",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "75b00fdd3f8f7697a2eb5d3bc9eb7952fc16fc1fca1bcb956c4e010b4ce495e8",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only(self, storage_info, num_syms):\n        self.lib.list_versions(latest_only=True)\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws = AWSVersionSymbols.SETUP_CLASS.setup_environment()\n        info = aws.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:81",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "77bb76dacb5466520ad09dc5359902e13a6425155bd1fd0dd1eb250f394d0ddb",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_skip_snapshots(self, storage_info, num_syms):\n        self.lib.list_versions(skip_snapshots=self.aws.last_snapshot)\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws = AWSVersionSymbols.SETUP_CLASS.setup_environment()\n        info = aws.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:81",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3f8aa1851aab45ed29160759afa95b81d140340729dc2cbaf3b04dda5800b833",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_snapshot(self, storage_info, num_syms):\n        self.lib.list_versions(snapshot=self.aws.last_snapshot)\n\n    def setup(self, storage_info, num_syms):\n        self.aws = GeneralSetupSymbolsVersionsSnapshots.from_storage_info(storage_info)\n        self.lib = self.aws.get_library(num_syms)\n\n    def setup_cache(self):\n        aws = AWSVersionSymbols.SETUP_CLASS.setup_environment()\n        info = aws.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        aws.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:81",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2fc5d8c935ffb8a674f88c6645ca56370dfb61851afcdf0f80e60086979aba43",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.peakmem_read": {
        "code": "class LMDBReadWrite:\n    def peakmem_read(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "name": "real_read_write.AWSReadWrite.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e4db4eb9016252cc2bad76ba08ee7ecf9e3ebc6a778540f68267f3f0799aec66"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {
        "code": "class LMDBReadWrite:\n    def peakmem_read_with_column_float(self, storage_info, num_rows):\n        COLS = [\"float2\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3f48fb1fa12934bc8f29c840dfa8bde8340c6b78a6d7f4790a56d40052a73e50"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {
        "code": "class LMDBReadWrite:\n    def peakmem_read_with_columns_all_types(self, storage_info, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "22f393a2fd235c3782d3e90eba20bcec002731fbee7e200a310a10dd8837de78"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class LMDBReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, date_range=self.last_20).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0aa87cfa8e3c6995280d940769b6af445d5cc399c9b407d5359abf4bf7ffbfd9"
    },
    "real_read_write.AWSReadWrite.peakmem_write": {
        "code": "class LMDBReadWrite:\n    def peakmem_write(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.write_library.write(symbol=sym, data=self.to_write_df)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "name": "real_read_write.AWSReadWrite.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "832682a84bb4809a90ef66e36aa8c8b3afcbc4eeb65731e8f95e484defcbc098"
    },
    "real_read_write.AWSReadWrite.peakmem_write_staged": {
        "code": "class LMDBReadWrite:\n    def peakmem_write_staged(self, storage_info, num_rows):\n        lib = self.write_library\n        lib.write(f\"sym\", self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "name": "real_read_write.AWSReadWrite.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "51e7ae300716354dbd82cf1daad962041fe2e263c9834fff94e25be1204b2e8f"
    },
    "real_read_write.AWSReadWrite.time_read": {
        "code": "class LMDBReadWrite:\n    def time_read(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3aa4fbc05600d7c25e5d0707b9ffdd13dcb3f97b71a560a859635848204dd116",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_column_float": {
        "code": "class LMDBReadWrite:\n    def time_read_with_column_float(self, storage_info, num_rows):\n        COLS = [\"float2\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b30d52fba59959a63ce616f01bed6dd7f1667a8f1552de38f991f4c5362a012a",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {
        "code": "class LMDBReadWrite:\n    def time_read_with_columns_all_types(self, storage_info, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a8062e1b6951296e53217ad888369b5ac1db202bd6a057005a17cb7925ef8ee8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class LMDBReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, date_range=self.last_20).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4b05cadb89b90c76b6714f9ac5dca703d6f5ce40312d5471ce997979c74077f7",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write": {
        "code": "class LMDBReadWrite:\n    def time_write(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.write_library.write(symbol=sym, data=self.to_write_df)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "cd1e2d7c3e9482942693e00c912767a6ec944994930d18451574aafbbaeaca68",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write_staged": {
        "code": "class LMDBReadWrite:\n    def time_write_staged(self, storage_info, num_rows):\n        lib = self.write_library\n        lib.write(f\"sym\", self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\nclass AWSReadWrite:\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        aws_setup = AWSReadWrite.SETUP_CLASS.setup_environment()\n        return aws_setup.get_storage_info()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:204",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "572f489277a78ff2cb5700ed79959ca08b9e4540c5caf87f8be00eb880504477",
        "warmup_time": 0
    },
    "real_read_write.LMDBReadWrite.peakmem_read": {
        "code": "class LMDBReadWrite:\n    def peakmem_read(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write.LMDBReadWrite.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7d6c057be0bf944061400232400ea13c8bad84d221601873273433e344248ca6"
    },
    "real_read_write.LMDBReadWrite.peakmem_read_with_column_float": {
        "code": "class LMDBReadWrite:\n    def peakmem_read_with_column_float(self, storage_info, num_rows):\n        COLS = [\"float2\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write.LMDBReadWrite.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f99243ecc747828b4f4b17b4abb95f17b741121c07bc2fa3e1e840a5d18653ec"
    },
    "real_read_write.LMDBReadWrite.peakmem_read_with_columns_all_types": {
        "code": "class LMDBReadWrite:\n    def peakmem_read_with_columns_all_types(self, storage_info, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write.LMDBReadWrite.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5675be19d4dd752dfa5a2ad72eb7036e82860c98a095e0c305fe54ed9dafec1a"
    },
    "real_read_write.LMDBReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class LMDBReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, date_range=self.last_20).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write.LMDBReadWrite.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae103b5d2d38787508ab3087f4d5e8a868c6e7e59ffbb5d5f06cab1c969a504f"
    },
    "real_read_write.LMDBReadWrite.peakmem_write": {
        "code": "class LMDBReadWrite:\n    def peakmem_write(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.write_library.write(symbol=sym, data=self.to_write_df)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write.LMDBReadWrite.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "66ddb1280ee577b514456f53c652743a72741648800ac19b17f4ed09e92d8beb"
    },
    "real_read_write.LMDBReadWrite.peakmem_write_staged": {
        "code": "class LMDBReadWrite:\n    def peakmem_write_staged(self, storage_info, num_rows):\n        lib = self.write_library\n        lib.write(f\"sym\", self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write.LMDBReadWrite.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "338750dbe353d919ca3d64bed0bac540bdfa97f0c58b4a1fc4f2dfe2d3343b4e"
    },
    "real_read_write.LMDBReadWrite.time_read": {
        "code": "class LMDBReadWrite:\n    def time_read(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write.LMDBReadWrite.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "1e4f456ed691b5c4e3265657521c880272b373d7decc33de53d3446d2db13121",
        "warmup_time": 0
    },
    "real_read_write.LMDBReadWrite.time_read_with_column_float": {
        "code": "class LMDBReadWrite:\n    def time_read_with_column_float(self, storage_info, num_rows):\n        COLS = [\"float2\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write.LMDBReadWrite.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "bbc6654cb1a66659ef92f6bb22eed20400f7fe6f7b6a3c453295a5b1521542b6",
        "warmup_time": 0
    },
    "real_read_write.LMDBReadWrite.time_read_with_columns_all_types": {
        "code": "class LMDBReadWrite:\n    def time_read_with_columns_all_types(self, storage_info, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, columns=COLS).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write.LMDBReadWrite.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0da03af5ca996c4c871384bc8359dc1f36c867343d8cc68c7b84e79f5587a50c",
        "warmup_time": 0
    },
    "real_read_write.LMDBReadWrite.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class LMDBReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.setup_env.get_library().read(symbol=sym, date_range=self.last_20).data\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write.LMDBReadWrite.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b0651fc8e0ae03cad012f3f7935e04530d18dc54a9693323b51946c8cac48302",
        "warmup_time": 0
    },
    "real_read_write.LMDBReadWrite.time_write": {
        "code": "class LMDBReadWrite:\n    def time_write(self, storage_info, num_rows):\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.write_library.write(symbol=sym, data=self.to_write_df)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write.LMDBReadWrite.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "de9263de50fdb6af214f9b0a81b3d28e7f552cdd88420ba45fcc6644664a19e5",
        "warmup_time": 0
    },
    "real_read_write.LMDBReadWrite.time_write_staged": {
        "code": "class LMDBReadWrite:\n    def time_write_staged(self, storage_info, num_rows):\n        lib = self.write_library\n        lib.write(f\"sym\", self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, storage_info, num_rows):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.setup_env: ReadWriteBenchmarkSettings = ReadWriteBenchmarkSettings.from_storage_info(storage_info)\n        sym = self.setup_env.get_symbol_name(num_rows, None)\n        self.to_write_df = self.setup_env.get_library().read(symbol=sym).data\n        self.last_20 = self.setup_env.get_last_x_percent_date_range(num_rows, 20)\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.setup_env.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardowns\n        '''\n        lmdb_setup = LMDBReadWrite.SETUP_CLASS.setup_environment()\n        info = lmdb_setup.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        lmdb_setup.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write.LMDBReadWrite.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500000",
                "5000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:97",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8b0865d24ccea511ec9db954dea35797a30d467065bebc0dd74789e0ef70b9e4",
        "warmup_time": 0
    },
    "real_read_write_wide.AWSWideDataFrameTests.peakmem_read_wide": {
        "code": "class AWSWideDataFrameTests:\n    def peakmem_read_wide(self, storage_info, num_rows, num_cols):\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.storage.get_library().read(symbol=sym)\n\n    def setup(self, storage_info, num_rows, num_cols):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.storage = GeneralSetupLibraryWithSymbols.from_storage_info(storage_info)\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.to_write_df = self.storage.get_library().read(symbol=sym).data\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.storage.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardown\n        '''\n        setup_env = AWSWideDataFrameTests.SETUP_CLASS.setup_environment()\n        info = setup_env.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        setup_env.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write_wide.AWSWideDataFrameTests.peakmem_read_wide",
        "param_names": [
            "num_rows",
            "num_cols"
        ],
        "params": [
            [
                "2500",
                "3000"
            ],
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write_wide:51",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8766de18cf0cc5aa01c0365d6a846b92f02c80650d4a5db0ada3ce92ed44a57d"
    },
    "real_read_write_wide.AWSWideDataFrameTests.peakmem_write_wide": {
        "code": "class AWSWideDataFrameTests:\n    def peakmem_write_wide(self, storage_info, num_rows, num_cols):\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.write_library.write(symbol=sym, data=self.to_write_df)\n\n    def setup(self, storage_info, num_rows, num_cols):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.storage = GeneralSetupLibraryWithSymbols.from_storage_info(storage_info)\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.to_write_df = self.storage.get_library().read(symbol=sym).data\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.storage.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardown\n        '''\n        setup_env = AWSWideDataFrameTests.SETUP_CLASS.setup_environment()\n        info = setup_env.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        setup_env.logger().info(f\"storage info object: {info}\")\n        return info",
        "name": "real_read_write_wide.AWSWideDataFrameTests.peakmem_write_wide",
        "param_names": [
            "num_rows",
            "num_cols"
        ],
        "params": [
            [
                "2500",
                "3000"
            ],
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write_wide:51",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3726fcc9e3e1a536d6101f2e669c4929ab8ce7a44dc430a565496fd809032f2c"
    },
    "real_read_write_wide.AWSWideDataFrameTests.time_read_wide": {
        "code": "class AWSWideDataFrameTests:\n    def time_read_wide(self, storage_info, num_rows, num_cols):\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.storage.get_library().read(symbol=sym)\n\n    def setup(self, storage_info, num_rows, num_cols):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.storage = GeneralSetupLibraryWithSymbols.from_storage_info(storage_info)\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.to_write_df = self.storage.get_library().read(symbol=sym).data\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.storage.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardown\n        '''\n        setup_env = AWSWideDataFrameTests.SETUP_CLASS.setup_environment()\n        info = setup_env.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        setup_env.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write_wide.AWSWideDataFrameTests.time_read_wide",
        "number": 3,
        "param_names": [
            "num_rows",
            "num_cols"
        ],
        "params": [
            [
                "2500",
                "3000"
            ],
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write_wide:51",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "31f6dcdcaa4c32f8c80df9c9bb9351bce53cd20535e8b8eaeb7ac8e740206171",
        "warmup_time": 0
    },
    "real_read_write_wide.AWSWideDataFrameTests.time_write_wide": {
        "code": "class AWSWideDataFrameTests:\n    def time_write_wide(self, storage_info, num_rows, num_cols):\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.write_library.write(symbol=sym, data=self.to_write_df)\n\n    def setup(self, storage_info, num_rows, num_cols):\n        '''\n        This setup method for read and writes can be executed only once\n        No need to be executed before each test. That is why we define\n        `repeat` as 1\n        '''\n        ## Construct back from arctic url the object\n        self.storage = GeneralSetupLibraryWithSymbols.from_storage_info(storage_info)\n        sym = self.storage.get_symbol_name(num_rows, num_cols)\n        self.to_write_df = self.storage.get_library().read(symbol=sym).data\n        ##\n        ## Writing into library that has suffix same as process\n        ## will protect ASV processes from writing on one and same symbol\n        ## this way each one is going to have its unique library\n        self.write_library = self.storage.get_modifiable_library(os.getpid())\n\n    def setup_cache(self):\n        '''\n        Always provide implementation of setup_cache in\n        the child class\n    \n        And always return storage info which should\n        be first parameter for setup, tests and teardown\n        '''\n        setup_env = AWSWideDataFrameTests.SETUP_CLASS.setup_environment()\n        info = setup_env.get_storage_info()\n        # NOTE: use only logger defined by setup class\n        setup_env.logger().info(f\"storage info object: {info}\")\n        return info",
        "min_run_count": 1,
        "name": "real_read_write_wide.AWSWideDataFrameTests.time_write_wide",
        "number": 3,
        "param_names": [
            "num_rows",
            "num_cols"
        ],
        "params": [
            [
                "2500",
                "3000"
            ],
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write_wide:51",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4506dee1fcb6a7cd973a259fabd00503773c550c962319d2305b7b9a135372b8",
        "warmup_time": 0
    },
    "resample.Resample.peakmem_resample": {
        "code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)",
        "name": "resample.Resample.peakmem_resample",
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "setup_cache_key": "resample:37",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "760c9d62e17a5467f1e93abb258d89057e8fdf9ee67d98ceb376e731157a4d2e"
    },
    "resample.Resample.time_resample": {
        "code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)",
        "min_run_count": 2,
        "name": "resample.Resample.time_resample",
        "number": 5,
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:37",
        "type": "time",
        "unit": "seconds",
        "version": "1381d2db90e66cb5cd04febf62398827a3ac9928795eaced908daec35d5c0c31",
        "warmup_time": -1
    },
    "resample.ResampleWide.peakmem_resample_wide": {
        "code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "name": "resample.ResampleWide.peakmem_resample_wide",
        "param_names": [],
        "params": [],
        "setup_cache_key": "resample:103",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"
    },
    "resample.ResampleWide.time_resample_wide": {
        "code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "min_run_count": 2,
        "name": "resample.ResampleWide.time_resample_wide",
        "number": 5,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:103",
        "type": "time",
        "unit": "seconds",
        "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2",
        "warmup_time": -1
    },
    "sample.SamplePeakmem.peakmem_pass_only": {
        "code": "class SamplePeakmem:\n    def peakmem_pass_only(self, arr):\n    #def peakmem_pass_only(self):\n        pass\n\n    def setup(self, arr):\n    #def setup(self):\n        logger.info(f\"Setup started\")\n        self.arr = [100.0] * (1024 * 1024)\n        logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger.info(f\"Setup CACHE start\")\n        arr = [100.0] * (1024 * 1024)\n        arr1 = [100.0] * (1024 * 10024)\n        pass\n        return (arr, arr1)",
        "name": "sample.SamplePeakmem.peakmem_pass_only",
        "param_names": [],
        "params": [],
        "setup_cache_key": "sample:43",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ccf4cfa51a3b1bac0a76030ab726f2f4eb86426780df5a760de302293f653e71"
    },
    "version": 2,
    "version_chain.IterateVersionChain.time_list_undeleted_versions": {
        "code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_list_undeleted_versions",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:36",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "6bdd43d7f191d2bbbd30ef740909969e25cbe1cec77f1755c5c3ba58a77f2b88",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_load_all_versions": {
        "code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_load_all_versions",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:36",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c40fe3123db9e5d6fdf5f35caecaf42d266328deb78c237e293096ae3a4bcf98",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_alternating": {
        "code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_alternating",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:36",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ec1a61c37c4cc7317cfafe554f3eeb7fe2a426068ec412c1d7c6b78f510f6c45",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_from_epoch": {
        "code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_from_epoch",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:36",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "5c6aace0b39c7a75f064a61c182cbbb42a35f0e0ee46546579bc641e68dc954a",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_v0": {
        "code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_v0",
        "number": 10,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:36",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4bf693e490128c1cff7500c93799432e7bf150925d3714757219604aa7fa5e9c",
        "warmup_time": -1
    }
}