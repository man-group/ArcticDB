{
    "arrow.ArrowReadNumeric.peakmem_read": {
        "code": "class ArrowReadNumeric:\n    def peakmem_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name)\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowReadNumeric.peakmem_read",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "100000",
                "100000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:35",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "401a0e348c8f3688dc1c38a277ab98b1f06fcc80ade1aa6932ae88695a26de8c"
    },
    "arrow.ArrowReadNumeric.time_read": {
        "code": "class ArrowReadNumeric:\n    def time_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name)\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowReadNumeric.time_read",
        "number": 5,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "100000",
                "100000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:35",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "d02fbb19f932977919ddecc359f8e4af3de9730c7c6b7314beb4b33d0a3b4587",
        "warmup_time": 0
    },
    "arrow.ArrowReadStrings.peakmem_read": {
        "code": "class ArrowReadStrings:\n    def peakmem_read(self, rows, date_range, unique_string_count):\n        self.lib.read(self.symbol_name(rows, unique_string_count), date_range=self.date_range)\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name)\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowReadStrings.peakmem_read",
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count"
        ],
        "params": [
            [
                "10000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "1",
                "100",
                "100000"
            ]
        ],
        "setup_cache_key": "arrow:91",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e4507030a33067ee8dc5e23a40f2ef36232ac8e8cf42fc70b77bdd0ca16c72ea"
    },
    "arrow.ArrowReadStrings.time_read": {
        "code": "class ArrowReadStrings:\n    def time_read(self, rows, date_range, unique_string_count):\n        self.lib.read(self.symbol_name(rows, unique_string_count), date_range=self.date_range)\n\n    def setup(self, rows, date_range, unique_string_count):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.EXPERIMENTAL_ARROW)\n        self.lib = self.ac.get_library(self.lib_name)\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowReadStrings.time_read",
        "number": 5,
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count"
        ],
        "params": [
            [
                "10000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "1",
                "100",
                "100000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:91",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4ff5c086861a792b7bc9591bbe8d74f6c29d420929db8f89fa6781dbb508b349",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3b847508b2a63c58c508dc7e7aec08547e2ca09a57bcf3f6777619a5cb149b7f"
    },
    "basic_functions.BasicFunctions.peakmem_read_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a4ca2ebef508c6560b98af94d69ba7cd459cc02976eaed2aad41ac976d932b11"
    },
    "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_ultra_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "74f0d776147883a788c405dd90a8490ea4b766f2946aaf5ab43cc48076c2b929"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "34501bd0311c8b2644012b8f713f8a596bba14dbf1976cbbded37060cf77709f"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "908e8e1658fc2235b08f2996edb57c49852791e6f6c187165b3b931e7d7e896c"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7870e015641ad2975bbc58ad43428c0e65d3771d1cd2838d2f72f4d6c57aa926"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0db13340295d95fa06e3f786be93bbc345a0f67ffe4fdcd226189c2b82aecb5e"
    },
    "basic_functions.BasicFunctions.peakmem_write_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write_short_wide",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3128ed9e1a62c74b07c0c51e215080ecb9ccb8284e6221dadb0d4b229acceb2d"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "42c10f3b6072be6b54db49e98670de0ce65dc2d7e543f6b1af53bd4fd28bba5f"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "99231807c0927256747827b6d0d3bf8d565cae9f2b6955d40e3c403ff162daac",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "18cb78627bc67fc910cdce20943fb32ba17bc8271a1b13de73393d76c1411f9e",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_ultra_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_ultra_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.ULTRA_SHORT_WIDE_DF_ROWS)]\n        lib.read(\"ultra_short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_ultra_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f5a362303294a64862824aa956d62c05bac490feb7438d742720d341274beeb7",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "97fab62dd3036b086e61d8e193b02c3c566b76c76cb514b44dfd3d9090a2bbe7",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8fe0b03e88febe68a480effc73ec81b1c3bbb24fa31f8d4868db79760d58ccdd",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(BasicFunctions.DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "50281bd8fb1b296b9de072d72b5bb207d9b5d1700949b1d911f7475339110289",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a399d5220450caaa75806e22756b1bfcf8b27c050fa15dd9165be085ae2b0b63",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write_short_wide": {
        "code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_short_wide",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7d9de7789dd1f63d916bd3e86d00c633177462a29f323774fd022cebf75e92ab",
        "warmup_time": 0
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 5,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:48",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "00d21f9de9837bdad48c5ad99e7c3312e1a2825b119b58548d7fd703481d9501",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1154d4d002a16465c10fd5e41721ed25ee8d4a5fa3790c7718d7f309f1b8b29c"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "eb2f74f4dab10d4472f2349a2d539c9609baaaa6debf7206c064a2b93c495bfc"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "114711c1c4ebf66c64ab33fb81f5f8cc73ce78984e2472f492d0c80f96f331d8"
    },
    "basic_functions.BatchBasicFunctions.peakmem_update_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_update_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "11bde9a31b891ee985296085ae2b9f7805556601060b2e993f257d8ed4a2144d"
    },
    "basic_functions.BatchBasicFunctions.peakmem_write_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e29d7bab3ecd450da657f11c448f62bbb39d5a2607fb6f4cc5b92db2dee50dc9"
    },
    "basic_functions.BatchBasicFunctions.time_read_batch": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "95a87e9c330d6f1b83b13cdf87403e69a4c2d4a3992430942454acfd5021b73a",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_pure": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9a6a0b018e486ac4f51dcd497fd51d6394c9b92418337aab1e6bbc5347b3674c",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2c798cded56db601f7a5e81cf0d0e77407d3717695a4c4863d4e68c96618393b",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "04e761907c8cd5b7d21ef3beadec4150292a508c0de7c9fd36305f118c97ceb3",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_update_batch": {
        "code": "class BatchBasicFunctions:\n    def time_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_update_batch",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ae298ed42a2bc58a2d14509b1578a2ecd7c11cab6f15200ef8ebfbc4f9924a27",
        "warmup_time": 0
    },
    "basic_functions.BatchBasicFunctions.time_write_batch": {
        "code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_write_batch",
        "number": 1,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:181",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "49522447c3d9ac6f75f9df9a159dbbaeb95553440cf4bbb98303fce0c490bd66",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_large": {
        "code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_large",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8d1e9a72db76b8a1a5e0215330c54476f80a05dbea2174186950370cd831245e",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_short_wide": {
        "code": "class ModificationFunctions:\n    def time_append_short_wide(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_short_wide[rows].pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b15a42ff2e9792de5c9bee4cbf108d57071a6c2b2b504997406e7a848b83b0dc",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_append_single": {
        "code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_append_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "52acfbd4409bdcadf8af0cf5bd559122d5d467b56650bbb381c2d332a1aece2e",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete": {
        "code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "672259b2998d092f121f5de4c7b4327ebe5c3e444b677994c276755dc72bcffa",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_multiple_versions": {
        "code": "class ModificationFunctions:\n    def time_delete_multiple_versions(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym_delete_multiple\", list(range(99)))\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_multiple_versions",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2f548b89ece0762d3a4ca804f4cebacdc407c72b720b63927a70a48213bd7b95",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_over_time": {
        "code": "class ModificationFunctions:\n    def time_delete_over_time(self, lad: LargeAppendDataModify, rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_over_time",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "77d3d9f015b1ff0e318bd419a22aeca9b56c22cd174c0efd80e2909b9b68ceb5",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_delete_short_wide": {
        "code": "class ModificationFunctions:\n    def time_delete_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_delete_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2409d72e307e71748c55e973bf651e1c775a5ed31869e089c243223efbb83df3",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_half": {
        "code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_half",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7bde302f3062e94d0fe774eeebb7ad10585c4e40e525dbd3e05b25c0a7798ce1",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_short_wide": {
        "code": "class ModificationFunctions:\n    def time_update_short_wide(self, lad: LargeAppendDataModify, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_short_wide",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c012d3639c8e84e4a2e654631211e110644fd75bf36de8576515355a72d9409c",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_single": {
        "code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "d0b4f92fc686ee91643527d18268011dbe2053d2f5463efb671d28d1fe2e388a",
        "warmup_time": 0
    },
    "basic_functions.ModificationFunctions.time_update_upsert": {
        "code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n    \n        # Used by time_delete_multiple_versions\n        for i in range(100):\n            self.lib.write(\"sym_delete_multiple\", pd.DataFrame(), prune_previous_versions=False)\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "basic_functions.ModificationFunctions.time_update_upsert",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "1500000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:339",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "916a69cac80c70e2fe053b743904459170731ac657c2efbe0222daafcdd9ba9e",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "331c9abb4f84b01dd28765b77a88e069ec6d6b70617a12dd5aa9c3e14ca6a6ad"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae73602827d4d1b739e519e8ca6a847c5938a5744ebf371ca78511b0be1bf16f"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e708975c50d2b70ebdff11efa45f9fd15ceee9861301d5552f1e8ebe2cb4d1bd"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {
        "code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3407eb183cf3c02afbeaf04e6c31bf6b5aaf615458cd8e2ad46a21b4d2af80e2"
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "bf5e390b01e356685500d464be897fe7cb51531dcd92fccedec980f97f361e3c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ceeb3b3e6049c66cb2ecabbb16485e4555cefc7920697c7a34de08993be14af0",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "fa74284d1e48fd396138a5f50c53d92829194b7be1f0caa8f441f8820db4157c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_readall": {
        "code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_readall",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:74",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "3cd2c7d90725498da459157638eb15b5a3fcc68aa91684951717ed5ab1c8ca63",
        "warmup_time": 0
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_create_dataframe(self, tpl):\n        df, dict = tpl\n        df = pd.DataFrame(dict)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_create_dataframe",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3580ee58cdf2db23481a370c46cd60e321e4c5b6b763d8a56a905cd857b05b65"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_arctic(self, tpl):\n        self.lib.read(ComparisonBenchmarks.SYMBOL)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_arctic",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f13bf3d2bdd1f6129281823f3ac4a1c6c7d7b97be21efe3ae0d1065b07f89050"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_read_dataframe_parquet(self, tpl):\n        pd.read_parquet(self.path_to_read)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_read_dataframe_parquet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7d5db25fdc879f9e0da495f2f228353425dd662cc2ad93eecf662c2dd9be1c55"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_arctic(self, tpl):\n        df, dict = tpl\n        self.lib.write(\"symbol\", df)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_arctic",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cd90a2c0309608e8c86b47a91b2a4c26ba9ba6b7bc50ca115f0d62b4ab82d064"
    },
    "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet": {
        "code": "class ComparisonBenchmarks:\n    def peakmem_write_dataframe_parquet(self, tpl):\n        df, dict = tpl\n        df.to_parquet(self.path, index=True)\n\n    def setup(self, tpl):\n        df, dict = tpl\n        self.ac = Arctic(ComparisonBenchmarks.URL)\n        self.lib = self.ac[ComparisonBenchmarks.LIB_NAME]\n        self.path = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.path_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.path)\n        df.to_parquet(self.path_to_read, index=True)\n\n    def setup_cache(self):\n        start = time.time()\n        df, dict = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return (df, dict)",
        "name": "comparison_benchmarks.ComparisonBenchmarks.peakmem_write_dataframe_parquet",
        "param_names": [],
        "params": [],
        "setup_cache_key": "comparison_benchmarks:44",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "166a93976c1ae0088bbb2477ee1d3e92672e7477a811b64747ee2b97bb078f26"
    },
    "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, param: int):\n        self.logger.info(f\"LIBRARY: {self.lib}\")\n        self.logger.info(f\"Created Symbol: {self.symbol}\")\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, param: int):\n        self.ac = Arctic(FinalizeStagedData.CONNECTION_STRING)\n        self.lib = self.ac.get_library(self.lib_name)\n        self.symbol = f\"symbol{param}\"\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache(CachedDFGenerator(350000, [5]))\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:47",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b49303c7a22a40468340ba6846719d83029e19a7355bb1d0e330142132a132b3"
    },
    "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, param: int):\n        self.logger.info(f\"LIBRARY: {self.lib}\")\n        self.logger.info(f\"Created Symbol: {self.symbol}\")\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, param: int):\n        self.ac = Arctic(FinalizeStagedData.CONNECTION_STRING)\n        self.lib = self.ac.get_library(self.lib_name)\n        self.symbol = f\"symbol{param}\"\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache(CachedDFGenerator(350000, [5]))\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "repeat": 5,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:47",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "b083f4862ad8234395f2d583c00eca4ee85bd17a2142e225946d02fbebbb849c",
        "warmup_time": 0
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def peakmem_finalize_staged_data(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().peakmem_finalize_staged_data(param)\n\n    def setup(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        if not SLOW_TESTS:\n            return #Avoid setup when skipping\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        start = time.time()\n        self._setup_cache(cachedDF)\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.peakmem_finalize_staged_data",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:103",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d5455e784a299c90ae432d9871ef498f6a227a5fba2fd0657fbd3ee095170560"
    },
    "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data": {
        "code": "class FinalizeStagedDataWiderDataframeX3:\n    def time_finalize_staged_data(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().time_finalize_staged_data(param)\n\n    def setup(self, param: int):\n        if not SLOW_TESTS:\n            raise SkipNotImplemented(\"Slow tests are skipped\")\n        super().setup(param)\n\n    def setup_cache(self):\n        # Generating dataframe with all kind of supported data type\n        if not SLOW_TESTS:\n            return #Avoid setup when skipping\n        cachedDF = CachedDFGenerator(\n            350000, [5, 25, 50]\n        )  # 3 times wider DF with bigger string columns\n        start = time.time()\n        self._setup_cache(cachedDF)\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedDataWiderDataframeX3.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "repeat": 5,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:103",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "d9b33220bbcc9d5d991bb7a81142342743f0a04cbf0612ec3c22896feb296974",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.peakmem_list_symbols": {
        "code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_functions.ListFunctions.peakmem_list_symbols",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "36bbe8ba3ff6df423837100203b3182d32efce65a17df876ac1369d05d7523fc"
    },
    "list_functions.ListFunctions.peakmem_list_versions": {
        "code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_functions.ListFunctions.peakmem_list_versions",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4d771cfea089c1fb8c4b9b8331803fa8aeca814cc8c2d0558485705521bb184b"
    },
    "list_functions.ListFunctions.time_has_symbol": {
        "code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_has_symbol",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "033af6c71329e0162ae9ea485b64a83dba030888400a9e1d0c5c33d686bd0880",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.time_list_symbols": {
        "code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_symbols",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "272c3535600411e60dd72ff988009bbc491bca183158c04d2aa82748e95bfc33",
        "warmup_time": 0
    },
    "list_functions.ListFunctions.time_list_versions": {
        "code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_versions",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:29",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2c963b861564347f6311befd6189eb42e4b882c06fb468060e38b9cda65cd12b",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_no_metadata_list",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "2fe5cfa52bf33e64fd73e89643e27d8f47d02159488e14d0359cacfa93e63e4a"
    },
    "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def peakmem_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "list_snapshots.SnaphotFunctions.peakmem_snapshots_with_metadata_list_with_load_meta",
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae3140e42a763dd7a75cc5a8df60881378dec6ce6708ac127ccc8dcf367626c0"
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_no_metadata_list(self, symbols_x_snaps_per_sym):\n        list = self.lib_no_meta.list_snapshots()\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_no_metadata_list",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8350189525f2502cc7949369b1dff78862d303da0fa2d8e27a523411f3a501bc",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_with_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=True)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_with_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "eb27c5627ff775cc2a8f0f8841f5b28b79f0ee10c3420e77eca1f8ca4582bd0e",
        "warmup_time": 0
    },
    "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta": {
        "code": "class SnaphotFunctions:\n    def time_snapshots_with_metadata_list_without_load_meta(self, symbols_x_snaps_per_sym):\n        list = self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, symbols_x_snaps_per_sym):\n        num_symbols = self.get_symbols(symbols_x_snaps_per_sym)\n        self.ac = Arctic(SnaphotFunctions.ARCTIC_URL)\n        self.lib = self.ac[self.get_lib_name(num_symbols, True)]\n        self.lib_no_meta = self.ac[self.get_lib_name(num_symbols, False)]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "list_snapshots.SnaphotFunctions.time_snapshots_with_metadata_list_without_load_meta",
        "number": 5,
        "param_names": [
            "symbols_x_snaps_per_sym"
        ],
        "params": [
            [
                "'20x10'",
                "'40x20'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:46",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "68900e75770793f36f0f940ebab65e148dee97f5c8cebe43c4ebd3ba5350dfbf",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ea9562ac454e814117fbe50a9b5d235a37110c1745fa24bd71db420baaf072ef"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "69f411de823212396215fb4912da15f56a16ca931fd2cfcf9533204b56e696ba"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ecadcf0caca3e2382953be630263bde2c01f7ac011ac2a140080e00f3a6ebdad"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cd35c81c46cd4eb64ccab7a6099884a9f99832a5ece8bb0bc387c0ed402f1536"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1806b540ceeb66fb0bc4476c5b07fca6d5d5edb13e441fb4a2471751a99ff7cb"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "923c4661c84571a4abb8cf5d774b2e0ec674bf671b3b11bc26ed561a0c417ef7"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0af435e83cbba85506f4bbf0fe355dd839b3e55fd81aa7e3600fcb443dc682ee"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2ac4df4d7d9b744d192742ba5b8c00c2f79a143cd72c4a7b7f63785dea19e219",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e16e60780a624d94310ad7e6059e97827feb6b4b6bc2d757a1e89f67c5e7ddd5",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, num_rows):\n        pattern = f\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "104a945e32f971e4b887da7b6cc6e6e728fc78ef6a77c2f6aed0693c24113234",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f2bd34ad9342852559d34262fc059be4c6f3122909b31ee078847b1a1b93907f",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "af4473b7d93a3c30d6cfc9d8bcde05e126a1df7948b5877c1604f2882a037768",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "9d0961c0292eff90c651a1989e525c7ce5ab63baa888ac4e2ccdb88b89cc9f2e",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e32d4b66ff29029ccf51f51511afe9792412ba38d2422d44b03b7e7e8710e38b",
        "warmup_time": 0
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:33",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2b556ee72fdb8b5b25072c8e0df67fb3264fc3f2fa724c453a6522ef98392f93",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5f940b32e17b1e08e0e79df3ddb81fd60d6217ec6274ee308a3cccf5a90cc72f"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e5f752bdf60df192471e9f0a0bb7ee74f3582679fd461247cda321614ecfc952"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_date_range)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] > 2\n        assert read_batch_result[-1].data.shape[0] > 2\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0bc60840654ec805851574b1f8ef76987cbd0ac99806d08abf47e7a5c415fd4c"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert write_batch_result[0].symbol in self.symbols\n        assert write_batch_result[-1].symbol in self.symbols\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e3de99f1307e75a7b5fddd8d7f3e4fba1975fda0f0f196603caa638b4cb3569f"
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "919b109aa3f63f22826be5a5f1255dcd06e284fac7035d1d2b8446ef182d4f3f",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "c4f2b10ea3bae71c069942dc0b9ed61b161076d0c9ed9e9a3eabdd56aa720675",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        self.lib.read_batch(self.read_reqs_date_range)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "21629b37558919e369c6b23aab6179b57f09fc95247b5819b07cac4a46ce608c",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        #Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        #Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        #Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        #Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_write_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:53",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "7dc9bf11079cd6affdcf109a9d3f2ea057c2f52f63593fd9260004930ac7a6e6",
        "warmup_time": 0
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_create_then_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6d29e295671e2ea22e0d30278ef69d9d7740a8e9c7dd806efa5d6144cc3647da"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_read_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # measures base memory which need to be deducted from\n            # any measurements with actual operations\n            # see discussion above\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            pd.read_parquet(self.parquet_to_read )\n        elif btype == ARCTICDB_LMDB:\n            self.lib.read(self.SYMBOL)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_read.read(self.s3_symbol)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_read_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "659f77bdc21378d3f9210e0f791c2cea56ca30f7a06644ec82d3d8cad9964efe"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def peakmem_write_dataframe(self, tpl, btype):\n        df, dict = tpl\n        if btype == BASE_MEMORY:\n            # What is the tool mem load?\n            return\n        if btype == CREATE_DATAFRAME:\n            df = pd.DataFrame(dict)\n        elif btype == PANDAS_PARQUET:\n            df.to_parquet(self.parquet_to_write, index=True)\n        elif btype == ARCTICDB_LMDB:\n            self.lib.write(\"symbol\", df)\n        elif btype == ARCTICDB_AMAZON_S3:\n            self.s3_lib_write.write(self.s3_symbol, df)\n        else:\n            raise Exception(f\"Unsupported type: {btype}\")\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.peakmem_write_dataframe",
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6e7f2fc17ff2e3550a927ac3659eae946384ea1f88bfb66049eae9f7969b6f02"
    },
    "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe": {
        "code": "class RealComparisonBenchmarks:\n    def time_create_then_write_dataframe(self, tpl, btype):\n        self.create_then_write_dataframe(tpl, btype)\n\n    def setup(self, tpl, btype):\n        df : pd.DataFrame\n        dict: Dict[str, Any]\n        df, dict = tpl\n        self.manager = self.get_library_manager()\n        self.logger = self.get_logger()\n        self.logger.info(f\"Setup started\")\n        # LMDB Setup\n        self.ac = Arctic(RealComparisonBenchmarks.URL)\n        self.lib = self.ac[RealComparisonBenchmarks.LIB_NAME]\n        self.parquet_to_write = f\"{tempfile.gettempdir()}/df.parquet\"\n        self.parquet_to_read = f\"{tempfile.gettempdir()}/df_to_read.parquet\"\n        self.delete_if_exists(self.parquet_to_write)\n        df.to_parquet(self.parquet_to_read , index=True)\n    \n        # With shared storage we create different libs for each process\n        self.s3_lib_write = self.manager.get_library(LibraryType.MODIFIABLE)\n        self.s3_lib_read = self.manager.get_library(LibraryType.PERSISTENT)\n        self.s3_symbol = RealComparisonBenchmarks.SYMBOL\n        self.logger.info(f\"Setup ended\")\n\n    def setup_cache(self):\n        logger = self.get_logger()\n        logger.info(f\"Setup CACHE start\")\n        manager = self.get_library_manager()\n        symbol = RealComparisonBenchmarks.SYMBOL\n        num_rows = RealComparisonBenchmarks.NUMBER_ROWS\n    \n        st = time.time()\n        dict = self.create_dict(num_rows)\n        df = pd.DataFrame(dict)\n        logger.info(f\"DF with {num_rows} rows generated for {time.time() - st}\")\n    \n        # Prepare local LMDB lib\n        ac = Arctic(RealComparisonBenchmarks.URL)\n        ac.delete_library(RealComparisonBenchmarks.LIB_NAME)\n        lib = ac.create_library(RealComparisonBenchmarks.LIB_NAME)\n        lib.write(symbol=symbol, data=df)\n    \n        # Prepare persistent library if does not exist\n        manager.clear_all_benchmark_libs()\n        if not manager.has_library(LibraryType.PERSISTENT):\n            s3_lib = manager.get_library(LibraryType.PERSISTENT)\n            s3_lib.write(symbol, df)\n        return (df, dict)",
        "min_run_count": 1,
        "name": "real_comparison_benchmarks.RealComparisonBenchmarks.time_create_then_write_dataframe",
        "number": 2,
        "param_names": [
            "backend_type"
        ],
        "params": [
            [
                "'no-operation-load'",
                "'create-df-pandas-from_dict'",
                "'pandas-parquet'",
                "'arcticdb-lmdb'",
                "'arcticdb-amazon-s3'"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_comparison_benchmarks:76",
        "timeout": 60000,
        "type": "time",
        "unit": "seconds",
        "version": "f775e01172180c26b5d2a21e04b658cb3389adb8256920c58a20a2798f39131c",
        "warmup_time": 0
    },
    "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class AWSFinalizeStagedData:\n    def peakmem_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, self.df_cache.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache",
        "name": "real_finalize_staged_data.AWSFinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "num_chunks"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_finalize_staged_data:43",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "807052d5c2c0c054cc0b91e1255cb54af1e074c1e216974289db9fa905eff800"
    },
    "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data": {
        "code": "class AWSFinalizeStagedData:\n    def time_finalize_staged_data(self, cache: CachedDFGenerator, param: int):\n        self.logger.info(f\"Library: {self.lib}\")\n        self.logger.info(f\"Symbol: {self.symbol}\")\n        assert self.symbol in self.lib.get_staged_symbols()\n        self.lib.finalize_staged_data(self.symbol, mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, cache, num_chunks: int):\n        self.df_cache: CachedDFGenerator = cache\n        self.logger = self.get_logger()\n    \n        self.lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n    \n        INITIAL_TIMESTAMP: TimestampNumber = TimestampNumber(\n            0, self.df_cache.TIME_UNIT\n        )  # Synchronize index frequency\n    \n        df = self.df_cache.generate_dataframe_timestamp_indexed(200, 0, self.df_cache.TIME_UNIT)\n        list_of_chunks = [10000] * num_chunks\n        self.symbol = f\"symbol_{os.getpid()}\"\n    \n        self.lib.write(self.symbol, data=df, prune_previous_versions=True)\n        stage_chunks(self.lib, self.symbol, self.df_cache, INITIAL_TIMESTAMP, list_of_chunks)\n\n    def setup_cache(self):\n        # Preconditions for this test\n        assert AWSFinalizeStagedData.number == 1\n        assert AWSFinalizeStagedData.repeat == 1\n        assert AWSFinalizeStagedData.rounds == 1\n        assert AWSFinalizeStagedData.warmup_time == 0\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        df_cache = CachedDFGenerator(500000, [5])\n        return df_cache",
        "min_run_count": 1,
        "name": "real_finalize_staged_data.AWSFinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "num_chunks"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_finalize_staged_data:43",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e637474610e2fa47bd54a3077f46e8b35f94f808c4aadccbd01e8ff63d51dc06",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.peakmem_list_symbols": {
        "code": "class AWSListSymbols:\n    def peakmem_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs",
        "name": "real_list_operations.AWSListSymbols.peakmem_list_symbols",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "real_list_operations:51",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fd9a84c3b5ca3f9971e5460376018e832367ebcb746eb52aff37606f622eff17"
    },
    "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting": {
        "code": "class AWSListSymbols:\n    def time_has_symbol_nonexisting(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.has_symbol(\"250_sym\")\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_has_symbol_nonexisting",
        "number": 1,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:51",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e61b7963339a5c66c99a44b8a5ed4246ce52e18c56de05ad969ccfe36e1007df",
        "warmup_time": 0
    },
    "real_list_operations.AWSListSymbols.time_list_symbols": {
        "code": "class AWSListSymbols:\n    def time_list_symbols(self, num_syms):\n        assert self.test_counter == 1, \"Test executed only once in setup-teardown cycle\"\n        self.lib.list_symbols()\n        self.test_counter += 1\n\n    def setup(self, num_syms):\n        self.setup_library()\n        self.lib = self.get_library_manager().get_library(AWSListSymbols.library_type, num_syms)\n        self.test_counter = 1\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        self.lib._nvs.version_store._clear_symbol_list_keys() # clear cache\n\n    def setup_cache(self):\n        assert AWSListSymbols.number == 1, \"There must be always one test between setup and tear down\"\n        self.get_library_manager().log_info() # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_list_operations.AWSListSymbols.time_list_symbols",
        "number": 1,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:51",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "740a4dee3a32d8cb9246934e1860041a0a4d07cf870f17a851c804c95797d293",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fbbf510c321854b58ec444401f123c69f5740b256525334c302218a2ae6d0066"
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_snapshots_without_metadata",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "34c30a2205a1449369b92853def2c677da806e1fd0c092f8ff90a06fec22e7eb"
    },
    "real_list_operations.AWSVersionSymbols.peakmem_list_versions": {
        "code": "class AWSVersionSymbols:\n    def peakmem_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "name": "real_list_operations.AWSVersionSymbols.peakmem_list_versions",
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "68626a85492c785d254c3253ac781ae868deafb7d32980f62984e011ac0c5f07"
    },
    "real_list_operations.AWSVersionSymbols.time_list_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0a97c0c02ba988159c2ba09b8e7f02ca34474bf249980aaea55c37d440d7721d",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata": {
        "code": "class AWSVersionSymbols:\n    def time_list_snapshots_without_metadata(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_snapshots(load_metadata=False)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_snapshots_without_metadata",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "6221253e0b62c144c7223b4969b58a430a1717462bd939900b602a5481d24bc2",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions()\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2f3663667131b7a0be1abbf0400dafceb3af124a460735034097a907a3f9bcb9",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "193b11a4617959d826ba0c91617e552bdffd44feb3b78ced90c5300c0c0450ec",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_latest_only_and_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(latest_only=True, skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_latest_only_and_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d16cac6fad1d99375dbec55fcb0890d7444f076796e6952dab891f2d79b4b70c",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_skip_snapshots(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(skip_snapshots=True)\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_skip_snapshots",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3493206fe0748c6626346d9cb1573b137114e32a4a30dc7aa082ba90ccb3c57a",
        "warmup_time": 0
    },
    "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot": {
        "code": "class AWSVersionSymbols:\n    def time_list_versions_snapshot(self, last_snapshot_names_dict, num_syms):\n        self.lib.list_versions(snapshot=last_snapshot_names_dict[num_syms])\n\n    def setup(self, last_snapshot_names_dict, num_syms):\n        self.population_policy = self.get_population_policy()\n        self.lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, num_syms)\n        self.test_counter = 1\n        expected_num_versions = AWSVersionSymbols.mean_number_versions_per_symbol * num_syms\n        self.get_logger().info(f\"Library {str(self.lib)}\")\n        symbols_list = self.lib.list_symbols()\n        assert num_syms == len(symbols_list), f\"The library contains expected number of symbols {symbols_list}\"\n        mes = f\"There are sufficient versions (at least {expected_num_versions - 1}, num symbols {len(symbols_list)})\"\n        assert (expected_num_versions - 1) >= len(symbols_list), mes\n        assert last_snapshot_names_dict[num_syms] is not None\n\n    def setup_cache(self):\n        num_rows = AWSListSymbols.number_rows\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        last_snapshot_names_dict = {}\n        for number_symbols in AWSVersionSymbols.params:\n            start = time.time()\n            policy.set_parameters([num_rows] * number_symbols, AWSVersionSymbols.number_columns)\n            if not manager.has_library(AWSListSymbols.library_type, number_symbols):\n                populate_library(manager, policy, AWSVersionSymbols.library_type, number_symbols)\n                self.get_logger().info(f\"Generated {number_symbols} with {num_rows} each for {time.time()- start}\")\n            else:\n                self.get_logger().info(f\"Library already exists, population skipped\")\n            # Getting one snapshot - the last\n            lib = self.get_library_manager().get_library(AWSVersionSymbols.library_type, number_symbols)\n            snapshot_name = lib.list_snapshots(load_metadata=False)[-1]\n            last_snapshot_names_dict[number_symbols] = snapshot_name\n        manager.log_info() # Always log the ArcticURIs\n        return last_snapshot_names_dict",
        "min_run_count": 1,
        "name": "real_list_operations.AWSVersionSymbols.time_list_versions_snapshot",
        "number": 3,
        "param_names": [
            "num_syms"
        ],
        "params": [
            [
                "25",
                "50"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_list_operations:129",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2aff065897965f79c4a26f9107a7acda0fa16d88dbfb3ffc50cf98ca90570155",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e97486b955cfba59bd57e26e7f67dfd28fa68fe5b41d4000cad23410f78e4a0f",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f8d864264ef6e4f853192984c4f67100dbcc9fb5ea225dcf5a3b4e00947ff62c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "6685c85cff6e17f615694061ea490e2b14d432c1628252a29d07fdcc04b97c2c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a96bc9d0a016f29c9671d713ad7a2fdfd945381695c2227c7716468b8eaebd1d",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "bbc2c8763db23c7c27e02f3361ee26cb68e9ead689a18a7e7bbc81edafa7a5a5",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:215",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b690d7836ba77e8f6ecd134bf9c3877c8c3f5b1a5d94f4d842b3b192ec8bbe07",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete(self, cache, num_rows):\n        self.lib.delete(self.symbol)\n        self.symbol_deleted = True\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:261",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4588f423aa2b7d1ded777f24c8ddd1b19a282bd7c7a6f15c012fc0cf1acbdd36",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete_over_time(self, cache, num_rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(25):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:261",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "978d41f95903f476a5a0c703ce1cd6ffdf3386ad6dd9023851d9f784159d567f",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ae924cc65f3fda9e7c64fcd76f6a542603cfb7242333cab7d762a62689a44aa3",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "afabfcaa402bc4f4a8dd332050aaa65770b2d746b2ea6235ec9421e461dd4975",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "c3a2c9b358a8c4ffd9af01ffa0b9474e2e317579a1030aeea626be4f621274a2",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e865f1e39380722e6d3bfe6e3a56d2fae9389a0d95cd11c29b6f34f2007a389a",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ba848d44eee0ef4595475f4def6ae17301c551096480c10b75562fd8f5c2598c",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:73",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0470f242734c94e8f6d30f4241c13ffe6cc8b53cb2bad1973799878b94c3cccd",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9e0bfdbf626113f82fdfd1ffcd6e02ac70422c95277dc9b2b71b9eff44dd5844"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ded8cd45317d8b9ef7d267bbe014125a0cb892b87d4f249a7b113c7fa8d09df0"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1e538c9cadf80b7110caa89fd8da6c930281e5f0d8f0f77b874ff590cd359fcb"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4a5c834b77a9e0290a0acc55ea4aad637b747af99e048bd0f7a953f8b5fc763f"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9fcfddabe4a1ac18529dc03b6cc3aaad336249b652774a16aaba62e2132702b5"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6f7f2edde2507709dc18b41fa471455f6689bc62c76f009646bcfa9337ca8485"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "56097ba0e08cd9b953d9731b60fc7c04ecba690f615098d26be69fc8cf6f105f"
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "5051b70ae6f5cbc9493af6ec64ea7f644db64a60d0a58f5de1930870d44c02a4",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ef199c9d78fc43c44984acaf08e9867a6efa89ab5b0012e51ecd4e5fd1fedce4",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_projection",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "df07f61fb088929ad901e2fea6f172430f060daaadd6457d9eea51bad1129a8d",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_1",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "9720f704e40e9de6ae39cdab8db71d34c19b46fc05f4151e49ecebd50f01fd0d",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_3",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b0ebeed0c3c098fafd4f9f1f1b4031dcddfeec4eb44ec0789a60f9b9ff3df04b",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_4",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "da26f9fa31e449bf2696324ee811ee3a949836935c265a28d8728bce3ca03579",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol =  self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:66",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "7542c6a85b7d4791481f3106886009ee800f6fdfb132008b9f6fa60c6ed93848",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0689b459c6c7bb49e10edc6df8a37976cdefa6017472521a35896aeddf267ae0"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "61215cb103adee49fa6f3f56e1f611dcd697f9b44e8f2be23d35b3c6b997883b"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "285c5f2f9ba3da391ed62f3dfcf5e754258744e6ee152ccf26bf8a2ca571c972"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "67423f3bf528cd9f5242633d93b484b558436e5c843c939d5071217f435ef906"
    },
    "real_read_write.AWSReadWrite.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0625aa02db843284e2bd88936c72d6ed89e4f8399179c6230fffb4e15096e141"
    },
    "real_read_write.AWSReadWrite.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7ba6fea1dcf0c0e751683e095293ce820525ee930e234bbd516af15b7636d80f"
    },
    "real_read_write.AWSReadWrite.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "612120fc41c174f1a09dadd9b53367c87432990752569e4fbfccc84a8d0094b6",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f2264b620c766af4a90a0eb7e2e06b5fbb47f300449e8ed6a8e3a265e695b9ff",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8c7fb09e61e2d7bd0f229b2ca10e61cde1f54eadb187d6f9f8136eddf32d7b1b",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "84f2f904f9ca0aa633bec423ac9385f780529edd8cf618e13690b07781a90b23",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2e85d7bf669c8aab495b34003fc9d5f536e37ee77b0154c98debb39612a1898e",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        '''\n        In setup_cache we only populate the persistent libraries if they are missing.\n        '''\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info() # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:79",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "aa2c3915c2cc65992b8d64669e2bc74ccfbd4d83469b0eb253739b11211b46be",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "bcdfc7aa551bc9ae43c6bf8dbb5ea2b9b92e03e7e6da8487ac110b302443beb4"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d8263d005135750e39a3e13bd7de6702642ec7991a7dfde787b4bdba7a47125c"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "febc2581d6ee0495f21fe39fd5b9e139ee3784074ebef2305ea52a167d60a449"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ac2ade0b551d6e6eca41a7b098855fc013f95c1ad95a97d15fcb716a99784bca"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ebfae974146eb06e3015c78f663185f041fecba4f2d21265a95a0f8e0b3ed114"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "8e0b84e936c8b20c9ab1100d19b36f06abe0690a3d42f1a121d487556d98fa66"
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "02de728e3f80213fda1fc979b4aaf61786cd350dc31f266000478ce15d5c04e0",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e730587e51a99d028e7438d4874d1e0b1f098b49ec2773dbabf9b06d7c562315",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "69b8202ed5e2e62aefd161b99db3b55667643789c66fc99599abf2649c144ab7",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "35c60f81beba373aaf062739dc364a9835f7f7bcccd9ac4ae7187f2f97a6a0c5",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "dc443b21285eae3a4009ce860f6d4be6cb0164245da60aad1b63ec6fecd5d4f8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n       super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:228",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ed3fedef28f86763035af2a64963966bf6724343de519001c9ac1a4a72d84928",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "777451970fc2c9ba7c40648d51397be114005fca10b35c2beb816a0c5b11eb4e"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "94cd6191a28611f9fdfb6f92e01fa9ccec156d60cc7bda30471fc0b6b27d8c28"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6b035607aea2f64b0266d23fd98a15543efda0abc94c68902099d2525db7050d"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "840af2cee86e8ba04910ceb2c04cc3c6e732c2a9eea52a2ab3cecf1a35767444"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c55080c0aa9714cf9bbac9058877813c9fd7bab7326f66618725b67b14f21372"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write_staged",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a07309f742f72469c5f54a0962205f9272af5d741b615c6e5536f97ee16356ee"
    },
    "real_read_write.AWSWideDataFrameTests.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "355ba97b40b39aa52586e966f70b2710a64b1694030d66d69a94845d11620e12",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "5653d388b62f876452148f5aff5c29e03eda6a7548b406fbdc324ab2365bcdbe",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\",\"string10\",\"bool\", \"int64\",\"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "7f4881ccccbe07a5dc1f77fcf1e75aaa331f0f76a590eda75c876742eb30c613",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4feff55363038593799ca793ef2904d32a76d0d794f5466ed75c56ba867bd1d3",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ed88fa1910599e13c65662744b40b1396536a03e2397cbf12f39f37b81c8f81b",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write_staged",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:200",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a26f1ab74733278e53ed0c0913f466bfdb150dd2fa8d5a481715bfa3caf2f978",
        "warmup_time": 0
    },
    "resample.Resample.peakmem_resample": {
        "code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "resample.Resample.peakmem_resample",
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "setup_cache_key": "resample:45",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "99c6acb45f29b2d72d54020d96990507084b8fd1c9918c594e9fa09670dec465"
    },
    "resample.Resample.time_resample": {
        "code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            pass\n            # Use this when upgrading to ASV 0.6.0 or later\n            # raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "resample.Resample.time_resample",
        "number": 7,
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "resample:45",
        "type": "time",
        "unit": "seconds",
        "version": "1f835087b54bb1d48a11dcbdb503bbbc4c57d6c5f54b6f0f6cf22c68c2187a5c",
        "warmup_time": -1
    },
    "resample.ResampleWide.peakmem_resample_wide": {
        "code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "name": "resample.ResampleWide.peakmem_resample_wide",
        "param_names": [],
        "params": [],
        "setup_cache_key": "resample:131",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"
    },
    "resample.ResampleWide.time_resample_wide": {
        "code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "min_run_count": 2,
        "name": "resample.ResampleWide.time_resample_wide",
        "number": 5,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "resample:131",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2",
        "warmup_time": 0
    },
    "version": 2,
    "version_chain.IterateVersionChain.time_list_undeleted_versions": {
        "code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_list_undeleted_versions",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8cf2d8b7302ee0311a2bab73cb1ab31134c9676a39bc5e517411e3192a89ead7",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_load_all_versions": {
        "code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_load_all_versions",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "32c93e66abfbbbaa80b6cdf40c50fc82e21aa1de964c5962ae200444ff26f252",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_alternating": {
        "code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_alternating",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f1f008d2c2efb9386c21fdd3539bb3601b1078e323613d38f13ddadb066cb004",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_from_epoch": {
        "code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_from_epoch",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ee44eb3fe3eecad30de7d9349e47e68cdeb430326b5713b7ae4bfd7abdb63707",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_v0": {
        "code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_v0",
        "number": 13,
        "param_names": [
            "num_versions",
            "caching",
            "deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "0.0",
                "0.99"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "version_chain:42",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "4de46121ac1a914c7a5d77c82aa535e29da68e0e2acc7e17e483d168cac49db3",
        "warmup_time": -1
    }
}