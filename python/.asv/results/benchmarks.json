{
    "arrow.ArrowNumeric.peakmem_read": {
        "code": "class ArrowNumeric:\n    def peakmem_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowNumeric.peakmem_read",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fea10ab7115b7e7be437e715eeb12d0e612b564841f428284d671d5cb7ac19bd"
    },
    "arrow.ArrowNumeric.peakmem_write": {
        "code": "class ArrowNumeric:\n    def peakmem_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowNumeric.peakmem_write",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b69f2e665657c5347cab7e09a6abe6ba6fd15ba460efd5bb65174d16a86c1e88"
    },
    "arrow.ArrowNumeric.time_read": {
        "code": "class ArrowNumeric:\n    def time_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowNumeric.time_read",
        "number": 0,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "5a886d69ac180d8a3f8cb7b8d851da8f710cdf2a89ef94bdb6ccee92dfff3408",
        "warmup_time": -1
    },
    "arrow.ArrowNumeric.time_write": {
        "code": "class ArrowNumeric:\n    def time_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowNumeric.time_write",
        "number": 0,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "9170a039dc500cb3b447a2654628e4f3b38d37248f0d3627a554d4fe1bd4b46a",
        "warmup_time": -1
    },
    "arrow.ArrowStrings.peakmem_read": {
        "code": "class ArrowStrings:\n    def peakmem_read(self, rows, date_range, unique_string_count, arrow_string_format):\n        self._check_should_run_benchmark(date_range=date_range, arrow_string_format=arrow_string_format)\n        self.lib.read(\n            self.symbol_name(rows, unique_string_count),\n            date_range=self.date_range,\n            arrow_string_format_default=arrow_string_format,\n        )\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowStrings.peakmem_read",
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "44f0a54520c8f9423d047f524c28873223c84717c1073bd180b017f300bd3486"
    },
    "arrow.ArrowStrings.peakmem_write": {
        "code": "class ArrowStrings:\n    def peakmem_write(self, rows, date_range, unique_string_count, arrow_string_format):\n        # No point in running with all read time options\n        if date_range is None and arrow_string_format == ArrowOutputStringFormat.CATEGORICAL:\n            self.fresh_lib.write(self.symbol_name(rows, unique_string_count), self.table, index_column=\"ts\")\n        else:\n            raise SkipNotImplemented\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowStrings.peakmem_write",
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f391977543e46b15a88a5882f4f80eb41e5285a9d2fa8398b322a24e3f574a62"
    },
    "arrow.ArrowStrings.time_read": {
        "code": "class ArrowStrings:\n    def time_read(self, rows, date_range, unique_string_count, arrow_string_format):\n        self._check_should_run_benchmark(date_range=date_range, arrow_string_format=arrow_string_format)\n        self.lib.read(\n            self.symbol_name(rows, unique_string_count),\n            date_range=self.date_range,\n            arrow_string_format_default=arrow_string_format,\n        )\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowStrings.time_read",
        "number": 0,
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "08e1367be8f34d8df67bdf98157985f6623a74631e19dcd63924dde8d0e74176",
        "warmup_time": -1
    },
    "arrow.ArrowStrings.time_write": {
        "code": "class ArrowStrings:\n    def time_write(self, rows, date_range, unique_string_count, arrow_string_format):\n        # No point in running with all read time options\n        if date_range is None and arrow_string_format == ArrowOutputStringFormat.CATEGORICAL:\n            self.fresh_lib.write(self.symbol_name(rows, unique_string_count), self.table, index_column=\"ts\")\n        else:\n            raise SkipNotImplemented\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowStrings.time_write",
        "number": 0,
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "aee7692a967a399e46745ef2b201b715b725357ffdfe81a6219225984d2e2a79",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, *args):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7e0663f5f53179157fae210d779bcfcedfc83dbfd58837a07832da84d46e9ef5"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, *args):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ccfd86bc53c7545e3e514a4987ec117bbd9a875392f85c425723498ba396f13d"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, *args):\n        self.lib.read(f\"sym\", date_range=DATE_RANGE).data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "bcf899a767d13af99a959348d88abe7a173ee6e83e832ee27788b4cf4a7f0a2f"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, *args):\n        q = QueryBuilder().date_range(DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder",
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3b436ea2a28f6ae6734c17537690d92e3e61af5ee518b7f0c1f58f73f7ddbb56"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, *args):\n        self.lib.write(f\"peakmem_write_sym\", self.df)\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "81d79c51e980ab5ab2560d3dda7254baf61f7cc7dceef9fb70fa6ab52994f5f0"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, *args):\n        self.lib.write(f\"peakmem_write_staged\", self.df, staged=True)\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9ef0afeb65168b10326e44ed91dad7829e7137c0b123c262767cd9253539e37f"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, *args):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 0,
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "ef0619fc498a3a5658c891fce06a85f1aeffd64f289e692da87bd4b03ba84340",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, *args):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 0,
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "a0398df225735b7a669dc096f0240601ce85c1775810b16047b68850efbbcdf3",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, *args):\n        self.lib.read(f\"sym\", date_range=DATE_RANGE).data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 0,
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "024f460f41be247529b37396e2de7afff3e0f936c016f218e72cbcbbdd90b1a7",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, *args):\n        q = QueryBuilder().date_range(DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder",
        "number": 0,
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "63fc85a93f0380d2d1b3bcc502a4ed14add0cebe5b2a00744fad43d706fafafb",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, *args):\n        self.lib.write(f\"time_write_sym\", self.df)\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 0,
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "e160ee5f841133bf7e547cb508e465001813ccfcdca7e5bb6fd9919872144091",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, *args):\n        self.lib.write(f\"time_write_staged\", self.df, staged=True)\n\n    def setup(self, libs_for_storage, rows, use_query_stats, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        if use_query_stats:\n            if storage != Storage.AMAZON:\n                raise SkipNotImplemented(\"Query stats only supported against S3\")\n            qs.enable()\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 0,
        "param_names": [
            "num_rows",
            "use_query_stats",
            "storage"
        ],
        "params": [
            [
                "5000000",
                "10000000"
            ],
            [
                "True",
                "False"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:50",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "6b0ab769d32ef0743e047b41f528f691d89b59f258adb993d3ad62c1657bf018",
        "warmup_time": 0.2
    },
    "basic_functions.BatchFunctions.peakmem_read_batch": {
        "code": "class BatchFunctions:\n    def peakmem_read_batch(self, libs_for_storage, rows, num_symbols, storage):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "name": "basic_functions.BatchFunctions.peakmem_read_batch",
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:269",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f4a967266cd6ea1d0d981ad126d561f2f00dbb8eeab69cc3e865dffa411e06f7"
    },
    "basic_functions.BatchFunctions.peakmem_read_batch_with_columns": {
        "code": "class BatchFunctions:\n    def peakmem_read_batch_with_columns(self, libs_for_storage, rows, num_symbols, storage):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "name": "basic_functions.BatchFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:269",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c0ae8f163bf19c07653b6f60ab6e995158aa563e662c2d8ccd347a5ba505688c"
    },
    "basic_functions.BatchFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BatchFunctions:\n    def peakmem_read_batch_with_date_ranges(self, libs_for_storage, rows, num_symbols, storage):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "name": "basic_functions.BatchFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:269",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d362cfb67572e9f68968dfe29777803a15c4691f01653af49ea32bdb7c158b01"
    },
    "basic_functions.BatchFunctions.time_read_batch": {
        "code": "class BatchFunctions:\n    def time_read_batch(self, libs_for_storage, rows, num_symbols, storage):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BatchFunctions.time_read_batch",
        "number": 0,
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:269",
        "type": "time",
        "unit": "seconds",
        "version": "9c67c6ac2385d57d12dc1d8d49b7a0d0711d60e36d0e7d6a99a23e5fcc7cfbb5",
        "warmup_time": -1
    },
    "basic_functions.BatchFunctions.time_read_batch_with_columns": {
        "code": "class BatchFunctions:\n    def time_read_batch_with_columns(self, libs_for_storage, rows, num_symbols, storage):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BatchFunctions.time_read_batch_with_columns",
        "number": 0,
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:269",
        "type": "time",
        "unit": "seconds",
        "version": "a93ba8dc7f258d45759f62c1d3d2a48bcf6ea4d091a58f36008ed020b4eeefb9",
        "warmup_time": -1
    },
    "basic_functions.BatchFunctions.time_read_batch_with_date_ranges": {
        "code": "class BatchFunctions:\n    def time_read_batch_with_date_ranges(self, libs_for_storage, rows, num_symbols, storage):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BatchFunctions.time_read_batch_with_date_ranges",
        "number": 0,
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:269",
        "type": "time",
        "unit": "seconds",
        "version": "64aec41d93970093050e6a4b50565a81f071487c2b3a63f9e6fabe9afe0b0f7f",
        "warmup_time": -1
    },
    "basic_functions.BatchFunctions.time_update_batch": {
        "code": "class BatchFunctions:\n    def time_update_batch(self, libs_for_storage, rows, num_symbols, storage):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, libs_for_storage, rows, num_symbols, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BatchFunctions.time_update_batch",
        "number": 0,
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:269",
        "type": "time",
        "unit": "seconds",
        "version": "f2cdd6b45cd2d49f2358097a18969b2ad5f2efc6cf26a854ab889d6b23fc4c4f",
        "warmup_time": -1
    },
    "basic_functions.BatchWrite.peakmem_write_batch": {
        "code": "class BatchWrite:\n    def peakmem_write_batch(self, lib_for_storage, rows, num_symbols, storage):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.lib.write_batch(payloads)\n\n    def setup(self, lib_for_storage, rows, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "name": "basic_functions.BatchWrite.peakmem_write_batch",
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:227",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "434b63682c3d0bae77a89fe975756e348383dffe6c1f6a37b8149c6342e38e26"
    },
    "basic_functions.BatchWrite.time_write_batch": {
        "code": "class BatchWrite:\n    def time_write_batch(self, lib_for_storage, rows, num_symbols, storage):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.lib.write_batch(payloads)\n\n    def setup(self, lib_for_storage, rows, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.BatchWrite.time_write_batch",
        "number": 0,
        "param_names": [
            "num_rows",
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:227",
        "type": "time",
        "unit": "seconds",
        "version": "5f2cd16f87dc39fde97363a9689a3c011855cb7b83e7af0c50d3403c655e0ff8",
        "warmup_time": -1
    },
    "basic_functions.ShortWideRead.peakmem_read": {
        "code": "class ShortWideRead:\n    def peakmem_read(self, *args):\n        self.lib.read(f\"sym\")\n\n    def setup(self, libs_for_storage, rows, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        libs_for_storage = dict()\n        library_names = [_lib_name(rows) for rows in ShortWideRead.rows]\n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in ShortWideRead.rows}\n    \n        for storage in STORAGES:\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n    \n        for rows in ShortWideRead.rows:\n            df = self.dfs[rows]\n            lib_name = _lib_name(rows)\n            for storage in STORAGES:\n                lib = libs_for_storage[storage][lib_name]\n                if not lib:\n                    continue\n                lib.write(\"sym\", df)\n    \n        return libs_for_storage",
        "name": "basic_functions.ShortWideRead.peakmem_read",
        "param_names": [
            "num_rows",
            "storage"
        ],
        "params": [
            [
                "1",
                "5000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:180",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "363b22cc6dfbdaa2b6928899e70c650f82208162a4e40578121c5a6ca92e24a6"
    },
    "basic_functions.ShortWideRead.time_read": {
        "code": "class ShortWideRead:\n    def time_read(self, *args):\n        self.lib.read(f\"sym\")\n\n    def setup(self, libs_for_storage, rows, storage):\n        self.lib = libs_for_storage[storage][_lib_name(rows)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        libs_for_storage = dict()\n        library_names = [_lib_name(rows) for rows in ShortWideRead.rows]\n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in ShortWideRead.rows}\n    \n        for storage in STORAGES:\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n    \n        for rows in ShortWideRead.rows:\n            df = self.dfs[rows]\n            lib_name = _lib_name(rows)\n            for storage in STORAGES:\n                lib = libs_for_storage[storage][lib_name]\n                if not lib:\n                    continue\n                lib.write(\"sym\", df)\n    \n        return libs_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.ShortWideRead.time_read",
        "number": 0,
        "param_names": [
            "num_rows",
            "storage"
        ],
        "params": [
            [
                "1",
                "5000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:180",
        "type": "time",
        "unit": "seconds",
        "version": "fa3800476c3a8197f7666ce731230a0e1427c8f4d709ea03772d325d40184a68",
        "warmup_time": -1
    },
    "basic_functions.ShortWideWrite.peakmem_write": {
        "code": "class ShortWideWrite:\n    def peakmem_write(self, *args):\n        self.lib.write(\"sym\", self.df)\n\n    def setup(self, lib_for_storage, rows, cols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.df = generate_random_floats_dataframe(rows, cols)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(STORAGES)\n        return lib_for_storage",
        "name": "basic_functions.ShortWideWrite.peakmem_write",
        "param_names": [
            "rows",
            "cols",
            "storages"
        ],
        "params": [
            [
                "1",
                "5000"
            ],
            [
                "30000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:145",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e7e630990fa7fd9ff165290297cc42118a322f07b4d95abfae6060760c83baea"
    },
    "basic_functions.ShortWideWrite.peakmem_write_staged": {
        "code": "class ShortWideWrite:\n    def peakmem_write_staged(self, *args):\n        self.lib.write(f\"peakmem_write_staged\", self.df, staged=True)\n\n    def setup(self, lib_for_storage, rows, cols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.df = generate_random_floats_dataframe(rows, cols)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(STORAGES)\n        return lib_for_storage",
        "name": "basic_functions.ShortWideWrite.peakmem_write_staged",
        "param_names": [
            "rows",
            "cols",
            "storages"
        ],
        "params": [
            [
                "1",
                "5000"
            ],
            [
                "30000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "basic_functions:145",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1f8533a2f894ad65e2fcfefcfadf85262eff10e7ff0ce3787dc6b7e709984dcc"
    },
    "basic_functions.ShortWideWrite.time_write": {
        "code": "class ShortWideWrite:\n    def time_write(self, *args):\n        self.lib.write(\"sym\", self.df)\n\n    def setup(self, lib_for_storage, rows, cols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.df = generate_random_floats_dataframe(rows, cols)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(STORAGES)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.ShortWideWrite.time_write",
        "number": 0,
        "param_names": [
            "rows",
            "cols",
            "storages"
        ],
        "params": [
            [
                "1",
                "5000"
            ],
            [
                "30000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:145",
        "type": "time",
        "unit": "seconds",
        "version": "695127b76341fdc0fa06c864b14adaa64531d019157792896f7e40fa99fdea56",
        "warmup_time": -1
    },
    "basic_functions.ShortWideWrite.time_write_staged": {
        "code": "class ShortWideWrite:\n    def time_write_staged(self, *args):\n        self.lib.write(f\"time_write_staged\", self.df, staged=True)\n\n    def setup(self, lib_for_storage, rows, cols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.df = generate_random_floats_dataframe(rows, cols)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(STORAGES)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "basic_functions.ShortWideWrite.time_write_staged",
        "number": 0,
        "param_names": [
            "rows",
            "cols",
            "storages"
        ],
        "params": [
            [
                "1",
                "5000"
            ],
            [
                "30000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:145",
        "type": "time",
        "unit": "seconds",
        "version": "897174943e95686578befc161ffa75dd3b93b60e28885ef4005962198f3d2697",
        "warmup_time": -1
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "331c9abb4f84b01dd28765b77a88e069ec6d6b70617a12dd5aa9c3e14ca6a6ad"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae73602827d4d1b739e519e8ca6a847c5938a5744ebf371ca78511b0be1bf16f"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e708975c50d2b70ebdff11efa45f9fd15ceee9861301d5552f1e8ebe2cb4d1bd"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {
        "code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3407eb183cf3c02afbeaf04e6c31bf6b5aaf615458cd8e2ad46a21b4d2af80e2"
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "bf5e390b01e356685500d464be897fe7cb51531dcd92fccedec980f97f361e3c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ceeb3b3e6049c66cb2ecabbb16485e4555cefc7920697c7a34de08993be14af0",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "fa74284d1e48fd396138a5f50c53d92829194b7be1f0caa8f441f8820db4157c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_readall": {
        "code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_readall",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "3cd2c7d90725498da459157638eb15b5a3fcc68aa91684951717ed5ab1c8ca63",
        "warmup_time": 0
    },
    "cpp_microbenchmarks.BM_arrow_convert_multiple_record_batches_to_segment.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_arrow_convert_multiple_record_batches_to_segment.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'10/100000/10/1'",
                "'10000000/10/100/1'",
                "'10000000/10/10000/1'",
                "'1000/10/1000/1'",
                "'10/100000/10/0'",
                "'10000000/10/100/0'",
                "'10000000/10/10000/0'",
                "'1000/10/1000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_arrow_convert_single_record_batch_to_segment.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_arrow_convert_single_record_batch_to_segment.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'10/100000/-1/1'",
                "'10/100000/0/1'",
                "'10/100000/50000/1'",
                "'10000000/10/-1/1'",
                "'10000000/10/0/1'",
                "'10000000/10/5/1'",
                "'10/100000/-1/0'",
                "'10/100000/0/0'",
                "'10/100000/50000/0'",
                "'10000000/10/-1/0'",
                "'10000000/10/0/0'",
                "'10000000/10/5/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_arrow_string_handler.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_arrow_string_handler.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'10000/1/0/0/0'",
                "'100000/1/0/0/0'",
                "'10000/10000/0/0/0'",
                "'100000/100000/0/0/0'",
                "'10000/1/5000/0/0'",
                "'100000/1/50000/0/0'",
                "'10000/1/10000/0/0'",
                "'100000/1/100000/0/0'",
                "'10000/1/0/1/0'",
                "'100000/1/0/1/0'",
                "'10000/10000/0/1/0'",
                "'100000/100000/0/1/0'",
                "'10000/1/5000/1/0'",
                "'100000/1/50000/1/0'",
                "'10000/1/10000/1/0'",
                "'100000/1/100000/1/0'",
                "'10000/1/0/2/0'",
                "'100000/1/0/2/0'",
                "'10000/10000/0/2/0'",
                "'100000/100000/0/2/0'",
                "'10000/1/5000/2/0'",
                "'100000/1/50000/2/0'",
                "'10000/1/10000/2/0'",
                "'100000/1/100000/2/0'",
                "'10000/1/0/0/1'",
                "'100000/1/0/0/1'",
                "'10000/10000/0/0/1'",
                "'100000/100000/0/0/1'",
                "'10000/1/0/1/1'",
                "'100000/1/0/1/1'",
                "'10000/10000/0/1/1'",
                "'100000/100000/0/1/1'",
                "'10000/1/0/2/1'",
                "'100000/1/0/2/1'",
                "'10000/10000/0/2/1'",
                "'100000/100000/0/2/1'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_chunked_buffer_allocate_with_ensure.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_chunked_buffer_allocate_with_ensure.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/203/1/0'",
                "'100000/203/0/0'",
                "'100000/203/0/2'",
                "'10000/2003/1/0'",
                "'10000/2003/0/0'",
                "'10000/2003/0/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_chunked_buffer_random_access.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_chunked_buffer_random_access.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/203/1/0'",
                "'100000/203/0/0'",
                "'100000/203/0/2'",
                "'10000/2003/1/0'",
                "'10000/2003/0/0'",
                "'10000/2003/0/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_hash_grouping_int_int16_t_.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_hash_grouping_int_int16_t_.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/10/2'",
                "'100000/10000/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_hash_grouping_int_int32_t_.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_hash_grouping_int_int32_t_.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/10/2'",
                "'100000/100000/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_hash_grouping_int_int64_t_.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_hash_grouping_int_int64_t_.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/10/2'",
                "'100000/100000/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_hash_grouping_int_int8_t_.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_hash_grouping_int_int8_t_.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/10/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_hash_grouping_string.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_hash_grouping_string.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/10/2/10'",
                "'100000/100000/2/10'",
                "'100000/10/2/100'",
                "'100000/100000/2/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_iterate_with_iterator.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_iterate_with_iterator.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_iterate_with_scalar_at.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_iterate_with_scalar_at.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_merge_interleaved.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_merge_interleaved.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'10000/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_merge_ordered.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_merge_ordered.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'10000/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_packed_bits_to_buffer.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_packed_bits_to_buffer.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'",
                "'1000000'",
                "'10000000'",
                "'100000000'",
                "'1000000000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_regex_match.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_regex_match.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1000/1'",
                "'100000/1000/0'",
                "'100000/10000/1'",
                "'100000/10000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_search_sorted_random.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_search_sorted_random.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_search_sorted_single_value.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_search_sorted_single_value.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_sort_ordered.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_sort_ordered.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_sort_shuffled.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_sort_shuffled.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100'",
                "'1000000/1'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_sort_sparse.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_sort_sparse.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_bitset_bitset.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_bitset_bitset.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_bitset_bool.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_bitset_bool.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1/1'",
                "'100000/1/0'",
                "'100000/0/1'",
                "'100000/0/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_bool_bool.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_bool_bool.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1/1'",
                "'100000/1/0'",
                "'100000/0/1'",
                "'100000/0/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_dense_col_dense_col.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_dense_col_dense_col.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_dense_col_empty.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_dense_col_empty.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_dense_col_sparse_col.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_dense_col_sparse_col.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_dense_col_val.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_dense_col_val.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_sparse_col_empty.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_sparse_col_empty.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_sparse_col_sparse_col.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_sparse_col_sparse_col.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_sparse_col_val.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_sparse_col_val.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_val_empty.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_val_empty.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_numeric_val_val.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_numeric_val_val.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_dense_col_dense_col.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_dense_col_dense_col.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100000/1'",
                "'100000/100000/0'",
                "'100000/2/1'",
                "'100000/2/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_dense_col_empty.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_dense_col_empty.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1/100000'",
                "'100000/0/100000'",
                "'100000/1/2'",
                "'100000/0/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_dense_col_sparse_col.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_dense_col_sparse_col.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100000/1'",
                "'100000/100000/0'",
                "'100000/2/1'",
                "'100000/2/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_dense_col_val.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_dense_col_val.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1/100000'",
                "'100000/0/100000'",
                "'100000/1/2'",
                "'100000/0/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_sparse_col_empty.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_sparse_col_empty.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1/100000'",
                "'100000/0/100000'",
                "'100000/1/2'",
                "'100000/0/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_sparse_col_sparse_col.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_sparse_col_sparse_col.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/100000/1'",
                "'100000/100000/0'",
                "'100000/2/1'",
                "'100000/2/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_sparse_col_val.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_sparse_col_val.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1/100000'",
                "'100000/0/100000'",
                "'100000/1/2'",
                "'100000/0/2'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_val_empty.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_val_empty.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000/1'",
                "'100000/0'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "cpp_microbenchmarks.BM_ternary_string_val_val.track_time_ms": {
        "code": "class <locals>:\n    def track_time_ms(self, params):\n        if not CPP_BENCHMARKS_BINARY.exists():\n            raise FileNotFoundError(\n                f\"C++ benchmark binary not found: {CPP_BENCHMARKS_BINARY}\\n\"\n                f\"Build it with:\\n\"\n                f\"  cmake --preset linux-release -DTEST=ON cpp\\n\"\n                f\"  cmake --build cpp/out/linux-release-build --target benchmarks\\n\"\n                f\"or set ARCTICDB_CPP_BENCHMARKS_BINARY to the correct path.\"\n            )\n        full_name = f\"{func_name}/{params}\" if params is not None else func_name\n        return run_benchmark(full_name, func_name)",
        "name": "cpp_microbenchmarks.BM_ternary_string_val_val.track_time_ms",
        "param_names": [
            "params"
        ],
        "params": [
            [
                "'100000'"
            ]
        ],
        "timeout": 3600,
        "type": "track",
        "unit": "ms",
        "version": "e8c576f1d487f7f7331a39fb79564e75f17ca0faf14620a4c2dd8c5fb841ff70"
    },
    "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, *args):\n        staged_symbols = self.lib.get_staged_symbols()\n        assert self.symbol + \"-mem\" in staged_symbols\n        self.lib.finalize_staged_data(self.symbol + \"-mem\", mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, lib_for_storage, num_chunks, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        assert len(self.lib.list_symbols()) == 0  # check we are in a clean state\n        initial_timestamp = TimestampNumber(0, self.df_generator.TIME_UNIT)\n    \n        list_of_chunks = [10_000] * num_chunks\n    \n        for suffix in (\"-time\", \"-mem\"):\n            symbol = _symbol_name(num_chunks) + suffix\n            stage_chunks(self.lib, symbol, self.df_generator, initial_timestamp, list_of_chunks)\n            self.logger.info(f\"Created Symbol: {symbol}\")\n        self.symbol = _symbol_name(num_chunks)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "num_chunks",
            "storage"
        ],
        "params": [
            [
                "10",
                "100"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:45",
        "timeout": 100,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "defbeee92bd3cdca701262dd6032fc454c3f47d67b7f38713a7e8f89e5f719fb"
    },
    "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, *args):\n        staged_symbols = self.lib.get_staged_symbols()\n        assert self.symbol + \"-time\" in staged_symbols\n        self.lib.finalize_staged_data(self.symbol + \"-time\", mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, lib_for_storage, num_chunks, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        assert len(self.lib.list_symbols()) == 0  # check we are in a clean state\n        initial_timestamp = TimestampNumber(0, self.df_generator.TIME_UNIT)\n    \n        list_of_chunks = [10_000] * num_chunks\n    \n        for suffix in (\"-time\", \"-mem\"):\n            symbol = _symbol_name(num_chunks) + suffix\n            stage_chunks(self.lib, symbol, self.df_generator, initial_timestamp, list_of_chunks)\n            self.logger.info(f\"Created Symbol: {symbol}\")\n        self.symbol = _symbol_name(num_chunks)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "num_chunks",
            "storage"
        ],
        "params": [
            [
                "10",
                "100"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:45",
        "timeout": 100,
        "type": "time",
        "unit": "seconds",
        "version": "c3ec2b9e88db58166ea78b66b71354d3b13c8ca077e865208dd8848b788f7f8d",
        "warmup_time": 0
    },
    "list_snapshots.Snapshots.peakmem_list_snapshots": {
        "code": "class Snapshots:\n    def peakmem_list_snapshots(\n        self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata\n    ):\n        res = self.lib.list_snapshots(load_metadata=load_metadata)\n        assert len(res) == num_snapshots, f\"Expected {num_snapshots} snapshots but were {len(res)}\"\n\n    def setup(self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata):\n        self.lib = libs_for_storage[storage][get_lib_name(num_symbols, num_snapshots, metadata_entries)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        write_parameters = list(itertools.product(self.num_symbols, self.num_snapshots, self.metadata_entries))\n        assert write_parameters\n        libs_for_storage = dict()\n        library_names = [\n            get_lib_name(num_syms=n_syms, num_snaps=n_snaps, metadata_size=md_size)\n            for n_syms, n_snaps, md_size in write_parameters\n        ]\n        simple_df = pd.DataFrame({\"a\": [1]})\n    \n        for storage in self.storages:\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            if not is_storage_enabled(storage):\n                continue\n    \n            for n_syms, n_snaps, md_size in write_parameters:\n                lib_name = get_lib_name(n_syms, n_snaps, md_size)\n                lib = libs_for_storage[storage][lib_name]\n                print(f\"lib_name={lib_name}, lib={lib}\", file=sys.stderr)\n                if lib is None:\n                    continue\n                writes = [WritePayload(f\"sym_{i}\", simple_df) for i in range(n_syms)]\n    \n                lib.write_batch(writes)\n    \n                metadata = get_metadata(md_size)\n                for i in range(n_snaps):\n                    lib.snapshot(f\"snap_{i}\", metadata=metadata)\n    \n        return libs_for_storage",
        "name": "list_snapshots.Snapshots.peakmem_list_snapshots",
        "param_names": [
            "storage",
            "num_symbols",
            "num_snapshots",
            "metadata_entries",
            "load_metadata"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "1",
                "1000"
            ],
            [
                "1",
                "1000"
            ],
            [
                "0",
                "10000"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "list_snapshots:45",
        "timeout": 3000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4597353428e441b79a00a18fa58c2bb98bdd2257331524e2581cf26c8b3f82c9"
    },
    "list_snapshots.Snapshots.time_list_snapshots": {
        "code": "class Snapshots:\n    def time_list_snapshots(\n        self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata\n    ):\n        res = self.lib.list_snapshots(load_metadata=load_metadata)\n        assert len(res) == num_snapshots, f\"Expected {num_snapshots} snapshots but were {len(res)}\"\n\n    def setup(self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata):\n        self.lib = libs_for_storage[storage][get_lib_name(num_symbols, num_snapshots, metadata_entries)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        write_parameters = list(itertools.product(self.num_symbols, self.num_snapshots, self.metadata_entries))\n        assert write_parameters\n        libs_for_storage = dict()\n        library_names = [\n            get_lib_name(num_syms=n_syms, num_snaps=n_snaps, metadata_size=md_size)\n            for n_syms, n_snaps, md_size in write_parameters\n        ]\n        simple_df = pd.DataFrame({\"a\": [1]})\n    \n        for storage in self.storages:\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            if not is_storage_enabled(storage):\n                continue\n    \n            for n_syms, n_snaps, md_size in write_parameters:\n                lib_name = get_lib_name(n_syms, n_snaps, md_size)\n                lib = libs_for_storage[storage][lib_name]\n                print(f\"lib_name={lib_name}, lib={lib}\", file=sys.stderr)\n                if lib is None:\n                    continue\n                writes = [WritePayload(f\"sym_{i}\", simple_df) for i in range(n_syms)]\n    \n                lib.write_batch(writes)\n    \n                metadata = get_metadata(md_size)\n                for i in range(n_snaps):\n                    lib.snapshot(f\"snap_{i}\", metadata=metadata)\n    \n        return libs_for_storage",
        "min_run_count": 2,
        "name": "list_snapshots.Snapshots.time_list_snapshots",
        "number": 0,
        "param_names": [
            "storage",
            "num_symbols",
            "num_snapshots",
            "metadata_entries",
            "load_metadata"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "1",
                "1000"
            ],
            [
                "1",
                "1000"
            ],
            [
                "0",
                "10000"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:45",
        "timeout": 3000,
        "type": "time",
        "unit": "seconds",
        "version": "c292f6f8757182d8ac67ddb990a81389675b5d264b32a5b2f720514bf703f6a9",
        "warmup_time": -1
    },
    "list_symbols.ListSymbolsWithoutCache.peakmem_list_symbols": {
        "code": "class ListSymbolsWithoutCache:\n    def peakmem_list_symbols(self, *args):\n        self._check_test_counter()\n        self.lib.list_symbols()\n\n    def setup(self, lib_for_storage, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.test_counter = 1\n    \n        simple_df = pd.DataFrame({\"a\": [1]})\n        write_payloads = [WritePayload(f\"{i}\", simple_df) for i in range(num_symbols)]\n        self.lib.write_batch(write_payloads)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "name": "list_symbols.ListSymbolsWithoutCache.peakmem_list_symbols",
        "param_names": [
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "100",
                "1000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "list_symbols:39",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a14720848297f912d486f8af9f7e910ffce6362511a459d4a7115f6090ec3a88"
    },
    "list_symbols.ListSymbolsWithoutCache.time_has_symbol": {
        "code": "class ListSymbolsWithoutCache:\n    def time_has_symbol(self, *args):\n        self._check_test_counter()\n        for i in range(50, 150):\n            self.lib.has_symbol(f\"{i}\")\n\n    def setup(self, lib_for_storage, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.test_counter = 1\n    \n        simple_df = pd.DataFrame({\"a\": [1]})\n        write_payloads = [WritePayload(f\"{i}\", simple_df) for i in range(num_symbols)]\n        self.lib.write_batch(write_payloads)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "list_symbols.ListSymbolsWithoutCache.time_has_symbol",
        "number": 1,
        "param_names": [
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "100",
                "1000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_symbols:39",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "eb6f2f7d851aa1291ea7212dd0ab46e0f861324a868f66ea7f88c5b1228091af",
        "warmup_time": 0
    },
    "list_symbols.ListSymbolsWithoutCache.time_list_symbols": {
        "code": "class ListSymbolsWithoutCache:\n    def time_list_symbols(self, *args):\n        self._check_test_counter()\n        self.lib.list_symbols()\n\n    def setup(self, lib_for_storage, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.test_counter = 1\n    \n        simple_df = pd.DataFrame({\"a\": [1]})\n        write_payloads = [WritePayload(f\"{i}\", simple_df) for i in range(num_symbols)]\n        self.lib.write_batch(write_payloads)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "list_symbols.ListSymbolsWithoutCache.time_list_symbols",
        "number": 1,
        "param_names": [
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "100",
                "1000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_symbols:39",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "843dd34bf1cd6875d10798c9616b378671efa66c112b250876340d4ad6ba8ddc",
        "warmup_time": 0
    },
    "list_versions.ListVersions.peakmem_list_versions": {
        "code": "class ListVersions:\n    def peakmem_list_versions(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        assert self.lib.list_versions(\n            symbol=symbol, snapshot=snapshot, latest_only=latest_only, skip_snapshots=skip_snapshots\n        )\n\n    def setup(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        self.check_if_skipped_parameter(\n            storage, num_symbols, num_versions, num_snapshots, symbol, snapshot, latest_only, skip_snapshots\n        )\n        self.lib = lib_for_storage[storage][self._lib_name(num_symbols, num_versions, num_snapshots)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        storages = self.params[0]\n        num_symbols = self.params[1]\n        num_versions = self.params[2]\n        num_snapshots = self.params[3]\n    \n        libs_for_storage = dict()\n    \n        library_names = [\n            self._lib_name(n_symbols, n_versions, n_snaps)\n            for n_symbols, n_versions, n_snaps in itertools.product(num_symbols, num_versions, num_snapshots)\n        ]\n    \n        for storage in storages:\n            if not is_storage_enabled(storage):\n                continue\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            for syms in num_symbols:\n                write_payloads = [WritePayload(self._sym_name(sym), 0) for sym in range(syms)]\n                for versions in num_versions:\n                    for snapshots in num_snapshots:\n                        lib_name = self._lib_name(syms, versions, snapshots)\n                        lib = libs_for_storage[storage][lib_name]\n                        for _ in range(versions):\n                            lib.write_pickle_batch(write_payloads)\n    \n                        for snapshot in range(snapshots):\n                            lib.snapshot(\n                                self._snap_name(snapshot),\n                                versions={self._sym_name(sym): random.randint(0, versions - 1) for sym in range(syms)},\n                            )\n    \n        return libs_for_storage",
        "name": "list_versions.ListVersions.peakmem_list_versions",
        "param_names": [
            "storage",
            "num_symbols",
            "num_versions",
            "num_snapshots",
            "symbol",
            "snapshot",
            "latest_only",
            "skip_snapshots"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "10",
                "100"
            ],
            [
                "1",
                "10"
            ],
            [
                "0",
                "100"
            ],
            [
                "None",
                "'0_sym'"
            ],
            [
                "None",
                "'0_snap'"
            ],
            [
                "True",
                "False"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "list_versions:79",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "85b027e3b7e31f6c03592d2354402f6eed8251d0848f8670d9450b3428fa75cd"
    },
    "list_versions.ListVersions.time_list_versions": {
        "code": "class ListVersions:\n    def time_list_versions(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        assert self.lib.list_versions(\n            symbol=symbol, snapshot=snapshot, latest_only=latest_only, skip_snapshots=skip_snapshots\n        )\n\n    def setup(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        self.check_if_skipped_parameter(\n            storage, num_symbols, num_versions, num_snapshots, symbol, snapshot, latest_only, skip_snapshots\n        )\n        self.lib = lib_for_storage[storage][self._lib_name(num_symbols, num_versions, num_snapshots)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        storages = self.params[0]\n        num_symbols = self.params[1]\n        num_versions = self.params[2]\n        num_snapshots = self.params[3]\n    \n        libs_for_storage = dict()\n    \n        library_names = [\n            self._lib_name(n_symbols, n_versions, n_snaps)\n            for n_symbols, n_versions, n_snaps in itertools.product(num_symbols, num_versions, num_snapshots)\n        ]\n    \n        for storage in storages:\n            if not is_storage_enabled(storage):\n                continue\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            for syms in num_symbols:\n                write_payloads = [WritePayload(self._sym_name(sym), 0) for sym in range(syms)]\n                for versions in num_versions:\n                    for snapshots in num_snapshots:\n                        lib_name = self._lib_name(syms, versions, snapshots)\n                        lib = libs_for_storage[storage][lib_name]\n                        for _ in range(versions):\n                            lib.write_pickle_batch(write_payloads)\n    \n                        for snapshot in range(snapshots):\n                            lib.snapshot(\n                                self._snap_name(snapshot),\n                                versions={self._sym_name(sym): random.randint(0, versions - 1) for sym in range(syms)},\n                            )\n    \n        return libs_for_storage",
        "min_run_count": 2,
        "name": "list_versions.ListVersions.time_list_versions",
        "number": 5,
        "param_names": [
            "storage",
            "num_symbols",
            "num_versions",
            "num_snapshots",
            "symbol",
            "snapshot",
            "latest_only",
            "skip_snapshots"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "10",
                "100"
            ],
            [
                "1",
                "10"
            ],
            [
                "0",
                "100"
            ],
            [
                "None",
                "'0_sym'"
            ],
            [
                "None",
                "'0_snap'"
            ],
            [
                "True",
                "False"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_versions:79",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "2238d0faf29b5206ab9dd61d1acea68ae7883b8ac95bc8c0fafaefb85ff1527b",
        "warmup_time": 0.1
    },
    "modification_functions.DeleteMultipleVersions.time_delete_multiple_versions": {
        "code": "class DeleteMultipleVersions:\n    def time_delete_multiple_versions(self):\n        self.lib.delete(self.sym, list(range(99)))\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.ac.delete_library(\"test_lib\")\n        self.lib = self.ac.create_library(\"test_lib\")\n        self.sym = f\"sym_{random.randint(0, 1_000_000)}\"\n    \n        for i in range(100):\n            self.lib.write(self.sym, pd.DataFrame({\"a\": [0]}), prune_previous_versions=False)",
        "min_run_count": 2,
        "name": "modification_functions.DeleteMultipleVersions.time_delete_multiple_versions",
        "number": 1,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "type": "time",
        "unit": "seconds",
        "version": "5ff8e79225dba7a420094c75efc2b950b018c92c5dafe32e54c2ec67f7b5054f",
        "warmup_time": 0
    },
    "modification_functions.DeleteOverTimeModificationFunctions.time_delete_over_time": {
        "code": "class DeleteOverTimeModificationFunctions:\n    def time_delete_over_time(self):\n        sym = f\"sym_{random.randint(0, 1_000_000)}\"\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(sym, pd.DataFrame({\"a\": [1]}))\n                self.lib.delete(sym)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.ac.delete_library(\"test_lib\")\n        self.lib = self.ac.create_library(\"test_lib\")",
        "min_run_count": 2,
        "name": "modification_functions.DeleteOverTimeModificationFunctions.time_delete_over_time",
        "number": 0,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "type": "time",
        "unit": "seconds",
        "version": "9fccfe476a9df615a7647e269f078d8d154caf9a77a75d0666b465c250e09aba",
        "warmup_time": -1
    },
    "modification_functions.Deletion.time_delete": {
        "code": "class Deletion:\n    def time_delete(self, *args):\n        assert self.lib.has_symbol(\"sym\")\n        self.lib.delete(\"sym\")\n\n    def setup(self, lib_for_storage, rows_and_cols, storage):\n        lib = lib_for_storage[storage]\n        if lib is None:\n            raise SkipNotImplemented\n        self.lib = lib\n        self.lib._nvs.version_store.clear()\n    \n        rows, cols = rows_and_cols\n        df = generate_random_floats_dataframe_with_index(num_rows=rows, num_cols=cols)\n        self.lib.write(\"sym\", df)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(Deletion.storages)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "modification_functions.Deletion.time_delete",
        "number": 1,
        "param_names": [
            "rows_and_cols",
            "storage"
        ],
        "params": [
            [
                "(1000000, 2)",
                "(10000000, 2)",
                "(5000, 30000)"
            ],
            [
                "<Storage.AMAZON: 1>",
                "<Storage.LMDB: 2>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:128",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "c17ba8e2a399ad6fdc7bfc55cbd281b444e1d51b6d8d30e5b92f69773ab9c82f",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_append_large": {
        "code": "class ModificationFunctions:\n    def time_append_large(self, *args):\n        self.lib.append(self.sym, self.df_append_large)\n\n    def setup(self, libs_for_storage, rows_and_cols, storage):\n        self.lib = libs_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        rows, cols = rows_and_cols\n    \n        self.sym = _sym_name(rows, cols)\n        assert self.lib.has_symbol(self.sym)\n        self.lib._nvs.restore_version(self.sym, 0)\n        assert self.lib.get_description(self.sym).row_count == rows\n    \n        self.df_update_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(0.5, rows)\n        )\n        self.df_update_half = generate_random_floats_dataframe_with_index(\n            rows // 2, cols, end_timestamp=get_time_at_fraction_of_df(0.75, rows)\n        )\n        self.df_update_outside_date_range = generate_random_floats_dataframe_with_index(\n            rows, cols, end_timestamp=get_time_at_fraction_of_df(1.5, rows)\n        )\n        self.df_append_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(1.1, rows)\n        )\n        self.df_append_large = generate_random_floats_dataframe_with_index(rows, cols)\n        append_index = pd.date_range(start=\"1/2/2023\", periods=rows, freq=\"ms\")\n        self.df_append_large.index = append_index\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_append_large",
        "number": 1,
        "param_names": [
            "rows_and_cols",
            "storage"
        ],
        "params": [
            [
                "(1000000, 2)",
                "(10000000, 2)",
                "(5000, 30000)"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:51",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "32a9cc94de517e097bf5bff157750bcff696b67ce78c3853c59f8863ac23ff4e",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_append_single": {
        "code": "class ModificationFunctions:\n    def time_append_single(self, *args):\n        self.lib.append(self.sym, self.df_append_single)\n\n    def setup(self, libs_for_storage, rows_and_cols, storage):\n        self.lib = libs_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        rows, cols = rows_and_cols\n    \n        self.sym = _sym_name(rows, cols)\n        assert self.lib.has_symbol(self.sym)\n        self.lib._nvs.restore_version(self.sym, 0)\n        assert self.lib.get_description(self.sym).row_count == rows\n    \n        self.df_update_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(0.5, rows)\n        )\n        self.df_update_half = generate_random_floats_dataframe_with_index(\n            rows // 2, cols, end_timestamp=get_time_at_fraction_of_df(0.75, rows)\n        )\n        self.df_update_outside_date_range = generate_random_floats_dataframe_with_index(\n            rows, cols, end_timestamp=get_time_at_fraction_of_df(1.5, rows)\n        )\n        self.df_append_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(1.1, rows)\n        )\n        self.df_append_large = generate_random_floats_dataframe_with_index(rows, cols)\n        append_index = pd.date_range(start=\"1/2/2023\", periods=rows, freq=\"ms\")\n        self.df_append_large.index = append_index\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_append_single",
        "number": 1,
        "param_names": [
            "rows_and_cols",
            "storage"
        ],
        "params": [
            [
                "(1000000, 2)",
                "(10000000, 2)",
                "(5000, 30000)"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:51",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "49c73b3f13c02528fd4d86c53972a65f71734c613fe70930029d8cc8db1ffb1c",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_update_half": {
        "code": "class ModificationFunctions:\n    def time_update_half(self, *args):\n        self.lib.update(self.sym, self.df_update_half)\n\n    def setup(self, libs_for_storage, rows_and_cols, storage):\n        self.lib = libs_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        rows, cols = rows_and_cols\n    \n        self.sym = _sym_name(rows, cols)\n        assert self.lib.has_symbol(self.sym)\n        self.lib._nvs.restore_version(self.sym, 0)\n        assert self.lib.get_description(self.sym).row_count == rows\n    \n        self.df_update_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(0.5, rows)\n        )\n        self.df_update_half = generate_random_floats_dataframe_with_index(\n            rows // 2, cols, end_timestamp=get_time_at_fraction_of_df(0.75, rows)\n        )\n        self.df_update_outside_date_range = generate_random_floats_dataframe_with_index(\n            rows, cols, end_timestamp=get_time_at_fraction_of_df(1.5, rows)\n        )\n        self.df_append_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(1.1, rows)\n        )\n        self.df_append_large = generate_random_floats_dataframe_with_index(rows, cols)\n        append_index = pd.date_range(start=\"1/2/2023\", periods=rows, freq=\"ms\")\n        self.df_append_large.index = append_index\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_update_half",
        "number": 1,
        "param_names": [
            "rows_and_cols",
            "storage"
        ],
        "params": [
            [
                "(1000000, 2)",
                "(10000000, 2)",
                "(5000, 30000)"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:51",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "177355a8cc7d43d09fd9958c0da1c443b58461dd703f9aa77cb2ba7d4cea5d15",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_update_outside_date_range": {
        "code": "class ModificationFunctions:\n    def time_update_outside_date_range(self, *args):\n        self.lib.update(self.sym, self.df_update_outside_date_range, upsert=True)\n\n    def setup(self, libs_for_storage, rows_and_cols, storage):\n        self.lib = libs_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        rows, cols = rows_and_cols\n    \n        self.sym = _sym_name(rows, cols)\n        assert self.lib.has_symbol(self.sym)\n        self.lib._nvs.restore_version(self.sym, 0)\n        assert self.lib.get_description(self.sym).row_count == rows\n    \n        self.df_update_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(0.5, rows)\n        )\n        self.df_update_half = generate_random_floats_dataframe_with_index(\n            rows // 2, cols, end_timestamp=get_time_at_fraction_of_df(0.75, rows)\n        )\n        self.df_update_outside_date_range = generate_random_floats_dataframe_with_index(\n            rows, cols, end_timestamp=get_time_at_fraction_of_df(1.5, rows)\n        )\n        self.df_append_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(1.1, rows)\n        )\n        self.df_append_large = generate_random_floats_dataframe_with_index(rows, cols)\n        append_index = pd.date_range(start=\"1/2/2023\", periods=rows, freq=\"ms\")\n        self.df_append_large.index = append_index\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_update_outside_date_range",
        "number": 1,
        "param_names": [
            "rows_and_cols",
            "storage"
        ],
        "params": [
            [
                "(1000000, 2)",
                "(10000000, 2)",
                "(5000, 30000)"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:51",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "a69bff9c3b8f61ef96e6df1de568c35c58ff5f2bb3b4efee021b7072b6f4eff2",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_update_single": {
        "code": "class ModificationFunctions:\n    def time_update_single(self, *args):\n        self.lib.update(self.sym, self.df_update_single)\n\n    def setup(self, libs_for_storage, rows_and_cols, storage):\n        self.lib = libs_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        rows, cols = rows_and_cols\n    \n        self.sym = _sym_name(rows, cols)\n        assert self.lib.has_symbol(self.sym)\n        self.lib._nvs.restore_version(self.sym, 0)\n        assert self.lib.get_description(self.sym).row_count == rows\n    \n        self.df_update_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(0.5, rows)\n        )\n        self.df_update_half = generate_random_floats_dataframe_with_index(\n            rows // 2, cols, end_timestamp=get_time_at_fraction_of_df(0.75, rows)\n        )\n        self.df_update_outside_date_range = generate_random_floats_dataframe_with_index(\n            rows, cols, end_timestamp=get_time_at_fraction_of_df(1.5, rows)\n        )\n        self.df_append_single = generate_random_floats_dataframe_with_index(\n            1, cols, end_timestamp=get_time_at_fraction_of_df(1.1, rows)\n        )\n        self.df_append_large = generate_random_floats_dataframe_with_index(rows, cols)\n        append_index = pd.date_range(start=\"1/2/2023\", periods=rows, freq=\"ms\")\n        self.df_append_large.index = append_index\n\n    def setup_cache(self):\n        start = time.time()\n        libs_for_storage = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return libs_for_storage",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_update_single",
        "number": 1,
        "param_names": [
            "rows_and_cols",
            "storage"
        ],
        "params": [
            [
                "(1000000, 2)",
                "(10000000, 2)",
                "(5000, 30000)"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:51",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "eee3f8539d658de25ad38be90b670970a13fc5030a402331fbe2e1a191fe18ff",
        "warmup_time": 0
    },
    "query_builder.QueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, *args):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b9f803274f2a3faa5ea7e421370c808397f3198f12d1cc06bf034b28a353784d"
    },
    "query_builder.QueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, lib_for_storage, num_rows, storage):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4a453d4ce8e5ecf76b202279fab7c38a66974cdb108a66037dcc8d04741f5f79"
    },
    "query_builder.QueryBuilderFunctions.peakmem_projection": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_projection(self, *args):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9e614afe71db7792c672ae0a860437217a0a4f87293318363ed90190e7033259"
    },
    "query_builder.QueryBuilderFunctions.peakmem_query_1": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_query_1(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e414dd1e883e4d39957fdf690dbd35e54830167af27a5379c23c1e8451a184dd"
    },
    "query_builder.QueryBuilderFunctions.peakmem_query_3": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_query_3(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fdb6378d8f4151d61530ee4e0f3db7dd1dbf01aa09d60e4c5bbc647bc7ae31e7"
    },
    "query_builder.QueryBuilderFunctions.peakmem_query_4": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_query_4(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "810cdde366536268b1dac926c600c95ce25db177c9a6daf79a9c1a2220d5bddf"
    },
    "query_builder.QueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class QueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "name": "query_builder.QueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "dca72a263e021b08204f3ef8deb8f78e355189494cf7178f270c95f1f569a1cd"
    },
    "query_builder.QueryBuilderFunctions.time_filtering_numeric": {
        "code": "class QueryBuilderFunctions:\n    def time_filtering_numeric(self, *args):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_filtering_numeric",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "9c9e9cd8492f587c0ffc69bc7367905281a11229bea0b1f7042214de7895f436",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class QueryBuilderFunctions:\n    def time_filtering_string_isin(self, lib_for_storage, num_rows, storage):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_filtering_string_isin",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "e3264e4f0b655d3b381488fe3d0906362671d2e640b3ca0c729a644d237e0bb1",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_filtering_string_regex_match": {
        "code": "class QueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, *args):\n        pattern = r\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_filtering_string_regex_match",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "57afa0ac498853e31288aced64f87c946d84a2db67af6058f77bc8bd6cd6ec32",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_projection": {
        "code": "class QueryBuilderFunctions:\n    def time_projection(self, *args):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_projection",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "60d5bdab793a5bfc17913dd4f0af6224f86247b49afc9a064384b640c66f460f",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_query_1": {
        "code": "class QueryBuilderFunctions:\n    def time_query_1(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_query_1",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "ba8983e5c8df1cc9b61131f42ac04147af2bbca98b230f208b048dd1a6a80070",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_query_3": {
        "code": "class QueryBuilderFunctions:\n    def time_query_3(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_query_3",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "90510c3d0cf383bd728b23b90b4c30ed14423d81d410c8ebb477e52793733fde",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_query_4": {
        "code": "class QueryBuilderFunctions:\n    def time_query_4(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_query_4",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "32c12c7f850640e929401b67cc7be03231a37994a4dffdea7dc228e8765c7293",
        "warmup_time": 0.2
    },
    "query_builder.QueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class QueryBuilderFunctions:\n    def time_query_adv_query_2(self, *args):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(self.symbol, query_builder=q)\n\n    def setup(self, lib_for_storage, num_rows, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.symbol = _symbol_name(num_rows)\n\n    def setup_cache(self):\n        start = time.time()\n        lib_for_storage = create_libraries_across_storages(self.storages)\n    \n        for rows in QueryBuilderFunctions.num_rows:\n            df = generate_benchmark_df(rows)\n            sym = _symbol_name(rows)\n            for storage in QueryBuilderFunctions.storages:\n                if not is_storage_enabled(storage):\n                    continue\n                lib = lib_for_storage[storage]\n                self.logger.info(f\"writing {df.shape} under {sym}\")\n                lib.write(sym, df)\n    \n        self.logger.info(f\"setup_cache time: {time.time() - start}\")\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "query_builder.QueryBuilderFunctions.time_query_adv_query_2",
        "number": 0,
        "param_names": [
            "num_rows",
            "storages"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "query_builder:42",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "8f0f2114a5f0bb59e2c5857dfa305d134a8464c4270722fff6758ed44488a257",
        "warmup_time": 0.2
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_batch_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def peakmem_read_batch_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read_batch([get_symbol_name(num_dict_entry, i) for i in range(num_symbols)])\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_batch_nested_dict",
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "setup_cache_key": "recursive_normalizer:36",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "16dc7deb22ab10bbfebcb212e39030a1d8cb11a9b96418ebbd26fa1c2895ba3a"
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def peakmem_read_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read(get_symbol_name(num_dict_entry, 0))\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_nested_dict",
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "setup_cache_key": "recursive_normalizer:36",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "43ceef75cd18db9646dfe1635071f294e5faaf811066e3c31a632c0a4a05dd8c"
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_batch_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def time_read_batch_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read_batch([get_symbol_name(num_dict_entry, i) for i in range(num_symbols)])\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "min_run_count": 2,
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_batch_nested_dict",
        "number": 0,
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "recursive_normalizer:36",
        "type": "time",
        "unit": "seconds",
        "version": "bdd744509146dd4994139db5d82b087b93ec5c51109a23283b955b546efc13f1",
        "warmup_time": -1
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def time_read_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read(get_symbol_name(num_dict_entry, 0))\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "min_run_count": 2,
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_nested_dict",
        "number": 0,
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "recursive_normalizer:36",
        "type": "time",
        "unit": "seconds",
        "version": "9e8238a61a6de72778522f023e8d65de5be7558826343557dec84e06baa42663",
        "warmup_time": -1
    },
    "recursive_normalizer.LMDBRecursiveNormalizerWrite.peakmem_write_nested_dict": {
        "code": "class LMDBRecursiveNormalizerWrite:\n    def peakmem_write_nested_dict(self, num_dict_entry):\n        assert len(self.data) == num_dict_entry\n        self.lib._nvs.write(\n            f\"nested_dict_peakmem_write_nested_dict_{num_dict_entry}\",\n            self.data,\n            recursive_normalizers=True,\n        )\n\n    def setup(self, num_dict_entry):\n        self.lib = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI).get_library(\"lib\")\n        self.data = get_data(num_dict_entry)\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        ac.create_library(\"lib\")",
        "name": "recursive_normalizer.LMDBRecursiveNormalizerWrite.peakmem_write_nested_dict",
        "param_names": [
            "num_dict_entries"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "setup_cache_key": "recursive_normalizer:80",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9dcfed4cee0cbe00634c7e5f8eb217fe11cbb4069242637e41d00a1ff99aed4c"
    },
    "recursive_normalizer.LMDBRecursiveNormalizerWrite.time_write_nested_dict": {
        "code": "class LMDBRecursiveNormalizerWrite:\n    def time_write_nested_dict(self, num_dict_entry):\n        assert len(self.data) == num_dict_entry\n        self.lib._nvs.write(\n            f\"nested_dict_time_write_nested_dict_{num_dict_entry}\",\n            self.data,\n            recursive_normalizers=True,\n        )\n\n    def setup(self, num_dict_entry):\n        self.lib = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI).get_library(\"lib\")\n        self.data = get_data(num_dict_entry)\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        ac.create_library(\"lib\")",
        "min_run_count": 2,
        "name": "recursive_normalizer.LMDBRecursiveNormalizerWrite.time_write_nested_dict",
        "number": 0,
        "param_names": [
            "num_dict_entries"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "recursive_normalizer:80",
        "type": "time",
        "unit": "seconds",
        "version": "a2dfcb01776d52d843c514b129d2256a8031360caa00bc24b890b6dad9246191",
        "warmup_time": -1
    },
    "resample.Resample.peakmem_resample": {
        "code": "class Resample:\n    @skip_for_params(TIME_PARAMS)\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            self.skipped = True\n            raise NotImplementedError(f\"{aggregation} not supported on columns of type {col_type}\")\n    \n        self.skipped = False\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "resample.Resample.peakmem_resample",
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "3000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "setup_cache_key": "resample:53",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "20835daa6c6fd2af13420212c35b87cdac7153f782b2bc02cb2535131bd654d8"
    },
    "resample.Resample.time_resample": {
        "code": "class Resample:\n    @skip_for_params(PEAKMEM_PARAMS)\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            self.skipped = True\n            raise NotImplementedError(f\"{aggregation} not supported on columns of type {col_type}\")\n    \n        self.skipped = False\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "resample.Resample.time_resample",
        "number": 5,
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "3000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:53",
        "type": "time",
        "unit": "seconds",
        "version": "5a8f9ae968dbf11481dbe381bcb072c86f46190c9e10951ab2ec80ad9c9f116e",
        "warmup_time": -1
    },
    "resample.ResampleWide.peakmem_resample_wide": {
        "code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "name": "resample.ResampleWide.peakmem_resample_wide",
        "param_names": [],
        "params": [],
        "setup_cache_key": "resample:129",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"
    },
    "resample.ResampleWide.time_resample_wide": {
        "code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "min_run_count": 2,
        "name": "resample.ResampleWide.time_resample_wide",
        "number": 0,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:129",
        "type": "time",
        "unit": "seconds",
        "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2",
        "warmup_time": -1
    },
    "version": 2,
    "version_chain.IterateVersionChain.track_num_ver_reads_list_undeleted_versions": {
        "code": "class IterateVersionChain:\n    def track_num_ver_reads_list_undeleted_versions(self, num_versions, caching, deleted):\n        query_stats.reset_stats()\n        self.lib.list_versions(symbol=self.symbol(num_versions))\n        stats = query_stats.get_query_stats()\n        return count_version_reads(stats)\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        ac = self._setup()\n        if deleted:\n            self.lib = ac[IterateVersionChain.LIB_NAME_DELETED]\n        else:\n            self.lib = ac[IterateVersionChain.LIB_NAME_UNDELETED]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n        query_stats.enable()",
        "name": "version_chain.IterateVersionChain.track_num_ver_reads_list_undeleted_versions",
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "100"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "type": "track",
        "unit": "unit",
        "version": "6c43bd4b095463b0dedc0491cefbe92815b55b3fe31093e80d9fd8ab5cffb1e2"
    },
    "version_chain.IterateVersionChain.track_num_ver_reads_load_all_versions": {
        "code": "class IterateVersionChain:\n    def track_num_ver_reads_load_all_versions(self, num_versions, caching, deleted):\n        query_stats.reset_stats()\n        self.load_all(self.symbol(num_versions))\n        stats = query_stats.get_query_stats()\n        return count_version_reads(stats)\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        ac = self._setup()\n        if deleted:\n            self.lib = ac[IterateVersionChain.LIB_NAME_DELETED]\n        else:\n            self.lib = ac[IterateVersionChain.LIB_NAME_UNDELETED]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n        query_stats.enable()",
        "name": "version_chain.IterateVersionChain.track_num_ver_reads_load_all_versions",
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "100"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "type": "track",
        "unit": "unit",
        "version": "62bfd1aaf7ff82577a691181423531d06ed7a55d29e6e90203101c5f45516c9a"
    },
    "version_chain.IterateVersionChain.track_num_ver_reads_read_from_epoch": {
        "code": "class IterateVersionChain:\n    def track_num_ver_reads_read_from_epoch(self, num_versions, caching, deleted):\n        query_stats.reset_stats()\n        self.read_from_epoch(self.symbol(num_versions))\n        stats = query_stats.get_query_stats()\n        return count_version_reads(stats)\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        ac = self._setup()\n        if deleted:\n            self.lib = ac[IterateVersionChain.LIB_NAME_DELETED]\n        else:\n            self.lib = ac[IterateVersionChain.LIB_NAME_UNDELETED]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n        query_stats.enable()",
        "name": "version_chain.IterateVersionChain.track_num_ver_reads_read_from_epoch",
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "100"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "type": "track",
        "unit": "unit",
        "version": "b2c92fce8c863add9f716d7a2cefdca637b2117d8ba1ac7b0b8544cfa8e45015"
    },
    "version_chain.IterateVersionChain.track_num_ver_reads_read_v0": {
        "code": "class IterateVersionChain:\n    def track_num_ver_reads_read_v0(self, num_versions, caching, deleted):\n        query_stats.reset_stats()\n        self.read_v0(self.symbol(num_versions))\n        stats = query_stats.get_query_stats()\n        return count_version_reads(stats)\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        ac = self._setup()\n        if deleted:\n            self.lib = ac[IterateVersionChain.LIB_NAME_DELETED]\n        else:\n            self.lib = ac[IterateVersionChain.LIB_NAME_UNDELETED]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n        query_stats.enable()",
        "name": "version_chain.IterateVersionChain.track_num_ver_reads_read_v0",
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "100"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "type": "track",
        "unit": "unit",
        "version": "0ed06e659040e01d1c02c81778c1e343d0f7281301cd2b149a8f5afac64b8a0f"
    },
    "version_chain.IterateVersionChain.track_num_ver_reads_time_read_alternating": {
        "code": "class IterateVersionChain:\n    def track_num_ver_reads_time_read_alternating(self, num_versions, caching, deleted):\n        query_stats.reset_stats()\n        self.read_from_epoch(self.symbol(num_versions))\n        self.read_v0(self.symbol(num_versions))\n        stats = query_stats.get_query_stats()\n        return count_version_reads(stats)\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        ac = self._setup()\n        if deleted:\n            self.lib = ac[IterateVersionChain.LIB_NAME_DELETED]\n        else:\n            self.lib = ac[IterateVersionChain.LIB_NAME_UNDELETED]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n        query_stats.enable()",
        "name": "version_chain.IterateVersionChain.track_num_ver_reads_time_read_alternating",
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "100"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "type": "track",
        "unit": "unit",
        "version": "0055459dae3b5d58af29abca982fcdd1e187ff18810517da4c8f61dd7ec78e57"
    }
}