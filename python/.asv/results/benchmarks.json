{
    "arrow.ArrowNumeric.peakmem_read": {
        "code": "class ArrowNumeric:\n    def peakmem_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowNumeric.peakmem_read",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fea10ab7115b7e7be437e715eeb12d0e612b564841f428284d671d5cb7ac19bd"
    },
    "arrow.ArrowNumeric.peakmem_write": {
        "code": "class ArrowNumeric:\n    def peakmem_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowNumeric.peakmem_write",
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b69f2e665657c5347cab7e09a6abe6ba6fd15ba460efd5bb65174d16a86c1e88"
    },
    "arrow.ArrowNumeric.time_read": {
        "code": "class ArrowNumeric:\n    def time_read(self, rows, date_range):\n        self.lib.read(self.symbol_name(rows), date_range=self.date_range)\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowNumeric.time_read",
        "number": 0,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "5a886d69ac180d8a3f8cb7b8d851da8f710cdf2a89ef94bdb6ccee92dfff3408",
        "warmup_time": -1
    },
    "arrow.ArrowNumeric.time_write": {
        "code": "class ArrowNumeric:\n    def time_write(self, rows, date_range):\n        self.fresh_lib.write(f\"sym_{rows}\", self.table, index_column=\"ts\")\n\n    def setup(self, rows, date_range):\n        np.random.seed(42)\n        random.seed(42)\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = pa.Table.from_pandas(generate_pseudo_random_dataframe(rows))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowNumeric.time_write",
        "number": 0,
        "param_names": [
            "rows",
            "date_range"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ],
            [
                "None",
                "'middle'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:37",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "9170a039dc500cb3b447a2654628e4f3b38d37248f0d3627a554d4fe1bd4b46a",
        "warmup_time": -1
    },
    "arrow.ArrowStrings.peakmem_read": {
        "code": "class ArrowStrings:\n    def peakmem_read(self, rows, date_range, unique_string_count, arrow_string_format):\n        self._check_should_run_benchmark(date_range=date_range, arrow_string_format=arrow_string_format)\n        self.lib.read(\n            self.symbol_name(rows, unique_string_count),\n            date_range=self.date_range,\n            arrow_string_format_default=arrow_string_format,\n        )\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowStrings.peakmem_read",
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "44f0a54520c8f9423d047f524c28873223c84717c1073bd180b017f300bd3486"
    },
    "arrow.ArrowStrings.peakmem_write": {
        "code": "class ArrowStrings:\n    def peakmem_write(self, rows, date_range, unique_string_count, arrow_string_format):\n        # No point in running with all read time options\n        if date_range is None and arrow_string_format == ArrowOutputStringFormat.CATEGORICAL:\n            self.fresh_lib.write(self.symbol_name(rows, unique_string_count), self.table, index_column=\"ts\")\n        else:\n            raise SkipNotImplemented\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "arrow.ArrowStrings.peakmem_write",
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f391977543e46b15a88a5882f4f80eb41e5285a9d2fa8398b322a24e3f574a62"
    },
    "arrow.ArrowStrings.time_read": {
        "code": "class ArrowStrings:\n    def time_read(self, rows, date_range, unique_string_count, arrow_string_format):\n        self._check_should_run_benchmark(date_range=date_range, arrow_string_format=arrow_string_format)\n        self.lib.read(\n            self.symbol_name(rows, unique_string_count),\n            date_range=self.date_range,\n            arrow_string_format_default=arrow_string_format,\n        )\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowStrings.time_read",
        "number": 0,
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "08e1367be8f34d8df67bdf98157985f6623a74631e19dcd63924dde8d0e74176",
        "warmup_time": -1
    },
    "arrow.ArrowStrings.time_write": {
        "code": "class ArrowStrings:\n    def time_write(self, rows, date_range, unique_string_count, arrow_string_format):\n        # No point in running with all read time options\n        if date_range is None and arrow_string_format == ArrowOutputStringFormat.CATEGORICAL:\n            self.fresh_lib.write(self.symbol_name(rows, unique_string_count), self.table, index_column=\"ts\")\n        else:\n            raise SkipNotImplemented\n\n    def setup(self, rows, date_range, unique_string_count, arrow_string_format):\n        self.ac = Arctic(self.connection_string, output_format=OutputFormat.PYARROW)\n        self.lib = self.ac.get_library(self.lib_name_prewritten)\n        self.lib._nvs._set_allow_arrow_input()\n        if date_range is None:\n            self.date_range = None\n        else:\n            # Create a date range that excludes the first and last 10 rows of the data only\n            self.date_range = (pd.Timestamp(10), pd.Timestamp(rows - 10))\n        self.fresh_lib = self.get_fresh_lib()\n        self.fresh_lib._nvs._set_allow_arrow_input()\n        self.table = self._generate_table(rows, self.num_cols, unique_string_count)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "arrow.ArrowStrings.time_write",
        "number": 0,
        "param_names": [
            "rows",
            "date_range",
            "unique_string_count",
            "arrow_string_format"
        ],
        "params": [
            [
                "100000",
                "1000000"
            ],
            [
                "None",
                "'middle'"
            ],
            [
                "100",
                "10000"
            ],
            [
                "<ArrowOutputStringFormat.CATEGORICAL: 'CATEGORICAL'>",
                "<ArrowOutputStringFormat.DICTIONARY_ENCODED: 'DICTIONARY_ENCODED'>",
                "<ArrowOutputStringFormat.LARGE_STRING: 'LARGE_STRING'>",
                "<ArrowOutputStringFormat.SMALL_STRING: 'SMALL_STRING'>"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "arrow:111",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "aee7692a967a399e46745ef2b201b715b725357ffdfe81a6219225984d2e2a79",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "basic_functions:41",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4ed305e5b21d4c3df22cc239ad54e2ab9b8acf19c2a689e264e9145eca1a2094"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "basic_functions:41",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0655902640cee72832e909202e4187147f1378b546bd5b3518ab19f4004d5b32"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "basic_functions:41",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a12a8dd0cf3e4d5ab9c8535fcaec9647a533644933a4b5c643a7f99723015fa3"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges_query_builder",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "basic_functions:41",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a0dbc5843fc9f5b31ccf981b2a6b6569880b0841b684163e617368071e48e2c7"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "basic_functions:41",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "530fec1ac85f134bab26649b2579b205a87c1b01dc5d972f324e452e6a4633cd"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "basic_functions:41",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a19002933078e96e583a65bb547e86712f87c841e877a54fba4fce777ac6ab05"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:41",
        "type": "time",
        "unit": "seconds",
        "version": "0790a17ac39caedc1cea887331e48e2b607f4bbdbde59ebdf2a16e220e680c81",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:41",
        "type": "time",
        "unit": "seconds",
        "version": "d66332f438d375470e7b471b2641664ce03a5043765b8aecb701ba028940504d",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:41",
        "type": "time",
        "unit": "seconds",
        "version": "c1fdcddf679a65c0f96b011e2f091c21b3f6fa603b9dd9c3daf20698273e1245",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges_query_builder(self, rows):\n        q = QueryBuilder().date_range(DATE_RANGE)\n        self.lib.read(f\"sym\", query_builder=q).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges_query_builder",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:41",
        "type": "time",
        "unit": "seconds",
        "version": "e30effe8e084ada39a8d4f7e123d6253b483a434a3a59c65fcb1dc2a04377959",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:41",
        "type": "time",
        "unit": "seconds",
        "version": "eb258ad770b5f9d0220957f1fce0410b51de9964c6a1744919abda640feec054",
        "warmup_time": 0.2
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "basic_functions:41",
        "type": "time",
        "unit": "seconds",
        "version": "bb986153ea43fb78204ded8a63c102b4718197158c8f81d6c28e17cfa39d3282",
        "warmup_time": 0.2
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "setup_cache_key": "basic_functions:187",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1154d4d002a16465c10fd5e41721ed25ee8d4a5fa3790c7718d7f309f1b8b29c"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "setup_cache_key": "basic_functions:187",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "eb2f74f4dab10d4472f2349a2d539c9609baaaa6debf7206c064a2b93c495bfc"
    },
    "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "setup_cache_key": "basic_functions:187",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "114711c1c4ebf66c64ab33fb81f5f8cc73ce78984e2472f492d0c80f96f331d8"
    },
    "basic_functions.BatchBasicFunctions.peakmem_write_batch": {
        "code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "setup_cache_key": "basic_functions:187",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e29d7bab3ecd450da657f11c448f62bbb39d5a2607fb6f4cc5b92db2dee50dc9"
    },
    "basic_functions.BatchBasicFunctions.time_read_batch": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch",
        "number": 0,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:187",
        "type": "time",
        "unit": "seconds",
        "version": "95a87e9c330d6f1b83b13cdf87403e69a4c2d4a3992430942454acfd5021b73a",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns",
        "number": 0,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:187",
        "type": "time",
        "unit": "seconds",
        "version": "2c798cded56db601f7a5e81cf0d0e77407d3717695a4c4863d4e68c96618393b",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE) for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 0,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:187",
        "type": "time",
        "unit": "seconds",
        "version": "04e761907c8cd5b7d21ef3beadec4150292a508c0de7c9fd36305f118c97ceb3",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_update_batch": {
        "code": "class BatchBasicFunctions:\n    def time_update_batch(self, rows, num_symbols):\n        payloads = [UpdatePayload(f\"{sym}_sym\", self.update_df) for sym in range(num_symbols)]\n        results = self.lib.update_batch(payloads)\n        assert results[0].version >= 1\n        assert results[-1].version >= 1\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_update_batch",
        "number": 0,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:187",
        "type": "time",
        "unit": "seconds",
        "version": "ae298ed42a2bc58a2d14509b1578a2ecd7c11cab6f15200ef8ebfbc4f9924a27",
        "warmup_time": -1
    },
    "basic_functions.BatchBasicFunctions.time_write_batch": {
        "code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.update_df = generate_pseudo_random_dataframe(rows // 2)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "basic_functions.BatchBasicFunctions.time_write_batch",
        "number": 0,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "25000",
                "50000"
            ],
            [
                "100",
                "200"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.1,
        "setup_cache_key": "basic_functions:187",
        "type": "time",
        "unit": "seconds",
        "version": "49522447c3d9ac6f75f9df9a159dbbaeb95553440cf4bbb98303fce0c490bd66",
        "warmup_time": -1
    },
    "basic_functions.ShortWideRead.peakmem_read": {
        "code": "class ShortWideRead:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym_{rows}\")\n\n    def setup(self, rows):\n        ac = Arctic(ShortWideRead.CONNECTION_STRING)\n        self.lib = ac[\"lib\"]\n\n    def setup_cache(self):\n        ac = Arctic(ShortWideRead.CONNECTION_STRING)\n        lib = ac.create_library(\"lib\")\n        for r in ShortWideRead.ROWS:\n            df = generate_random_floats_dataframe(r, ShortWideRead.COLS)\n            lib.write(f\"sym_{r}\", df)",
        "name": "basic_functions.ShortWideRead.peakmem_read",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1",
                "5000"
            ]
        ],
        "setup_cache_key": "basic_functions:157",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0c9e4ed77543c683169797f90579b45d91798edff513d7b6eeac570b958822eb"
    },
    "basic_functions.ShortWideRead.time_read": {
        "code": "class ShortWideRead:\n    def time_read(self, rows):\n        self.lib.read(f\"sym_{rows}\")\n\n    def setup(self, rows):\n        ac = Arctic(ShortWideRead.CONNECTION_STRING)\n        self.lib = ac[\"lib\"]\n\n    def setup_cache(self):\n        ac = Arctic(ShortWideRead.CONNECTION_STRING)\n        lib = ac.create_library(\"lib\")\n        for r in ShortWideRead.ROWS:\n            df = generate_random_floats_dataframe(r, ShortWideRead.COLS)\n            lib.write(f\"sym_{r}\", df)",
        "min_run_count": 2,
        "name": "basic_functions.ShortWideRead.time_read",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1",
                "5000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:157",
        "type": "time",
        "unit": "seconds",
        "version": "de731d8892af34ed745e455464ee25c7a3144744c6a19440e2b1f4726623ad2c",
        "warmup_time": -1
    },
    "basic_functions.ShortWideWrite.peakmem_write": {
        "code": "class ShortWideWrite:\n    def peakmem_write(self, *args):\n        self.lib.write(\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(ShortWideWrite.CONNECTION_STRING)\n        self.lib = self.ac.create_library(\"lib\")\n        self.df = generate_random_floats_dataframe(rows, ShortWideWrite.COLS)",
        "name": "basic_functions.ShortWideWrite.peakmem_write",
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1",
                "5000"
            ]
        ],
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6f39a68c95fd453da7306d8525332a7b86b00e84643407c6e7e94f9d3560295f"
    },
    "basic_functions.ShortWideWrite.time_write": {
        "code": "class ShortWideWrite:\n    def time_write(self, *args):\n        self.lib.write(\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(ShortWideWrite.CONNECTION_STRING)\n        self.lib = self.ac.create_library(\"lib\")\n        self.df = generate_random_floats_dataframe(rows, ShortWideWrite.COLS)",
        "min_run_count": 2,
        "name": "basic_functions.ShortWideWrite.time_write",
        "number": 0,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1",
                "5000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "type": "time",
        "unit": "seconds",
        "version": "b65576b24b68bea4080deca2d075daaa74a809c4f9d379ea8143e02a7c3b8413",
        "warmup_time": -1
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_all",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "331c9abb4f84b01dd28765b77a88e069ec6d6b70617a12dd5aa9c3e14ca6a6ad"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_filter_two_aggregations(self, times_bigger):\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_filter_two_aggregations",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ae73602827d4d1b739e519e8ca6a847c5938a5744ebf371ca78511b0be1bf16f"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def peakmem_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_groupby_city_count_isin_filter",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e708975c50d2b70ebdff11efa45f9fd15ceee9861301d5552f1e8ebe2cb4d1bd"
    },
    "bi_benchmarks.BIBenchmarks.peakmem_query_readall": {
        "code": "class BIBenchmarks:\n    def peakmem_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "bi_benchmarks.BIBenchmarks.peakmem_query_readall",
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3407eb183cf3c02afbeaf04e6c31bf6b5aaf615458cd8e2ad46a21b4d2af80e2"
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_all(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_all(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_all",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "bf5e390b01e356685500d464be897fe7cb51531dcd92fccedec980f97f361e3c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_filter_two_aggregations(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_filter_two_aggregations(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_filter_two_aggregations",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ceeb3b3e6049c66cb2ecabbb16485e4555cefc7920697c7a34de08993be14af0",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter": {
        "code": "class BIBenchmarks:\n    def time_query_groupby_city_count_isin_filter(self, times_bigger) -> pd.DataFrame:\n        return self.query_groupby_city_count_isin_filter(times_bigger)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_groupby_city_count_isin_filter",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "fa74284d1e48fd396138a5f50c53d92829194b7be1f0caa8f441f8820db4157c",
        "warmup_time": 0
    },
    "bi_benchmarks.BIBenchmarks.time_query_readall": {
        "code": "class BIBenchmarks:\n    def time_query_readall(self, times_bigger):\n        self.lib.read(f\"{self.symbol}{times_bigger}\")\n\n    def setup(self, num_rows):\n        self.ac = Arctic(f\"lmdb://opensource_datasets_{self.lib_name}?map_size=20GB\")\n        self.lib = self.ac.get_library(self.lib_name)\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "bi_benchmarks.BIBenchmarks.time_query_readall",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "1",
                "10"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "bi_benchmarks:68",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "3cd2c7d90725498da459157638eb15b5a3fcc68aa91684951717ed5ab1c8ca63",
        "warmup_time": 0
    },
    "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def peakmem_finalize_staged_data(self, *args):\n        staged_symbols = self.lib.get_staged_symbols()\n        assert self.symbol + \"-mem\" in staged_symbols\n        self.lib.finalize_staged_data(self.symbol + \"-mem\", mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, lib_for_storage, num_chunks, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        assert len(self.lib.list_symbols()) == 0  # check we are in a clean state\n        initial_timestamp = TimestampNumber(0, self.df_generator.TIME_UNIT)\n    \n        list_of_chunks = [10_000] * num_chunks\n    \n        for suffix in (\"-time\", \"-mem\"):\n            symbol = _symbol_name(num_chunks) + suffix\n            stage_chunks(self.lib, symbol, self.df_generator, initial_timestamp, list_of_chunks)\n            self.logger.info(f\"Created Symbol: {symbol}\")\n        self.symbol = _symbol_name(num_chunks)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "name": "finalize_staged_data.FinalizeStagedData.peakmem_finalize_staged_data",
        "param_names": [
            "num_chunks",
            "storage"
        ],
        "params": [
            [
                "10",
                "100"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "finalize_staged_data:45",
        "timeout": 100,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "defbeee92bd3cdca701262dd6032fc454c3f47d67b7f38713a7e8f89e5f719fb"
    },
    "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data": {
        "code": "class FinalizeStagedData:\n    def time_finalize_staged_data(self, *args):\n        staged_symbols = self.lib.get_staged_symbols()\n        assert self.symbol + \"-time\" in staged_symbols\n        self.lib.finalize_staged_data(self.symbol + \"-time\", mode=StagedDataFinalizeMethod.WRITE)\n\n    def setup(self, lib_for_storage, num_chunks, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n    \n        assert len(self.lib.list_symbols()) == 0  # check we are in a clean state\n        initial_timestamp = TimestampNumber(0, self.df_generator.TIME_UNIT)\n    \n        list_of_chunks = [10_000] * num_chunks\n    \n        for suffix in (\"-time\", \"-mem\"):\n            symbol = _symbol_name(num_chunks) + suffix\n            stage_chunks(self.lib, symbol, self.df_generator, initial_timestamp, list_of_chunks)\n            self.logger.info(f\"Created Symbol: {symbol}\")\n        self.symbol = _symbol_name(num_chunks)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "min_run_count": 1,
        "name": "finalize_staged_data.FinalizeStagedData.time_finalize_staged_data",
        "number": 1,
        "param_names": [
            "num_chunks",
            "storage"
        ],
        "params": [
            [
                "10",
                "100"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "finalize_staged_data:45",
        "timeout": 100,
        "type": "time",
        "unit": "seconds",
        "version": "c3ec2b9e88db58166ea78b66b71354d3b13c8ca077e865208dd8848b788f7f8d",
        "warmup_time": 0
    },
    "list_snapshots.Snapshots.peakmem_list_snapshots": {
        "code": "class Snapshots:\n    def peakmem_list_snapshots(\n        self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata\n    ):\n        res = self.lib.list_snapshots(load_metadata=load_metadata)\n        assert len(res) == num_snapshots, f\"Expected {num_snapshots} snapshots but were {len(res)}\"\n\n    def setup(self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata):\n        self.lib = libs_for_storage[storage][get_lib_name(num_symbols, num_snapshots, metadata_entries)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        write_parameters = list(itertools.product(self.num_symbols, self.num_snapshots, self.metadata_entries))\n        assert write_parameters\n        libs_for_storage = dict()\n        library_names = [\n            get_lib_name(num_syms=n_syms, num_snaps=n_snaps, metadata_size=md_size)\n            for n_syms, n_snaps, md_size in write_parameters\n        ]\n        simple_df = pd.DataFrame({\"a\": [1]})\n    \n        for storage in self.storages:\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            if not is_storage_enabled(storage):\n                continue\n    \n            for n_syms, n_snaps, md_size in write_parameters:\n                lib_name = get_lib_name(n_syms, n_snaps, md_size)\n                lib = libs_for_storage[storage][lib_name]\n                print(f\"lib_name={lib_name}, lib={lib}\", file=sys.stderr)\n                if lib is None:\n                    continue\n                writes = [WritePayload(f\"sym_{i}\", simple_df) for i in range(n_syms)]\n    \n                lib.write_batch(writes)\n    \n                metadata = get_metadata(md_size)\n                for i in range(n_snaps):\n                    lib.snapshot(f\"snap_{i}\", metadata=metadata)\n    \n        return libs_for_storage",
        "name": "list_snapshots.Snapshots.peakmem_list_snapshots",
        "param_names": [
            "storage",
            "num_symbols",
            "num_snapshots",
            "metadata_entries",
            "load_metadata"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "1",
                "1000"
            ],
            [
                "1",
                "1000"
            ],
            [
                "0",
                "10000"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "list_snapshots:44",
        "timeout": 3000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4597353428e441b79a00a18fa58c2bb98bdd2257331524e2581cf26c8b3f82c9"
    },
    "list_snapshots.Snapshots.time_list_snapshots": {
        "code": "class Snapshots:\n    def time_list_snapshots(\n        self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata\n    ):\n        res = self.lib.list_snapshots(load_metadata=load_metadata)\n        assert len(res) == num_snapshots, f\"Expected {num_snapshots} snapshots but were {len(res)}\"\n\n    def setup(self, libs_for_storage, storage, num_symbols, num_snapshots, metadata_entries, load_metadata):\n        self.lib = libs_for_storage[storage][get_lib_name(num_symbols, num_snapshots, metadata_entries)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        write_parameters = list(itertools.product(self.num_symbols, self.num_snapshots, self.metadata_entries))\n        assert write_parameters\n        libs_for_storage = dict()\n        library_names = [\n            get_lib_name(num_syms=n_syms, num_snaps=n_snaps, metadata_size=md_size)\n            for n_syms, n_snaps, md_size in write_parameters\n        ]\n        simple_df = pd.DataFrame({\"a\": [1]})\n    \n        for storage in self.storages:\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            if not is_storage_enabled(storage):\n                continue\n    \n            for n_syms, n_snaps, md_size in write_parameters:\n                lib_name = get_lib_name(n_syms, n_snaps, md_size)\n                lib = libs_for_storage[storage][lib_name]\n                print(f\"lib_name={lib_name}, lib={lib}\", file=sys.stderr)\n                if lib is None:\n                    continue\n                writes = [WritePayload(f\"sym_{i}\", simple_df) for i in range(n_syms)]\n    \n                lib.write_batch(writes)\n    \n                metadata = get_metadata(md_size)\n                for i in range(n_snaps):\n                    lib.snapshot(f\"snap_{i}\", metadata=metadata)\n    \n        return libs_for_storage",
        "min_run_count": 2,
        "name": "list_snapshots.Snapshots.time_list_snapshots",
        "number": 0,
        "param_names": [
            "storage",
            "num_symbols",
            "num_snapshots",
            "metadata_entries",
            "load_metadata"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "1",
                "1000"
            ],
            [
                "1",
                "1000"
            ],
            [
                "0",
                "10000"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_snapshots:44",
        "timeout": 3000,
        "type": "time",
        "unit": "seconds",
        "version": "c292f6f8757182d8ac67ddb990a81389675b5d264b32a5b2f720514bf703f6a9",
        "warmup_time": -1
    },
    "list_symbols.ListSymbolsWithoutCache.peakmem_list_symbols": {
        "code": "class ListSymbolsWithoutCache:\n    def peakmem_list_symbols(self, *args):\n        self._check_test_counter()\n        self.lib.list_symbols()\n\n    def setup(self, lib_for_storage, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.test_counter = 1\n    \n        simple_df = pd.DataFrame({\"a\": [1]})\n        write_payloads = [WritePayload(f\"{i}\", simple_df) for i in range(num_symbols)]\n        self.lib.write_batch(write_payloads)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "name": "list_symbols.ListSymbolsWithoutCache.peakmem_list_symbols",
        "param_names": [
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "100",
                "1000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "setup_cache_key": "list_symbols:38",
        "timeout": 600,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a14720848297f912d486f8af9f7e910ffce6362511a459d4a7115f6090ec3a88"
    },
    "list_symbols.ListSymbolsWithoutCache.time_has_symbol": {
        "code": "class ListSymbolsWithoutCache:\n    def time_has_symbol(self, *args):\n        self._check_test_counter()\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, lib_for_storage, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.test_counter = 1\n    \n        simple_df = pd.DataFrame({\"a\": [1]})\n        write_payloads = [WritePayload(f\"{i}\", simple_df) for i in range(num_symbols)]\n        self.lib.write_batch(write_payloads)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "list_symbols.ListSymbolsWithoutCache.time_has_symbol",
        "number": 1,
        "param_names": [
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "100",
                "1000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_symbols:38",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "461e8b2adf939873268e712542116ec4e1f4935443dd70b0d66e951212a44656",
        "warmup_time": 0
    },
    "list_symbols.ListSymbolsWithoutCache.time_list_symbols": {
        "code": "class ListSymbolsWithoutCache:\n    def time_list_symbols(self, *args):\n        self._check_test_counter()\n        self.lib.list_symbols()\n\n    def setup(self, lib_for_storage, num_symbols, storage):\n        self.lib = lib_for_storage[storage]\n        if self.lib is None:\n            raise SkipNotImplemented\n        self.test_counter = 1\n    \n        simple_df = pd.DataFrame({\"a\": [1]})\n        write_payloads = [WritePayload(f\"{i}\", simple_df) for i in range(num_symbols)]\n        self.lib.write_batch(write_payloads)\n\n    def setup_cache(self):\n        lib_for_storage = create_libraries_across_storages(self.storages)\n        return lib_for_storage",
        "min_run_count": 2,
        "name": "list_symbols.ListSymbolsWithoutCache.time_list_symbols",
        "number": 1,
        "param_names": [
            "num_symbols",
            "storage"
        ],
        "params": [
            [
                "100",
                "1000"
            ],
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_symbols:38",
        "timeout": 600,
        "type": "time",
        "unit": "seconds",
        "version": "843dd34bf1cd6875d10798c9616b378671efa66c112b250876340d4ad6ba8ddc",
        "warmup_time": 0
    },
    "list_versions.ListVersions.peakmem_list_versions": {
        "code": "class ListVersions:\n    def peakmem_list_versions(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        assert self.lib.list_versions(\n            symbol=symbol, snapshot=snapshot, latest_only=latest_only, skip_snapshots=skip_snapshots\n        )\n\n    def setup(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        self.check_if_skipped_parameter(\n            storage, num_symbols, num_versions, num_snapshots, symbol, snapshot, latest_only, skip_snapshots\n        )\n        self.lib = lib_for_storage[storage][self._lib_name(num_symbols, num_versions, num_snapshots)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        storages = self.params[0]\n        num_symbols = self.params[1]\n        num_versions = self.params[2]\n        num_snapshots = self.params[3]\n    \n        libs_for_storage = dict()\n    \n        library_names = [\n            self._lib_name(n_symbols, n_versions, n_snaps)\n            for n_symbols, n_versions, n_snaps in itertools.product(num_symbols, num_versions, num_snapshots)\n        ]\n    \n        for storage in storages:\n            if not is_storage_enabled(storage):\n                continue\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            for syms in num_symbols:\n                write_payloads = [WritePayload(self._sym_name(sym), 0) for sym in range(syms)]\n                for versions in num_versions:\n                    for snapshots in num_snapshots:\n                        lib_name = self._lib_name(syms, versions, snapshots)\n                        lib = libs_for_storage[storage][lib_name]\n                        for _ in range(versions):\n                            lib.write_pickle_batch(write_payloads)\n    \n                        for snapshot in range(snapshots):\n                            lib.snapshot(\n                                self._snap_name(snapshot),\n                                versions={self._sym_name(sym): random.randint(0, versions - 1) for sym in range(syms)},\n                            )\n    \n        return libs_for_storage",
        "name": "list_versions.ListVersions.peakmem_list_versions",
        "param_names": [
            "storage",
            "num_symbols",
            "num_versions",
            "num_snapshots",
            "symbol",
            "snapshot",
            "latest_only",
            "skip_snapshots"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "10",
                "100"
            ],
            [
                "1",
                "10"
            ],
            [
                "0",
                "100"
            ],
            [
                "None",
                "'0_sym'"
            ],
            [
                "None",
                "'0_snap'"
            ],
            [
                "True",
                "False"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "list_versions:78",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "85b027e3b7e31f6c03592d2354402f6eed8251d0848f8670d9450b3428fa75cd"
    },
    "list_versions.ListVersions.time_list_versions": {
        "code": "class ListVersions:\n    def time_list_versions(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        assert self.lib.list_versions(\n            symbol=symbol, snapshot=snapshot, latest_only=latest_only, skip_snapshots=skip_snapshots\n        )\n\n    def setup(\n        self,\n        lib_for_storage,\n        storage,\n        num_symbols,\n        num_versions,\n        num_snapshots,\n        symbol,\n        snapshot,\n        latest_only,\n        skip_snapshots,\n    ):\n        self.check_if_skipped_parameter(\n            storage, num_symbols, num_versions, num_snapshots, symbol, snapshot, latest_only, skip_snapshots\n        )\n        self.lib = lib_for_storage[storage][self._lib_name(num_symbols, num_versions, num_snapshots)]\n        if self.lib is None:\n            raise SkipNotImplemented\n\n    def setup_cache(self):\n        storages = self.params[0]\n        num_symbols = self.params[1]\n        num_versions = self.params[2]\n        num_snapshots = self.params[3]\n    \n        libs_for_storage = dict()\n    \n        library_names = [\n            self._lib_name(n_symbols, n_versions, n_snaps)\n            for n_symbols, n_versions, n_snaps in itertools.product(num_symbols, num_versions, num_snapshots)\n        ]\n    \n        for storage in storages:\n            if not is_storage_enabled(storage):\n                continue\n            libraries = create_libraries(storage, library_names)\n            libs_for_storage[storage] = dict(zip(library_names, libraries))\n            for syms in num_symbols:\n                write_payloads = [WritePayload(self._sym_name(sym), 0) for sym in range(syms)]\n                for versions in num_versions:\n                    for snapshots in num_snapshots:\n                        lib_name = self._lib_name(syms, versions, snapshots)\n                        lib = libs_for_storage[storage][lib_name]\n                        for _ in range(versions):\n                            lib.write_pickle_batch(write_payloads)\n    \n                        for snapshot in range(snapshots):\n                            lib.snapshot(\n                                self._snap_name(snapshot),\n                                versions={self._sym_name(sym): random.randint(0, versions - 1) for sym in range(syms)},\n                            )\n    \n        return libs_for_storage",
        "min_run_count": 2,
        "name": "list_versions.ListVersions.time_list_versions",
        "number": 5,
        "param_names": [
            "storage",
            "num_symbols",
            "num_versions",
            "num_snapshots",
            "symbol",
            "snapshot",
            "latest_only",
            "skip_snapshots"
        ],
        "params": [
            [
                "<Storage.LMDB: 2>",
                "<Storage.AMAZON: 1>"
            ],
            [
                "10",
                "100"
            ],
            [
                "1",
                "10"
            ],
            [
                "0",
                "100"
            ],
            [
                "None",
                "'0_sym'"
            ],
            [
                "None",
                "'0_snap'"
            ],
            [
                "True",
                "False"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 0,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "list_versions:78",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "2238d0faf29b5206ab9dd61d1acea68ae7883b8ac95bc8c0fafaefb85ff1527b",
        "warmup_time": 0.1
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ea9562ac454e814117fbe50a9b5d235a37110c1745fa24bd71db420baaf072ef"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "69f411de823212396215fb4912da15f56a16ca931fd2cfcf9533204b56e696ba"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ecadcf0caca3e2382953be630263bde2c01f7ac011ac2a140080e00f3a6ebdad"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cd35c81c46cd4eb64ccab7a6099884a9f99832a5ece8bb0bc387c0ed402f1536"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1806b540ceeb66fb0bc4476c5b07fca6d5d5edb13e441fb4a2471751a99ff7cb"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "923c4661c84571a4abb8cf5d774b2e0ec674bf671b3b11bc26ed561a0c417ef7"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:33",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0af435e83cbba85506f4bbf0fe355dd839b3e55fd81aa7e3600fcb443dc682ee"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "2ac4df4d7d9b744d192742ba5b8c00c2f79a143cd72c4a7b7f63785dea19e219",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "e16e60780a624d94310ad7e6059e97827feb6b4b6bc2d757a1e89f67c5e7ddd5",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_regex_match(self, num_rows):\n        pattern = f\"^id\\d\\d\\d$\"\n        q = QueryBuilder()\n        q = q[q[\"id1\"].regex_match(pattern)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_regex_match",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "104a945e32f971e4b887da7b6cc6e6e728fc78ef6a77c2f6aed0693c24113234",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "f2bd34ad9342852559d34262fc059be4c6f3122909b31ee078847b1a1b93907f",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "af4473b7d93a3c30d6cfc9d8bcde05e126a1df7948b5877c1604f2882a037768",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "9d0961c0292eff90c651a1989e525c7ce5ab63baa888ac4e2ccdb88b89cc9f2e",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "e32d4b66ff29029ccf51f51511afe9792412ba38d2422d44b03b7e7e8710e38b",
        "warmup_time": 0.2
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 0,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "rounds": 2,
        "sample_time": 2,
        "setup_cache_key": "local_query_builder:33",
        "type": "time",
        "unit": "seconds",
        "version": "2b556ee72fdb8b5b25072c8e0df67fb3264fc3f2fa724c453a6522ef98392f93",
        "warmup_time": 0.2
    },
    "modification_functions.DeleteMultipleVersions.time_delete_multiple_versions": {
        "code": "class DeleteMultipleVersions:\n    def time_delete_multiple_versions(self):\n        self.lib.delete(self.sym, list(range(99)))\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.ac.delete_library(\"test_lib\")\n        self.lib = self.ac.create_library(\"test_lib\")\n        self.sym = f\"sym_{random.randint(0, 1_000_000)}\"\n    \n        for i in range(100):\n            self.lib.write(self.sym, pd.DataFrame({\"a\": [0]}), prune_previous_versions=False)",
        "min_run_count": 2,
        "name": "modification_functions.DeleteMultipleVersions.time_delete_multiple_versions",
        "number": 1,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "type": "time",
        "unit": "seconds",
        "version": "5ff8e79225dba7a420094c75efc2b950b018c92c5dafe32e54c2ec67f7b5054f",
        "warmup_time": 0
    },
    "modification_functions.DeleteOverTimeModificationFunctions.time_delete_over_time": {
        "code": "class DeleteOverTimeModificationFunctions:\n    def time_delete_over_time(self):\n        sym = f\"sym_{random.randint(0, 1_000_000)}\"\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(100):\n                self.lib.write(sym, pd.DataFrame({\"a\": [1]}))\n                self.lib.delete(sym)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.ac.delete_library(\"test_lib\")\n        self.lib = self.ac.create_library(\"test_lib\")",
        "min_run_count": 2,
        "name": "modification_functions.DeleteOverTimeModificationFunctions.time_delete_over_time",
        "number": 0,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "type": "time",
        "unit": "seconds",
        "version": "9fccfe476a9df615a7647e269f078d8d154caf9a77a75d0666b465c250e09aba",
        "warmup_time": -1
    },
    "modification_functions.ModificationFunctions.time_append_large": {
        "code": "class ModificationFunctions:\n    def time_append_large(self, lad: LargeAppendDataModify, rows):\n        large: pd.DataFrame = lad.df_append_large[rows].pop(0)\n        self.lib.append(\"sym\", large)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_append_large",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:61",
        "type": "time",
        "unit": "seconds",
        "version": "fb86070d8db7d4773154494c7220f394f778f12fb7743699da53caa56ef9e8dd",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_append_single": {
        "code": "class ModificationFunctions:\n    def time_append_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.append(\"sym\", self.df_append_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_append_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:61",
        "type": "time",
        "unit": "seconds",
        "version": "38998c47434fe01c7d369271a506dd30542427915409b7bdbcb60659fd2f3cf8",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_delete": {
        "code": "class ModificationFunctions:\n    def time_delete(self, lad: LargeAppendDataModify, rows):\n        self.lib.delete(\"sym\")\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_delete",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:61",
        "type": "time",
        "unit": "seconds",
        "version": "c49859046e3271f14a0b2ad1a8b19bbb81a74971b1f36968829958444d5c626f",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_update_half": {
        "code": "class ModificationFunctions:\n    def time_update_half(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_half)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_update_half",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:61",
        "type": "time",
        "unit": "seconds",
        "version": "f5bf7080565adb1ea9b6a39c44910eb51dde1069676886d2628087176897ef75",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_update_single": {
        "code": "class ModificationFunctions:\n    def time_update_single(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_single)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_update_single",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:61",
        "type": "time",
        "unit": "seconds",
        "version": "c4d4d94b931b60f5e9ef723c994f4688965d1021bc41031d99761221af51b474",
        "warmup_time": 0
    },
    "modification_functions.ModificationFunctions.time_update_upsert": {
        "code": "class ModificationFunctions:\n    def time_update_upsert(self, lad: LargeAppendDataModify, rows):\n        self.lib.update(\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, lad: LargeAppendDataModify, rows):\n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5, rows))\n        self.df_update_half = generate_pseudo_random_dataframe(rows // 2, \"s\", get_time_at_fraction_of_df(0.75, rows))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5, rows))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1, rows))\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n\n    def setup_cache(self):\n        start = time.time()\n        lad = self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")\n        return lad",
        "min_run_count": 2,
        "name": "modification_functions.ModificationFunctions.time_update_upsert",
        "number": 1,
        "param_names": [
            "rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:61",
        "type": "time",
        "unit": "seconds",
        "version": "60f6425aed7b7fe2b97435325f21ac1822a9dea5344f2eb6a14cf554cc9dfce7",
        "warmup_time": 0
    },
    "modification_functions.ShortWideModificationFunctions.time_append_short_wide": {
        "code": "class ShortWideModificationFunctions:\n    def time_append_short_wide(self, wide_append_data: ShortWideAppendData):\n        large: pd.DataFrame = wide_append_data.append_dfs.pop(0)\n        self.lib_short_wide.append(\"short_wide_sym\", large)\n\n    def setup(self, wide_append_data: ShortWideAppendData):\n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(self.WIDE_DF_ROWS, self.WIDE_DF_COLS)\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(self.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        wide_append_data = self._setup_cache()\n        return wide_append_data",
        "min_run_count": 2,
        "name": "modification_functions.ShortWideModificationFunctions.time_append_short_wide",
        "number": 1,
        "param_names": [],
        "params": [],
        "repeat": 2,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:154",
        "type": "time",
        "unit": "seconds",
        "version": "0718f2707bedadfbde65661a5faf806e09236332e3002438a0637f00e8cc6aa0",
        "warmup_time": 0
    },
    "modification_functions.ShortWideModificationFunctions.time_delete_short_wide": {
        "code": "class ShortWideModificationFunctions:\n    def time_delete_short_wide(self, wide_append_data: ShortWideAppendData):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, wide_append_data: ShortWideAppendData):\n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(self.WIDE_DF_ROWS, self.WIDE_DF_COLS)\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(self.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        wide_append_data = self._setup_cache()\n        return wide_append_data",
        "min_run_count": 2,
        "name": "modification_functions.ShortWideModificationFunctions.time_delete_short_wide",
        "number": 1,
        "param_names": [],
        "params": [],
        "repeat": 2,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:154",
        "type": "time",
        "unit": "seconds",
        "version": "ea0d16462ea088dd30272c8a6e35952e7b43bac6b1f2cac423de50271138d487",
        "warmup_time": 0
    },
    "modification_functions.ShortWideModificationFunctions.time_update_short_wide": {
        "code": "class ShortWideModificationFunctions:\n    def time_update_short_wide(self, wide_append_data: ShortWideAppendData):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, wide_append_data: ShortWideAppendData):\n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(self.WIDE_DF_ROWS, self.WIDE_DF_COLS)\n    \n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(self.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        wide_append_data = self._setup_cache()\n        return wide_append_data",
        "min_run_count": 2,
        "name": "modification_functions.ShortWideModificationFunctions.time_update_short_wide",
        "number": 1,
        "param_names": [],
        "params": [],
        "repeat": 2,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "modification_functions:154",
        "type": "time",
        "unit": "seconds",
        "version": "0c499be36152b0f962116c641928b8a0f2bfca6bab12e9c4d30461c8509bdc2b",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "12247b929d34d209286ab85918d78e86cc91561bd9a4cc7db06beaa001d21e0d"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] == num_rows\n        assert read_batch_result[-1].data.shape[0] == num_rows\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d8eeb21aa5b74f3f935a4b92c92f09f31d1f089b892b6ac712a476f33e854a4c"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_date_range)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert read_batch_result[0].data.shape[0] > 2\n        assert read_batch_result[-1].data.shape[0] > 2\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "aebe4be8cf7590f95c29a5f5e71088dc833e85dba49c4d159ccb6aced1f7ce90"
    },
    "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def peakmem_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n        # Quick check all is ok (will not affect bemchmarks)\n        assert write_batch_result[0].symbol in self.symbols\n        assert write_batch_result[-1].symbol in self.symbols\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "name": "real_batch_functions.AWSBatchBasicFunctions.peakmem_write_batch",
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b58eb8a420862c585bcfbdfe92d70ba068310e4c34c7846292f47dd7363cae27"
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "501017573a8a25827024b656a55df9ed1a621d3cdaac24c830c183d9b691463a",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_columns(self, num_symbols, num_rows):\n        read_batch_result = self.lib.read_batch(self.read_reqs_with_cols)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_columns",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8e28fd869de381aec95ddab625b7d4e17bd262a6238f21b980a1ded0903ef3c1",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class AWSBatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, num_symbols, num_rows):\n        self.lib.read_batch(self.read_reqs_date_range)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_read_batch_with_date_ranges",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "97ac25e33d8fb8d99080e1a9177918dc6bf503d3fc8bc429ae06dc746537f950",
        "warmup_time": 0
    },
    "real_batch_functions.AWSBatchBasicFunctions.time_write_batch": {
        "code": "class AWSBatchBasicFunctions:\n    def time_write_batch(self, num_symbols, num_rows):\n        payloads = [WritePayload(symbol, self.df) for symbol in self.symbols]\n        write_batch_result = self.write_lib.write_batch(payloads)\n\n    def setup(self, num_symbols, num_rows):\n        self.manager = self.get_library_manager()\n        self.population_policy = self.get_population_policy()\n        # We use the same generator as the policy\n    \n        self.lib: Library = self.manager.get_library(LibraryType.PERSISTENT, num_symbols)\n        self.write_lib: Library = self.manager.get_library(LibraryType.MODIFIABLE, num_symbols)\n        self.get_logger().info(f\"Library {self.lib}\")\n        self.get_logger().debug(f\"Symbols {self.lib.list_symbols()}\")\n    \n        # Get generated symbol names\n        self.symbols = []\n        for num_symb_idx in range(num_symbols):\n            # the name is constructed of 2 parts index + number of rows\n            sym_name = self.population_policy.get_symbol_name(num_symb_idx, num_rows)\n            if not self.lib.has_symbol(sym_name):\n                self.get_logger().error(f\"symbol not found {sym_name}\")\n            self.symbols.append(sym_name)\n    \n        # Construct read requests (will equal to number of symbols)\n        self.read_reqs = [ReadRequest(symbol) for symbol in self.symbols]\n    \n        # Construct dataframe that will be used for write requests, not whole DF (will equal to number of symbols)\n        self.df = self.population_policy.df_generator.get_dataframe(num_rows, AWSBatchBasicFunctions.number_columns)\n    \n        # Construct read requests based on 2 colmns, not whole DF (will equal to number of symbols)\n        COLS = self.df.columns[2:4]\n        self.read_reqs_with_cols = [ReadRequest(symbol, columns=COLS) for symbol in self.symbols]\n    \n        # Construct read request with date_range\n        self.date_range = self.get_last_x_percent_date_range(num_rows, 0.05)\n        self.read_reqs_date_range = [ReadRequest(symbol, date_range=self.date_range) for symbol in self.symbols]\n\n    def setup_cache(self):\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        logger = self.get_logger()\n        number_symbols_list, number_rows_list = AWSBatchBasicFunctions.params\n        for number_symbols in number_symbols_list:\n            lib_suffix = number_symbols\n            if not manager.has_library(LibraryType.PERSISTENT, lib_suffix):\n                start = time.time()\n                for number_rows in number_rows_list:\n                    policy.set_parameters([number_rows] * lib_suffix, AWSBatchBasicFunctions.number_columns)\n                    # the name of symbols during generation will have now 2 parameters:\n                    # the index of symbol + number of rows\n                    # that allows generating more than one symbol in a library\n                    policy.set_symbol_fixed_str(number_rows)\n                    populate_library(manager, policy, LibraryType.PERSISTENT, lib_suffix)\n                    logger.info(f\"Generated {number_symbols} with {number_rows} each for {time.time()- start}\")\n        manager.log_info()  # Always log the ArcticURIs",
        "min_run_count": 1,
        "name": "real_batch_functions.AWSBatchBasicFunctions.time_write_batch",
        "number": 3,
        "param_names": [
            "num_symbols",
            "num_rows"
        ],
        "params": [
            [
                "500",
                "1000"
            ],
            [
                "25000",
                "50000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_batch_functions:59",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "79c5307f83e8be6020c49ce96f9b943180776220a624f8853cb50c7d10db11bc",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:214",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e97486b955cfba59bd57e26e7f67dfd28fa68fe5b41d4000cad23410f78e4a0f",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:214",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f8d864264ef6e4f853192984c4f67100dbcc9fb5ea225dcf5a3b4e00947ff62c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:214",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "6685c85cff6e17f615694061ea490e2b14d432c1628252a29d07fdcc04b97c2c",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:214",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a96bc9d0a016f29c9671d713ad7a2fdfd945381695c2227c7716468b8eaebd1d",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:214",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "bbc2c8763db23c7c27e02f3361ee26cb68e9ead689a18a7e7bbc81edafa7a5a5",
        "warmup_time": 0
    },
    "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\nclass AWS30kColsWideDFLargeAppendTests:\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWS30kColsWideDFLargeAppendTests.warmup_time,\n            AWS30kColsWideDFLargeAppendTests.params,\n            AWS30kColsWideDFLargeAppendTests.number_columns,\n            AWS30kColsWideDFLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWS30kColsWideDFLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "2500",
                "5000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:214",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b690d7836ba77e8f6ecd134bf9c3877c8c3f5b1a5d94f4d842b3b192ec8bbe07",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete(self, cache, num_rows):\n        self.lib.delete(self.symbol)\n        self.symbol_deleted = True\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:260",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4588f423aa2b7d1ded777f24c8ddd1b19a282bd7c7a6f15c012fc0cf1acbdd36",
        "warmup_time": 0
    },
    "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time": {
        "code": "class AWSDeleteTestsFewLarge:\n    def time_delete_over_time(self, cache, num_rows):\n        with config_context(\"VersionMap.ReloadInterval\", 0):\n            for i in range(25):\n                self.lib.write(\"delete_over_time\", pd.DataFrame())\n                self.lib.delete(\"delete_over_time\")\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        writes_list = cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.setup_symbol(self.lib, writes_list)\n        self.get_logger().info(f\"Library {self.lib}\")\n        assert self.lib.has_symbol(self.symbol)\n        self.symbol_deleted = False\n\n    def setup_cache(self):\n        # warmup will execute tests additional time and we do not want that at all for write\n        # update and append tests. We want exact specified `number` of times to be executed between\n        assert AWSDeleteTestsFewLarge.warmup_time == 0, \"warm up must be 0\"\n        assert AWSDeleteTestsFewLarge.number == 1, \"delete works only once per setup=teardown\"\n    \n        num_sequential_dataframes = AWSDeleteTestsFewLarge.number_appends_to_symbol + 1  # for initial dataframe\n        cache = CacheForModifiableTests()\n    \n        generator = SequentialDataframesGenerator()\n    \n        for num_rows in AWSDeleteTestsFewLarge.params:\n            num_cols = AWSDeleteTestsFewLarge.number_columns\n            df_list = generator.generate_sequential_dataframes(\n                number_data_frames=num_sequential_dataframes,\n                number_rows=num_rows,\n                number_columns=num_cols,\n                start_timestamp=pd.Timestamp(\"1-1-1980\"),\n                freq=\"s\",\n            )\n            cache.write_and_append_dict[num_rows] = df_list\n    \n        manager = self.get_library_manager()\n        manager.clear_all_benchmark_libs()\n        manager.log_info()\n    \n        return cache",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSDeleteTestsFewLarge.time_delete_over_time",
        "number": 1,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 3,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:260",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "978d41f95903f476a5a0c703ce1cd6ffdf3386ad6dd9023851d9f784159d567f",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_large": {
        "code": "class AWSLargeAppendTests:\n    def time_append_large(self, cache, num_rows):\n        large: pd.DataFrame = self.appends_list.pop(0)\n        self.lib.append(self.symbol, large)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_large",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:72",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ae924cc65f3fda9e7c64fcd76f6a542603cfb7242333cab7d762a62689a44aa3",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_append_single": {
        "code": "class AWSLargeAppendTests:\n    def time_append_single(self, cache, num_rows):\n        self.lib.append(self.symbol, self.cache.append_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_append_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:72",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "afabfcaa402bc4f4a8dd332050aaa65770b2d746b2ea6235ec9421e461dd4975",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_full": {
        "code": "class AWSLargeAppendTests:\n    def time_update_full(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_full_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_full",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:72",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "c3a2c9b358a8c4ffd9af01ffa0b9474e2e317579a1030aeea626be4f621274a2",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_half": {
        "code": "class AWSLargeAppendTests:\n    def time_update_half(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_half_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_half",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:72",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e865f1e39380722e6d3bfe6e3a56d2fae9389a0d95cd11c29b6f34f2007a389a",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_single": {
        "code": "class AWSLargeAppendTests:\n    def time_update_single(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_single_dict[num_rows])\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_single",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:72",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ba848d44eee0ef4595475f4def6ae17301c551096480c10b75562fd8f5c2598c",
        "warmup_time": 0
    },
    "real_modification_functions.AWSLargeAppendTests.time_update_upsert": {
        "code": "class AWSLargeAppendTests:\n    def time_update_upsert(self, cache, num_rows):\n        self.lib.update(self.symbol, self.cache.update_upsert_dict[num_rows], upsert=True)\n\n    def setup(self, cache: CacheForModifiableTests, num_rows):\n        manager = self.get_library_manager()\n        self.cache = cache\n        writes_list = self.cache.write_and_append_dict[num_rows]\n    \n        self.lib = manager.get_library(LibraryType.MODIFIABLE)\n    \n        self.symbol = f\"symbol-{os.getpid()}\"\n        self.lib.write(self.symbol, writes_list[0])\n    \n        self.appends_list = writes_list[1:]\n\n    def setup_cache(self):\n        return self.initialize_cache(\n            AWSLargeAppendTests.warmup_time,\n            AWSLargeAppendTests.params,\n            AWSLargeAppendTests.number_columns,\n            AWSLargeAppendTests.number,\n        )",
        "min_run_count": 1,
        "name": "real_modification_functions.AWSLargeAppendTests.time_update_upsert",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "500000",
                "1000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_modification_functions:72",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "0470f242734c94e8f6d30f4241c13ffe6cc8b53cb2bad1973799878b94c3cccd",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "66587c7cad65fa03050a1e2d2cbdd37b215e36464be2dbca6667b52ada07b398"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7a402ec7e4f09ccc8191abff900aee941fe5b983d3a54c1128332802b3aeacb8"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "41f3abfee7b61aaf01d6a7c097f50c82a3cd5aa520735e8e56d1433980423e81"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1e238f958363598bb77ea0b61b18e872f9f2d45ace953b66286f6590b855138f"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3f46b68395cb394e790f846e8bb368416fa13d20e8963afe38202e0afb2a8012"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0b4095f9e4d0594d25e5334be3c0c9d41135b0460577f58f20ef358d83e58dd3"
    },
    "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n        assert data.shape[0] > 1\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_query_builder.AWSQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3d94e6e1d0c466cf98b0c7673d3e695b231110e5e0aff293d7a7f75f878f49cc"
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_numeric",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d8f0751dfa443b7fe80c43d18b9e89bebfb248d8fd2456719e5bd712c2f54905",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"v3\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_filtering_string_isin",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3fbe4bd3d18d1756b709637b51e6181d1753379ec47d30de348e3d82e8069750",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_projection": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        data: pd.DataFrame = self.lib.read(self.symbol, columns=[\"new_col\"], query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_projection",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3e7e763b14722ab6c81c7835c4bbee8fb086672ad73366beefd3c6bc7781172f",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_1": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_1",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "76a7f4b0952f07784150df2b7a382356a4be815385c5258323ffc27e1d385767",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_3": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_3",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "685f6a5908e27c6198503b4bc2330769d4c764d4e16beebf4e0355b5b2c6b627",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_4": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_4",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "de79858cd635f67551cee7c9c1439573bf926264dc8e11de497d7062c5589641",
        "warmup_time": 0
    },
    "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class AWSQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        data: pd.DataFrame = self.lib.read(self.symbol, query_builder=q).data\n\n    def setup(self, num_rows):\n        ## Construct back from arctic url the object\n        self.lib: Library = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.policy = self.get_population_policy()\n        self.symbol = self.policy.get_symbol_name(num_rows)\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_query_builder.AWSQueryBuilderFunctions.time_query_adv_query_2",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_query_builder:75",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "977b46a22c5f742940aa1d126fec2fccea49721ce34ed2ad73026225387a0a0b",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c9472d2ab25d1a30beb1146a9c649036af89bea84f2119d9cb184408139276f3"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cc3955cfed7c5684809c86a04041374c674f26e8721128acbcd80af06012128c"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fdc284437037f6f245607caf113a0e5f0a6c6dccc8e6ad9f471611574253c50b"
    },
    "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a09cc022e09a57064fca48fc68fed42d6b9593fbefddc51f8be856afb7ac710b"
    },
    "real_read_write.AWSReadWrite.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7876413187267e0867f03f798f317eaa3e3960ac2375ff5df6f2095520bb1ca5"
    },
    "real_read_write.AWSReadWrite.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "name": "real_read_write.AWSReadWrite.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d211408aa7db06df36befacdd5fd39b9422eb968773a6e2bd19f8d16745541ac"
    },
    "real_read_write.AWSReadWrite.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a2b8548a163367ba007992cefa84d7a83d4f60672b14b8a90bd4b2600b4d8131",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "470178c2a5f27c30784904befff88ed0b75125c5ad1a4d508ee2c6d79e1f3f99",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "2d8f9e98f36bf378b003b44866c8d3c39864f8160798e8b2cc7b475b074bdd38",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "8aec0888e02948dc708cc304400857ce54ab1f5b91fda2bc167bce90ea4c7299",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f77b10516f456c860eb05ec818f8242a43aa9adc54b34ef30eafd4098299322e",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWrite.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\n    def setup_cache(self):\n        \"\"\"\n        In setup_cache we only populate the persistent libraries if they are missing.\n        \"\"\"\n        manager = self.get_library_manager()\n        policy = self.get_population_policy()\n        populate_library_if_missing(manager, policy, LibraryType.PERSISTENT)\n        manager.log_info()  # Logs info about ArcticURI - do always use last",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWrite.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:86",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "e0cd4c4a06cec3813214e5ed2e32ea0a22bf2f55e6d9cebc4f16894b16710e36",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1cac6c9cf15d5fbf892a777498296d8d098711272b405b1c8e243e3e767e599b"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_column_float",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "4ff1a56334201630187cbd2ad88a520d04067e282d5a163c1d4a34230b997ab5"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "44fbc536228fe2270c8038e451223ded4f7941c9226bf95190a46b7b28f228b0"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "307ae3fb4ac3c92c44fb9c578b21907867846318087564daae4c61f32fd996fa"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3b402f9a631ceeb544fa7ee7b860e13ab870c7e61da809ba11d14c5973c97cda"
    },
    "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "real_read_write.AWSReadWriteWithQueryStats.peakmem_write_staged",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c1fdbb5013c30dd4c7a6461a1b9193097a831af4f6e639e8c3261c3817e6181f"
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "f7a86808b4972336b56bbe425b35ce39d7db682c525504edc5912736af82398e",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "b0a5713c3639a1d5084ba6933abf2bcb14df3db143f5bfe713fbb7e448068db9",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "748e8cf9fad63c651326e462f4021c28a2c128a1d7c4783fc2886a2451b19d99",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "d77a903a22862f45799bed1ae703c285e21602d82da264bf013ceba202c63cf8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "48483b15cbd6e2738ddf3f615179a9bafde8596faac8e1b331ad03df4b4c21d8",
        "warmup_time": 0
    },
    "real_read_write.AWSReadWriteWithQueryStats.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\nclass AWSReadWriteWithQueryStats:\n    def setup(self, num_rows):\n        super().setup(num_rows)\n        qs.enable()\n\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSReadWriteWithQueryStats.time_write_staged",
        "number": 3,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "2000000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:242",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "9805c7597bb79b23de9a89830bdaca81c17868b485d9ecd9ede6d0621460cf56",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read": {
        "code": "class AWSReadWrite:\n    def peakmem_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "777451970fc2c9ba7c40648d51397be114005fca10b35c2beb816a0c5b11eb4e"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_column_float",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "94cd6191a28611f9fdfb6f92e01fa9ccec156d60cc7bda30471fc0b6b27d8c28"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_columns_all_types",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c27180c5137daf6611cffdd96923d000f03bdc4f4c12b00435502b40d5abd4da"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def peakmem_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_read_with_date_ranges_last20_percent_rows",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "840af2cee86e8ba04910ceb2c04cc3c6e732c2a9eea52a2ab3cecf1a35767444"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write": {
        "code": "class AWSReadWrite:\n    def peakmem_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c55080c0aa9714cf9bbac9058877813c9fd7bab7326f66618725b67b14f21372"
    },
    "real_read_write.AWSWideDataFrameTests.peakmem_write_staged": {
        "code": "class AWSReadWrite:\n    def peakmem_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "name": "real_read_write.AWSWideDataFrameTests.peakmem_write_staged",
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a07309f742f72469c5f54a0962205f9272af5d741b615c6e5536f97ee16356ee"
    },
    "real_read_write.AWSWideDataFrameTests.time_read": {
        "code": "class AWSReadWrite:\n    def time_read(self, num_rows):\n        self.read_lib.read(self.symbol)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "355ba97b40b39aa52586e966f70b2710a64b1694030d66d69a94845d11620e12",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_column_float": {
        "code": "class AWSReadWrite:\n    def time_read_with_column_float(self, num_rows):\n        COLS = [\"float2\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_column_float",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "5653d388b62f876452148f5aff5c29e03eda6a7548b406fbdc324ab2365bcdbe",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types": {
        "code": "class AWSReadWrite:\n    def time_read_with_columns_all_types(self, num_rows):\n        COLS = [\"float2\", \"string10\", \"bool\", \"int64\", \"uint64\"]\n        self.read_lib.read(symbol=self.symbol, columns=COLS).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_columns_all_types",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "3ab9782a626f87b77d5fe64ffa7e83ae14ce2274f047d15f0c1af2c9bb8da30d",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows": {
        "code": "class AWSReadWrite:\n    def time_read_with_date_ranges_last20_percent_rows(self, num_rows):\n        self.read_lib.read(symbol=self.symbol, date_range=self.last_20).data\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_read_with_date_ranges_last20_percent_rows",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "4feff55363038593799ca793ef2904d32a76d0d794f5466ed75c56ba867bd1d3",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write": {
        "code": "class AWSReadWrite:\n    def time_write(self, num_rows):\n        self.write_lib.write(self.symbol, self.to_write_df)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "ed88fa1910599e13c65662744b40b1396536a03e2397cbf12f39f37b81c8f81b",
        "warmup_time": 0
    },
    "real_read_write.AWSWideDataFrameTests.time_write_staged": {
        "code": "class AWSReadWrite:\n    def time_write_staged(self, num_rows):\n        lib = self.write_lib\n        lib.write(self.symbol, self.to_write_df, staged=True)\n        lib._nvs.compact_incomplete(self.symbol, False, False)\n\n    def setup(self, num_rows):\n        self.population_policy = self.get_population_policy()\n        self.symbol = self.population_policy.get_symbol_name(num_rows)\n        # We use the same generator as the policy\n        self.to_write_df = self.population_policy.df_generator.get_dataframe(num_rows, 0)\n    \n        # Functions operating on differetent date ranges to be moved in some shared utils\n        self.last_20 = self.get_last_x_percent_date_range(num_rows, 20)\n    \n        self.read_lib = self.get_library_manager().get_library(LibraryType.PERSISTENT)\n        self.write_lib = self.get_library_manager().get_library(LibraryType.MODIFIABLE)\n        # We could also populate the library like so (we don't need )\n        # populate_library(self.write_lib, )\n\nclass AWSWideDataFrameTests:\n    def setup_cache(self):\n        # Each class that has specific setup and inherits from another class,\n        # must implement setup_cache\n        super().setup_cache()",
        "min_run_count": 1,
        "name": "real_read_write.AWSWideDataFrameTests.time_write_staged",
        "number": 3,
        "param_names": [
            "num_cols"
        ],
        "params": [
            [
                "15000",
                "30000"
            ]
        ],
        "repeat": 1,
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "real_read_write:211",
        "timeout": 1200,
        "type": "time",
        "unit": "seconds",
        "version": "a26f1ab74733278e53ed0c0913f466bfdb150dd2fa8d5a481715bfa3caf2f978",
        "warmup_time": 0
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_batch_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def peakmem_read_batch_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read_batch([get_symbol_name(num_dict_entry, i) for i in range(num_symbols)])\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_batch_nested_dict",
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "setup_cache_key": "recursive_normalizer:36",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "16dc7deb22ab10bbfebcb212e39030a1d8cb11a9b96418ebbd26fa1c2895ba3a"
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def peakmem_read_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read(get_symbol_name(num_dict_entry, 0))\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.peakmem_read_nested_dict",
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "setup_cache_key": "recursive_normalizer:36",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "43ceef75cd18db9646dfe1635071f294e5faaf811066e3c31a632c0a4a05dd8c"
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_batch_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def time_read_batch_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read_batch([get_symbol_name(num_dict_entry, i) for i in range(num_symbols)])\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "min_run_count": 2,
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_batch_nested_dict",
        "number": 0,
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "recursive_normalizer:36",
        "type": "time",
        "unit": "seconds",
        "version": "bdd744509146dd4994139db5d82b087b93ec5c51109a23283b955b546efc13f1",
        "warmup_time": -1
    },
    "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_nested_dict": {
        "code": "class LMDBRecursiveNormalizerRead:\n    def time_read_nested_dict(self, num_dict_entry, num_symbols):\n        self.lib.read(get_symbol_name(num_dict_entry, 0))\n\n    def setup(self, num_dict_entry, num_symbols):\n        self.lib = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI).get_library(\"lib\")\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerRead.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        lib = ac.create_library(\"lib\")\n    \n        max_num_symbols = max(self.params[1])\n        for num_dict_entry in self.params[0]:\n            data = get_data(num_dict_entry)\n            for symbol_idx in range(max_num_symbols):\n                symbol_name = get_symbol_name(num_dict_entry, symbol_idx)\n                lib._nvs.write(symbol_name, data, recursive_normalizers=True)",
        "min_run_count": 2,
        "name": "recursive_normalizer.LMDBRecursiveNormalizerRead.time_read_nested_dict",
        "number": 0,
        "param_names": [
            "num_dict_entries",
            "num_symbols"
        ],
        "params": [
            [
                "1000"
            ],
            [
                "5"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "recursive_normalizer:36",
        "type": "time",
        "unit": "seconds",
        "version": "9e8238a61a6de72778522f023e8d65de5be7558826343557dec84e06baa42663",
        "warmup_time": -1
    },
    "recursive_normalizer.LMDBRecursiveNormalizerWrite.peakmem_write_nested_dict": {
        "code": "class LMDBRecursiveNormalizerWrite:\n    def peakmem_write_nested_dict(self, num_dict_entry):\n        assert len(self.data) == num_dict_entry\n        self.lib._nvs.write(\n            f\"nested_dict_peakmem_write_nested_dict_{num_dict_entry}\",\n            self.data,\n            recursive_normalizers=True,\n        )\n\n    def setup(self, num_dict_entry):\n        self.lib = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI).get_library(\"lib\")\n        self.data = get_data(num_dict_entry)\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        ac.create_library(\"lib\")",
        "name": "recursive_normalizer.LMDBRecursiveNormalizerWrite.peakmem_write_nested_dict",
        "param_names": [
            "num_dict_entries"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "setup_cache_key": "recursive_normalizer:80",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9dcfed4cee0cbe00634c7e5f8eb217fe11cbb4069242637e41d00a1ff99aed4c"
    },
    "recursive_normalizer.LMDBRecursiveNormalizerWrite.time_write_nested_dict": {
        "code": "class LMDBRecursiveNormalizerWrite:\n    def time_write_nested_dict(self, num_dict_entry):\n        assert len(self.data) == num_dict_entry\n        self.lib._nvs.write(\n            f\"nested_dict_time_write_nested_dict_{num_dict_entry}\",\n            self.data,\n            recursive_normalizers=True,\n        )\n\n    def setup(self, num_dict_entry):\n        self.lib = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI).get_library(\"lib\")\n        self.data = get_data(num_dict_entry)\n\n    def setup_cache(self):\n        ac = Arctic(LMDBRecursiveNormalizerWrite.ARCTIC_URI)\n        if \"lib\" in ac:\n            ac.delete_library(\"lib\")\n    \n        ac.create_library(\"lib\")",
        "min_run_count": 2,
        "name": "recursive_normalizer.LMDBRecursiveNormalizerWrite.time_write_nested_dict",
        "number": 0,
        "param_names": [
            "num_dict_entries"
        ],
        "params": [
            [
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "recursive_normalizer:80",
        "type": "time",
        "unit": "seconds",
        "version": "a2dfcb01776d52d843c514b129d2256a8031360caa00bc24b890b6dad9246191",
        "warmup_time": -1
    },
    "resample.Resample.peakmem_resample": {
        "code": "class Resample:\n    @skip_for_params(TIME_PARAMS)\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            self.skipped = True\n            raise NotImplementedError(f\"{aggregation} not supported on columns of type {col_type}\")\n    \n        self.skipped = False\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "name": "resample.Resample.peakmem_resample",
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "3000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "setup_cache_key": "resample:53",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "20835daa6c6fd2af13420212c35b87cdac7153f782b2bc02cb2535131bd654d8"
    },
    "resample.Resample.time_resample": {
        "code": "class Resample:\n    @skip_for_params(PEAKMEM_PARAMS)\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        if (\n            col_type == \"datetime\"\n            and aggregation == \"sum\"\n            or col_type == \"str\"\n            and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]\n        ):\n            self.skipped = True\n            raise NotImplementedError(f\"{aggregation} not supported on columns of type {col_type}\")\n    \n        self.skipped = False\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "resample.Resample.time_resample",
        "number": 5,
        "param_names": [
            "num_rows",
            "downsampling_factor",
            "col_type",
            "aggregation"
        ],
        "params": [
            [
                "3000000",
                "10000000"
            ],
            [
                "10",
                "100",
                "100000"
            ],
            [
                "'bool'",
                "'int'",
                "'float'",
                "'datetime'",
                "'str'"
            ],
            [
                "'sum'",
                "'mean'",
                "'min'",
                "'max'",
                "'first'",
                "'last'",
                "'count'"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:53",
        "type": "time",
        "unit": "seconds",
        "version": "5a8f9ae968dbf11481dbe381bcb072c86f46190c9e10951ab2ec80ad9c9f116e",
        "warmup_time": -1
    },
    "resample.ResampleWide.peakmem_resample_wide": {
        "code": "class ResampleWide:\n    def peakmem_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "name": "resample.ResampleWide.peakmem_resample_wide",
        "param_names": [],
        "params": [],
        "setup_cache_key": "resample:129",
        "type": "peakmemory",
        "unit": "bytes",
        "version": "53f042192048c92d282637c1bbcee9e52dacec9086c534782de30d7ff67e77eb"
    },
    "resample.ResampleWide.time_resample_wide": {
        "code": "class ResampleWide:\n    def time_resample_wide(self):\n        self.lib.read(self.SYM, query_builder=self.query_builder)\n\n    def setup(self):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        aggs = dict()\n        for col in self.COLS:\n            aggs[col] = \"last\"\n        self.query_builder = QueryBuilder().resample(\"30us\").agg(aggs)\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        num_rows = 3000\n        index = pd.date_range(pd.Timestamp(0, unit=\"us\"), freq=\"us\", periods=num_rows)\n        data = dict()\n        for col in self.COLS:\n            data[col] = 100 * rng.random(num_rows, dtype=np.float64)\n        df = pd.DataFrame(data, index=index)\n        lib.write(self.SYM, df)",
        "min_run_count": 2,
        "name": "resample.ResampleWide.time_resample_wide",
        "number": 0,
        "param_names": [],
        "params": [],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "resample:129",
        "type": "time",
        "unit": "seconds",
        "version": "ece714f981e8de31ee8296644624bf8f5fb895e6bf48d64a6ae2a9c50c5db7a2",
        "warmup_time": -1
    },
    "version": 2,
    "version_chain.IterateVersionChain.time_list_undeleted_versions": {
        "code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        if deleted:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_TAIL_DELETED)\n        else:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_UNDELETED)\n        self.lib = ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_list_undeleted_versions",
        "number": 0,
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 5,
        "rounds": 2,
        "sample_time": 1,
        "setup_cache_key": "version_chain:49",
        "timeout": 1000,
        "type": "time",
        "unit": "seconds",
        "version": "c994d17e8987906e0964cc8145263dc8ddba0f0e1ec0b8fc729ea437a390605c",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_load_all_versions": {
        "code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        if deleted:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_TAIL_DELETED)\n        else:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_UNDELETED)\n        self.lib = ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_load_all_versions",
        "number": 0,
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 5,
        "rounds": 2,
        "sample_time": 1,
        "setup_cache_key": "version_chain:49",
        "timeout": 1000,
        "type": "time",
        "unit": "seconds",
        "version": "e68f85572eefce903e6c90ac005474274128cf893551579dded0dbfa891eb4bf",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_alternating": {
        "code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions))\n        self.read_v0(self.symbol(num_versions))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        if deleted:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_TAIL_DELETED)\n        else:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_UNDELETED)\n        self.lib = ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_alternating",
        "number": 0,
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 5,
        "rounds": 2,
        "sample_time": 1,
        "setup_cache_key": "version_chain:49",
        "timeout": 1000,
        "type": "time",
        "unit": "seconds",
        "version": "117a257c8033130bab1d83ea4c8993283739842e87dccd898107f8ecbd8fd714",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_from_epoch": {
        "code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        if deleted:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_TAIL_DELETED)\n        else:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_UNDELETED)\n        self.lib = ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_from_epoch",
        "number": 0,
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 5,
        "rounds": 2,
        "sample_time": 1,
        "setup_cache_key": "version_chain:49",
        "timeout": 1000,
        "type": "time",
        "unit": "seconds",
        "version": "65cf6d6226805c2655d981a065b9e345cc8b00424f8f35965407dfdc1b8da504",
        "warmup_time": -1
    },
    "version_chain.IterateVersionChain.time_read_v0": {
        "code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching == \"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching == \"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching == \"default\":\n            # Leave the default reload interval\n            pass\n    \n        if deleted:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_TAIL_DELETED)\n        else:\n            ac = Arctic(IterateVersionChain.CONNECTION_STRING_UNDELETED)\n        self.lib = ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions))\n\n    def setup_cache(self):\n        start = time.time()\n        self._setup_cache()\n        self.logger.info(f\"SETUP_CACHE TIME: {time.time() - start}\")",
        "min_run_count": 2,
        "name": "version_chain.IterateVersionChain.time_read_v0",
        "number": 0,
        "param_names": [
            "num_versions",
            "caching",
            "tail_deleted"
        ],
        "params": [
            [
                "25000"
            ],
            [
                "'forever'",
                "'default'",
                "'never'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "repeat": 5,
        "rounds": 2,
        "sample_time": 1,
        "setup_cache_key": "version_chain:49",
        "timeout": 1000,
        "type": "time",
        "unit": "seconds",
        "version": "e086c5206ded034b9ab56b93a7c774085fd00125211350aee5d2fa3b2a700321",
        "warmup_time": -1
    }
}