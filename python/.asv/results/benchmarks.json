{
    "basic_functions.BasicFunctions.peakmem_read": {
        "code": "class BasicFunctions:\n    def peakmem_read(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        [lib.read(f\"{sym}_sym\").data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c5aec6c6c0473d8bc76d9ee9e49666cb533fadc902f63da49d81d51262b51aca"
    },
    "basic_functions.BasicFunctions.peakmem_read_batch": {
        "code": "class BasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "848aa90516ee6297965afbf0ade938753828f43537bc2e47159339fdef65751c"
    },
    "basic_functions.BasicFunctions.peakmem_read_batch_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read_batch_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "dd753b503b4c6466bd6c0e166abe319fab90acbe8b8b72ac219393bcce4dc480"
    },
    "basic_functions.BasicFunctions.peakmem_read_batch_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=dr) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read_batch_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "97cf81d51ce5d7f2d72b9e1bc6bc9d201d6506fb218914dae9e872be0c590c98"
    },
    "basic_functions.BasicFunctions.peakmem_read_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read_short_wide",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "aa29d7c366133483a54489dec54422fb214b792cf58f7e74a8b803981a12168a"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_columns": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        [lib.read(f\"{sym}_sym\", columns=COLS).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_columns",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a0557e06ec9ce45e6a790831b98820ab7f9e9fcdc08d1944cb7869088bc7c426"
    },
    "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        [lib.read(f\"{sym}_sym\", date_range=dr).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "64588b255dcadca97a49da42f6ceee52194febf23fa3da431529cab46b5bb147"
    },
    "basic_functions.BasicFunctions.peakmem_write": {
        "code": "class BasicFunctions:\n    def peakmem_write(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_write",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "1bf92b4942f7d1013c86e5d4efd63d1fecb0ab2f2fb6f4ffa8d0790fb50f624d"
    },
    "basic_functions.BasicFunctions.peakmem_write_batch": {
        "code": "class BasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        df = self.df\n        payloads = [WritePayload(f\"{sym}_sym\", df) for sym in range(num_symbols)]\n        lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_write_batch",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "ac9541769beec9d47bb64406600e7a610a122fb187f28e8a5903526bf16014a1"
    },
    "basic_functions.BasicFunctions.peakmem_write_short_wide": {
        "code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_write_short_wide",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e1f4298bd5e6c512f0b5a7be2916ddbf20d256d4ef043d4a30d27ea7720bfd4b"
    },
    "basic_functions.BasicFunctions.peakmem_write_staged": {
        "code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df, staged=True)\n    \n        for sym in range(num_symbols):\n            lib._nvs.compact_incomplete(f\"{sym}_sym\", False, False)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "name": "basic_functions.BasicFunctions.peakmem_write_staged",
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d9b981399b4f03502d93f4fd607918960ee468ec0a7ee821121e13c9624b0217"
    },
    "basic_functions.BasicFunctions.time_read": {
        "code": "class BasicFunctions:\n    def time_read(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        [lib.read(f\"{sym}_sym\").data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f14eee77b5815fed53f18934d2221bdad341961c5e1c66145737b9c7369f90de",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch": {
        "code": "class BasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "1105c8663471c2de9d44562f639dca10115b5389148955fb3faee0fd754949ee",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch_pure": {
        "code": "class BasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch_pure",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "dce09f508518caed1d80fde228523d4e92a8207a5ba8acc66a6cf0ed30999f06",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        read_reqs = [ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch_with_columns",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "55e242190d9bee2bd297a66069f805d7d2a3bed04c751a80f3495057f41d0d88",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_batch_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        read_reqs = [ReadRequest(f\"{sym}_sym\", date_range=dr) for sym in range(num_symbols)]\n        lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_batch_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "917b7be653cd4752d70f0764766ea1e85ad305338e3c6352a9923253cc7de3e3",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_short_wide": {
        "code": "class BasicFunctions:\n    def time_read_short_wide(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_short_wide",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "7a8aebb9bfb5ae861080001e5e2dfe5dc5ec6f9cb0a18c5320e7228b48fd991d",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_with_columns": {
        "code": "class BasicFunctions:\n    def time_read_with_columns(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        COLS = [\"value\"]\n        [lib.read(f\"{sym}_sym\", columns=COLS).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_columns",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "a7de9888e084e882f1b8b694824aefa9a1b393ac921e98ec5b781441f782dc9e",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_read_with_date_ranges": {
        "code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows, num_symbols):\n        lib = self.ac[get_prewritten_lib_name(rows)]\n        dr = pd.date_range(\"2023-01-01\", \"2023-01-01\")\n        [lib.read(f\"{sym}_sym\", date_range=dr).data for sym in range(num_symbols)]\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_read_with_date_ranges",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b1f745f91cfe4a2a886688038612d4c8190f1d16d6b725e8c3ee8c3dd20569a8",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write": {
        "code": "class BasicFunctions:\n    def time_write(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b4e20bcf340f05bcbe839792dedccd04ca7c61c04481997a7a1f3a91b3435125",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_batch": {
        "code": "class BasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        df = self.df\n        payloads = [WritePayload(f\"{sym}_sym\", df) for sym in range(num_symbols)]\n        lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_batch",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8bddad28073908b80c7780eb3e0c76740fde9e88ab261961e4501be5227c12e3",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_short_wide": {
        "code": "class BasicFunctions:\n    def time_write_short_wide(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_short_wide",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "2088fffc825de2bcd9d28471c4d7092c42635cf1a3661e5739c412ceb2bf16e6",
        "warmup_time": -1
    },
    "basic_functions.BasicFunctions.time_write_staged": {
        "code": "class BasicFunctions:\n    def time_write_staged(self, rows, num_symbols):\n        lib = self.get_fresh_lib()\n        for sym in range(num_symbols):\n            lib.write(f\"{sym}_sym\", self.df, staged=True)\n    \n        for sym in range(num_symbols):\n            lib._nvs.compact_incomplete(f\"{sym}_sym\", False, False)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(5_000, 30_000)\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://basic_functions?map_size=10GB\")\n        num_rows, num_symbols = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in num_rows}\n        for rows in num_rows:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])\n        lib.write(\"short_wide_sym\", generate_random_floats_dataframe(5_000, 30_000))",
        "min_run_count": 2,
        "name": "basic_functions.BasicFunctions.time_write_staged",
        "number": 5,
        "param_names": [
            "rows",
            "num_symbols"
        ],
        "params": [
            [
                "100000",
                "150000"
            ],
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "basic_functions:23",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "b72c33b6e6b44ecdbef7f56a9ab4de3c76137d937f2ced3fd39d066f309c94e5",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.peakmem_list_symbols": {
        "code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_symbols",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0d50729844beae58e9cba083ee7214a55e5c1639b8439842ed1cdf080b7b2a4f"
    },
    "list_functions.ListFunctions.peakmem_list_versions": {
        "code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "name": "list_functions.ListFunctions.peakmem_list_versions",
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "c317f2aa935e2b739a1a232439412ad1da88ee9eb332daf0f560e00ec27f1143"
    },
    "list_functions.ListFunctions.time_has_symbol": {
        "code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_has_symbol",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8a8e9e8927b3b8698d1ef4bab4119455588fc62a3bbfa62321e6ce83f5f428ad",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.time_list_symbols": {
        "code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_symbols",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c91930aa368c8d3ad7bd56d7f827edcca3f16645c205bf6e042c57a5792a7113",
        "warmup_time": -1
    },
    "list_functions.ListFunctions.time_list_versions": {
        "code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        lib = self.ac[f\"{num_symbols}_num_symbols\"]\n        lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            self.ac.create_library(lib_name)\n            lib = self.ac[lib_name]\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))",
        "min_run_count": 2,
        "name": "list_functions.ListFunctions.time_list_versions",
        "number": 5,
        "param_names": [
            "num_symbols"
        ],
        "params": [
            [
                "500",
                "1000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "list_functions:22",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "ed4587ca351d24477560baa8e184e265b612140183e88c412c36bf8a9352191c",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "119932c4738eb092fbbd9da22b3ff1e16bd4d6052dace2da6748fff5b416462d"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        lib = self.ac[self.lib_name]\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e77e28f6a2cb96e5b549ca58b551462e0ff90d7704ad3cf12f3e2ad71b1ccd73"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "928c58e85b09b5969e7b2302b3a3a3c5af5d47ce5bd1cd5245387bf34fcb266b"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a1d501b169474fd16fdb1e90760157944483bba8d81eb3cb5791bbd487706ef6"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0b19c5c0e4be8742e99e32f13332544c343ac260474ee51bfb8911225c90b87b"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "76d23e33cb5a319de6d0e007054bc0c3588cde135a08797e8f0c84493ed30189"
    },
    "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2",
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "36489c1cb0a3965ff6003b10a5f5cf2f2354a619346e53c488b2a3d1a07fd4c2"
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "441c9db2c41da7c09c3f758c1cd0993dc5ea674f05d708e8997567826a961251",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        lib = self.ac[self.lib_name]\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f9d819943b527f80d002995694ba8cfc60f5ed75586144f6c4d1ba612a761294",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_projection": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "911828bde306e54bd165122188fb8acc406697e8cc94daa3e02596c1efc4530a",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "e0d8b8b13b6012a8fe388a57e1d816fc387312c6595ff5cbf0490ae32a11642e",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "22c320dbb7f3915712bffffef0c86ec9a51a167a8aee1af369ae9afceb47ad0b",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "f6c81bfbe6bd6a3ed5de7ede40c048ee2e7f0f2f844608169ddf1768a18aa900",
        "warmup_time": -1
    },
    "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n        self.lib_name = \"query_builder\"\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://query_builder?map_size=5GB\")\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = \"query_builder\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2",
        "number": 5,
        "param_names": [
            "num_rows"
        ],
        "params": [
            [
                "1000000",
                "10000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "local_query_builder:21",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "8167772e10cd53cd18fa670b57287145bdee6a010f1f8e07c9beec045706692e",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "10000000",
                "100000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:28",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "420c3ba188fd433b786e4be9cdb08340590d4595ac3a98badd6ebe63c41e7ecf",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "10000000",
                "100000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:28",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "c835f67ed597ec88b640012a8a87cd4fd26b5b208db2812274c5e90a0d7aa128",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "10000000",
                "100000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:28",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "aa0500f4f7201412558afeb82ada834bd10d4b56beae1dc16f73ccec69b33d45",
        "warmup_time": -1
    },
    "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {
        "code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        lib = self.ac[self.lib_name]\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        pass\n\n    def setup_cache(self):\n        self.ac = real_s3_from_environment_variables(shared_path=True).create_fixture().create_arctic()\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = \"query_builder_benchmark_lib\"\n        self.ac.delete_library(self.lib_name)\n        self.ac.create_library(self.lib_name)\n        lib = self.ac[self.lib_name]\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))",
        "min_run_count": 2,
        "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2",
        "number": 2,
        "param_names": [
            "param1"
        ],
        "params": [
            [
                "10000000",
                "100000000"
            ]
        ],
        "repeat": 0,
        "rounds": 2,
        "sample_time": 0.01,
        "setup_cache_key": "persistent_query_builder:28",
        "timeout": 6000,
        "type": "time",
        "unit": "seconds",
        "version": "827ddb04703d351338fde3b2a7eb5f147fa81c5269b630ba32554d8aacb396bc",
        "warmup_time": -1
    },
    "version": 2
}